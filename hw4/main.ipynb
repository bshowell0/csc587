{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43d0da7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857246fc",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "We'll define a helper function to visualize the agent's performance and functions to handle training and evaluation to keep the code modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4ba1af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def show_video(images, captions):\n",
    "  \"\"\" Show a sequence of images as an interactive plot. \"\"\"\n",
    "  if not images:\n",
    "      print(\"No images to display.\")\n",
    "      return\n",
    "  def f(i):\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(captions[i])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "  interact(f, i=IntSlider(min=0, max=len(images)-1, step=1, value=0))\n",
    "\n",
    "def train_q_table(env, num_steps, lr, gamma, decay_rate, seed):\n",
    "    \"\"\" Trains a Q-table for the given environment. \"\"\"\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    # Initialize Q-table\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    # Reset environment, seeding it for a reproducible training sequence\n",
    "    state, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(num_steps):\n",
    "        # Epsilon-greedy action selection\n",
    "        epsilon = math.exp(-decay_rate * step)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])  # Exploit\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Calculate target y based on Bellman equation\n",
    "        if done:\n",
    "            y = reward\n",
    "        else:\n",
    "            y = reward + gamma * np.max(q_table[next_state, :])\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] + lr * (y - q_table[state, action])\n",
    "\n",
    "        # Update state\n",
    "        if done:\n",
    "            state, _ = env.reset() # Reset for the next episode\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "def evaluate_policy(env, q_table, num_trials, seed):\n",
    "    \"\"\" Evaluates a Q-table policy over a number of trials. \"\"\"\n",
    "    success_count = 0\n",
    "    max_steps_per_trial = 100  # Prevent infinite loops in non-optimal policies\n",
    "\n",
    "    # Seed the environment's RNG to make the sequence of trials reproducible.\n",
    "    # This reset also provides the starting state for the first trial.\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        # For subsequent trials, just reset the state to the beginning.\n",
    "        # This will use the same, seeded RNG to produce the next episode in the sequence.\n",
    "        if i > 0:\n",
    "            state, _ = env.reset()\n",
    "\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done and steps < max_steps_per_trial:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            steps += 1\n",
    "\n",
    "        if done and reward == 1:\n",
    "            success_count += 1\n",
    "\n",
    "    return success_count / num_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74923c24",
   "metadata": {},
   "source": [
    "## Non-Slippery Frozen Lake\n",
    "\n",
    "First, we'll learn a Q-table for the deterministic version (`is_slippery=False`). In this environment, the agent's actions always succeed as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84232c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global seed for the notebook\n",
    "seed = 1234\n",
    "\n",
    "# Environment setup\n",
    "env_non_slippery = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ac08a",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm\n",
    "\n",
    "We implement the Q-learning algorithm as described in the pseudo-code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a590f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table learned for non-slippery environment.\n"
     ]
    }
   ],
   "source": [
    "# Q-learning parameters\n",
    "num_steps = 2000\n",
    "lr = 0.01\n",
    "gamma = 0.99\n",
    "decay_rate = 0.0001\n",
    "\n",
    "# Train the Q-table\n",
    "q_table_non_slippery = train_q_table(\n",
    "    env=env_non_slippery,\n",
    "    num_steps=num_steps,\n",
    "    lr=lr,\n",
    "    gamma=gamma,\n",
    "    decay_rate=decay_rate,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "print(\"Q-table learned for non-slippery environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2840d",
   "metadata": {},
   "source": [
    "### Testing the Learned Policy\n",
    "\n",
    "Now we test the policy learned by the Q-table. Since the environment is deterministic and the Q-table is well-trained, the agent should find the optimal path to the goal every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da22653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Reached the goal in 6 steps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73c5639abdb4183b4075621b837d577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=6), Output()), _dom_classes=('widget-interact',)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the policy\n",
    "state, _ = env_non_slippery.reset(seed=seed)\n",
    "done = False\n",
    "max_test_steps = 10000\n",
    "renders = []\n",
    "captions = []\n",
    "step_count = 0\n",
    "\n",
    "# Initial state\n",
    "renders.append(env_non_slippery.render())\n",
    "captions.append(f'Step: {step_count} State: {state} Action: N/A Reward: N/A')\n",
    "\n",
    "while not done and step_count < max_test_steps:\n",
    "    step_count += 1\n",
    "    # Choose action with max utility from the Q-table\n",
    "    action = np.argmax(q_table_non_slippery[state, :])\n",
    "\n",
    "    # Apply action\n",
    "    next_state, reward, done, _, _ = env_non_slippery.step(action)\n",
    "\n",
    "    # Render and store frame for video\n",
    "    renders.append(env_non_slippery.render())\n",
    "    captions.append(f'Step: {step_count} State: {next_state} Action: {action} Reward: {reward}')\n",
    "\n",
    "    # Update state\n",
    "    state = next_state\n",
    "\n",
    "if done and reward == 1:\n",
    "    print(f\"Success! Reached the goal in {step_count} steps.\")\n",
    "else:\n",
    "    print(f\"Failure. Did not reach the goal after {step_count} steps.\")\n",
    "\n",
    "# Display the successful run\n",
    "show_video(renders, captions)\n",
    "env_non_slippery.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09a827",
   "metadata": {},
   "source": [
    "## Slippery Frozen Lake\n",
    "\n",
    "Now we will learn a Q-table for the slippery version of Frozen Lake (`is_slippery=True`), where actions do not always have the intended effect. This makes the problem significantly harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ec6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup for the slippery version\n",
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f92e00",
   "metadata": {},
   "source": [
    "### Training and Evaluation (2,000 steps)\n",
    "\n",
    "First, we train with the same parameters as the non-slippery case (2,000 steps). We expect the performance to be poor due to the environment's stochasticity and the short training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c16072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate over 1000 trials (2,000 training steps): 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Train with 2,000 steps\n",
    "q_table_slippery_2k = train_q_table(\n",
    "    env=env_slippery,\n",
    "    num_steps=2000,\n",
    "    lr=0.01,\n",
    "    gamma=0.99,\n",
    "    decay_rate=0.0001,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Evaluate the policy over 1000 trials\n",
    "success_rate_2k = evaluate_policy(env_slippery, q_table_slippery_2k, num_trials=1000, seed=seed)\n",
    "\n",
    "print(f\"Success rate over 1000 trials (2,000 training steps): {success_rate_2k:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7f237",
   "metadata": {},
   "source": [
    "### Training and Evaluation (200,000 steps)\n",
    "\n",
    "To improve performance, we drastically increase the number of training steps to 200,000 and use a slower epsilon decay rate to encourage more exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451f25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate over 1000 trials (200,000 training steps): 71.80%\n"
     ]
    }
   ],
   "source": [
    "# Train with 200,000 steps and a slower decay rate\n",
    "num_steps_200k = 200000\n",
    "decay_rate_slower = 0.00001\n",
    "\n",
    "q_table_slippery_200k = train_q_table(\n",
    "    env=env_slippery,\n",
    "    num_steps=num_steps_200k,\n",
    "    lr=0.01,\n",
    "    gamma=0.99,\n",
    "    decay_rate=decay_rate_slower,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Evaluate the new policy\n",
    "success_rate_200k = evaluate_policy(env_slippery, q_table_slippery_200k, num_trials=1000, seed=seed)\n",
    "env_slippery.close()\n",
    "\n",
    "print(f\"Success rate over 1000 trials (200,000 training steps): {success_rate_200k:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c145c",
   "metadata": {},
   "source": [
    "### Why did the success rate improve?\n",
    "\n",
    "The success rate improved significantly for two main reasons:\n",
    "\n",
    "1.  **More Training Steps:** The slippery environment is stochastic, meaning the same action in the same state can lead to different next states. A small number of training steps (like 2,000) is insufficient for the agent to experience the full range of possible outcomes for each state-action pair. By increasing the training steps to 200,000, the agent gathers a much larger and more representative sample of transitions. This allows the Q-values to converge more closely to their true expected values, which properly account for the environment's randomness.\n",
    "\n",
    "2.  **Slower Epsilon Decay:** The decay rate for epsilon was reduced from 0.0001 to 0.00001. A slower decay means that the `epsilon` value stays higher for longer, forcing the agent to perform more random (exploratory) actions throughout the extended training period. In a stochastic environment, thorough exploration is critical to avoid settling on a suboptimal policy that might seem good based on early, lucky outcomes. The prolonged exploration ensures the agent discovers more robust paths to the goal that are less susceptible to the environment's slipperiness.\n",
    "\n",
    "In summary, the combination of significantly more experience and a more patient exploration strategy allowed the agent to build a much more accurate and robust model of the stochastic environment, leading to a higher success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac9bf7",
   "metadata": {},
   "source": [
    "# Part 2: n-step Deep Q-Learning (Cart Pole)\n",
    "\n",
    "In this part, we implement an n-step Deep Q-Learning agent to solve the CartPole-v1 environment. We will use a Keras neural network to approximate the Q-function. The agent will be trained by running full episodes and then performing a batch update on the network using the collected experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e272928",
   "metadata": {},
   "source": [
    "## Setting up the Environment and Model\n",
    "\n",
    "First, we set the seed for reproducibility across all relevant libraries. Then we define a function to create our Q-network. The network will be a simple multi-layer perceptron (MLP) with one hidden layer, as specified in the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9121a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "seed = 1234\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "\n",
    "# Set seeds\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "\n",
    "def create_q_model():\n",
    "    \"\"\"Creates a Keras model for the Q-network.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=state_shape),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(num_actions, activation=\"linear\") # Linear activation for Q-values\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e84d23",
   "metadata": {},
   "source": [
    "## N-step Q-Learning Implementation\n",
    "\n",
    "Here is the core of the n-step Q-learning algorithm. We will run for a specified number of episodes. In each episode, we collect states, actions, and rewards. At the end of the episode, we calculate the discounted cumulative rewards (also called returns) and use them as targets to update our Q-network in a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3110b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dql(num_episodes, decay_rate, learning_rate):\n",
    "    \"\"\"Trains a Deep Q-Learning model on the CartPole environment.\"\"\"\n",
    "    q_network = create_q_model()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    global_step_counter = 0\n",
    "    episode_rewards_history = []\n",
    "\n",
    "    # Maximum possible reward for normalization, as per instructions.\n",
    "    # The environment caps episodes at 500 steps.\n",
    "    max_possible_reward = 500.0\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    pbar = tqdm(range(num_episodes))\n",
    "    for episode in pbar:\n",
    "        # ---- 1. Collect experience by playing one episode ----\n",
    "        state, _ = env.reset()\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            global_step_counter += 1\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            epsilon = math.exp(-decay_rate * global_step_counter)\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()  # Explore\n",
    "            else:\n",
    "                q_values = q_network(tf.expand_dims(state, 0))\n",
    "                action = tf.argmax(q_values[0]).numpy()  # Exploit\n",
    "\n",
    "            # Apply action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Store experience\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_rewards_history.append(len(episode_rewards))\n",
    "\n",
    "        # ---- 2. Calculate discounted cumulative rewards (returns) ----\n",
    "        n_steps = len(episode_rewards)\n",
    "        returns = np.zeros_like(episode_rewards, dtype=np.float32)\n",
    "        running_return = 0.0\n",
    "        # Iterate backwards from the last step\n",
    "        for i in reversed(range(n_steps)):\n",
    "            running_return = episode_rewards[i] + gamma * running_return\n",
    "            returns[i] = running_return\n",
    "\n",
    "        # ---- 3. Normalize returns ----\n",
    "        returns_normalized = returns / max_possible_reward\n",
    "\n",
    "        # ---- 4. Prepare data for batch update ----\n",
    "        states_tensor = tf.convert_to_tensor(episode_states, dtype=tf.float32)\n",
    "        actions_tensor = tf.convert_to_tensor(episode_actions, dtype=tf.int32)\n",
    "        returns_tensor = tf.convert_to_tensor(returns_normalized, dtype=tf.float32)\n",
    "\n",
    "        # Create indices for tf.gather_nd to select the Q-values of actions taken\n",
    "        action_indices = tf.stack([tf.range(n_steps, dtype=tf.int32), actions_tensor], axis=1)\n",
    "\n",
    "        # ---- 5. Update the Q-network ----\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predict Q-values for all actions for the states in the episode\n",
    "            all_q_values = q_network(states_tensor)\n",
    "            # Select the Q-values for the actions that were actually taken\n",
    "            action_q_values = tf.gather_nd(all_q_values, action_indices)\n",
    "            # Calculate the loss between the predicted Q-values and the calculated returns\n",
    "            loss = mse_loss(returns_tensor, action_q_values)\n",
    "\n",
    "        # Calculate gradients and update the model\n",
    "        grads = tape.gradient(loss, q_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards_history[-100:])\n",
    "            pbar.set_description(f\"Episode {episode + 1}/{num_episodes}, Avg Reward (last 100): {avg_reward:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "    return q_network, episode_rewards_history\n",
    "\n",
    "def evaluate_policy_dql(env, q_network, num_trials=100):\n",
    "    \"\"\"Evaluates the performance of a trained DQL policy.\"\"\"\n",
    "    print(f\"\\nEvaluating policy over {num_trials} trials...\")\n",
    "    total_steps = 0\n",
    "    success_count = 0\n",
    "    all_steps = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            q_values = q_network(tf.expand_dims(state, 0))\n",
    "            action = tf.argmax(q_values[0]).numpy()\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "            steps += 1\n",
    "\n",
    "        all_steps.append(steps)\n",
    "        total_steps += steps\n",
    "        if steps >= 200:\n",
    "            success_count += 1\n",
    "\n",
    "    avg_steps = total_steps / num_trials\n",
    "    success_rate = success_count / num_trials\n",
    "\n",
    "    print(f\"Average steps per episode: {avg_steps:.2f}\")\n",
    "    print(f\"Success rate (>= 200 steps): {success_rate:.2%}\")\n",
    "    return avg_steps, success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d022176",
   "metadata": {},
   "source": [
    "### Experiment 1: Standard Training\n",
    "\n",
    "We train for 3000 episodes with a slow epsilon decay rate. This gives the agent ample time to explore the environment and learn a robust policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d521a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bshowell/Desktop/school/spring 24-25/587/.venv/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 3000/3000, Avg Reward (last 100): 113.36, Epsilon: 0.1610: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [01:31<00:00, 32.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "\n",
      "Evaluating policy over 100 trials...\n",
      "Average steps per episode: 269.60\n",
      "Success rate (>= 200 steps): 96.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(269.6, 0.96)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Parameters from instructions\n",
    "num_episodes_exp1 = 3000\n",
    "decay_rate_exp1 = 0.00001\n",
    "learning_rate_exp1 = 0.001\n",
    "\n",
    "trained_model_exp1, _ = train_dql(num_episodes_exp1, decay_rate_exp1, learning_rate_exp1)\n",
    "evaluate_policy_dql(env, trained_model_exp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd537ca",
   "metadata": {},
   "source": [
    "### Experiment 2: Rapid Decay and Short Training\n",
    "\n",
    "Now, we drastically change the parameters. We train for only 100 episodes and use a much faster epsilon decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d94ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 100/100, Avg Reward (last 100): 18.09, Epsilon: 0.1638: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 74.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "\n",
      "Evaluating policy over 100 trials...\n",
      "Average steps per episode: 15.80\n",
      "Success rate (>= 200 steps): 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Modified parameters\n",
    "num_episodes_exp2 = 100\n",
    "decay_rate_exp2 = 0.001\n",
    "learning_rate_exp2 = 0.001\n",
    "\n",
    "trained_model_exp2, _ = train_dql(num_episodes_exp2, decay_rate_exp2, learning_rate_exp2)\n",
    "evaluate_policy_dql(env, trained_model_exp2)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ec72f",
   "metadata": {},
   "source": [
    "### Why did the performance drop so drastically in Experiment 2?\n",
    "\n",
    "The performance plummeted in the second experiment due to two interconnected factors:\n",
    "\n",
    "1.  **Premature Exploitation:** The `decay_rate` was increased from `0.00001` to `0.001`. This causes the exploration probability, `epsilon`, to decrease very rapidly. The agent stops taking random actions and starts exploiting its learned knowledge far too early. Since it has only been trained for a few episodes, its \"knowledge\" is based on a tiny, unrepresentative sample of the environment. It likely latches onto a poor, short-sighted strategy and never explores enough to find the better, long-term solution required to balance the pole for 200+ steps.\n",
    "\n",
    "2.  **Insufficient Training Data:** Training for only 100 episodes is not enough time for the neural network to learn the complex dynamics of the CartPole environment. In the early stages, episodes are very short as the agent's policy is essentially random. With only 100 short episodes, the total number of (state, action, reward) samples collected is extremely small. The Q-network cannot generalize from such sparse data and fails to learn a meaningful policy.\n",
    "\n",
    "In essence, the second experiment combines the worst of both worlds: it forces the agent to commit to a strategy before it has had a chance to explore (due to rapid epsilon decay) and it doesn't provide enough experience for any strategy it learns to be a good one (due to the low number of episodes). The first experiment succeeded because the long training duration and slow decay rate allowed for a healthy balance between exploration and exploitation, which is crucial for effective reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2f404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
