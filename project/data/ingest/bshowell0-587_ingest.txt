Directory structure:
â””â”€â”€ 587/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ hw1/
    â”‚   â”œâ”€â”€ Tensorflow_and_Keras_Tutorial.ipynb
    â”‚   â””â”€â”€ main.ipynb
    â”œâ”€â”€ hw2/
    â”‚   â”œâ”€â”€ instructions.md
    â”‚   â”œâ”€â”€ linear_autoencoder.ipynb
    â”‚   â”œâ”€â”€ main.ipynb
    â”‚   â”œâ”€â”€ temp.ipynb
    â”‚   â””â”€â”€ temp.py
    â”œâ”€â”€ hw3/
    â”‚   â”œâ”€â”€ frey_diffusion_example.py
    â”‚   â”œâ”€â”€ gan_mnist.py
    â”‚   â”œâ”€â”€ instructions.md
    â”‚   â”œâ”€â”€ main.ipynb
    â”‚   â”œâ”€â”€ temp.ipynb
    â”‚   â””â”€â”€ temp.py
    â”œâ”€â”€ hw4/
    â”‚   â”œâ”€â”€ hw4_gym_examples.py
    â”‚   â”œâ”€â”€ instructions.md
    â”‚   â”œâ”€â”€ main.ipynb
    â”‚   â”œâ”€â”€ temp.ipynb
    â”‚   â””â”€â”€ temp.py
    â”œâ”€â”€ paper/
    â”‚   â”œâ”€â”€ .DS_Store
    â”‚   â”œâ”€â”€ con/
    â”‚   â”œâ”€â”€ pro/
    â”‚   â””â”€â”€ summary/
    â”œâ”€â”€ project/
    â”‚   â”œâ”€â”€ notes.md
    â”‚   â”œâ”€â”€ proposal.md
    â”‚   â””â”€â”€ .DS_Store
    â””â”€â”€ random/

================================================
File: README.md
================================================
587 repo



================================================
File: hw1/Tensorflow_and_Keras_Tutorial.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Tensorflow / Keras Tutorial ##

Tensorflow and Keras are Python libraries developed by Google for deep learning.
"""

import tensorflow as tf
from tensorflow import keras

"""
With Tensorflow you can build arbitrary "computation graphs" which specify the flow of computations from input to output.  The computation can happen on the CPU or GPU and can even be spread over multiple GPUs or distributed across multiple machines.

To use a GPU in Google Colab, be sure to select GPU under Runtime > Change Runtime Type > Hardware Accelerator.
"""

a = tf.constant(5.)
b = tf.constant(5.)
(a + b).numpy()
# Output:
#   np.float32(10.0)

"""
You can define variables in Tensorflow, which can be initialized and later updated.  These could be used to represent weights in a neural network, for example.
"""

x = tf.Variable(initial_value=5,name='x')
y = x + 2
y
# Output:
#   <tf.Tensor: shape=(), dtype=int32, numpy=7>

"""
The real magic of Tensorflow is that it provides automatic differentiation.  You can ask it for the derivative of any tensor with respect to any variable and it will automatically compute it for you.

In Tensorflow 2, automatic differentation is enabled using GradientTape.
"""

x = tf.Variable(initial_value=5.,name='x')

with tf.GradientTape() as tape:
  y = 5*x + 2.
tape.gradient(y,x)
# Output:
#   <tf.Tensor: shape=(), dtype=float32, numpy=5.0>

"""
We can use variables and automatic differentation to set up linear regression, for example.
"""

import numpy as np

w = tf.Variable(initial_value=np.random.randn(2,1),name='w')
b = tf.Variable(initial_value=np.zeros((1,1)),name='b')
x = [[1,2]]
y = [[3]]
# w1 * x1 + w2 * x2 + b = 3

with tf.GradientTape() as tape:
  z = x @ w + b
  loss = tf.reduce_mean((z-y)**2)
dLdw,dLdb = tape.gradient(loss,[w,b])
print('dL/dw:',dLdw)
print('dL/db:',dLdb)
# Output:
#   dL/dw: tf.Tensor(

#   [[-2.71152596]

#    [-5.42305191]], shape=(2, 1), dtype=float64)

#   dL/db: tf.Tensor([[-2.71152596]], shape=(1, 1), dtype=float64)


"""
Tensorflow provides common neural network functions in the tf.nn package.
"""

x = tf.Variable(initial_value=-1.,name='x')
with tf.GradientTape() as tape:
  y = tf.nn.relu(x)
tape.gradient(y,x)
# Output:
#   <tf.Tensor: shape=(), dtype=float32, numpy=0.0>

"""
## Building a multi-layer perceptron in Tensorflow ##

Let's make a multi-layer perceptron in Tensorflow.  Thanks to autodiff, we don't need to worry about the Jacobians -- Tensorflow can figure it out for us!

First we will make a random, non-linearly separable dataset.
"""

from sklearn.datasets import make_moons
import numpy as np
from matplotlib import pyplot as plt

x,y = make_moons(n_samples=500,noise=0.1)
x = x.astype('float32')
y = y.astype('float32')
y = np.expand_dims(y,axis=-1)

plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.Set1,
            edgecolor='k')
plt.axis('equal')
plt.title('moons')
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>

"""
Now let's create variables for the weights and biases in the hidden and output layers.
"""

# hidden layer
w_h = tf.Variable(initial_value=np.random.randn(2,3)*0.01,dtype='float32',name='w_h')
b_h = tf.Variable(initial_value=np.zeros(3),dtype='float32',name='b_h')

# output layer
w_z = tf.Variable(initial_value=np.random.randn(3,1)*0.01,dtype='float32',name='w_z')
b_z = tf.Variable(initial_value=np.zeros(1),dtype='float32',name='b_z')

"""
Now we will run gradient descent.  We will put our forward pass code inside GradientTape so that the gradients will be computed.  At the end of each iteration we use the gradients to update the weights and biases.
"""

num_iter = 2000
lr = 1.

vars = [w_h,b_h,w_z,b_z]
losses = []

for iter in range(num_iter):
  with tf.GradientTape() as tape:
    # hidden layer 1
    h = x @ w_h + b_h
    h = tf.nn.tanh(h)

    # output layer
    z = h @ w_z + b_z

    # binary cross-entropy loss
    loss = y * tf.nn.softplus(-z) + (1-y)*tf.nn.softplus(z)
    loss = tf.reduce_mean(loss)

  # get gradients
  grads = tape.gradient(loss,vars)

  # gradient descent
  for grad,var in zip(grads,vars):
    var.assign_add(-lr*grad)

  losses.append(loss.numpy())

  if iter % 50 == 0:
    print('step %d: loss: %0.2f'%(iter,loss))
# Output:
#   step 0: loss: 0.69

#   step 50: loss: 0.27

#   step 100: loss: 0.26

#   step 150: loss: 0.26

#   step 200: loss: 0.26

#   step 250: loss: 0.26

#   step 300: loss: 0.26

#   step 350: loss: 0.26

#   step 400: loss: 0.26

#   step 450: loss: 0.26

#   step 500: loss: 0.26

#   step 550: loss: 0.26

#   step 600: loss: 0.26

#   step 650: loss: 0.26

#   step 700: loss: 0.26

#   step 750: loss: 0.26

#   step 800: loss: 0.25

#   step 850: loss: 0.25

#   step 900: loss: 0.25

#   step 950: loss: 0.25

#   step 1000: loss: 0.25

#   step 1050: loss: 0.25

#   step 1100: loss: 0.25

#   step 1150: loss: 0.25

#   step 1200: loss: 0.25

#   step 1250: loss: 0.25

#   step 1300: loss: 0.24

#   step 1350: loss: 0.24

#   step 1400: loss: 0.24

#   step 1450: loss: 0.24

#   step 1500: loss: 0.23

#   step 1550: loss: 0.23

#   step 1600: loss: 0.22

#   step 1650: loss: 0.14

#   step 1700: loss: 0.05

#   step 1750: loss: 0.03

#   step 1800: loss: 0.03

#   step 1850: loss: 0.02

#   step 1900: loss: 0.02

#   step 1950: loss: 0.02


"""
Plot of loss over time:
"""

plt.plot(losses)
plt.xlabel('iteration')
plt.ylabel('loss')
plt.title('Loss over time')
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>

"""
Now we can run the trained neural network on the data and check the accuracy.
"""

# forward pass
h2 = x @ w_h + b_h
h2 = tf.nn.tanh(h)
z = h2 @ w_z + b_z

# compute accuracy
pred = (z>0).numpy().astype('int32')
acc = np.count_nonzero(pred==y)/len(y)
print('Accuracy: %.2f%%'%(acc*100))
# Output:
#   Accuracy: 100.00%


"""
## Neural Networks in Keras ##

Keras makes it much easier to set up a neural network.  It is an object-oriented framework that provides classes for many common neural network layers, activations, losses, optimizers, and metrics, as well as many other convenient features.

The entire network is encapsulated in a Model class which can be trained using .fit().
"""

from tensorflow.keras import layers
model = tf.keras.Sequential(
    [layers.Input((2,)),
     layers.Dense(3,activation='tanh',name='h'),
     layers.Dense(1,activation='sigmoid',name='z')
    ]
)
opt = tf.keras.optimizers.SGD(learning_rate=lr,momentum=0.)
model.compile(loss='binary_crossentropy',optimizer=opt)

history = model.fit(x,y,batch_size=len(x),epochs=2000,verbose=False)

plt.plot(history.history['loss'])
plt.xlabel('iteration')
plt.ylabel('loss')
plt.title('Loss over time')
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>

pred = (model.predict(x)>0.5).astype('int32')
acc = np.count_nonzero(pred==y)/len(y)
print('Accuracy: %.2f%%'%(acc*100))
# Output:
#   [1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step

#   Accuracy: 100.00%


"""
### Custom training loop

Sometimes you need to write a custom training loop with a Keras model.  This can be done using GradientTape.

Note that we don't need to compile the model in this case.
"""

"""
First we make some dummy data for a least squares regression problem.
"""

x = np.random.uniform(-np.pi,np.pi,size=(1000,1))
y = np.sin(x)+np.random.normal(scale=0.1,size=x.shape)

plt.scatter(x,y)
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>

from tensorflow.keras import layers
model = tf.keras.Sequential(
    [
     layers.InputLayer((1,)),
     layers.Dense(3,activation='tanh',name='h'),
     layers.Dense(1,activation=None,name='z')
    ]
)

# create optimizer
opt = tf.keras.optimizers.Adam(.01)

loss_history = []
for i in range(2000):
  # turn on gradient tape to track loss computations
  # so that we can calculate gradient of the loss later
  with tf.GradientTape() as tape:
    # get predictions from model
    y_pred = model(x)
    # calculate mean squared error
    L = tf.reduce_mean((y-y_pred)**2)

  # get gradients of loss w.r.t. model parameters
  grads = tape.gradient(L,model.trainable_variables)

  # update model parameters using optimizer
  opt.apply_gradients(zip(grads,model.trainable_variables))

  # append loss to history so we can plot it later
  loss_history.append(L.numpy())


plt.plot(loss_history)
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>

pred = model.predict(x)
plt.scatter(x,y)
plt.scatter(x,pred)
plt.legend(['gt','pred'])
plt.show()
# Output:
#   [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step

#   <Figure size 640x480 with 1 Axes>

"""
## Convolutional Neural Networks in Keras ##

Keras provides convolutional layers as well.  In Tensorflow and Keras, when using convolutional layers, the data has shape (B,H,W,C) where B is the batch size, H is the image height, W is the image width, and C is the number of channels.
"""

"""
First let's load the small UC Merced dataset from Tensorflow Datasets.  Description from the website:

"UC Merced is a 21 class land use remote sensing image dataset, with 100 images per class. The images were manually extracted from large images from the USGS National Map Urban Area Imagery collection for various urban areas around the country. The pixel resolution of this public domain imagery is 0.3 m.

While most images are 256x256 pixels, there are 44 images with different shape."

We will resize all the images to 32x32 to make the CNN training faster.

"""

import tensorflow_datasets as tfds
ds_train, ds_info = tfds.load('uc_merced', split='train', shuffle_files=True, with_info=True)

x = []
y = []
for ex in ds_train:
  im = tf.image.resize(ex['image'],(32,32),method='area')
  im = im/255.
  label = ex['label']
  x.append(im)
  y.append(label)
x = tf.stack(x)
y = tf.stack(y)
print('x shape:',x.shape)
print('y shape:',y.shape)
# Output:
#   x shape: (2100, 32, 32, 3)

#   y shape: (2100,)


"""
Here are some example images with their labels.
"""

for i in range(5):
  plt.imshow(x[i].numpy())
  plt.title(y[i].numpy())
  plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>
#   <Figure size 640x480 with 1 Axes>
#   <Figure size 640x480 with 1 Axes>
#   <Figure size 640x480 with 1 Axes>
#   <Figure size 640x480 with 1 Axes>

"""
Now let's build a little VGG-style network.

We will use the Adam optimizer which is typically much better than vanilla SGD.

We will use the validation_split feature of Keras to automatically withold a validation set during training.
"""

from tensorflow.keras import layers

lr = 3e-4
batch_size = 32
epochs = 200

model = tf.keras.Sequential(
    [
     layers.Conv2D(32,3,activation='relu',padding='same',name='conv1a'),
     layers.Conv2D(32,3,activation='relu',padding='same',name='conv1b'),
     layers.MaxPooling2D(2,2),
     layers.Conv2D(64,3,activation='relu',padding='same',name='conv2a'),
     layers.Conv2D(64,3,activation='relu',padding='same',name='conv2b'),
     layers.MaxPooling2D(2,2),
     layers.Conv2D(128,3,activation='relu',padding='same',name='conv3a'),
     layers.Conv2D(128,3,activation='relu',padding='same',name='conv3b'),
     layers.MaxPooling2D(2,2),
     layers.Flatten(),
     layers.Dense(1024,activation='relu',name='dense1'),
     layers.Dense(21,activation='softmax',name='z')
    ]
)
opt = tf.keras.optimizers.Adam(learning_rate=lr)
model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])

history = model.fit(x,y,batch_size=batch_size,epochs=epochs,validation_split=0.1,verbose=True)

print(model.summary())
# Output:
#   Epoch 1/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m10s[0m 75ms/step - accuracy: 0.0478 - loss: 3.0341 - val_accuracy: 0.1333 - val_loss: 2.8115

#   Epoch 2/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.1445 - loss: 2.6930 - val_accuracy: 0.2143 - val_loss: 2.3073

#   Epoch 3/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.2438 - loss: 2.2828 - val_accuracy: 0.2381 - val_loss: 2.4129

#   Epoch 4/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 0.3202 - loss: 2.0438 - val_accuracy: 0.4048 - val_loss: 1.8862

#   Epoch 5/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.4105 - loss: 1.8197 - val_accuracy: 0.4952 - val_loss: 1.5841

#   Epoch 6/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.4961 - loss: 1.6197 - val_accuracy: 0.4381 - val_loss: 1.7112

#   Epoch 7/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.4755 - loss: 1.5500 - val_accuracy: 0.5714 - val_loss: 1.3809

#   Epoch 8/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.6106 - loss: 1.2273 - val_accuracy: 0.5714 - val_loss: 1.3508

#   Epoch 9/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 0.6016 - loss: 1.2115 - val_accuracy: 0.5095 - val_loss: 1.3646

#   Epoch 10/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.6708 - loss: 0.9991 - val_accuracy: 0.5667 - val_loss: 1.3268

#   Epoch 11/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.6936 - loss: 0.9073 - val_accuracy: 0.3952 - val_loss: 2.1023

#   Epoch 12/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.6636 - loss: 1.0208 - val_accuracy: 0.6048 - val_loss: 1.2110

#   Epoch 13/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.7405 - loss: 0.7798 - val_accuracy: 0.5952 - val_loss: 1.4852

#   Epoch 14/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.7646 - loss: 0.6946 - val_accuracy: 0.6190 - val_loss: 1.2374

#   Epoch 15/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.8353 - loss: 0.4835 - val_accuracy: 0.6429 - val_loss: 1.1331

#   Epoch 16/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 0.8640 - loss: 0.4076 - val_accuracy: 0.6333 - val_loss: 1.3560

#   Epoch 17/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 0.8702 - loss: 0.3994 - val_accuracy: 0.6333 - val_loss: 1.4143

#   Epoch 18/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 0.8911 - loss: 0.3359 - val_accuracy: 0.6095 - val_loss: 1.3947

#   Epoch 19/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9483 - loss: 0.1740 - val_accuracy: 0.6429 - val_loss: 1.3509

#   Epoch 20/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9164 - loss: 0.2732 - val_accuracy: 0.6429 - val_loss: 1.3222

#   Epoch 21/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.9774 - loss: 0.1023 - val_accuracy: 0.6619 - val_loss: 1.6127

#   Epoch 22/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.9680 - loss: 0.1042 - val_accuracy: 0.6381 - val_loss: 1.7236

#   Epoch 23/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.9645 - loss: 0.1226 - val_accuracy: 0.6429 - val_loss: 1.5513

#   Epoch 24/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.9657 - loss: 0.1117 - val_accuracy: 0.6571 - val_loss: 1.6012

#   Epoch 25/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.9717 - loss: 0.0842 - val_accuracy: 0.6476 - val_loss: 1.4998

#   Epoch 26/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9931 - loss: 0.0311 - val_accuracy: 0.6714 - val_loss: 1.4841

#   Epoch 27/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.6619 - val_loss: 1.7086

#   Epoch 28/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.9967 - loss: 0.0119 - val_accuracy: 0.5810 - val_loss: 3.1106

#   Epoch 29/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.8373 - loss: 0.5663 - val_accuracy: 0.6333 - val_loss: 1.5162

#   Epoch 30/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9858 - loss: 0.0774 - val_accuracy: 0.6571 - val_loss: 1.6551

#   Epoch 31/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9985 - loss: 0.0149 - val_accuracy: 0.6286 - val_loss: 1.7474

#   Epoch 32/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.9925 - loss: 0.0321 - val_accuracy: 0.6333 - val_loss: 1.9866

#   Epoch 33/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9764 - loss: 0.0905 - val_accuracy: 0.6476 - val_loss: 1.6705

#   Epoch 34/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9996 - loss: 0.0143 - val_accuracy: 0.6571 - val_loss: 1.8059

#   Epoch 35/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.9998 - loss: 0.0044 - val_accuracy: 0.6476 - val_loss: 2.0226

#   Epoch 36/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.9923 - loss: 0.0258 - val_accuracy: 0.5762 - val_loss: 2.7601

#   Epoch 37/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9332 - loss: 0.2412 - val_accuracy: 0.6714 - val_loss: 1.6676

#   Epoch 38/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.9932 - loss: 0.0356 - val_accuracy: 0.6619 - val_loss: 1.6885

#   Epoch 39/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - accuracy: 0.9690 - loss: 0.0917 - val_accuracy: 0.6524 - val_loss: 1.5932

#   Epoch 40/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 0.9957 - loss: 0.0192 - val_accuracy: 0.6667 - val_loss: 1.8352

#   Epoch 41/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 0.9702 - loss: 0.1044 - val_accuracy: 0.6381 - val_loss: 1.9019

#   Epoch 42/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 0.9911 - loss: 0.0520 - val_accuracy: 0.6476 - val_loss: 1.8693

#   Epoch 43/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 0.9985 - loss: 0.0069 - val_accuracy: 0.6619 - val_loss: 1.8541

#   Epoch 44/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.9999 - loss: 0.0019 - val_accuracy: 0.6619 - val_loss: 1.9681

#   Epoch 45/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.6619 - val_loss: 1.9822

#   Epoch 46/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.6714 - val_loss: 2.0215

#   Epoch 47/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.6714 - val_loss: 2.0529

#   Epoch 48/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 6.0582e-04 - val_accuracy: 0.6714 - val_loss: 2.0802

#   Epoch 49/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 6.2273e-04 - val_accuracy: 0.6762 - val_loss: 2.1068

#   Epoch 50/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 5.3340e-04 - val_accuracy: 0.6762 - val_loss: 2.1277

#   Epoch 51/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 5.5560e-04 - val_accuracy: 0.6810 - val_loss: 2.1529

#   Epoch 52/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 4.0832e-04 - val_accuracy: 0.6714 - val_loss: 2.1744

#   Epoch 53/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 3.0389e-04 - val_accuracy: 0.6714 - val_loss: 2.2035

#   Epoch 54/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 2.8675e-04 - val_accuracy: 0.6810 - val_loss: 2.2156

#   Epoch 55/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 2.9226e-04 - val_accuracy: 0.6714 - val_loss: 2.2321

#   Epoch 56/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 2.0928e-04 - val_accuracy: 0.6714 - val_loss: 2.2514

#   Epoch 57/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 2.2105e-04 - val_accuracy: 0.6714 - val_loss: 2.2642

#   Epoch 58/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 1.8863e-04 - val_accuracy: 0.6714 - val_loss: 2.2811

#   Epoch 59/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.6356e-04 - val_accuracy: 0.6714 - val_loss: 2.2968

#   Epoch 60/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 2.1624e-04 - val_accuracy: 0.6762 - val_loss: 2.3040

#   Epoch 61/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.6511e-04 - val_accuracy: 0.6714 - val_loss: 2.3202

#   Epoch 62/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.2731e-04 - val_accuracy: 0.6667 - val_loss: 2.3358

#   Epoch 63/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 1.3613e-04 - val_accuracy: 0.6667 - val_loss: 2.3516

#   Epoch 64/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 1.1120e-04 - val_accuracy: 0.6619 - val_loss: 2.3690

#   Epoch 65/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 1.2394e-04 - val_accuracy: 0.6762 - val_loss: 2.3772

#   Epoch 66/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 1.1988e-04 - val_accuracy: 0.6667 - val_loss: 2.3877

#   Epoch 67/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 1.1387e-04 - val_accuracy: 0.6667 - val_loss: 2.4039

#   Epoch 68/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 9.8014e-05 - val_accuracy: 0.6667 - val_loss: 2.4155

#   Epoch 69/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 7.8640e-05 - val_accuracy: 0.6667 - val_loss: 2.4318

#   Epoch 70/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 8.9662e-05 - val_accuracy: 0.6667 - val_loss: 2.4444

#   Epoch 71/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 7.9099e-05 - val_accuracy: 0.6667 - val_loss: 2.4490

#   Epoch 72/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 6.5766e-05 - val_accuracy: 0.6667 - val_loss: 2.4615

#   Epoch 73/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 7.3703e-05 - val_accuracy: 0.6667 - val_loss: 2.4718

#   Epoch 74/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 6.8790e-05 - val_accuracy: 0.6810 - val_loss: 2.4795

#   Epoch 75/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 6.5850e-05 - val_accuracy: 0.6714 - val_loss: 2.4915

#   Epoch 76/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 5.7185e-05 - val_accuracy: 0.6667 - val_loss: 2.5057

#   Epoch 77/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 5.5458e-05 - val_accuracy: 0.6667 - val_loss: 2.5160

#   Epoch 78/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 5.3543e-05 - val_accuracy: 0.6667 - val_loss: 2.5235

#   Epoch 79/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 5.0341e-05 - val_accuracy: 0.6667 - val_loss: 2.5381

#   Epoch 80/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 5.1070e-05 - val_accuracy: 0.6762 - val_loss: 2.5441

#   Epoch 81/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 4.8139e-05 - val_accuracy: 0.6667 - val_loss: 2.5613

#   Epoch 82/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 4.5692e-05 - val_accuracy: 0.6667 - val_loss: 2.5703

#   Epoch 83/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 4.8713e-05 - val_accuracy: 0.6667 - val_loss: 2.5779

#   Epoch 84/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 3.5579e-05 - val_accuracy: 0.6667 - val_loss: 2.5948

#   Epoch 85/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 4.0455e-05 - val_accuracy: 0.6619 - val_loss: 2.6039

#   Epoch 86/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 3.9836e-05 - val_accuracy: 0.6667 - val_loss: 2.6115

#   Epoch 87/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 3.5778e-05 - val_accuracy: 0.6667 - val_loss: 2.6225

#   Epoch 88/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 3.3980e-05 - val_accuracy: 0.6667 - val_loss: 2.6326

#   Epoch 89/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 3.0665e-05 - val_accuracy: 0.6667 - val_loss: 2.6400

#   Epoch 90/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 3.3943e-05 - val_accuracy: 0.6667 - val_loss: 2.6472

#   Epoch 91/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 3.5671e-05 - val_accuracy: 0.6667 - val_loss: 2.6530

#   Epoch 92/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 2.7241e-05 - val_accuracy: 0.6667 - val_loss: 2.6632

#   Epoch 93/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 2.8363e-05 - val_accuracy: 0.6667 - val_loss: 2.6728

#   Epoch 94/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 2.5216e-05 - val_accuracy: 0.6667 - val_loss: 2.6815

#   Epoch 95/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 2.3748e-05 - val_accuracy: 0.6667 - val_loss: 2.6889

#   Epoch 96/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 2.5721e-05 - val_accuracy: 0.6667 - val_loss: 2.7015

#   Epoch 97/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 2.3123e-05 - val_accuracy: 0.6619 - val_loss: 2.7070

#   Epoch 98/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 2.3109e-05 - val_accuracy: 0.6667 - val_loss: 2.7175

#   Epoch 99/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 2.7439e-05 - val_accuracy: 0.6667 - val_loss: 2.7278

#   Epoch 100/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 2.0516e-05 - val_accuracy: 0.6667 - val_loss: 2.7354

#   Epoch 101/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.7752e-05 - val_accuracy: 0.6619 - val_loss: 2.7400

#   Epoch 102/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.9034e-05 - val_accuracy: 0.6619 - val_loss: 2.7489

#   Epoch 103/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.9544e-05 - val_accuracy: 0.6619 - val_loss: 2.7604

#   Epoch 104/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 1.8553e-05 - val_accuracy: 0.6619 - val_loss: 2.7719

#   Epoch 105/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.3428e-05 - val_accuracy: 0.6714 - val_loss: 2.7803

#   Epoch 106/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.4197e-05 - val_accuracy: 0.6571 - val_loss: 2.7946

#   Epoch 107/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.8452e-05 - val_accuracy: 0.6619 - val_loss: 2.7986

#   Epoch 108/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 1.5268e-05 - val_accuracy: 0.6619 - val_loss: 2.8084

#   Epoch 109/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.2744e-05 - val_accuracy: 0.6667 - val_loss: 2.8151

#   Epoch 110/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 1.2410e-05 - val_accuracy: 0.6619 - val_loss: 2.8224

#   Epoch 111/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 1.3890e-05 - val_accuracy: 0.6619 - val_loss: 2.8385

#   Epoch 112/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 1.3410e-05 - val_accuracy: 0.6619 - val_loss: 2.8476

#   Epoch 113/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.1921e-05 - val_accuracy: 0.6619 - val_loss: 2.8482

#   Epoch 114/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.1879e-05 - val_accuracy: 0.6619 - val_loss: 2.8549

#   Epoch 115/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.0416e-05 - val_accuracy: 0.6619 - val_loss: 2.8655

#   Epoch 116/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 9.1863e-06 - val_accuracy: 0.6619 - val_loss: 2.8743

#   Epoch 117/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.0451e-05 - val_accuracy: 0.6667 - val_loss: 2.8895

#   Epoch 118/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 8.4601e-06 - val_accuracy: 0.6667 - val_loss: 2.8931

#   Epoch 119/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 9.2958e-06 - val_accuracy: 0.6667 - val_loss: 2.8968

#   Epoch 120/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 7.8688e-06 - val_accuracy: 0.6667 - val_loss: 2.9031

#   Epoch 121/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 8.7560e-06 - val_accuracy: 0.6667 - val_loss: 2.9140

#   Epoch 122/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 8.7750e-06 - val_accuracy: 0.6667 - val_loss: 2.9242

#   Epoch 123/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 7.3452e-06 - val_accuracy: 0.6667 - val_loss: 2.9380

#   Epoch 124/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 7.3261e-06 - val_accuracy: 0.6762 - val_loss: 2.9395

#   Epoch 125/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 8.0147e-06 - val_accuracy: 0.6667 - val_loss: 2.9531

#   Epoch 126/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 7.6023e-06 - val_accuracy: 0.6667 - val_loss: 2.9598

#   Epoch 127/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 6.1699e-06 - val_accuracy: 0.6667 - val_loss: 2.9644

#   Epoch 128/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 6.6822e-06 - val_accuracy: 0.6667 - val_loss: 2.9727

#   Epoch 129/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 6.3732e-06 - val_accuracy: 0.6667 - val_loss: 2.9839

#   Epoch 130/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 5.7918e-06 - val_accuracy: 0.6667 - val_loss: 2.9915

#   Epoch 131/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 5.9157e-06 - val_accuracy: 0.6667 - val_loss: 2.9995

#   Epoch 132/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 5.8190e-06 - val_accuracy: 0.6667 - val_loss: 3.0071

#   Epoch 133/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 5.5707e-06 - val_accuracy: 0.6667 - val_loss: 3.0121

#   Epoch 134/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 4.9221e-06 - val_accuracy: 0.6667 - val_loss: 3.0232

#   Epoch 135/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 4.5770e-06 - val_accuracy: 0.6667 - val_loss: 3.0270

#   Epoch 136/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 4.7226e-06 - val_accuracy: 0.6667 - val_loss: 3.0343

#   Epoch 137/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 4.3521e-06 - val_accuracy: 0.6667 - val_loss: 3.0442

#   Epoch 138/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 4.6982e-06 - val_accuracy: 0.6667 - val_loss: 3.0541

#   Epoch 139/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 5.3171e-06 - val_accuracy: 0.6667 - val_loss: 3.0635

#   Epoch 140/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 5.1087e-06 - val_accuracy: 0.6667 - val_loss: 3.0688

#   Epoch 141/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 3.6774e-06 - val_accuracy: 0.6667 - val_loss: 3.0770

#   Epoch 142/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 3.8180e-06 - val_accuracy: 0.6619 - val_loss: 3.0830

#   Epoch 143/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 3.1711e-06 - val_accuracy: 0.6714 - val_loss: 3.0922

#   Epoch 144/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 3.7143e-06 - val_accuracy: 0.6667 - val_loss: 3.1010

#   Epoch 145/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 3.4945e-06 - val_accuracy: 0.6667 - val_loss: 3.1109

#   Epoch 146/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 3.4652e-06 - val_accuracy: 0.6667 - val_loss: 3.1158

#   Epoch 147/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 3.2842e-06 - val_accuracy: 0.6667 - val_loss: 3.1214

#   Epoch 148/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 3.3636e-06 - val_accuracy: 0.6667 - val_loss: 3.1350

#   Epoch 149/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 3.3548e-06 - val_accuracy: 0.6667 - val_loss: 3.1440

#   Epoch 150/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 3.1185e-06 - val_accuracy: 0.6667 - val_loss: 3.1552

#   Epoch 151/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 2.8861e-06 - val_accuracy: 0.6667 - val_loss: 3.1588

#   Epoch 152/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 2.7785e-06 - val_accuracy: 0.6667 - val_loss: 3.1665

#   Epoch 153/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 2.7366e-06 - val_accuracy: 0.6667 - val_loss: 3.1746

#   Epoch 154/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 2.5798e-06 - val_accuracy: 0.6619 - val_loss: 3.1800

#   Epoch 155/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 2.0835e-06 - val_accuracy: 0.6714 - val_loss: 3.1907

#   Epoch 156/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 2.1703e-06 - val_accuracy: 0.6619 - val_loss: 3.2037

#   Epoch 157/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 2.4034e-06 - val_accuracy: 0.6667 - val_loss: 3.2094

#   Epoch 158/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 2.3605e-06 - val_accuracy: 0.6619 - val_loss: 3.2164

#   Epoch 159/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 2.0573e-06 - val_accuracy: 0.6714 - val_loss: 3.2237

#   Epoch 160/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 2.1003e-06 - val_accuracy: 0.6619 - val_loss: 3.2315

#   Epoch 161/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 2.1611e-06 - val_accuracy: 0.6619 - val_loss: 3.2387

#   Epoch 162/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.7193e-06 - val_accuracy: 0.6714 - val_loss: 3.2462

#   Epoch 163/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.9557e-06 - val_accuracy: 0.6619 - val_loss: 3.2531

#   Epoch 164/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.5455e-06 - val_accuracy: 0.6667 - val_loss: 3.2586

#   Epoch 165/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.5347e-06 - val_accuracy: 0.6619 - val_loss: 3.2658

#   Epoch 166/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.7725e-06 - val_accuracy: 0.6619 - val_loss: 3.2771

#   Epoch 167/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 1.4668e-06 - val_accuracy: 0.6619 - val_loss: 3.2891

#   Epoch 168/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.3851e-06 - val_accuracy: 0.6619 - val_loss: 3.2963

#   Epoch 169/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.5491e-06 - val_accuracy: 0.6619 - val_loss: 3.3005

#   Epoch 170/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.4823e-06 - val_accuracy: 0.6619 - val_loss: 3.3053

#   Epoch 171/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.3593e-06 - val_accuracy: 0.6619 - val_loss: 3.3152

#   Epoch 172/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.2701e-06 - val_accuracy: 0.6714 - val_loss: 3.3230

#   Epoch 173/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.3671e-06 - val_accuracy: 0.6619 - val_loss: 3.3329

#   Epoch 174/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.2815e-06 - val_accuracy: 0.6619 - val_loss: 3.3381

#   Epoch 175/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 6ms/step - accuracy: 1.0000 - loss: 1.3343e-06 - val_accuracy: 0.6667 - val_loss: 3.3621

#   Epoch 176/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 1.2065e-06 - val_accuracy: 0.6619 - val_loss: 3.3606

#   Epoch 177/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 1.1368e-06 - val_accuracy: 0.6619 - val_loss: 3.3690

#   Epoch 178/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 1.1769e-06 - val_accuracy: 0.6619 - val_loss: 3.3735

#   Epoch 179/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 1.0551e-06 - val_accuracy: 0.6619 - val_loss: 3.3900

#   Epoch 180/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 8.7567e-07 - val_accuracy: 0.6619 - val_loss: 3.3938

#   Epoch 181/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 9.3565e-07 - val_accuracy: 0.6619 - val_loss: 3.3988

#   Epoch 182/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - accuracy: 1.0000 - loss: 9.6018e-07 - val_accuracy: 0.6619 - val_loss: 3.4068

#   Epoch 183/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - accuracy: 1.0000 - loss: 9.0583e-07 - val_accuracy: 0.6714 - val_loss: 3.4145

#   Epoch 184/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 9.4274e-07 - val_accuracy: 0.6619 - val_loss: 3.4240

#   Epoch 185/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 9.3529e-07 - val_accuracy: 0.6619 - val_loss: 3.4311

#   Epoch 186/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 9.3208e-07 - val_accuracy: 0.6619 - val_loss: 3.4338

#   Epoch 187/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 8.3402e-07 - val_accuracy: 0.6619 - val_loss: 3.4428

#   Epoch 188/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 8.0313e-07 - val_accuracy: 0.6619 - val_loss: 3.4511

#   Epoch 189/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 7.1364e-07 - val_accuracy: 0.6619 - val_loss: 3.4594

#   Epoch 190/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 6.9744e-07 - val_accuracy: 0.6619 - val_loss: 3.4692

#   Epoch 191/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 6.9370e-07 - val_accuracy: 0.6619 - val_loss: 3.4732

#   Epoch 192/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 6.6996e-07 - val_accuracy: 0.6619 - val_loss: 3.4829

#   Epoch 193/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - accuracy: 1.0000 - loss: 7.1049e-07 - val_accuracy: 0.6619 - val_loss: 3.4889

#   Epoch 194/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 1.0000 - loss: 6.1551e-07 - val_accuracy: 0.6619 - val_loss: 3.4916

#   Epoch 195/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 7.1358e-07 - val_accuracy: 0.6619 - val_loss: 3.5038

#   Epoch 196/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 1.0000 - loss: 5.4614e-07 - val_accuracy: 0.6619 - val_loss: 3.5133

#   Epoch 197/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 5.1840e-07 - val_accuracy: 0.6714 - val_loss: 3.5152

#   Epoch 198/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 6.1261e-07 - val_accuracy: 0.6714 - val_loss: 3.5284

#   Epoch 199/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 5.6258e-07 - val_accuracy: 0.6619 - val_loss: 3.5364

#   Epoch 200/200

#   [1m60/60[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 7ms/step - accuracy: 1.0000 - loss: 6.4150e-07 - val_accuracy: 0.6619 - val_loss: 3.5496

#   [1mModel: "sequential_2"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ conv1a ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m32[0m, [38;5;34m32[0m, [38;5;34m32[0m)     â”‚           [38;5;34m896[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv1b ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m32[0m, [38;5;34m32[0m, [38;5;34m32[0m)     â”‚         [38;5;34m9,248[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ max_pooling2d ([38;5;33mMaxPooling2D[0m)    â”‚ ([38;5;45mNone[0m, [38;5;34m16[0m, [38;5;34m16[0m, [38;5;34m32[0m)     â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2a ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m16[0m, [38;5;34m16[0m, [38;5;34m64[0m)     â”‚        [38;5;34m18,496[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2b ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m16[0m, [38;5;34m16[0m, [38;5;34m64[0m)     â”‚        [38;5;34m36,928[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ max_pooling2d_1 ([38;5;33mMaxPooling2D[0m)  â”‚ ([38;5;45mNone[0m, [38;5;34m8[0m, [38;5;34m8[0m, [38;5;34m64[0m)       â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv3a ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m8[0m, [38;5;34m8[0m, [38;5;34m128[0m)      â”‚        [38;5;34m73,856[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv3b ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m8[0m, [38;5;34m8[0m, [38;5;34m128[0m)      â”‚       [38;5;34m147,584[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ max_pooling2d_2 ([38;5;33mMaxPooling2D[0m)  â”‚ ([38;5;45mNone[0m, [38;5;34m4[0m, [38;5;34m4[0m, [38;5;34m128[0m)      â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten ([38;5;33mFlatten[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m2048[0m)           â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense1 ([38;5;33mDense[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m1024[0m)           â”‚     [38;5;34m2,098,176[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ z ([38;5;33mDense[0m)                       â”‚ ([38;5;45mNone[0m, [38;5;34m21[0m)             â”‚        [38;5;34m21,525[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m7,220,129[0m (27.54 MB)

#   [1m Trainable params: [0m[38;5;34m2,406,709[0m (9.18 MB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   [1m Optimizer params: [0m[38;5;34m4,813,420[0m (18.36 MB)

#   None


"""
Plotting the training and validation loss and accuracy, we see that we are severely overfitting.  We could play with regularization and data augmentation to try to reduce the overfitting.
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('iteration')
plt.ylabel('loss')
plt.title('Loss over time')
plt.legend(['train','val'])
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.xlabel('iteration')
plt.ylabel('acc')
plt.title('Accuracy over time')
plt.legend(['train','val'])
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>



================================================
File: hw1/main.ipynb
================================================
# Jupyter notebook converted to Python script.

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras import layers, models, Model
import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import rotate

# load dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# convert to float32 and scale to [-1, 1]
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# map [0, 255] to [0, 2], then subtract 1 to get [-1, 1]
x_train = (x_train / 127.5) - 1.0
x_test = (x_test / 127.5) - 1.0

print(f"x_train shape: {x_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"x_train dtype: {x_train.dtype}")
print(f"x_train min value: {np.min(x_train)}")
print(f"x_train max value: {np.max(x_train)}")

num_images_to_show = 5
fig, axes = plt.subplots(1, num_images_to_show, figsize=(10, 3))
for i in range(num_images_to_show):
    axes[i].imshow(x_train[i], cmap='gray')
    axes[i].set_title(f"Label: {y_train[i]}")
    axes[i].axis('off')
plt.tight_layout()
plt.show()
# Output:
#   <Figure size 1000x300 with 5 Axes>

model = models.Sequential([
    layers.Input(shape=(28, 28, 1)),
    layers.Conv2D(50, kernel_size=(5, 5), padding='valid', activation='relu', name='conv1'),
    # (28-5+1, 28-5+1, 50) = (24, 24, 50)
    layers.MaxPooling2D(pool_size=(2, 2), strides=2, name='pool1'),
    # (24/2, 24/2, 50) = (12, 12, 50)
    layers.Conv2D(20, kernel_size=(5, 5), padding='valid', activation='relu', name='conv2'),
    # (12-5+1, 12-5+1, 20) = (8, 8, 20)
    layers.MaxPooling2D(pool_size=(2, 2), strides=2, name='pool2'),
    # (8/2, 8/2, 20) = (4, 4, 20)
    layers.Flatten(name='flatten'),
    # 4 * 4 * 20 = 320
    layers.Dense(200, activation='relu', name='dense1'),
    # 200
    layers.Dense(2, activation=None, name='embedding'),
    # 2
    layers.Dense(10, activation='softmax', name='output')
    # 10
])

model.summary()
# Output:
#   [1mModel: "sequential"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ conv1 ([38;5;33mConv2D[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m24[0m, [38;5;34m24[0m, [38;5;34m50[0m)     â”‚         [38;5;34m1,300[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ pool1 ([38;5;33mMaxPooling2D[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m12[0m, [38;5;34m12[0m, [38;5;34m50[0m)     â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2 ([38;5;33mConv2D[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m8[0m, [38;5;34m8[0m, [38;5;34m20[0m)       â”‚        [38;5;34m25,020[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ pool2 ([38;5;33mMaxPooling2D[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m4[0m, [38;5;34m4[0m, [38;5;34m20[0m)       â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten ([38;5;33mFlatten[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m320[0m)            â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense1 ([38;5;33mDense[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m200[0m)            â”‚        [38;5;34m64,200[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ embedding ([38;5;33mDense[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)              â”‚           [38;5;34m402[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ output ([38;5;33mDense[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m10[0m)             â”‚            [38;5;34m30[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m90,952[0m (355.28 KB)

#   [1m Trainable params: [0m[38;5;34m90,952[0m (355.28 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)


learning_rate = 0.01
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)

model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

batch_size = 100
epochs = 20 # could increase if I want better accuracy
validation_split = 0.1

history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=validation_split,
                    verbose=1) # Set verbose=1 to see progress per epoch
# Output:
#   Epoch 1/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.2141 - loss: 2.0850 - val_accuracy: 0.4660 - val_loss: 1.3204

#   Epoch 2/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.5235 - loss: 1.2473 - val_accuracy: 0.6588 - val_loss: 0.8882

#   Epoch 3/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.6814 - loss: 0.8782 - val_accuracy: 0.8093 - val_loss: 0.6099

#   Epoch 4/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.8035 - loss: 0.6430 - val_accuracy: 0.8790 - val_loss: 0.4667

#   Epoch 5/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.8602 - loss: 0.5165 - val_accuracy: 0.8970 - val_loss: 0.3967

#   Epoch 6/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 13ms/step - accuracy: 0.8905 - loss: 0.4274 - val_accuracy: 0.9138 - val_loss: 0.3397

#   Epoch 7/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 13ms/step - accuracy: 0.9069 - loss: 0.3719 - val_accuracy: 0.9267 - val_loss: 0.3013

#   Epoch 8/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.9184 - loss: 0.3313 - val_accuracy: 0.9383 - val_loss: 0.2629

#   Epoch 9/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9274 - loss: 0.2930 - val_accuracy: 0.9480 - val_loss: 0.2288

#   Epoch 10/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.9356 - loss: 0.2652 - val_accuracy: 0.9443 - val_loss: 0.2396

#   Epoch 11/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9417 - loss: 0.2351 - val_accuracy: 0.9552 - val_loss: 0.2022

#   Epoch 12/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9465 - loss: 0.2138 - val_accuracy: 0.9570 - val_loss: 0.1884

#   Epoch 13/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9482 - loss: 0.2104 - val_accuracy: 0.9595 - val_loss: 0.1835

#   Epoch 14/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9543 - loss: 0.1922 - val_accuracy: 0.9585 - val_loss: 0.1874

#   Epoch 15/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.9554 - loss: 0.1782 - val_accuracy: 0.9603 - val_loss: 0.1740

#   Epoch 16/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9590 - loss: 0.1666 - val_accuracy: 0.9627 - val_loss: 0.1651

#   Epoch 17/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9630 - loss: 0.1501 - val_accuracy: 0.9653 - val_loss: 0.1611

#   Epoch 18/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9633 - loss: 0.1467 - val_accuracy: 0.9643 - val_loss: 0.1637

#   Epoch 19/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.9649 - loss: 0.1418 - val_accuracy: 0.9643 - val_loss: 0.1581

#   Epoch 20/20

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.9662 - loss: 0.1303 - val_accuracy: 0.9662 - val_loss: 0.1498


# plotting loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Curves')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# plotting Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Curves')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()
# Output:
#   <Figure size 1200x500 with 2 Axes>

# evaluate on training set
train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)
print(f"Training Accuracy: {train_acc*100:.2f}%")
print(f"Training Loss: {train_loss:.4f}")

# evaluate on test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {test_acc*100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")
# Output:
#   Training Accuracy: 96.71%

#   Training Loss: 0.1277

#   Test Accuracy: 96.08%

#   Test Loss: 0.1629


"""
Overall, there is not much overfitting, though we can see the training loss dip below the validation loss after around epoch 16, and the training and validation accuracies roughly converge around epoch 13. In terms of accuracy, it does quite well, with 96.71% training accuracy and 96.08% test accuracy. With both of those values being so close to one another we can be confident that the model is barely if at all overfit on the data while being quite accurate.
"""

embedding_model = Model(inputs=model.inputs, outputs=model.get_layer('embedding').output)

# subset
num_samples_to_plot = 5000
indices = np.random.choice(x_train.shape[0], num_samples_to_plot, replace=False)
x_train_subset = x_train[indices]
y_train_subset = y_train[indices]

# # full dataset
# x_train_subset = x_train[:]
# y_train_subset = y_train[:]

train_embeddings = embedding_model.predict(x_train_subset)

# Create scatter plot, colored by class label
plt.figure(figsize=(10, 8))
scatter = plt.scatter(train_embeddings[:, 0], train_embeddings[:, 1], c=y_train_subset, cmap='tab10', alpha=0.7, s=10)
plt.title('2D Embedding of Training Data')
plt.xlabel('Embedding Dimension 1')
plt.ylabel('Embedding Dimension 2')
unique_labels = np.unique(y_train_subset)
string_labels = [str(label) for label in unique_labels]
plt.legend(handles=scatter.legend_elements()[0], labels=string_labels)
plt.grid(True)
plt.show()
# Output:
#   [1m157/157[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 3ms/step

#   /Users/bshowell/Desktop/school/spring 24-25/587/.venv/lib/python3.12/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.

#   Expected: ['keras_tensor']

#   Received: inputs=Tensor(shape=(None, 28, 28))

#     warnings.warn(msg)

#   <Figure size 1000x800 with 1 Axes>

"""
- The shape observed in this 2D embedding of the training data reveals the kinds of transformations the model does to classify the data. Each number ends up in its own distinct clustering. It is likely that the few overlapping areas of the clusters in the middle of the graph are the less confident guesses and misclassifications of the model, since it can't be sure which category it definitely falls into.
- The embedding layer was purposefully chosen to be of size 2 to be able to be plottable like this. It has the linear activation via the `activation=None` coming from the dense 200 dimensional layer before, so the model learns to convert from 200d space to 2d in a way that the final output layer can classify.
```python
layers.Dense(200, activation='relu', name='dense1'),
layers.Dense(2, activation=None, name='embedding'),
layers.Dense(10, activation='softmax', name='output')
```
"""

# get index for one of each digit
digit_indices = [np.where(y_test == i)[0][0] for i in range(10)]
example_images = x_test[digit_indices]
example_labels = y_test[digit_indices]

rotation_angles = np.arange(0, 361, 10) # 0 to 360 in 10 degree increments

original_model_predictions = {}

print("Running rotation analysis on original model...")
for i in range(10):
    digit = example_labels[i]
    original_image = example_images[i] # Shape (28, 28, 1)
    original_model_predictions[digit] = {}
    print(f"  Processing Digit: {digit}")

    for angle in rotation_angles:
        rotated_img = rotate(original_image, angle, reshape=False, mode='constant', cval=-1.0, order=1)
        rotated_img = rotated_img.astype('float32')
        rotated_img = np.expand_dims(rotated_img, axis=0)

        probabilities = model.predict(rotated_img, verbose=0)[0]
        original_model_predictions[digit][angle] = probabilities

print("Rotation analysis complete.")
# Output:
#   Running rotation analysis on original model...

#     Processing Digit: 0

#     Processing Digit: 1

#     Processing Digit: 2

#     Processing Digit: 3

#     Processing Digit: 4

#     Processing Digit: 5

#     Processing Digit: 6

#     Processing Digit: 7

#     Processing Digit: 8

#     Processing Digit: 9

#   Rotation analysis complete.


# example plot confidence for 9
digit_to_plot = 9
angles = list(original_model_predictions[digit_to_plot].keys())
confidences = [np.max(original_model_predictions[digit_to_plot][angle]) for angle in angles]
predicted_classes = [np.argmax(original_model_predictions[digit_to_plot][angle]) for angle in angles]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(angles, confidences)
plt.title(f'Confidence vs. Rotation Angle (Digit {digit_to_plot}) - Original Model')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Max Probability (Confidence)')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(angles, predicted_classes, marker='o', linestyle='-')
plt.title(f'Predicted Class vs. Rotation Angle (Digit {digit_to_plot}) - Original Model')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Predicted Class')
plt.yticks(range(10))
plt.grid(True)

plt.tight_layout()
plt.show()

# Output:
#   <Figure size 1200x500 with 2 Axes>

# example plot confidence for 2
digit_to_plot = 2
angles = list(original_model_predictions[digit_to_plot].keys())
confidences = [np.max(original_model_predictions[digit_to_plot][angle]) for angle in angles]
predicted_classes = [np.argmax(original_model_predictions[digit_to_plot][angle]) for angle in angles]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(angles, confidences)
plt.title(f'Confidence vs. Rotation Angle (Digit {digit_to_plot}) - Original Model')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Max Probability (Confidence)')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(angles, predicted_classes, marker='o', linestyle='-')
plt.title(f'Predicted Class vs. Rotation Angle (Digit {digit_to_plot}) - Original Model')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Predicted Class')
plt.yticks(range(10))
plt.grid(True)

plt.tight_layout()
plt.show()

# Output:
#   <Figure size 1200x500 with 2 Axes>

"""
We would assume that an upside down 6 will be classified as a 9.  But what happens in between?  What does an upside down 2 get classified as?  Does the networkâ€™s confidence in its predictions change as you rotate the digits?
- I analyzed 9 being rotated instead of 6 to make the graph start at the top just to make it easier for myself, doesn't change the analysis.
- As it gets rotated, it does indeed get classified as a 6 near angles of 180 degrees.
- At intermediate angles, the model is much more unsure in its confidence, and guesses a bunch of different numbers.
- Upside down, 2 is still guessed as a 2, since generally (unless you draw it unusually) a two is vertically symmetric.
- The model is most confident at the edges of the graph (near zero and 360 degree rotations) as expected, and it also has a higher confidence around 180, since a lot of digits are vertically symmetric as mentioned
"""

rotation_factor = 0.2

augmented_model = models.Sequential([
    layers.Input(shape=(28, 28, 1)),
    layers.RandomRotation(factor=rotation_factor, fill_mode='constant', fill_value=-1.0),
    # rest is the same as original model
    layers.Conv2D(50, kernel_size=(5, 5), padding='valid', activation='relu', name='conv1'),
    layers.MaxPooling2D(pool_size=(2, 2), strides=2, name='pool1'),
    layers.Conv2D(20, kernel_size=(5, 5), padding='valid', activation='relu', name='conv2'),
    layers.MaxPooling2D(pool_size=(2, 2), strides=2, name='pool2'),
    layers.Flatten(name='flatten'),
    layers.Dense(200, activation='relu', name='dense1'),
    layers.Dense(2, activation=None, name='embedding'),
    layers.Dense(10, activation='softmax', name='output')
])

augmented_model.summary()
# Output:
#   [1mModel: "sequential_2"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ random_rotation_1               â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m, [38;5;34m1[0m)      â”‚             [38;5;34m0[0m â”‚

#   â”‚ ([38;5;33mRandomRotation[0m)                â”‚                        â”‚               â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv1 ([38;5;33mConv2D[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m24[0m, [38;5;34m24[0m, [38;5;34m50[0m)     â”‚         [38;5;34m1,300[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ pool1 ([38;5;33mMaxPooling2D[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m12[0m, [38;5;34m12[0m, [38;5;34m50[0m)     â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2 ([38;5;33mConv2D[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m8[0m, [38;5;34m8[0m, [38;5;34m20[0m)       â”‚        [38;5;34m25,020[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ pool2 ([38;5;33mMaxPooling2D[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m4[0m, [38;5;34m4[0m, [38;5;34m20[0m)       â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten ([38;5;33mFlatten[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m320[0m)            â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense1 ([38;5;33mDense[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m200[0m)            â”‚        [38;5;34m64,200[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ embedding ([38;5;33mDense[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)              â”‚           [38;5;34m402[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ output ([38;5;33mDense[0m)                  â”‚ ([38;5;45mNone[0m, [38;5;34m10[0m)             â”‚            [38;5;34m30[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m90,952[0m (355.28 KB)

#   [1m Trainable params: [0m[38;5;34m90,952[0m (355.28 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)


learning_rate = 0.01
optimizer_aug = tf.keras.optimizers.SGD(learning_rate=learning_rate)

augmented_model.compile(optimizer=optimizer_aug,
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy'])

batch_size = 100
epochs = 30 # added epochs for added complexity
validation_split = 0.1

print("Training augmented model...")
history_aug = augmented_model.fit(x_train, y_train, # use original training data
                                  batch_size=batch_size,
                                  epochs=epochs,
                                  validation_split=validation_split,
                                  verbose=1)
print("Augmented model training complete.")

# Output:
#   Training augmented model...

#   Epoch 1/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.1457 - loss: 2.2560 - val_accuracy: 0.3147 - val_loss: 1.9011

#   Epoch 2/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.3365 - loss: 1.8012 - val_accuracy: 0.4438 - val_loss: 1.3938

#   Epoch 3/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.4623 - loss: 1.4020 - val_accuracy: 0.5907 - val_loss: 1.1023

#   Epoch 4/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.5970 - loss: 1.1642 - val_accuracy: 0.6705 - val_loss: 0.9035

#   Epoch 5/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.6543 - loss: 0.9904 - val_accuracy: 0.7150 - val_loss: 0.8200

#   Epoch 6/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.7105 - loss: 0.8540 - val_accuracy: 0.7552 - val_loss: 0.7110

#   Epoch 7/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.7525 - loss: 0.7669 - val_accuracy: 0.8122 - val_loss: 0.5879

#   Epoch 8/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.7881 - loss: 0.6796 - val_accuracy: 0.8377 - val_loss: 0.5162

#   Epoch 9/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.8181 - loss: 0.6218 - val_accuracy: 0.8570 - val_loss: 0.4772

#   Epoch 10/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.8347 - loss: 0.5794 - val_accuracy: 0.8892 - val_loss: 0.4043

#   Epoch 11/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.8525 - loss: 0.5286 - val_accuracy: 0.8975 - val_loss: 0.3842

#   Epoch 12/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.8670 - loss: 0.4871 - val_accuracy: 0.9055 - val_loss: 0.3571

#   Epoch 13/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.8735 - loss: 0.4587 - val_accuracy: 0.9072 - val_loss: 0.3349

#   Epoch 14/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.8839 - loss: 0.4378 - val_accuracy: 0.9232 - val_loss: 0.3031

#   Epoch 15/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.8935 - loss: 0.3958 - val_accuracy: 0.9257 - val_loss: 0.2995

#   Epoch 16/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.8999 - loss: 0.3827 - val_accuracy: 0.9170 - val_loss: 0.3072

#   Epoch 17/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.9029 - loss: 0.3696 - val_accuracy: 0.9320 - val_loss: 0.2769

#   Epoch 18/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9059 - loss: 0.3629 - val_accuracy: 0.9353 - val_loss: 0.2792

#   Epoch 19/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9125 - loss: 0.3446 - val_accuracy: 0.9293 - val_loss: 0.2739

#   Epoch 20/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9164 - loss: 0.3290 - val_accuracy: 0.9412 - val_loss: 0.2455

#   Epoch 21/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9173 - loss: 0.3240 - val_accuracy: 0.9293 - val_loss: 0.2703

#   Epoch 22/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 14ms/step - accuracy: 0.9198 - loss: 0.3169 - val_accuracy: 0.9430 - val_loss: 0.2399

#   Epoch 23/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9256 - loss: 0.2905 - val_accuracy: 0.9367 - val_loss: 0.2569

#   Epoch 24/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9275 - loss: 0.2922 - val_accuracy: 0.9470 - val_loss: 0.2217

#   Epoch 25/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9298 - loss: 0.2807 - val_accuracy: 0.9478 - val_loss: 0.2176

#   Epoch 26/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9295 - loss: 0.2758 - val_accuracy: 0.9505 - val_loss: 0.2065

#   Epoch 27/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9333 - loss: 0.2655 - val_accuracy: 0.9518 - val_loss: 0.2003

#   Epoch 28/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9361 - loss: 0.2566 - val_accuracy: 0.9535 - val_loss: 0.1998

#   Epoch 29/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 14ms/step - accuracy: 0.9369 - loss: 0.2550 - val_accuracy: 0.9550 - val_loss: 0.1932

#   Epoch 30/30

#   [1m540/540[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m8s[0m 15ms/step - accuracy: 0.9372 - loss: 0.2540 - val_accuracy: 0.9570 - val_loss: 0.1845

#   Augmented model training complete.


# loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history_aug.history['loss'], label='Training Loss')
plt.plot(history_aug.history['val_loss'], label='Validation Loss')
plt.title('Augmented Model Loss Curves')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# accuracy
plt.subplot(1, 2, 2)
plt.plot(history_aug.history['accuracy'], label='Training Accuracy')
plt.plot(history_aug.history['val_accuracy'], label='Validation Accuracy')
plt.title('Augmented Model Accuracy Curves')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()
# Output:
#   <Figure size 1200x500 with 2 Axes>

test_loss_aug, test_acc_aug = augmented_model.evaluate(x_test, y_test, verbose=0)
print(f"Augmented Model Test Accuracy: {test_acc_aug*100:.2f}%")
print(f"Augmented Model Test Loss: {test_loss_aug:.4f}")
# Output:
#   Augmented Model Test Accuracy: 95.38%

#   Augmented Model Test Loss: 0.1967


augmented_model_predictions = {}

print("Running rotation analysis on augmented model...")
for i in range(10):
    digit = example_labels[i]
    original_image = example_images[i]
    augmented_model_predictions[digit] = {}
    print(f"  Processing Digit: {digit}")

    for angle in rotation_angles:
        rotated_img = rotate(original_image, angle, reshape=False, mode='constant', cval=-1.0, order=1)
        rotated_img = rotated_img.astype('float32')
        rotated_img = np.expand_dims(rotated_img, axis=0)

        probabilities = augmented_model.predict(rotated_img, verbose=0)[0]
        augmented_model_predictions[digit][angle] = probabilities

print("Rotation analysis complete for augmented model.")
# Output:
#   Running rotation analysis on augmented model...

#     Processing Digit: 0

#     Processing Digit: 1

#     Processing Digit: 2

#     Processing Digit: 3

#     Processing Digit: 4

#     Processing Digit: 5

#     Processing Digit: 6

#     Processing Digit: 7

#     Processing Digit: 8

#     Processing Digit: 9

#   Rotation analysis complete for augmented model.


# plot confidence for 9 for both models
digit_to_plot = 9
angles = list(original_model_predictions[digit_to_plot].keys())

confidences_orig = [np.max(original_model_predictions[digit_to_plot][angle]) for angle in angles]
predicted_classes_orig = [np.argmax(original_model_predictions[digit_to_plot][angle]) for angle in angles]

confidences_aug = [np.max(augmented_model_predictions[digit_to_plot][angle]) for angle in angles]
predicted_classes_aug = [np.argmax(augmented_model_predictions[digit_to_plot][angle]) for angle in angles]


plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
plt.plot(angles, confidences_orig, label='Original')
plt.plot(angles, confidences_aug, label='Augmented', linestyle='--')
plt.title(f'Confidence vs. Rotation Angle (Digit {digit_to_plot})')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Max Probability (Confidence)')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(angles, predicted_classes_orig, marker='o', linestyle='-', label='Original')
plt.plot(angles, predicted_classes_aug, marker='x', linestyle='--', label='Augmented')
plt.title(f'Predicted Class vs. Rotation Angle (Digit {digit_to_plot})')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Predicted Class')
plt.yticks(range(10))
plt.legend()
plt.grid(True)

# plot probability of the correct class
correct_class = digit_to_plot
prob_correct_orig = [original_model_predictions[digit_to_plot][angle][correct_class] for angle in angles]
prob_correct_aug = [augmented_model_predictions[digit_to_plot][angle][correct_class] for angle in angles]

plt.subplot(2, 2, 3)
plt.plot(angles, prob_correct_orig, label='Original')
plt.plot(angles, prob_correct_aug, label='Augmented', linestyle='--')
plt.title(f'Prob. of Correct Class ({correct_class}) vs. Rotation')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel(f'Probability of Class {correct_class}')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
# Output:
#   <Figure size 1200x1000 with 3 Axes>

# plot confidence for 2 for both models
digit_to_plot = 2
angles = list(original_model_predictions[digit_to_plot].keys())

confidences_orig = [np.max(original_model_predictions[digit_to_plot][angle]) for angle in angles]
predicted_classes_orig = [np.argmax(original_model_predictions[digit_to_plot][angle]) for angle in angles]

confidences_aug = [np.max(augmented_model_predictions[digit_to_plot][angle]) for angle in angles]
predicted_classes_aug = [np.argmax(augmented_model_predictions[digit_to_plot][angle]) for angle in angles]


plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
plt.plot(angles, confidences_orig, label='Original')
plt.plot(angles, confidences_aug, label='Augmented', linestyle='--')
plt.title(f'Confidence vs. Rotation Angle (Digit {digit_to_plot})')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Max Probability (Confidence)')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(angles, predicted_classes_orig, marker='o', linestyle='-', label='Original')
plt.plot(angles, predicted_classes_aug, marker='x', linestyle='--', label='Augmented')
plt.title(f'Predicted Class vs. Rotation Angle (Digit {digit_to_plot})')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel('Predicted Class')
plt.yticks(range(10))
plt.legend()
plt.grid(True)

# plot probability of the correct class
correct_class = digit_to_plot
prob_correct_orig = [original_model_predictions[digit_to_plot][angle][correct_class] for angle in angles]
prob_correct_aug = [augmented_model_predictions[digit_to_plot][angle][correct_class] for angle in angles]

plt.subplot(2, 2, 3)
plt.plot(angles, prob_correct_orig, label='Original')
plt.plot(angles, prob_correct_aug, label='Augmented', linestyle='--')
plt.title(f'Prob. of Correct Class ({correct_class}) vs. Rotation')
plt.xlabel('Rotation Angle (degrees)')
plt.ylabel(f'Probability of Class {correct_class}')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
# Output:
#   <Figure size 1200x1000 with 3 Axes>

"""
- The model is significantly better at classifying digits after undergoing this rotation training
- With 2, for example, the model can accurately predict the 2 at every angle of rotation, whereas before, it would guess each other digit at least once throughout the rotation process. 
- For the 9, it goes between a 6 and a 9 being guessed, with between 70 and 90 degrees it being guessed as a 5, which makes sense if you draw a 5 and rotate your paper 90 degrees, it is roughly 5 shaped.
- The confidence is also affected, with it staying above 80% confident except for an interesting and unexpected dip around 270 degrees for various digits. But overall the confidence remains high whereas it could drop to around 40% before.
- Test accuracy did drop a bit, but that makes sense, since the test images are not rotated, so the more specialized model can perform better with less variation.
"""



================================================
File: hw2/instructions.md
================================================
# **Homework 2: Unsupervised Representation Learning**

100 points
Due date: Sunday, May 4, 11:59pm

In this homework you will explore how autoencoders can generate useful representations of data in an unsupervised manner.  You will train autoencoders on the MNIST and Frey datasets, visualize what filters the network learns, and test the quality of the learned representation for data compression, generation, and classification.

Your implementation should use the Keras module in Tensorflow 2 (import tensorflow.keras).

# **Part 1: Frey face compression and generation**

## **1.1 Autoencoder setup and training**

##

Load the Frey dataset and convert the images to have `float32` type and a range of `[-1 1]`. Display some of the images.

Set up an autoencoder, following the example in the linear autoencoder notebook.  Unlike the linear autoencoder, your autoencoder should use a multi-layer perceptron for the encoder and decoder and have a two-dimensional latent space.  The hidden layers in your encoder and decoder should have some non-linear activation such as ReLU or Leaky ReLU, except the embedding or bottleneck layer (last layer of encoder) which should have a tanh activation.  The last layer of the decoder should have size `28*20` and should have linear (None) activation.

The exact design of the encoder and decoder is up to you.  For reference, I used a two-layer MLP for the encoder and decoder, and the number of channels was `64-32-2-32-64`.

## **1.2 Analysis and Visualization**

Test the ability of the autoencoder to compress and decompress the images.  Compare some input images to their reconstructions after running the autoencoder.  What effect does the autoencoder have on the images?  How does it compare to the linear autoencoder?

Visualize the output of the encoder (run on the training data) as a scatter plot.  Give some observations about the output.  Does it seem to be using all of the possible output space?

Generate new faces by decoding a set of embedding points on `[-1 1] x [-1 1]` grid (see `np.meshgrid`).  Give some observations about the resulting images.

Test interpolation between two images (see the example from the linear autoencoder notebook).  How do the interpolations compare to the linear autoencoder?

# **Part II: MNIST digit classification with unsupervised pre-training**

Set up and train a similar MLP autoencoder on the MNIST dataset.  Use only digits 0 and 1 to make the task a little easier.  Use mean squared error loss for this dataset--I found it to work more reliably than mean absolute error.

After training the autoencoder, obtain the embedding vectors of all training and testing images.

Then, create and train another network that will classify the embedding vectors produced by your encoder.  Train the network on the training data and test it (model.evaluate()) on the testing data.  What test accuracy are you able to achieve?  Discuss the effectiveness of unsupervised pre-training in this experiment.



================================================
File: hw2/linear_autoencoder.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Linear autoencoders

In this notebook we explore the use of linear autoencoders to compress data and synthesize new data.

A autoencoder, in its simplest form, is a neural network that is trained to match its output $z$ to its input $x$.  A linear autoencoder does not use any non-linear activation functions, so that the entire learned function is linear:

$$\mathbf{z} = \mathbf{W}_2(\mathbf{W}_1\mathbf{x}+\mathbf{b}_1)+\mathbf{b}_2$$

The autoencoder is composed of two parts: an encoder and a decoder.  We will denote the encoded version of the input as $\mathbf{h}$:

$$\mathbf{h} = \mathbf{W}_1\mathbf{x}+\mathbf{b}_1$$
$$\mathbf{z}= \mathbf{W}_2\mathbf{h}+\mathbf{b}_2$$

In the typical undercomplete autoencoder, $\mathbf{h}$ is smaller than $\mathbf{x}$ so we can think of the neural network as learning to compress and decompress the data.

Once we have trained the neural network, we can synthesize new data by randomly sampling $\mathbf{h}$ and decoding it.

In this notebook we train a linear autoencoder on the Frey dataset which contains about 2,000 faces of a single person's face with different poses and expressions.  Then we explore how well the autoencoder can compress and decompress data, synthesize new data, and interpolate between faces.
"""

import numpy as np
import matplotlib as mpl
mpl.rc('image', cmap='gray')
from matplotlib import pyplot as plt

"""
## Data loading and pre-processing

Here we download and unpack the Frey dataset.  The dataset consists of grayscale images, 28 pixels high and 20 pixels wide.
"""

from tensorflow.keras.utils import get_file
from scipy.io import loadmat

path = get_file('frey_rawface.mat','https://www.dropbox.com/scl/fi/m70sh4ef39pvy01czc63r/frey_rawface.mat?rlkey=5v6meiap55z68ada2roxwxuql&dl=1')
data = np.transpose(loadmat(path)['ff'])
images = np.reshape(data,(-1,28,20))

np.random.seed(1234)
np.random.shuffle(images)

print(images.shape)
for i in range(5):
  plt.subplot(1,5,i+1)
  plt.imshow(images[i])
  plt.axis('off')
plt.show()
# Output:
#   (1965, 28, 20)

#   <Figure size 640x480 with 5 Axes>

"""
We split the data into training and testing splits (the data was shuffled above) and then convert to floating point on [-1 1] range.
"""

x_train = images[0:1800]
x_test = images[1800:]
x_train = (x_train.astype('float32')/255.)*2-1
x_test = (x_test.astype('float32')/255.)*2-1

"""
## Building the autoencoder model in Keras

To train the autoencoder, we will build a simple two-layer model in Keras: one layer for the encoder and one layer for the decoder.  At training time, we run data through the autoencoder and apply a reconstruction loss term such as mean squared error or mean absolute error.

However, at test time, we would like to be able to run the encoder or decoder separately.  For this reason, we make separate encoder and decoder models and then connect them in an autoencoder model.

"""

from tensorflow.keras.layers import Input, Flatten, Dense, Reshape
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD, Adam

def build_encoder(latent_dim):
  inputs = Input((28,20),name='encoder_input')
  flat = Flatten()(inputs) # flatten to a vector
  embedding = Dense(latent_dim,activation=None,name='encoder')(flat)
  return Model(inputs=inputs,outputs=embedding)

def build_decoder(latent_dim):
  inputs = Input((latent_dim,),name='decoder_input')
  reconstruction = Dense(28*20,activation=None,name='decoder')(inputs)
  reshaped = Reshape((28,20))(reconstruction) # reshape back to an image
  return Model(inputs=inputs,outputs=reshaped)

def build_autoencoder(encoder,decoder):
  inputs = Input((28,20),name='autoencoder_input')
  embedding = encoder(inputs)
  reconstruction = decoder(embedding)
  return Model(inputs=inputs,outputs=reconstruction)

latent_dim = 32
encoder = build_encoder(latent_dim)
decoder = build_decoder(latent_dim)
autoencoder = build_autoencoder(encoder,decoder)

opt = Adam(3e-4)
autoencoder.compile(opt,loss='mean_squared_error')

history = autoencoder.fit(x_train,x_train,batch_size=32,epochs=300,verbose=False,validation_split=0.1)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
# Output:
#   [<matplotlib.lines.Line2D at 0x143a33350>]
#   <Figure size 640x480 with 1 Axes>

"""
## Testing image reconstruction with the autoencoder

First we test the ability of the autoencoder to compress and then decompress data.  Note that the autoencoder compresses the input of size 560 down to a 32-dimensional vector and then decompresses it.  That is a 17.5x compression rate.

It can be seen that the reconstructed images look quite similar to the input, although some high-resolution detail is lost.

"""

result = autoencoder.predict(x_train)
for i in range(5):
  plt.subplot(2,5,i+1)
  plt.imshow(x_train[i],cmap='gray')
  plt.axis('off')
  plt.subplot(2,5,5+i+1)
  plt.imshow(result[i],cmap='gray')
  plt.axis('off')
plt.show()
# Output:
#   [1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 694us/step

#   <Figure size 640x480 with 10 Axes>

"""
## Visualizing the embedding

By running the encoder without the decoder, we can visualize the embedding of the input images into 32-dimensional space.  Here we only visualize the first two dimensions of the embedding.
"""

encoder_result = encoder.predict(x_train)
plt.scatter(encoder_result[:,0],encoder_result[:,1])
plt.show()
# Output:
#   [1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 573us/step

#   <Figure size 640x480 with 1 Axes>

"""
## Randomly synthesizing new data

To randomly synthesize new data, we can attempt to select random embedding vectors which are similar to the embedding vectors of the training data.

To this, I chose to model the embedding with a multivariate Gaussian -- from the plot above, it is clear that this is only going to be an approximation.
"""

mu = np.mean(encoder_result,axis=0)
sigma = np.cov(encoder_result.T)

random_embeddings = np.random.multivariate_normal(mu,sigma,size=(32))

"""
By decoding the random embeddings, we can randomly generate new faces in the "face space" learned by the autoencoder.

Some images look plausible but many have artifacts.  Some faces look like a blend of many different faces which don't really cohere together.
"""

plt.figure(figsize=(20,10))
decoder_result = decoder.predict(random_embeddings)
n = 0
for i in range(4):
  for j in range(8):
    plt.subplot(4,8,n+1)
    plt.imshow(decoder_result[n],cmap='gray')
    plt.axis('off')
    n = n + 1
plt.show()
# Output:
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step

#   <Figure size 2000x1000 with 32 Axes>

"""
## Inspecting the decoder weights

The decoder applies a linear transformation to the embedding to produce the output image:

$$\mathbf{z} = \mathbf{W}\mathbf{h}+\mathbf{b}$$

Let $h_1,\ldots,h_n$ be the coefficients of $\mathbf{h}$ and $\mathbf{w}_1,\ldots,\mathbf{w}_n$ be the columns of $\mathbf{W}$.

It can be easily seen that $\mathbf{z}$ is a linear combination of the columns of $\mathbf{W}$, where the coefficients in the linear combination come from $\mathbf{h}$:

$$\mathbf{z} = h_1\mathbf{w}_1 + \cdots + h_n\mathbf{w}_n + \mathbf{b}$$

By inspecting the weights in the decoder, we can see that each column vector of $\mathbf{W}$ looks like a different face; the decoder is blending these different faces together to produce the output.
"""

plt.figure(figsize=(20,10))
weights = decoder.get_weights()[0]
n = 0
for i in range(4):
  for j in range(8):
    plt.subplot(4,8,n+1)
    plt.imshow(np.reshape(weights[n],(28,20)),cmap='gray')
    plt.axis('off')
    n = n + 1
plt.show()

# Output:
#   <Figure size 2000x1000 with 32 Axes>

bias = decoder.get_weights()[1]
bias = np.reshape(bias,(28,20))
plt.imshow(bias)
plt.show()
# Output:
#   <Figure size 640x480 with 1 Axes>

"""
## Interpolating between faces

Now we see if we can interpolate between faces by stepping between their embedding vectors.

For two images $\mathbf{x}_1$ and $\mathbf{x}_2$, we first compute their encodings $\mathbf{h}_1$ and $\mathbf{h}_2$.  Then we synthesize images in-between by computing encodings between $\mathbf{h}_1$ and $\mathbf{h}_2$ and decoding them:

$$\mathbf{h} = (1-\alpha)\mathbf{h}_1 + \alpha\mathbf{h}_2$$


"""

def interpolate_between(imgA_index,imgB_index):
  plt.figure(figsize=(20,10))
  imgA = x_test[imgA_index]
  imgB = x_test[imgB_index]

  imgA_encoded = encoder.predict(x_test[imgA_index:(imgA_index+1)])
  imgB_encoded = encoder.predict(x_test[imgB_index:(imgB_index+1)])

  direction = imgB_encoded - imgA_encoded
  print(direction)

  alphas = np.linspace(0,1,num=10)
  for n,alpha in enumerate(alphas):
    alpha = alphas[n]
    img_encoded = (1-alpha)*imgA_encoded+alpha*imgB_encoded
    img = decoder.predict(img_encoded)
    plt.subplot(1,10,n+1)
    plt.imshow(np.squeeze(img))
    plt.axis('off')
  plt.show()

interpolate_between(0,2)
# Output:
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 18ms/step

#   [[ 0.98034555 -0.62201715  0.00564986 -0.5538416   1.0653322  -1.2977206

#     -0.41937852 -0.751601    0.46354967 -0.1530832  -0.5221958  -0.07718146

#      0.563241    0.6046201   1.3386538   1.2363371  -0.3715783   0.48640758

#      0.3271409   0.6809768   0.9906703   0.34347957  0.9261408   0.90509164

#     -0.00418222 -0.06946945 -0.32979962 -0.60743773 -0.0460625  -0.36114103

#      0.6174585   0.73963106]]

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 28ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 18ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 18ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 19ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 18ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 18ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 19ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 19ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step

#   <Figure size 2000x1000 with 10 Axes>

"""
The linear autoencoder in this case actually does a good job of interpolating between the two images!
"""



================================================
File: hw2/main.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Unsupervised Representation Learning

You will explore how autoencoders can generate useful representations of data in an unsupervised manner. You will train autoencoders on the MNIST and Frey datasets, visualize what filters the network learns, and test the quality of the learned representation for data compression, generation, and classification.

Your implementation should use the Keras module in Tensorflow 2 (import tensorflow.keras).
"""

"""
# **Part 1: Frey face compression and generation**
"""

"""
## **1.1 Autoencoder setup and training**
"""

"""
Load the Frey dataset and convert the images to have `float32` type and a range of `[-1 1]`. Display some of the images.

Set up an autoencoder, following the example in the linear autoencoder notebook. Unlike the linear autoencoder, your autoencoder should use a multi-layer perceptron for the encoder and decoder and have a two-dimensional latent space. The hidden layers in your encoder and decoder should have some non-linear activation such as ReLU or Leaky ReLU, except the embedding or bottleneck layer (last layer of encoder) which should have a tanh activation. The last layer of the decoder should have size `28*20` and should have linear (None) activation.

The exact design of the encoder and decoder is up to you. For reference, I used a two-layer MLP for the encoder and decoder, and the number of channels was `64-32-2-32-64`.
"""

import numpy as np
import matplotlib as mpl
mpl.rc('image', cmap='gray')
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input, Flatten, Dense, Reshape, LeakyReLU, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import get_file
from scipy.io import loadmat

"""
### Data Loading and Pre-processing
"""

path = get_file('frey_rawface.mat','https://www.dropbox.com/scl/fi/m70sh4ef39pvy01czc63r/frey_rawface.mat?rlkey=5v6meiap55z68ada2roxwxuql&dl=1')
data = np.transpose(loadmat(path)['ff'])
# reshape to (num_images, height, width)
images = np.reshape(data,(-1,28,20))

np.random.seed(1234)
np.random.shuffle(images)

print(f"Dataset shape: {images.shape}")
print(f"Pixel value range before scaling: [{np.min(images)}, {np.max(images)}]")
# Output:
#   Dataset shape: (1965, 28, 20)

#   Pixel value range before scaling: [8, 238]


# display some images
print("Sample images from the dataset:")
plt.figure(figsize=(10, 4))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(images[i])
    plt.title(f"Image {i}")
    plt.axis('off')
plt.show()
# Output:
#   Sample images from the dataset:

#   <Figure size 1000x400 with 5 Axes>

# split data and preprocess
# using the same split ratio as the reference notebook
split_idx = 1800
x_train_orig = images[0:split_idx]
x_test_orig = images[split_idx:]

# convert to float32 and scale to [-1, 1]
x_train = (x_train_orig.astype('float32') / 255.0) * 2.0 - 1.0
x_test = (x_test_orig.astype('float32') / 255.0) * 2.0 - 1.0

img_height, img_width = x_train.shape[1], x_train.shape[2]
img_shape = (img_height, img_width)
flat_img_size = img_height * img_width

print(f"\nTraining data shape: {x_train.shape}")
print(f"Testing data shape: {x_test.shape}")
print(f"Pixel value range after scaling: [{np.min(x_train)}, {np.max(x_train)}]")
print(f"Image shape: {img_shape}")
print(f"Flattened image size: {flat_img_size}")
# Output:
#   

#   Training data shape: (1800, 28, 20)

#   Testing data shape: (165, 28, 20)

#   Pixel value range after scaling: [-0.9372549057006836, 0.8666666746139526]

#   Image shape: (28, 20)

#   Flattened image size: 560


"""
### Building the MLP Autoencoder
"""

# define latent dimension
latent_dim = 2

# define activation for hidden layers
# activation_fn = 'relu'
# using LeakyReLU as suggested alternative
activation_fn = LeakyReLU(alpha=0.2)

# using the suggested architecture 64-32-2-32-64
def build_mlp_encoder(latent_dim, img_shape):
    inputs = Input(shape=img_shape, name='encoder_input')
    flat = Flatten()(inputs)
    x = Dense(64)(flat)
    x = activation_fn(x)
    x = Dense(32)(x)
    x = activation_fn(x)
    # bottleneck layer with tanh activation
    embedding = Dense(latent_dim, activation='tanh', name='encoder_embedding')(x)
    return Model(inputs=inputs, outputs=embedding, name='encoder')

def build_mlp_decoder(latent_dim, flat_img_size, img_shape):
    inputs = Input(shape=(latent_dim,), name='decoder_input')
    x = Dense(32)(inputs)
    x = activation_fn(x)
    x = Dense(64)(x)
    x = activation_fn(x)
    # output layer with linear activation
    reconstruction_flat = Dense(flat_img_size, activation=None, name='decoder_output_flat')(x)
    # reshape back to image dimensions
    reshaped = Reshape(img_shape, name='decoder_output_reshaped')(reconstruction_flat)
    return Model(inputs=inputs, outputs=reshaped, name='decoder')

def build_mlp_autoencoder(encoder, decoder, img_shape):
    inputs = Input(shape=img_shape, name='autoencoder_input')
    embedding = encoder(inputs)
    reconstruction = decoder(embedding)
    return Model(inputs=inputs, outputs=reconstruction, name='autoencoder')

encoder_mlp = build_mlp_encoder(latent_dim, img_shape)
decoder_mlp = build_mlp_decoder(latent_dim, flat_img_size, img_shape)
autoencoder_mlp = build_mlp_autoencoder(encoder_mlp, decoder_mlp, img_shape)

print("Encoder Summary:")
encoder_mlp.summary()
print("\nDecoder Summary:")
decoder_mlp.summary()
print("\nAutoencoder Summary:")
autoencoder_mlp.summary()
# Output:
#   Encoder Summary:

#   /Users/bshowell/Desktop/school/spring 24-25/587/.venv/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.

#     warnings.warn(

#   [1mModel: "encoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)       [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape     [0m[1m [0mâ”ƒ[1m [0m[1m   Param #[0m[1m [0mâ”ƒ[1m [0m[1mConnected to     [0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ encoder_input       â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m)    â”‚          [38;5;34m0[0m â”‚ -                 â”‚

#   â”‚ ([38;5;33mInputLayer[0m)        â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten ([38;5;33mFlatten[0m)   â”‚ ([38;5;45mNone[0m, [38;5;34m560[0m)       â”‚          [38;5;34m0[0m â”‚ encoder_input[[38;5;34m0[0m]â€¦ â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense ([38;5;33mDense[0m)       â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)        â”‚     [38;5;34m35,904[0m â”‚ flatten[[38;5;34m0[0m][[38;5;34m0[0m]     â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ leaky_re_lu         â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)        â”‚          [38;5;34m0[0m â”‚ dense[[38;5;34m0[0m][[38;5;34m0[0m],      â”‚

#   â”‚ ([38;5;33mLeakyReLU[0m)         â”‚                   â”‚            â”‚ dense_1[[38;5;34m0[0m][[38;5;34m0[0m]     â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_1 ([38;5;33mDense[0m)     â”‚ ([38;5;45mNone[0m, [38;5;34m32[0m)        â”‚      [38;5;34m2,080[0m â”‚ leaky_re_lu[[38;5;34m0[0m][[38;5;34m0[0m] â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ encoder_embedding   â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)         â”‚         [38;5;34m66[0m â”‚ leaky_re_lu[[38;5;34m1[0m][[38;5;34m0[0m] â”‚

#   â”‚ ([38;5;33mDense[0m)             â”‚                   â”‚            â”‚                   â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m38,050[0m (148.63 KB)

#   [1m Trainable params: [0m[38;5;34m38,050[0m (148.63 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   Decoder Summary:

#   [1mModel: "decoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)       [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape     [0m[1m [0mâ”ƒ[1m [0m[1m   Param #[0m[1m [0mâ”ƒ[1m [0m[1mConnected to     [0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ decoder_input       â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)         â”‚          [38;5;34m0[0m â”‚ -                 â”‚

#   â”‚ ([38;5;33mInputLayer[0m)        â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_2 ([38;5;33mDense[0m)     â”‚ ([38;5;45mNone[0m, [38;5;34m32[0m)        â”‚         [38;5;34m96[0m â”‚ decoder_input[[38;5;34m0[0m]â€¦ â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ leaky_re_lu         â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)        â”‚          [38;5;34m0[0m â”‚ dense_2[[38;5;34m0[0m][[38;5;34m0[0m],    â”‚

#   â”‚ ([38;5;33mLeakyReLU[0m)         â”‚                   â”‚            â”‚ dense_3[[38;5;34m0[0m][[38;5;34m0[0m]     â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_3 ([38;5;33mDense[0m)     â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)        â”‚      [38;5;34m2,112[0m â”‚ leaky_re_lu[[38;5;34m2[0m][[38;5;34m0[0m] â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ decoder_output_flat â”‚ ([38;5;45mNone[0m, [38;5;34m560[0m)       â”‚     [38;5;34m36,400[0m â”‚ leaky_re_lu[[38;5;34m3[0m][[38;5;34m0[0m] â”‚

#   â”‚ ([38;5;33mDense[0m)             â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ decoder_output_resâ€¦ â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m)    â”‚          [38;5;34m0[0m â”‚ decoder_output_fâ€¦ â”‚

#   â”‚ ([38;5;33mReshape[0m)           â”‚                   â”‚            â”‚                   â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m38,608[0m (150.81 KB)

#   [1m Trainable params: [0m[38;5;34m38,608[0m (150.81 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   Autoencoder Summary:

#   [1mModel: "autoencoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ autoencoder_input ([38;5;33mInputLayer[0m)  â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m)         â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ encoder ([38;5;33mFunctional[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)              â”‚        [38;5;34m38,050[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ decoder ([38;5;33mFunctional[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m)         â”‚        [38;5;34m38,608[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m76,658[0m (299.45 KB)

#   [1m Trainable params: [0m[38;5;34m76,658[0m (299.45 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)


"""
### Compiling and Training the Autoencoder
"""

opt = Adam(learning_rate=3e-4)
autoencoder_mlp.compile(optimizer=opt, loss='mean_squared_error')

# train the autoencoder
batch_size = 64
epochs = 300

print("\nTraining MLP Autoencoder...")
history_mlp = autoencoder_mlp.fit(x_train, x_train,
                                  batch_size=batch_size,
                                  epochs=epochs,
                                  verbose=0,
                                  validation_split=0.1)
print("Training complete.")
# Output:
#   

#   Training MLP Autoencoder...

#   Training complete.


plt.figure(figsize=(10, 5))
plt.plot(history_mlp.history['loss'], label='Training Loss')
plt.plot(history_mlp.history['val_loss'], label='Validation Loss')
plt.title('MLP Autoencoder Model loss')
plt.ylabel('Loss (MSE)')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()
# Output:
#   <Figure size 1000x500 with 1 Axes>

"""
## **1.2 Analysis and Visualization**
"""

"""
### Image Reconstruction
"""

"""
Test the ability of the autoencoder to compress and decompress the images. Compare some input images to their reconstructions after running the autoencoder.
"""

# get reconstructions for a few training images
n_display = 10
reconstructions = autoencoder_mlp.predict(x_train[:n_display])

plt.figure(figsize=(12, 4))
for i in range(n_display):
    # display original images (scaled back to [0, 1] for imshow)
    ax = plt.subplot(2, n_display, i + 1)
    plt.imshow((x_train[i] + 1.0) / 2.0)
    plt.title("Original")
    plt.axis('off')

    ax = plt.subplot(2, n_display, i + 1 + n_display)
    plt.imshow((reconstructions[i] + 1.0) / 2.0)
    plt.title("Reconstructed")
    plt.axis('off')
plt.tight_layout()
plt.show()
# Output:
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step

#   <Figure size 1200x400 with 20 Axes>

"""
The reconstructions capture the general pose and expression quite well, but they appear a lot blurrier than the originals, losing fine details. But this is expected because of the extreme compression into a 2D latent space.
Compared to a linear autoencoder with a small latent space (like 2D that we've been doing), the non-linear version might produce slightly more plausible but still blurry faces, which is able to handle variations in expression or pose in a more structured way comparatively. The linear version, however, produced overall sharper results than this non-linear version.
"""

"""
### Latent Space Visualization
"""

"""
Visualize the output of the encoder (run on the training data) as a scatter plot.
"""

# encode the training data to get latent vectors
latent_vectors_train = encoder_mlp.predict(x_train)

print(f"Shape of latent vectors: {latent_vectors_train.shape}")

plt.figure(figsize=(8, 8))
plt.scatter(latent_vectors_train[:, 0], latent_vectors_train[:, 1], s=5, alpha=0.6)
plt.title('Frey Dataset Latent Space (2D)')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')
plt.xlim([-1.1, 1.1])
plt.ylim([-1.1, 1.1])
plt.grid(True)
plt.gca().set_aspect('equal', adjustable='box')
plt.show()
# Output:
#   [1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 763us/step

#   Shape of latent vectors: (1800, 2)

#   <Figure size 800x800 with 1 Axes>

"""
**Observations:**
The points seem to form a specific, double curvedish shape cluster within the 2D space, showing how the autoencoder learned to map the high-dimensional face variations into a lower-dimensional space. There are denser areas, which probably correspond to common poses/expressions.
The points are all confined within the space but the points are not evenly spread. There is a large gap in the upper left quadrant that is empty of points, suggesting that that area did not get used in the mapping.
"""

"""
### Generating New Faces from Latent Space Grid
"""

"""
Generate new faces by decoding a set of embedding points on `[-1 1] x [-1 1]` grid.
"""

n_points = 10 # create 10x10 grid
grid_x = np.linspace(-1.0, 1.0, n_points)
grid_y = np.linspace(1.0, -1.0, n_points)

plt.figure(figsize=(10, 10))
for i, yi in enumerate(grid_y):
    for j, xi in enumerate(grid_x):
        # create latent vector for this grid point
        latent_vector = np.array([[xi, yi]])
        # decode
        generated_image = decoder_mlp.predict(latent_vector, verbose=0)
        # display the generated image
        ax = plt.subplot(n_points, n_points, i * n_points + j + 1)
        plt.imshow((generated_image[0] + 1.0) / 2.0)
        plt.axis('off')

plt.suptitle("Faces Generated from Latent Space Grid", y=0.92)
plt.subplots_adjust(wspace=0.05, hspace=0.05)
plt.show()
# Output:
#   <Figure size 1000x1000 with 100 Axes>

"""
The generated images show gradual transitions across the grid, changing pose (left-right looking) primarily along the x axis and expression moreso along the y axis, although this isn't exact at all. For example, The smiling expression seems to generally live in the middle-left area in the 2d space.
Faces generated from the areas outside the main data distribution (seen in the scatter plot) look very distorted. For example, the entire upper left quadrant, which as seen from the scatterplot above, has no points that map there, are all distorted and distinctly un-face-like.
"""

"""
### Interpolation Between Faces
"""

"""
Test interpolation between two images.
"""

def interpolate_between_mlp(imgA_index, imgB_index, num_steps=10):
    plt.figure(figsize=(12, 3))

    imgA = x_test[imgA_index]
    imgB = x_test[imgB_index]

    imgA_encoded = encoder_mlp.predict(x_test[imgA_index:(imgA_index+1)])
    imgB_encoded = encoder_mlp.predict(x_test[imgB_index:(imgB_index+1)])

    print(f"Image A ({imgA_index}) encoding: {imgA_encoded}")
    print(f"Image B ({imgB_index}) encoding: {imgB_encoded}")

    alphas = np.linspace(0, 1, num=num_steps)
    for n, alpha in enumerate(alphas):
        # interpolate in latent space
        img_encoded = (1 - alpha) * imgA_encoded + alpha * imgB_encoded
        # decode the interpolated latent vector
        img_decoded = decoder_mlp.predict(img_encoded, verbose=0)

        # display the interpolated image (scaled back to [0, 1])
        ax = plt.subplot(1, num_steps, n + 1)
        plt.imshow((np.squeeze(img_decoded) + 1.0) / 2.0)
        plt.title(f"$\\alpha={alpha:.2f}$")
        plt.axis('off')
    plt.suptitle(f"Interpolation between Test Image {imgA_index} and {imgB_index}", y=1.02)
    plt.show()

interpolate_between_mlp(0, 1)
interpolate_between_mlp(2, 3)
interpolate_between_mlp(0, 4)
interpolate_between_mlp(-2, 4)
# Output:
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 18ms/step

#   Image A (0) encoding: [[-0.17391585 -0.7568977 ]]

#   Image B (1) encoding: [[ 0.03400124 -0.8826001 ]]

#   <Figure size 1200x300 with 10 Axes>
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step

#   Image A (2) encoding: [[-0.35129598 -0.23541437]]

#   Image B (3) encoding: [[-0.04651837 -0.04395724]]

#   <Figure size 1200x300 with 10 Axes>
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 19ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step

#   Image A (0) encoding: [[-0.17391585 -0.7568977 ]]

#   Image B (4) encoding: [[-0.05698666 -0.9192494 ]]

#   <Figure size 1200x300 with 10 Axes>
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step

#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step

#   Image A (-2) encoding: [[-0.32184973 -0.18038742]]

#   Image B (4) encoding: [[-0.05698666 -0.9192494 ]]

#   <Figure size 1200x300 with 10 Axes>

"""
Overall, it produces smooth transitions between the start and end faces. The intermediate images look like very plausible faces, capturing changes in pose and expression more naturally than other techniques you could try like pixel averaging. However, probably due to the 2D bottleneck, the images remain blurry. Compared to the 32D linear autoencoder interpolation (which was also quite good), this 2D non-linear interpolation might show more significant changes in features across the steps, as it compresses information more drastically, but the overall image fidelity seems to be lower.
"""

"""
# **Part II: MNIST digit classification with unsupervised pre-training**

Set up and train a similar MLP autoencoder on the MNIST dataset. Use only digits 0 and 1 to make the task a little easier. Use mean squared error loss for this dataset--I found it to work more reliably than mean absolute error.

After training the autoencoder, obtain the embedding vectors of all training and testing images.

Then, create and train another network that will classify the embedding vectors produced by your encoder. Train the network on the training data and test it (model.evaluate()) on the testing data. What test accuracy are you able to achieve? Discuss the effectiveness of unsupervised pre-training in this experiment.
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Input, Flatten, Dense, Reshape, LeakyReLU, Activation, Dropout
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam

"""
### MNIST Data Loading and Preprocessing (Digits 0 and 1 only)
"""

(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()

# filter for digits 0 and 1
train_mask = np.isin(y_train_mnist, [0, 1])
test_mask = np.isin(y_test_mnist, [0, 1])

x_train_01 = x_train_mnist[train_mask]
y_train_01 = y_train_mnist[train_mask]
x_test_01 = x_test_mnist[test_mask]
y_test_01 = y_test_mnist[test_mask]

print(f"Original MNIST train shape: {x_train_mnist.shape}")
print(f"Filtered MNIST train shape (0s and 1s): {x_train_01.shape}")
print(f"Original MNIST test shape: {x_test_mnist.shape}")
print(f"Filtered MNIST test shape (0s and 1s): {x_test_01.shape}")
# Output:
#   Original MNIST train shape: (60000, 28, 28)

#   Filtered MNIST train shape (0s and 1s): (12665, 28, 28)

#   Original MNIST test shape: (10000, 28, 28)

#   Filtered MNIST test shape (0s and 1s): (2115, 28, 28)


# display some filtered images
print("Sample MNIST images (Digits 0 and 1):")
plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(x_train_01[i], cmap='gray')
    plt.title(f"Label: {y_train_01[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()
# Output:
#   Sample MNIST images (Digits 0 and 1):

#   <Figure size 1000x400 with 10 Axes>

# preprocess the data
x_train_01 = (x_train_01.astype('float32') / 255.0) * 2.0 - 1.0
x_test_01 = (x_test_01.astype('float32') / 255.0) * 2.0 - 1.0

mnist_img_height, mnist_img_width = x_train_01.shape[1], x_train_01.shape[2]
mnist_img_shape = (mnist_img_height, mnist_img_width)
mnist_flat_img_size = mnist_img_height * mnist_img_width

print(f"\nMNIST Image shape: {mnist_img_shape}")
print(f"Flattened MNIST image size: {mnist_flat_img_size}")
print(f"Pixel value range after scaling: [{np.min(x_train_01)}, {np.max(x_train_01)}]")
# Output:
#   

#   MNIST Image shape: (28, 28)

#   Flattened MNIST image size: 784

#   Pixel value range after scaling: [-1.0, 1.0]


"""
### Building and Training the MNIST Autoencoder
"""

mnist_latent_dim = 16
mnist_activation_fn = LeakyReLU(alpha=0.2)

def build_mnist_encoder(latent_dim, img_shape):
  inputs = Input(shape=img_shape, name='encoder_input')
  flat = Flatten()(inputs)
  x = Dense(128)(flat)
  x = mnist_activation_fn(x)
  x = Dense(64)(x)
  x = mnist_activation_fn(x)
  # bottleneck layer with tanh activation
  embedding = Dense(latent_dim, activation='tanh', name='encoder_embedding')(x)
  return Model(inputs=inputs, outputs=embedding, name='mnist_encoder')

def build_mnist_decoder(latent_dim, flat_img_size, img_shape):
  inputs = Input(shape=(latent_dim,), name='decoder_input')
  x = Dense(64)(inputs)
  x = mnist_activation_fn(x)
  x = Dense(128)(x)
  x = mnist_activation_fn(x)
  # output layer with linear activation
  reconstruction_flat = Dense(flat_img_size, activation=None, name='decoder_output_flat')(x)
  # reshape back to image dimensions
  reshaped = Reshape(img_shape, name='decoder_output_reshaped')(reconstruction_flat)
  return Model(inputs=inputs, outputs=reshaped, name='mnist_decoder')

def build_mnist_autoencoder(encoder, decoder, img_shape):
  inputs = Input(shape=img_shape, name='autoencoder_input')
  embedding = encoder(inputs)
  reconstruction = decoder(embedding)
  return Model(inputs=inputs, outputs=reconstruction, name='mnist_autoencoder')

mnist_encoder = build_mnist_encoder(mnist_latent_dim, mnist_img_shape)
mnist_decoder = build_mnist_decoder(mnist_latent_dim, mnist_flat_img_size, mnist_img_shape)
mnist_autoencoder = build_mnist_autoencoder(mnist_encoder, mnist_decoder, mnist_img_shape)

print("MNIST Encoder Summary:")
mnist_encoder.summary()
print("\nMNIST Decoder Summary:")
mnist_decoder.summary()
print("\nMNIST Autoencoder Summary:")
mnist_autoencoder.summary()
# Output:
#   MNIST Encoder Summary:

#   [1mModel: "mnist_encoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)       [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape     [0m[1m [0mâ”ƒ[1m [0m[1m   Param #[0m[1m [0mâ”ƒ[1m [0m[1mConnected to     [0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ encoder_input       â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m)    â”‚          [38;5;34m0[0m â”‚ -                 â”‚

#   â”‚ ([38;5;33mInputLayer[0m)        â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten_1 ([38;5;33mFlatten[0m) â”‚ ([38;5;45mNone[0m, [38;5;34m784[0m)       â”‚          [38;5;34m0[0m â”‚ encoder_input[[38;5;34m0[0m]â€¦ â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_4 ([38;5;33mDense[0m)     â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚    [38;5;34m100,480[0m â”‚ flatten_1[[38;5;34m0[0m][[38;5;34m0[0m]   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ leaky_re_lu_1       â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚          [38;5;34m0[0m â”‚ dense_4[[38;5;34m0[0m][[38;5;34m0[0m],    â”‚

#   â”‚ ([38;5;33mLeakyReLU[0m)         â”‚                   â”‚            â”‚ dense_5[[38;5;34m0[0m][[38;5;34m0[0m]     â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_5 ([38;5;33mDense[0m)     â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)        â”‚      [38;5;34m8,256[0m â”‚ leaky_re_lu_1[[38;5;34m0[0m]â€¦ â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ encoder_embedding   â”‚ ([38;5;45mNone[0m, [38;5;34m16[0m)        â”‚      [38;5;34m1,040[0m â”‚ leaky_re_lu_1[[38;5;34m1[0m]â€¦ â”‚

#   â”‚ ([38;5;33mDense[0m)             â”‚                   â”‚            â”‚                   â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m109,776[0m (428.81 KB)

#   [1m Trainable params: [0m[38;5;34m109,776[0m (428.81 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   MNIST Decoder Summary:

#   [1mModel: "mnist_decoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)       [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape     [0m[1m [0mâ”ƒ[1m [0m[1m   Param #[0m[1m [0mâ”ƒ[1m [0m[1mConnected to     [0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ decoder_input       â”‚ ([38;5;45mNone[0m, [38;5;34m16[0m)        â”‚          [38;5;34m0[0m â”‚ -                 â”‚

#   â”‚ ([38;5;33mInputLayer[0m)        â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_6 ([38;5;33mDense[0m)     â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)        â”‚      [38;5;34m1,088[0m â”‚ decoder_input[[38;5;34m0[0m]â€¦ â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ leaky_re_lu_1       â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚          [38;5;34m0[0m â”‚ dense_6[[38;5;34m0[0m][[38;5;34m0[0m],    â”‚

#   â”‚ ([38;5;33mLeakyReLU[0m)         â”‚                   â”‚            â”‚ dense_7[[38;5;34m0[0m][[38;5;34m0[0m]     â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_7 ([38;5;33mDense[0m)     â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚      [38;5;34m8,320[0m â”‚ leaky_re_lu_1[[38;5;34m2[0m]â€¦ â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ decoder_output_flat â”‚ ([38;5;45mNone[0m, [38;5;34m784[0m)       â”‚    [38;5;34m101,136[0m â”‚ leaky_re_lu_1[[38;5;34m3[0m]â€¦ â”‚

#   â”‚ ([38;5;33mDense[0m)             â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ decoder_output_resâ€¦ â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m)    â”‚          [38;5;34m0[0m â”‚ decoder_output_fâ€¦ â”‚

#   â”‚ ([38;5;33mReshape[0m)           â”‚                   â”‚            â”‚                   â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m110,544[0m (431.81 KB)

#   [1m Trainable params: [0m[38;5;34m110,544[0m (431.81 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   MNIST Autoencoder Summary:

#   [1mModel: "mnist_autoencoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ autoencoder_input ([38;5;33mInputLayer[0m)  â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m)         â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ mnist_encoder ([38;5;33mFunctional[0m)      â”‚ ([38;5;45mNone[0m, [38;5;34m16[0m)             â”‚       [38;5;34m109,776[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ mnist_decoder ([38;5;33mFunctional[0m)      â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m)         â”‚       [38;5;34m110,544[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m220,320[0m (860.62 KB)

#   [1m Trainable params: [0m[38;5;34m220,320[0m (860.62 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)


mnist_opt = Adam(learning_rate=3e-4)
mnist_autoencoder.compile(optimizer=mnist_opt, loss='mean_squared_error')

# Train the MNIST autoencoder
mnist_batch_size = 128
mnist_epochs = 50

print("\nTraining MNIST Autoencoder (Digits 0 & 1)...")
history_mnist_ae = mnist_autoencoder.fit(x_train_01, x_train_01,
                                         batch_size=mnist_batch_size,
                                         epochs=mnist_epochs,
                                         verbose=0,
                                         validation_data=(x_test_01, x_test_01))
print("Training complete.")
# Output:
#   

#   Training MNIST Autoencoder (Digits 0 & 1)...

#   Training complete.


# plot training & validation loss values for MNIST AE
plt.figure(figsize=(10, 5))
plt.plot(history_mnist_ae.history['loss'], label='Training Loss')
plt.plot(history_mnist_ae.history['val_loss'], label='Validation Loss')
plt.title('MNIST Autoencoder Model loss (Digits 0 & 1)')
plt.ylabel('Loss (MSE)')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()
# Output:
#   <Figure size 1000x500 with 1 Axes>

# visualize some reconstructions for MNIST 0s and 1s
n_display_mnist = 10
mnist_reconstructions = mnist_autoencoder.predict(x_test_01[:n_display_mnist])

plt.figure(figsize=(12, 4))
for i in range(n_display_mnist):
    ax = plt.subplot(2, n_display_mnist, i + 1)
    plt.imshow((x_test_01[i] + 1.0) / 2.0, cmap='gray')
    plt.title("Original")
    plt.axis('off')

    ax = plt.subplot(2, n_display_mnist, i + 1 + n_display_mnist)
    plt.imshow((mnist_reconstructions[i] + 1.0) / 2.0, cmap='gray')
    plt.title("Reconstructed")
    plt.axis('off')
plt.tight_layout()
plt.show()
# Output:
#   [1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step

#   <Figure size 1200x400 with 20 Axes>

"""
### Generating Embeddings
"""

print("Generating embeddings for training and testing data...")
train_embeddings = mnist_encoder.predict(x_train_01)
test_embeddings = mnist_encoder.predict(x_test_01)

print(f"Shape of training embeddings: {train_embeddings.shape}")
print(f"Shape of testing embeddings: {test_embeddings.shape}")
# Output:
#   Generating embeddings for training and testing data...

#   [1m396/396[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 384us/step

#   [1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 470us/step

#   Shape of training embeddings: (12665, 16)

#   Shape of testing embeddings: (2115, 16)


"""
### Building and Training the Classifier on Embeddings
"""

classifier = Sequential([
    Input(shape=(mnist_latent_dim,), name='classifier_input'),
    Dense(32, activation='relu'), # small hidden layer
    Dropout(0.5), # dropout for regularization
    Dense(1, activation='sigmoid', name='classifier_output') # binary classification
], name='embedding_classifier')

print("\nClassifier Summary:")
classifier.summary()
# Output:
#   

#   Classifier Summary:

#   [1mModel: "embedding_classifier"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ dense_8 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m32[0m)             â”‚           [38;5;34m544[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dropout ([38;5;33mDropout[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m32[0m)             â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ classifier_output ([38;5;33mDense[0m)       â”‚ ([38;5;45mNone[0m, [38;5;34m1[0m)              â”‚            [38;5;34m33[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m577[0m (2.25 KB)

#   [1m Trainable params: [0m[38;5;34m577[0m (2.25 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)


classifier.compile(optimizer=Adam(learning_rate=1e-3),
                   loss='binary_crossentropy',
                   metrics=['accuracy'])

classifier_batch_size = 64
classifier_epochs = 50

print("\nTraining Classifier on Embeddings...")
history_classifier = classifier.fit(train_embeddings, y_train_01,
                                    batch_size=classifier_batch_size,
                                    epochs=classifier_epochs,
                                    verbose=0,
                                    validation_data=(test_embeddings, y_test_01))
print("Classifier training complete.")
# Output:
#   

#   Training Classifier on Embeddings...

#   Classifier training complete.


# plot training & validation accuracy and loss for the classifier
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history_classifier.history['accuracy'], label='Training Accuracy')
plt.plot(history_classifier.history['val_accuracy'], label='Validation Accuracy')
plt.title('Classifier Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history_classifier.history['loss'], label='Training Loss')
plt.plot(history_classifier.history['val_loss'], label='Validation Loss')
plt.title('Classifier Loss')
plt.ylabel('Loss (Binary Crossentropy)')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)

plt.tight_layout()
plt.show()
# Output:
#   <Figure size 1200x500 with 2 Axes>

"""
### Evaluating the Classifier
"""

loss, accuracy = classifier.evaluate(test_embeddings, y_test_01, verbose=0)
print(f"\nClassifier Test Loss: {loss:.4f}")
print(f"Classifier Test Accuracy: {accuracy:.4f}")
# Output:
#   

#   Classifier Test Loss: 0.0050

#   Classifier Test Accuracy: 0.9991


"""
The classifier achieved a test accuracy of 99.91%
The high test accuracy achieved demonstrates that the unsupervised autoencoder learned a compressed representation (latent space) that very well was able to capture the distinguishing features between digits 0 and 1, even though it wasn't explicitly trained for classification. This shows the power of unsupervised pre-training for dimensionality reduction and feature extraction. Training a classifier on these low-dimensional embeddings is much more computationally efficient and can yield really good results, suggesting the learned features are very relevant. It's possible a classifier trained directly on pixels might achieve slightly higher accuracy, but it would operate on much higher dimensional data so with this high of an accuracy, that would be pointless to do.
"""



================================================
File: hw2/temp.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# **Part II: MNIST digit classification with unsupervised pre-training**

Set up and train a similar MLP autoencoder on the MNIST dataset. Use only digits 0 and 1 to make the task a little easier. Use mean squared error loss for this dataset--I found it to work more reliably than mean absolute error.

After training the autoencoder, obtain the embedding vectors of all training and testing images.

Then, create and train another network that will classify the embedding vectors produced by your encoder. Train the network on the training data and test it (model.evaluate()) on the testing data. What test accuracy are you able to achieve? Discuss the effectiveness of unsupervised pre-training in this experiment.
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Input, Flatten, Dense, Reshape, LeakyReLU, Activation, Dropout
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam

"""
### MNIST Data Loading and Preprocessing (Digits 0 and 1 only)
"""

# Load MNIST dataset
(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()

# Filter for digits 0 and 1
train_mask = np.isin(y_train_mnist, [0, 1])
test_mask = np.isin(y_test_mnist, [0, 1])

x_train_01 = x_train_mnist[train_mask]
y_train_01 = y_train_mnist[train_mask]
x_test_01 = x_test_mnist[test_mask]
y_test_01 = y_test_mnist[test_mask]

print(f"Original MNIST train shape: {x_train_mnist.shape}")
print(f"Filtered MNIST train shape (0s and 1s): {x_train_01.shape}")
print(f"Original MNIST test shape: {x_test_mnist.shape}")
print(f"Filtered MNIST test shape (0s and 1s): {x_test_01.shape}")

# Display some filtered images
print("Sample MNIST images (Digits 0 and 1):")
plt.figure(figsize=(10, 4))
for i in range(10):
  plt.subplot(2, 5, i+1)
  plt.imshow(x_train_01[i], cmap='gray')
  plt.title(f"Label: {y_train_01[i]}")
  plt.axis('off')
plt.tight_layout()
plt.show()

# Preprocess the data
# Convert to float32 and scale to [-1, 1] (consistent with Part 1)
x_train_01 = (x_train_01.astype('float32') / 255.0) * 2.0 - 1.0
x_test_01 = (x_test_01.astype('float32') / 255.0) * 2.0 - 1.0

# Keep track of original image shape and flattened size
mnist_img_height, mnist_img_width = x_train_01.shape[1], x_train_01.shape[2]
mnist_img_shape = (mnist_img_height, mnist_img_width)
mnist_flat_img_size = mnist_img_height * mnist_img_width

print(f"\nMNIST Image shape: {mnist_img_shape}")
print(f"Flattened MNIST image size: {mnist_flat_img_size}")
print(f"Pixel value range after scaling: [{np.min(x_train_01)}, {np.max(x_train_01)}]")

"""
### Building and Training the MNIST Autoencoder
"""

# Define latent dimension and architecture parameters
# Let's use a slightly larger latent dim than for Frey, e.g., 16
mnist_latent_dim = 16
# Define activation for hidden layers
mnist_activation_fn = LeakyReLU(alpha=0.2) # Or 'relu'

# Using a similar structure: e.g., 128-64-latent_dim-64-128
def build_mnist_encoder(latent_dim, img_shape):
  inputs = Input(shape=img_shape, name='encoder_input')
  flat = Flatten()(inputs)
  x = Dense(128)(flat)
  x = mnist_activation_fn(x)
  x = Dense(64)(x)
  x = mnist_activation_fn(x)
  # Bottleneck layer with tanh activation
  embedding = Dense(latent_dim, activation='tanh', name='encoder_embedding')(x)
  return Model(inputs=inputs, outputs=embedding, name='mnist_encoder')

def build_mnist_decoder(latent_dim, flat_img_size, img_shape):
  inputs = Input(shape=(latent_dim,), name='decoder_input')
  x = Dense(64)(inputs)
  x = mnist_activation_fn(x)
  x = Dense(128)(x)
  x = mnist_activation_fn(x)
  # Output layer with linear activation
  reconstruction_flat = Dense(flat_img_size, activation=None, name='decoder_output_flat')(x)
  # Reshape back to image dimensions
  reshaped = Reshape(img_shape, name='decoder_output_reshaped')(reconstruction_flat)
  return Model(inputs=inputs, outputs=reshaped, name='mnist_decoder')

def build_mnist_autoencoder(encoder, decoder, img_shape):
  inputs = Input(shape=img_shape, name='autoencoder_input')
  embedding = encoder(inputs)
  reconstruction = decoder(embedding)
  return Model(inputs=inputs, outputs=reconstruction, name='mnist_autoencoder')

# Instantiate the models
mnist_encoder = build_mnist_encoder(mnist_latent_dim, mnist_img_shape)
mnist_decoder = build_mnist_decoder(mnist_latent_dim, mnist_flat_img_size, mnist_img_shape)
mnist_autoencoder = build_mnist_autoencoder(mnist_encoder, mnist_decoder, mnist_img_shape)

# Print model summaries
print("MNIST Encoder Summary:")
mnist_encoder.summary()
print("\nMNIST Decoder Summary:")
mnist_decoder.summary()
print("\nMNIST Autoencoder Summary:")
mnist_autoencoder.summary()

# Compile the MNIST autoencoder
# Use Adam optimizer and Mean Squared Error (MSE) loss as requested
mnist_opt = Adam(learning_rate=3e-4)
mnist_autoencoder.compile(optimizer=mnist_opt, loss='mean_squared_error')

# Train the MNIST autoencoder
mnist_batch_size = 128
mnist_epochs = 50 # MNIST might train faster, adjust as needed

print("\nTraining MNIST Autoencoder (Digits 0 & 1)...")
history_mnist_ae = mnist_autoencoder.fit(x_train_01, x_train_01,
                                         batch_size=mnist_batch_size,
                                         epochs=mnist_epochs,
                                         verbose=1,
                                         validation_data=(x_test_01, x_test_01)) # Evaluate AE on test set
print("Training complete.")

# Plot training & validation loss values for MNIST AE
plt.figure(figsize=(10, 5))
plt.plot(history_mnist_ae.history['loss'], label='Training Loss')
plt.plot(history_mnist_ae.history['val_loss'], label='Validation Loss')
plt.title('MNIST Autoencoder Model loss (Digits 0 & 1)')
plt.ylabel('Loss (MSE)')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

# Visualize some reconstructions for MNIST 0s and 1s
n_display_mnist = 10
mnist_reconstructions = mnist_autoencoder.predict(x_test_01[:n_display_mnist])

plt.figure(figsize=(12, 4))
for i in range(n_display_mnist):
    # Display original images
    ax = plt.subplot(2, n_display_mnist, i + 1)
    plt.imshow((x_test_01[i] + 1.0) / 2.0, cmap='gray') # Scale back
    plt.title("Original")
    plt.axis('off')

    # Display reconstructed images
    ax = plt.subplot(2, n_display_mnist, i + 1 + n_display_mnist)
    plt.imshow((mnist_reconstructions[i] + 1.0) / 2.0, cmap='gray') # Scale back
    plt.title("Reconstructed")
    plt.axis('off')
plt.tight_layout()
plt.show()

"""
### Generating Embeddings
"""

# Use the trained encoder to generate latent vectors (embeddings)
print("Generating embeddings for training and testing data...")
train_embeddings = mnist_encoder.predict(x_train_01)
test_embeddings = mnist_encoder.predict(x_test_01)

print(f"Shape of training embeddings: {train_embeddings.shape}")
print(f"Shape of testing embeddings: {test_embeddings.shape}")

"""
### Building and Training the Classifier on Embeddings
"""

# Build a simple classifier model
# Input dimension is the latent dimension of the autoencoder
classifier = Sequential([
    Input(shape=(mnist_latent_dim,), name='classifier_input'),
    Dense(32, activation='relu'), # A small hidden layer
    Dropout(0.5), # Add dropout for regularization
    Dense(1, activation='sigmoid', name='classifier_output') # Binary classification (0 or 1)
], name='embedding_classifier')

print("\nClassifier Summary:")
classifier.summary()

# Compile the classifier
# Use binary crossentropy for 0/1 classification
classifier.compile(optimizer=Adam(learning_rate=1e-3),
                   loss='binary_crossentropy',
                   metrics=['accuracy'])

# Train the classifier using the embeddings and original labels
classifier_batch_size = 64
classifier_epochs = 50 # Adjust as needed

print("\nTraining Classifier on Embeddings...")
history_classifier = classifier.fit(train_embeddings, y_train_01,
                                    batch_size=classifier_batch_size,
                                    epochs=classifier_epochs,
                                    verbose=1,
                                    validation_data=(test_embeddings, y_test_01))
print("Classifier training complete.")

# Plot training & validation accuracy and loss for the classifier
plt.figure(figsize=(12, 5))

# Plot Accuracy
plt.subplot(1, 2, 1)
plt.plot(history_classifier.history['accuracy'], label='Training Accuracy')
plt.plot(history_classifier.history['val_accuracy'], label='Validation Accuracy')
plt.title('Classifier Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.grid(True)

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history_classifier.history['loss'], label='Training Loss')
plt.plot(history_classifier.history['val_loss'], label='Validation Loss')
plt.title('Classifier Loss')
plt.ylabel('Loss (Binary Crossentropy)')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)

plt.tight_layout()
plt.show()

"""
### Evaluating the Classifier
"""

# Evaluate the classifier on the test embeddings
loss, accuracy = classifier.evaluate(test_embeddings, y_test_01, verbose=0)
print(f"\nClassifier Test Loss: {loss:.4f}")
print(f"Classifier Test Accuracy: {accuracy:.4f}")

"""
**Discussion:**

*   *What test accuracy are you able to achieve?*
    *   *(Space for observation: State the final test accuracy reported by `model.evaluate()` above.)*
    *   *Example observation: The classifier achieved a test accuracy of X.XXXX.*
*   *Discuss the effectiveness of unsupervised pre-training in this experiment.*
    *   *(Space for observation: Consider the achieved accuracy. Is it high? Given that the autoencoder was trained *without* labels, how well did it capture features useful for discriminating between 0s and 1s? How does the dimensionality reduction (784 -> latent_dim) affect this? Could we expect similar/better/worse results training a similar classifier directly on the 784 pixels?)*
    *   *Example observation: The high test accuracy achieved (likely > 0.99 if training went well, as 0 vs 1 is an easy MNIST task) demonstrates that the unsupervised autoencoder learned a compressed representation (latent space) that effectively captured the distinguishing features between digits 0 and 1, even though it wasn't explicitly trained for classification. This shows the power of unsupervised pre-training for dimensionality reduction and feature extraction. Training a classifier on these low-dimensional embeddings is computationally efficient and can yield excellent results, suggesting the learned features are highly relevant. It's possible a classifier trained directly on pixels might achieve slightly higher accuracy, but it would operate on much higher dimensional data.*
"""

"""
End of Part II. This section covered training an autoencoder on MNIST digits 0/1, generating embeddings, and training/evaluating a classifier on those embeddings.
"""



================================================
File: hw2/temp.py
================================================
# %% [markdown]
# # **Part II: MNIST digit classification with unsupervised pre-training**
#
# Set up and train a similar MLP autoencoder on the MNIST dataset. Use only digits 0 and 1 to make the task a little easier. Use mean squared error loss for this dataset--I found it to work more reliably than mean absolute error.
#
# After training the autoencoder, obtain the embedding vectors of all training and testing images.
#
# Then, create and train another network that will classify the embedding vectors produced by your encoder. Train the network on the training data and test it (model.evaluate()) on the testing data. What test accuracy are you able to achieve? Discuss the effectiveness of unsupervised pre-training in this experiment.

# %%
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Input, Flatten, Dense, Reshape, LeakyReLU, Activation, Dropout
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam

# %% [markdown]
# ### MNIST Data Loading and Preprocessing (Digits 0 and 1 only)

# %%
# Load MNIST dataset
(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()

# Filter for digits 0 and 1
train_mask = np.isin(y_train_mnist, [0, 1])
test_mask = np.isin(y_test_mnist, [0, 1])

x_train_01 = x_train_mnist[train_mask]
y_train_01 = y_train_mnist[train_mask]
x_test_01 = x_test_mnist[test_mask]
y_test_01 = y_test_mnist[test_mask]

print(f"Original MNIST train shape: {x_train_mnist.shape}")
print(f"Filtered MNIST train shape (0s and 1s): {x_train_01.shape}")
print(f"Original MNIST test shape: {x_test_mnist.shape}")
print(f"Filtered MNIST test shape (0s and 1s): {x_test_01.shape}")

# %%
# Display some filtered images
print("Sample MNIST images (Digits 0 and 1):")
plt.figure(figsize=(10, 4))
for i in range(10):
  plt.subplot(2, 5, i+1)
  plt.imshow(x_train_01[i], cmap='gray')
  plt.title(f"Label: {y_train_01[i]}")
  plt.axis('off')
plt.tight_layout()
plt.show()

# %%
# Preprocess the data
# Convert to float32 and scale to [-1, 1] (consistent with Part 1)
x_train_01 = (x_train_01.astype('float32') / 255.0) * 2.0 - 1.0
x_test_01 = (x_test_01.astype('float32') / 255.0) * 2.0 - 1.0

# Keep track of original image shape and flattened size
mnist_img_height, mnist_img_width = x_train_01.shape[1], x_train_01.shape[2]
mnist_img_shape = (mnist_img_height, mnist_img_width)
mnist_flat_img_size = mnist_img_height * mnist_img_width

print(f"\nMNIST Image shape: {mnist_img_shape}")
print(f"Flattened MNIST image size: {mnist_flat_img_size}")
print(f"Pixel value range after scaling: [{np.min(x_train_01)}, {np.max(x_train_01)}]")

# %% [markdown]
# ### Building and Training the MNIST Autoencoder

# %%
# Define latent dimension and architecture parameters
# Let's use a slightly larger latent dim than for Frey, e.g., 16
mnist_latent_dim = 16
# Define activation for hidden layers
mnist_activation_fn = LeakyReLU(alpha=0.2) # Or 'relu'

# Using a similar structure: e.g., 128-64-latent_dim-64-128
def build_mnist_encoder(latent_dim, img_shape):
  inputs = Input(shape=img_shape, name='encoder_input')
  flat = Flatten()(inputs)
  x = Dense(128)(flat)
  x = mnist_activation_fn(x)
  x = Dense(64)(x)
  x = mnist_activation_fn(x)
  # Bottleneck layer with tanh activation
  embedding = Dense(latent_dim, activation='tanh', name='encoder_embedding')(x)
  return Model(inputs=inputs, outputs=embedding, name='mnist_encoder')

def build_mnist_decoder(latent_dim, flat_img_size, img_shape):
  inputs = Input(shape=(latent_dim,), name='decoder_input')
  x = Dense(64)(inputs)
  x = mnist_activation_fn(x)
  x = Dense(128)(x)
  x = mnist_activation_fn(x)
  # Output layer with linear activation
  reconstruction_flat = Dense(flat_img_size, activation=None, name='decoder_output_flat')(x)
  # Reshape back to image dimensions
  reshaped = Reshape(img_shape, name='decoder_output_reshaped')(reconstruction_flat)
  return Model(inputs=inputs, outputs=reshaped, name='mnist_decoder')

def build_mnist_autoencoder(encoder, decoder, img_shape):
  inputs = Input(shape=img_shape, name='autoencoder_input')
  embedding = encoder(inputs)
  reconstruction = decoder(embedding)
  return Model(inputs=inputs, outputs=reconstruction, name='mnist_autoencoder')

# Instantiate the models
mnist_encoder = build_mnist_encoder(mnist_latent_dim, mnist_img_shape)
mnist_decoder = build_mnist_decoder(mnist_latent_dim, mnist_flat_img_size, mnist_img_shape)
mnist_autoencoder = build_mnist_autoencoder(mnist_encoder, mnist_decoder, mnist_img_shape)

# Print model summaries
print("MNIST Encoder Summary:")
mnist_encoder.summary()
print("\nMNIST Decoder Summary:")
mnist_decoder.summary()
print("\nMNIST Autoencoder Summary:")
mnist_autoencoder.summary()

# %%
# Compile the MNIST autoencoder
# Use Adam optimizer and Mean Squared Error (MSE) loss as requested
mnist_opt = Adam(learning_rate=3e-4)
mnist_autoencoder.compile(optimizer=mnist_opt, loss='mean_squared_error')

# %%
# Train the MNIST autoencoder
mnist_batch_size = 128
mnist_epochs = 50 # MNIST might train faster, adjust as needed

print("\nTraining MNIST Autoencoder (Digits 0 & 1)...")
history_mnist_ae = mnist_autoencoder.fit(x_train_01, x_train_01,
                                         batch_size=mnist_batch_size,
                                         epochs=mnist_epochs,
                                         verbose=1,
                                         validation_data=(x_test_01, x_test_01)) # Evaluate AE on test set
print("Training complete.")

# %%
# Plot training & validation loss values for MNIST AE
plt.figure(figsize=(10, 5))
plt.plot(history_mnist_ae.history['loss'], label='Training Loss')
plt.plot(history_mnist_ae.history['val_loss'], label='Validation Loss')
plt.title('MNIST Autoencoder Model loss (Digits 0 & 1)')
plt.ylabel('Loss (MSE)')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

# %%
# Visualize some reconstructions for MNIST 0s and 1s
n_display_mnist = 10
mnist_reconstructions = mnist_autoencoder.predict(x_test_01[:n_display_mnist])

plt.figure(figsize=(12, 4))
for i in range(n_display_mnist):
    # Display original images
    ax = plt.subplot(2, n_display_mnist, i + 1)
    plt.imshow((x_test_01[i] + 1.0) / 2.0, cmap='gray') # Scale back
    plt.title("Original")
    plt.axis('off')

    # Display reconstructed images
    ax = plt.subplot(2, n_display_mnist, i + 1 + n_display_mnist)
    plt.imshow((mnist_reconstructions[i] + 1.0) / 2.0, cmap='gray') # Scale back
    plt.title("Reconstructed")
    plt.axis('off')
plt.tight_layout()
plt.show()


# %% [markdown]
# ### Generating Embeddings

# %%
# Use the trained encoder to generate latent vectors (embeddings)
print("Generating embeddings for training and testing data...")
train_embeddings = mnist_encoder.predict(x_train_01)
test_embeddings = mnist_encoder.predict(x_test_01)

print(f"Shape of training embeddings: {train_embeddings.shape}")
print(f"Shape of testing embeddings: {test_embeddings.shape}")

# %% [markdown]
# ### Building and Training the Classifier on Embeddings

# %%
# Build a simple classifier model
# Input dimension is the latent dimension of the autoencoder
classifier = Sequential([
    Input(shape=(mnist_latent_dim,), name='classifier_input'),
    Dense(32, activation='relu'), # A small hidden layer
    Dropout(0.5), # Add dropout for regularization
    Dense(1, activation='sigmoid', name='classifier_output') # Binary classification (0 or 1)
], name='embedding_classifier')

print("\nClassifier Summary:")
classifier.summary()

# %%
# Compile the classifier
# Use binary crossentropy for 0/1 classification
classifier.compile(optimizer=Adam(learning_rate=1e-3),
                   loss='binary_crossentropy',
                   metrics=['accuracy'])

# %%
# Train the classifier using the embeddings and original labels
classifier_batch_size = 64
classifier_epochs = 50 # Adjust as needed

print("\nTraining Classifier on Embeddings...")
history_classifier = classifier.fit(train_embeddings, y_train_01,
                                    batch_size=classifier_batch_size,
                                    epochs=classifier_epochs,
                                    verbose=1,
                                    validation_data=(test_embeddings, y_test_01))
print("Classifier training complete.")

# %%
# Plot training & validation accuracy and loss for the classifier
plt.figure(figsize=(12, 5))

# Plot Accuracy
plt.subplot(1, 2, 1)
plt.plot(history_classifier.history['accuracy'], label='Training Accuracy')
plt.plot(history_classifier.history['val_accuracy'], label='Validation Accuracy')
plt.title('Classifier Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.grid(True)

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history_classifier.history['loss'], label='Training Loss')
plt.plot(history_classifier.history['val_loss'], label='Validation Loss')
plt.title('Classifier Loss')
plt.ylabel('Loss (Binary Crossentropy)')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)

plt.tight_layout()
plt.show()

# %% [markdown]
# ### Evaluating the Classifier

# %%
# Evaluate the classifier on the test embeddings
loss, accuracy = classifier.evaluate(test_embeddings, y_test_01, verbose=0)
print(f"\nClassifier Test Loss: {loss:.4f}")
print(f"Classifier Test Accuracy: {accuracy:.4f}")

# %% [markdown]
# **Discussion:**
#
# *   *What test accuracy are you able to achieve?*
#     *   *(Space for observation: State the final test accuracy reported by `model.evaluate()` above.)*
#     *   *Example observation: The classifier achieved a test accuracy of X.XXXX.*
# *   *Discuss the effectiveness of unsupervised pre-training in this experiment.*
#     *   *(Space for observation: Consider the achieved accuracy. Is it high? Given that the autoencoder was trained *without* labels, how well did it capture features useful for discriminating between 0s and 1s? How does the dimensionality reduction (784 -> latent_dim) affect this? Could we expect similar/better/worse results training a similar classifier directly on the 784 pixels?)*
#     *   *Example observation: The high test accuracy achieved (likely > 0.99 if training went well, as 0 vs 1 is an easy MNIST task) demonstrates that the unsupervised autoencoder learned a compressed representation (latent space) that effectively captured the distinguishing features between digits 0 and 1, even though it wasn't explicitly trained for classification. This shows the power of unsupervised pre-training for dimensionality reduction and feature extraction. Training a classifier on these low-dimensional embeddings is computationally efficient and can yield excellent results, suggesting the learned features are highly relevant. It's possible a classifier trained directly on pixels might achieve slightly higher accuracy, but it would operate on much higher dimensional data.*

# %% [markdown]
# End of Part II. This section covered training an autoencoder on MNIST digits 0/1, generating embeddings, and training/evaluating a classifier on those embeddings.



================================================
File: hw3/frey_diffusion_example.py
================================================
# -*- coding: utf-8 -*-
"""frey_diffusion_example.ipynb

# Diffusion example

This notebook provides a simple implementation of a [Denoising Diffusion Probabilistic Model](https://arxiv.org/pdf/2006.11239.pdf) or DDPM (Ho et al., 2020).

The DDPM is a variational method, where introduce latent variables to help in modeling $p(\mathbf{x})$. Here we call the data $\mathbf{x}_0$ and we introduce a series of latent variables $\mathbf{x}_1,\ldots,\mathbf{x}_{T}$, each having the same size as $\mathbf{x}_0$.

In the *forward process*, we slowly add noise to $\mathbf{x}_0$ according to a fixed variance schedule $\beta_t$:

$$q(\mathbf{x}_t|\mathbf{x}_{t-1})=\mathcal{N}(\sqrt{1-\beta_t}\mathbf{x}_{t-1},\beta_t\mathbf{I})$$.

In other words, to sample $\mathbf{x}_t$, we add a normally-distributed noise vector $\mathbf{\epsilon_t}$ to $\mathbf{x}_{t-1}$. In the *reverse process*, we attempt to remove this noise to recover $\mathbf{x}_{t-1}$ from $\mathbf{x}_t$.
"""

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
import numpy as np
import math
from tqdm.auto import tqdm, trange
from tensorflow.keras.utils import get_file
from scipy.io import loadmat
from matplotlib import pyplot as plt

"""We will experiment with the Frey dataset which contains 28 x 20 grayscale images."""

def get_mnist_data():
    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
    x_train = np.expand_dims(x_train,axis=-1)
    x_train = x_train.astype('float32')
    x_train /= 255
    x_train = x_train * 2 - 1
    return x_train

def get_frey_data():
    path = get_file('frey_rawface.mat','https://www.dropbox.com/scl/fi/m70sh4ef39pvy01czc63r/frey_rawface.mat?rlkey=5v6meiap55z68ada2roxwxuql&dl=1')
    data = np.transpose(loadmat(path)['ff'])
    x_train = np.reshape(data,(-1,28,20,1))
    x_train = x_train.astype('float32')
    x_train /= 255
    x_train = x_train * 2 - 1
    return x_train

num_hidden = 128
num_layers = 12

T = 1000
betas = np.linspace(1e-4,.02,T)
sigmas = np.sqrt(betas)
alphas = 1 - betas
alphas_cumprod = np.cumprod(alphas,axis=-1)

batch_size = 128

"""In our example, the reverse process model is a convolutional neural network (CNN) which takes as input $\mathbf{x}_t$ and timestep $t$, and outputs its estimate of $\mathbf{\epsilon}_t$, the noise added at timestep $t$ in the forward process.

To condition the network on $t$, we use an [Embedding layer](https://keras.io/api/layers/core_layers/embedding/). The Embedding layer stores a list of T learnable vectors accessible by an index (the timestep). Each vector will have size $H \cdot W$ so that it can be resized into an $H \times W$ image and concatenated to the input image along the channels axis.
"""

def build_reverse_process_model(H,W,num_layers,num_hidden):
    """ Builds the reverse process model.

    Arguments:
        H: image height
        W: image width
        num_layers: number of layers in CNN
        num_hidden: width of each CNN layer

    Returns:
        Keras model
    """
    # create image and timestep inputs
    image_input = layers.Input((H,W,1))
    timestep_input = layers.Input((1,))

    # create embedding layer with T vectors of size H*W
    embedding = layers.Embedding(T,H*W,embeddings_initializer='glorot_normal')

    # look up embedding vectors for timesteps
    conditional = embedding(timestep_input)

    # reshape embedding vector into an image
    conditional = keras.layers.Reshape((H,W,1))(conditional)

    # concatenate to input along channels axis
    x = keras.layers.Concatenate()([image_input,conditional])

    # process in convolutional layers
    for i in range(num_layers):
        x = layers.Conv2D(num_hidden,3,activation='relu',padding='same',use_bias=True)(x)

    # output is estimate of noise
    x = layers.Conv2D(1,3,activation=None,padding='same')(x)

    model = keras.Model(inputs=[image_input,timestep_input],outputs=x)
    return model

"""The sampling procedure starts by sampling $\mathbf{x}_T$ as random noise from a unit normal distribution.    Then at each timestep from $T-1$ to $0$ it subtracts the noise estimate from the reverse process model.    The actual update formula is a bit more complicated and comes from the Ho et al. paper."""

def sample(model,shape):
    """ Samples from the diffusion model.

    Arguments:
        model: reverse process model
        shape: shape of data to be sampled; should be [N,H,W,1]
    Returns:
        Sampled images
    """
    # sample normally-distributed random noise (x_T)
    x = np.random.normal(size=shape)

    # iterate through timesteps from T-1 to 0
    for t in trange(T-1,-1,-1):
        # sample noise unless at final step (which is deterministic)
        z = np.random.normal(size=shape) if t > 0 else np.zeros(shape)

        # estimate correction using model conditioned on timestep
        eps = model.predict([x,np.ones((shape[0],1))*t],verbose=False)

        # apply update formula
        sigma = sigmas[t]
        a = alphas[t]
        a_bar = alphas_cumprod[t]
        x = 1/np.sqrt(a)*(x - (1-a)/np.sqrt(1-a_bar)*eps)+sigma*z
    return x

"""Now we get the data and build the model."""

x_train = get_frey_data()
H,W = x_train.shape[1:3]

model = build_reverse_process_model(H,W,num_layers,num_hidden)
model.summary()

"""At each training iteration, we do the following:
* Sample a random batch of images and timesteps
* Apply the forward process to obtain the noised version of each image at the corresponding timestep
* Estimate the noise using the reverse process model
* Compute mean squared error between the estimated and actual noise
"""

train_dataset = tf.data.Dataset.from_tensor_slices(x_train)
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

opt = keras.optimizers.Adam(3e-4)
loss_fn = keras.losses.MeanSquaredError()

epochs = 100
for epoch in range(epochs):
    total_loss = 0

    pbar = tqdm(total=len(train_dataset))
    for step, x_batch_train in enumerate(train_dataset):
        t = np.random.randint(T,size=len(x_batch_train))

        with tf.GradientTape() as tape:
            noise = np.random.normal(size=x_batch_train.shape)

            at = alphas_cumprod[t]
            at = np.reshape(at,(-1,1,1,1))
            inputs = np.sqrt(at) * x_batch_train + (1-at)*noise
            est_noise = model([tf.convert_to_tensor(inputs),tf.convert_to_tensor(t)])

            loss_value = loss_fn(noise, est_noise)

            total_loss += loss_value.numpy()

        grads = tape.gradient(loss_value, model.trainable_weights)
        opt.apply_gradients(zip(grads, model.trainable_weights))

        pbar.update(1)
    pbar.close()

    total_loss /= len(train_dataset)
    print(f'loss at epoch {epoch}: {total_loss}')

"""Once training has completed we can sample a batch of images and inspect the results."""

image_sample = sample(model,x_train[:10].shape)

fig,axes = plt.subplots(1,len(image_sample))
for i in range(len(image_sample)):
    axes[i].imshow(image_sample[i])
fig.show()

"""As yuo can see, the results are far from perfect, probably because of the limitations of our simple CNN model design. As you will see in the homework, a latent diffusion model can produce much better results even when using a simple neural network model."""




================================================
File: hw3/gan_mnist.py
================================================
# -*- coding: utf-8 -*-
"""gan_mnist.py

This notebook shows how to build a network similar to a DCGAN Deep Convolutional Generative Adversarial Network (DCGAN) for representation learning and image generation on the MNIST dataset.

"""

import numpy as np
import tensorflow as tf
import matplotlib as mpl
mpl.rc('image', cmap='gray')
from matplotlib import pyplot as plt

"""We grab the MNIST dataset and select just the zero digits."""

from tensorflow.keras.datasets import mnist

# x contains images, y contains integer labels (0-9)
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train[y_train<1]
y_train = y_train[y_train<1]

x_test = x_test[y_test<1]
y_test = y_test[y_test<1]

print(x_train.shape)
for i in range(5):
    plt.subplot(1,5,i+1)
    plt.imshow(x_train[i])
    plt.axis('off')
plt.show()

x_train = (x_train.astype('float32')/255.)*2-1
x_test = (x_test.astype('float32')/255.)*2-1

"""# Model definition

Here we build a generator and discriminator network. The generator uses Conv2DTranspose to upsample images and LeakyReLU as the activation function, as recommended in the DCGAN paper. The discriminator uses strided convolutions instead of max pooling, also following the recommendations of DCGAN.
"""

from tensorflow.keras.layers import Input, Flatten, Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose, MaxPooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD, Adam

def build_generator(latent_dim):
    inputs = Input((latent_dim,),name='generator_input')
    h1 = Dense(7*7*128,activation=None)(inputs)
    h1 = LeakyReLU(0.1)(h1)
    h1 = Reshape((7,7,128))(h1)
    h2 = Conv2DTranspose(256,3,strides=2,padding='same')(h1)
    h2 = LeakyReLU(0.1)(h2)
    output = Conv2DTranspose(1,3,strides=2,padding='same')(h2)
    return Model(inputs=inputs,outputs=output)

def build_discriminator():
    inputs = Input((28,28,1),name='encoder_input')
    h1 = Conv2D(256,3,strides=2,padding='same',activation='relu')(inputs)
    h2 = Conv2D(128,3,strides=2,padding='same',activation='relu')(h1)
    flat = Flatten()(h2)
    output = Dense(1,activation='sigmoid')(flat)
    return Model(inputs=inputs,outputs=output)

latent_dim = 2
generator = build_generator(latent_dim)
discriminator = build_discriminator()

print(generator.summary())
print(discriminator.summary())

"""# GAN Training

We have to manually set up a training loop because model.fit() doesn't support separate training of the generator and discriminator.
"""

g_opt = Adam(3e-4)
d_opt = Adam(3e-4)

num_iter = 5000
batch_size = 32

gen_loss_history = []
discrim_loss_history = []

# fixed set of random latent vectors to inspect development of generator output during training
latent_vectors = np.random.uniform(-1,1,size=(5,latent_dim))

# binary crossentropy loss function for discriminator training
loss_fn = tf.keras.losses.binary_crossentropy

for iter in range(num_iter):
    # select a random batch of training image indices
    inds = np.random.randint(len(x_train),size=batch_size)

    # get real data and labels
    x_real = x_train[inds]
    y_real = np.ones((batch_size,1))

    # get fake data (samples from generator) and labels
    x_fake = generator.predict(np.random.uniform(-1,1,size=(batch_size, latent_dim)), batch_size=batch_size, verbose=False)
    y_fake = np.zeros((batch_size,1))

    # compute discriminator loss -- label real images as 1 and fake images as 0
    with tf.GradientTape() as tape:
        real_loss = tf.reduce_mean(loss_fn(y_real,discriminator(x_real)))
        fake_loss = tf.reduce_mean(loss_fn(y_fake,discriminator(x_fake)))
        discrim_loss = 0.5*(real_loss + fake_loss)

    # compute gradients w.r.t. discriminator weights and update discriminator
    grads = tape.gradient(discrim_loss,discriminator.trainable_variables)
    d_opt.apply_gradients(zip(grads,discriminator.trainable_variables))

    # compute generator loss -- label fake images as 1
    with tf.GradientTape() as tape:
        x_gen = generator(np.random.uniform(-1,1,size=(batch_size, latent_dim)))
        gen_loss = tf.reduce_mean(loss_fn(y_real,discriminator(x_gen)))

    # generator updated
    grads = tape.gradient(gen_loss,generator.trainable_variables)
    g_opt.apply_gradients(zip(grads,generator.trainable_variables))

    # add losses to log
    gen_loss_history.append(gen_loss.numpy())
    discrim_loss_history.append(discrim_loss.numpy())

    # periodic summary output and generator sample visualization
    if iter % 100 == 0:
        print('iter %d: discriminator loss: %.2f\tgenerator loss: %.2f'%(iter+1,discrim_loss.numpy(),gen_loss.numpy()))
        x_gen = generator.predict(latent_vectors,verbose=False)
        for i in range(len(x_gen)):
            plt.subplot(1,len(x_gen),i+1)
            plt.imshow(np.squeeze(x_gen[i]))
        plt.show()

"""Plotting the loss curves"""

plt.plot(gen_loss_history)
plt.plot(discrim_loss_history)
plt.xlabel('iter')
plt.ylabel('loss')
plt.legend(['gen','discrim'])
plt.show()

"""## Generating new images

We generate images by sampling from a regularly spaced grid on [-1 1]x[-1 1].
"""

coords = np.linspace(-1,1,num=10)
x,y = np.meshgrid(coords,coords,indexing='xy')
embeddings = np.stack([x.flatten(),y.flatten()],axis=1)
plt.scatter(embeddings[:,0],embeddings[:,1])
plt.show()

plt.figure(figsize=(20, 20))
result = generator.predict(embeddings)
n = 0
for i in range(10):
    for j in range(10):
        plt.subplot(10,10,n+1)
        plt.imshow(np.squeeze(result[n]),cmap='gray')
        plt.axis('off')
        n = n + 1
plt.show()




================================================
File: hw3/instructions.md
================================================
# **Generative Models**

In this homework you will build and experiment with a convolutional GAN and a diffusion model for representation learning and image generation.

Your implementation should use the Keras module in Tensorflow 2 (import tensorflow.keras).

# **Part 1: GAN on Frey Dataset**

**1.1: Build the model**

Build and train a convolutional GAN on the Frey dataset, following the provided example notebook.

Your generator network should take in a latent vector and output an image. To do this, the first layer should be a dense layer with 7\*5\*128 outputs. Then reshape this output using a Reshape layer to size (7,5,128). Now you have converted your vector into a tensor of 128 7x5 images. Now you can use a Conv2DTranspose with stride 2 to upsample that into 256 14x10 images. Use one more Conv2DTranspose with stride 2 and 1 output channel to finally output a 28x20 image. Following the recommendations of DCGAN, use LeakyReLU activations with alpha=0.1 on all layers except the output layer.

The discriminator network should take an 28x20x1 image and output a probability value. Use Conv2D layers with stride 2 and ReLU activation. The first layer can have 256 channels and the second 128. Then flatten the tensor and do a final dense layer with a single output and sigmoid activation.

(Feel free to modify the network architecture and training settings as you like \-- this is not required, though.)

**1.2: Train the model**

Train the model following the MNIST example notebook.

Plot the discriminator and generator loss curves after training. Does the system seem to have converged?

**1.3: Evaluate the results**

Sample random images from `[-1 1]` box. Do the images appear similar to the training images? Do you see evidence of mode collapse in the results of GAN training? (Does the generator seem to be able to generate everything that is in the training set?)

# **Part 2: Latent Diffusion Model on Frey Dataset**

Build a latent diffusion model on the Frey dataset.  You will train a diffusion model to learn the distribution of the latent space of an autoencoder. To sample new images, you will first sample a latent vector using the diffusion model, and then decode it into an image.

**2.1: Build an autoencoder**

Train an MLP autoencoder on the Frey dataset. You can use the autoencoder you built in HW2. The bottleneck should be two-dimensional.

After training the autoencoder, extract and store the embedding vectors for the training images using encoder.predict().

**2.2: Build and train the reverse process model**

The reverse process model should take as input a two-dimensional embedding vector and the timestep `t` and output a two-dimensional vector. In-between should be a simple feedforward MLP. I used three hidden layers of width 128, but the design is up to you.


Use an Embedding layer to map the timestep to a vector the same size as your hidden layers. Then concatenate that vector to the two-dimensional input (along the channels axis) before processing with the MLP.

Train the model on the embedding vectors from part 2.1 using the custom training loop code provided in the diffusion example notebook.

**2.3: Evaluate the results**

Sample 1000 random 2D latent vectors using the diffusion sampling process (see the example notebook). Plot the training embedding vectors and your sampled 2D points on top. Did the diffusion model accurately learn the distribution of the latent space?

Then decode the first 10 sampled latent vectors into images. Do the images appear similar to the training images? Do you see evidence of mode collapse in the results of diffusion training? (Does the generator seem to be able to generate everything that is in the training set?)



================================================
File: hw3/main.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# **Generative Models - Part 1: GAN on Frey Dataset**
"""

import numpy as np
import tensorflow as tf
import matplotlib as mpl
mpl.rc('image', cmap='gray')
from matplotlib import pyplot as plt
from tensorflow.keras.layers import Input, Flatten, Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import get_file
from scipy.io import loadmat
import tensorflow.keras as keras
from tensorflow.keras import layers
from tensorflow.keras.utils import get_file
from tqdm.auto import tqdm, trange

"""
## 1. Data Loading and Preprocessing
"""

# Function to load and preprocess Frey dataset
# Adapted from frey_diffusion_example.py
def get_frey_data():
    """Loads and preprocesses the Frey dataset.

    Returns:
        numpy.ndarray: Frey face images, normalized to [-1, 1]
                       with shape (N, 28, 20, 1).
    """
    try:
        path = get_file('frey_rawface.mat',
                        'https://www.dropbox.com/scl/fi/m70sh4ef39pvy01czc63r/frey_rawface.mat?rlkey=5v6meiap55z68ada2roxwxuql&dl=1',
                        cache_subdir='datasets') # Added cache_subdir for better organization
        data = np.transpose(loadmat(path)['ff'])  # Shape (1965, 560)
        # Reshape to (num_images, height, width, channels)
        x_train = np.reshape(data, (-1, 28, 20, 1))
        x_train = x_train.astype('float32')
        x_train /= 255.0  # Normalize to [0, 1]
        x_train = (x_train * 2.0) - 1.0  # Normalize to [-1, 1]
        return x_train
    except Exception as e:
        print(f"Error loading Frey dataset: {e}")
        print("Please ensure you have an internet connection and scipy is installed.")
        print("Alternatively, download 'frey_rawface.mat' manually and place it in '~/.keras/datasets/'.")
        return None

# Load the data
x_train = get_frey_data()

IMG_H, IMG_W, IMG_C = x_train.shape[1:] # Should be 28, 20, 1
print(f"Loaded Frey dataset: {x_train.shape}")

# Display a few example images
print("Example images from Frey dataset:")
plt.figure(figsize=(10, 2))
for i in range(5):
    plt.subplot(1, 5, i + 1)
    # De-normalize for display: from [-1, 1] to [0, 1]
    img_display = (x_train[i] + 1.0) / 2.0
    plt.imshow(np.squeeze(img_display))
    plt.axis('off')
plt.show()
# Output:
#   Loaded Frey dataset: (1965, 28, 20, 1)

#   Example images from Frey dataset:

#   <Figure size 1000x200 with 5 Axes>

"""
## 1.1: Build the model

### Generator Network
The generator network takes a latent vector and outputs a 28x20x1 image.
- Input: latent vector
- Dense layer: 7\*5\*128 outputs, LeakyReLU(alpha=0.1)
- Reshape: (7,5,128)
- Conv2DTranspose 1: 256 filters, stride 2, padding 'same', LeakyReLU(alpha=0.1) -> (14,10,256)
- Conv2DTranspose 2: 1 filter (output channel), stride 2, padding 'same', no activation (linear) -> (28,20,1)
"""

def build_generator(latent_dim):
    inputs = Input((latent_dim,), name='generator_input')

    # Dense layer: 7*5*128 outputs
    x = Dense(7*5*128, activation=None)(inputs)
    x = LeakyReLU(alpha=0.1)(x)

    # Reshape to (7,5,128)
    x = Reshape((7,5,128))(x)

    # Conv2DTranspose 1: stride 2, 256 filters -> (14,10,256)
    x = Conv2DTranspose(256, kernel_size=3, strides=2, padding='same')(x) # Default kernel size 3 is common
    x = LeakyReLU(alpha=0.1)(x)

    # Conv2DTranspose 2: stride 2, 1 output channel -> (28,20,1)
    # Output layer, typically tanh for [-1,1] data, or linear as per MNIST example
    output = Conv2DTranspose(1, kernel_size=3, strides=2, padding='same', activation=None)(x)

    return Model(inputs=inputs, outputs=output, name='generator')

"""
### Discriminator Network
The discriminator network takes a 28x20x1 image and outputs a probability.
- Input: 28x20x1 image
- Conv2D 1: 256 filters, stride 2, padding 'same', ReLU activation
- Conv2D 2: 128 filters, stride 2, padding 'same', ReLU activation
- Flatten
- Dense: 1 output, sigmoid activation
"""

# Discriminator model definition
def build_discriminator(img_shape):
    inputs = Input(img_shape, name='discriminator_input')

    # Conv2D 1: 256 channels, stride 2, ReLU
    x = Conv2D(256, kernel_size=3, strides=2, padding='same', activation='relu')(inputs) # Default kernel size 3

    # Conv2D 2: 128 channels, stride 2, ReLU
    x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)

    # Flatten
    x = Flatten()(x)

    # Dense output layer: 1 output, sigmoid
    output = Dense(1, activation='sigmoid')(x)

    return Model(inputs=inputs, outputs=output, name='discriminator')

"""
### Model Instantiation and Summaries
"""


latent_dim = 100

generator = build_generator(latent_dim)
discriminator = build_discriminator((IMG_H, IMG_W, IMG_C))

print("\nGenerator Summary:")
generator.summary()
print("\nDiscriminator Summary:")
discriminator.summary()
# Output:
#   

#   Generator Summary:

#   /Users/bshowell/Desktop/school/spring 24-25/587/.venv/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.

#     warnings.warn(

#   [1mModel: "generator"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ generator_input ([38;5;33mInputLayer[0m)    â”‚ ([38;5;45mNone[0m, [38;5;34m100[0m)            â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense ([38;5;33mDense[0m)                   â”‚ ([38;5;45mNone[0m, [38;5;34m4480[0m)           â”‚       [38;5;34m452,480[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ leaky_re_lu ([38;5;33mLeakyReLU[0m)         â”‚ ([38;5;45mNone[0m, [38;5;34m4480[0m)           â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ reshape ([38;5;33mReshape[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m7[0m, [38;5;34m5[0m, [38;5;34m128[0m)      â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2d_transpose                â”‚ ([38;5;45mNone[0m, [38;5;34m14[0m, [38;5;34m10[0m, [38;5;34m256[0m)    â”‚       [38;5;34m295,168[0m â”‚

#   â”‚ ([38;5;33mConv2DTranspose[0m)               â”‚                        â”‚               â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ leaky_re_lu_1 ([38;5;33mLeakyReLU[0m)       â”‚ ([38;5;45mNone[0m, [38;5;34m14[0m, [38;5;34m10[0m, [38;5;34m256[0m)    â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2d_transpose_1              â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m, [38;5;34m1[0m)      â”‚         [38;5;34m2,305[0m â”‚

#   â”‚ ([38;5;33mConv2DTranspose[0m)               â”‚                        â”‚               â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m749,953[0m (2.86 MB)

#   [1m Trainable params: [0m[38;5;34m749,953[0m (2.86 MB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   Discriminator Summary:

#   [1mModel: "discriminator"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ discriminator_input             â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m, [38;5;34m1[0m)      â”‚             [38;5;34m0[0m â”‚

#   â”‚ ([38;5;33mInputLayer[0m)                    â”‚                        â”‚               â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2d ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m14[0m, [38;5;34m10[0m, [38;5;34m256[0m)    â”‚         [38;5;34m2,560[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ conv2d_1 ([38;5;33mConv2D[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m7[0m, [38;5;34m5[0m, [38;5;34m128[0m)      â”‚       [38;5;34m295,040[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten ([38;5;33mFlatten[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m4480[0m)           â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_1 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m1[0m)              â”‚         [38;5;34m4,481[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m302,081[0m (1.15 MB)

#   [1m Trainable params: [0m[38;5;34m302,081[0m (1.15 MB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)


"""
## GAN Training
We set up optimizers and a custom training loop.
"""

# Using Adam optimizer as in the MNIST GAN example
# Learning rates from the example for consistency
g_opt = Adam(learning_rate=3e-4)
d_opt = Adam(learning_rate=3e-4)

num_iter = 2000
batch_size = 32

gen_loss_history = []
discrim_loss_history = []

# Fixed set of random latent vectors to inspect generator's progress
num_samples_to_show = 5
latent_vectors_for_viz = np.random.uniform(-1, 1, size=(num_samples_to_show, latent_dim))

# Binary crossentropy loss function
loss_fn = tf.keras.losses.binary_crossentropy

print(f"\nStarting training for {num_iter} iterations with batch size {batch_size}...")
# Output:
#   

#   Starting training for 2000 iterations with batch size 32...


for iter_count in range(num_iter):
    # Select a random batch of real images
    inds = np.random.randint(0, x_train.shape[0], size=batch_size)
    x_real = x_train[inds]
    y_real = np.ones((batch_size, 1)) # Labels for real images: 1

    # Generate a batch of fake images
    random_latent_batch = np.random.uniform(-1, 1, size=(batch_size, latent_dim))
    x_fake = generator.predict(random_latent_batch, batch_size=batch_size, verbose=0)
    y_fake = np.zeros((batch_size, 1)) # Labels for fake images: 0

    # Discriminator loss
    with tf.GradientTape() as tape:
        pred_real = discriminator(x_real, training=True)
        real_loss = tf.reduce_mean(loss_fn(y_real, pred_real))

        pred_fake = discriminator(x_fake, training=True)
        fake_loss = tf.reduce_mean(loss_fn(y_fake, pred_fake))
        discrim_loss = 0.5 * (real_loss + fake_loss)

    # Compute gradients and update discriminator weights
    grads = tape.gradient(discrim_loss, discriminator.trainable_variables)
    d_opt.apply_gradients(zip(grads, discriminator.trainable_variables))

    # Generate a new batch of latent vectors
    random_latent_batch_for_gen = np.random.uniform(-1, 1, size=(batch_size, latent_dim))
    # Target labels for generator: we want discriminator to think these are real (label 1)
    y_gen_target = np.ones((batch_size, 1))

    # Generator loss
    with tf.GradientTape() as tape:
        x_gen = generator(random_latent_batch_for_gen, training=True)
        # Discriminator's weights are frozen during generator update (handled by tape.gradient target)
        pred_gen_for_loss = discriminator(x_gen, training=False) # As per example, D is fixed here
        gen_loss = tf.reduce_mean(loss_fn(y_gen_target, pred_gen_for_loss))

    # Compute gradients and update generator weights
    grads = tape.gradient(gen_loss, generator.trainable_variables)
    g_opt.apply_gradients(zip(grads, generator.trainable_variables))

    # Log losses
    gen_loss_history.append(gen_loss.numpy())
    discrim_loss_history.append(discrim_loss.numpy())

    # Periodic summary output and generator sample visualization
    if (iter_count + 1) % 100 == 0:
        print('Iter %d/%d: Discriminator Loss: %.4f, Generator Loss: %.4f' % (
            iter_count + 1, num_iter, discrim_loss.numpy(), gen_loss.numpy()))

        x_gen_samples = generator.predict(latent_vectors_for_viz, verbose=0)

        plt.figure(figsize=(num_samples_to_show * 2, 2))
        for i in range(num_samples_to_show):
            plt.subplot(1, num_samples_to_show, i + 1)
            # De-normalize for display: from [-1, 1] to [0, 1]
            img_display = (np.squeeze(x_gen_samples[i]) + 1.0) / 2.0
            plt.imshow(np.clip(img_display, 0.0, 1.0)) # Clip to ensure valid range for imshow
            plt.axis('off')
        plt.suptitle(f'Generated Samples at Iteration {iter_count + 1}')
        plt.show()

print("\nTraining finished.")
# Output:
#   Iter 100/2000: Discriminator Loss: 0.0661, Generator Loss: 2.7838

#   <Figure size 1000x200 with 5 Axes>
#   Iter 200/2000: Discriminator Loss: 0.1484, Generator Loss: 4.4530

#   <Figure size 1000x200 with 5 Axes>
#   Iter 300/2000: Discriminator Loss: 0.2674, Generator Loss: 3.8155

#   <Figure size 1000x200 with 5 Axes>
#   Iter 400/2000: Discriminator Loss: 0.2175, Generator Loss: 2.2137

#   <Figure size 1000x200 with 5 Axes>
#   Iter 500/2000: Discriminator Loss: 0.3816, Generator Loss: 2.3766

#   <Figure size 1000x200 with 5 Axes>
#   Iter 600/2000: Discriminator Loss: 0.4320, Generator Loss: 2.1901

#   <Figure size 1000x200 with 5 Axes>
#   Iter 700/2000: Discriminator Loss: 0.6409, Generator Loss: 1.3600

#   <Figure size 1000x200 with 5 Axes>
#   Iter 800/2000: Discriminator Loss: 0.2151, Generator Loss: 2.6106

#   <Figure size 1000x200 with 5 Axes>
#   Iter 900/2000: Discriminator Loss: 0.2110, Generator Loss: 2.7634

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1000/2000: Discriminator Loss: 0.2775, Generator Loss: 2.1866

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1100/2000: Discriminator Loss: 0.3202, Generator Loss: 2.1938

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1200/2000: Discriminator Loss: 0.4292, Generator Loss: 1.4540

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1300/2000: Discriminator Loss: 0.3782, Generator Loss: 1.9560

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1400/2000: Discriminator Loss: 0.2736, Generator Loss: 1.9458

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1500/2000: Discriminator Loss: 0.2602, Generator Loss: 2.1818

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1600/2000: Discriminator Loss: 0.3838, Generator Loss: 1.5973

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1700/2000: Discriminator Loss: 0.2687, Generator Loss: 1.9438

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1800/2000: Discriminator Loss: 0.4817, Generator Loss: 1.8319

#   <Figure size 1000x200 with 5 Axes>
#   Iter 1900/2000: Discriminator Loss: 0.3467, Generator Loss: 2.1938

#   <Figure size 1000x200 with 5 Axes>
#   Iter 2000/2000: Discriminator Loss: 0.3460, Generator Loss: 1.6515

#   <Figure size 1000x200 with 5 Axes>
#   

#   Training finished.


"""
### Plotting Loss Curves
"""

plt.figure(figsize=(10, 5))
plt.plot(gen_loss_history, label='Generator Loss')
plt.plot(discrim_loss_history, label='Discriminator Loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('GAN Training Losses on Frey Dataset')
plt.legend()
plt.grid(True)
plt.show()
# Output:
#   <Figure size 1000x500 with 1 Axes>

"""
### Generating and Showing Final Images
Let's generate a batch of new images using the trained generator.
"""

print("\nGenerating some final samples...")
num_final_samples = 10
# Ensure latent_dim is defined
final_latent_vectors = np.random.uniform(-1, 1, size=(num_final_samples, latent_dim))
final_samples = generator.predict(final_latent_vectors, verbose=0)

plt.figure(figsize=(12, 5)) # Adjusted for 2 rows of 5, or 1 row of 10
for i in range(num_final_samples):
    plt.subplot(2, 5, i + 1) # Assumes 10 samples to fit 2x5 grid
    img_display = (np.squeeze(final_samples[i]) + 1.0) / 2.0 # De-normalize
    plt.imshow(np.clip(img_display, 0.0, 1.0))
    plt.axis('off')
plt.suptitle('Final Generated Frey Face Samples')
plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to prevent suptitle overlap
plt.show()
# Output:
#   

#   Generating some final samples...

#   <Figure size 1200x500 with 10 Axes>

"""
The generation does seem to have roughly converged, at least stopped improving. 
"""

num_eval_samples = 20  # Generate a decent number of samples for evaluation

# Sample random latent vectors from the [-1, 1] box
random_latent_vectors_eval = np.random.uniform(-1, 1, size=(num_eval_samples, latent_dim))

# Generate images
print(f"Generating {num_eval_samples} images from random latent vectors...")
generated_images_eval = generator.predict(random_latent_vectors_eval, verbose=0)

# Display the generated images
print("Displaying randomly generated samples:")
cols = 5  # Number of columns in the plot
rows = (num_eval_samples + cols - 1) // cols # Calculate rows needed
plt.figure(figsize=(cols * 2, rows * 2.2)) # Adjust figure size

for i in range(num_eval_samples):
    plt.subplot(rows, cols, i + 1)
    # De-normalize for display: from [-1, 1] to [0, 1]
    img_display = (np.squeeze(generated_images_eval[i]) + 1.0) / 2.0
    plt.imshow(np.clip(img_display, 0.0, 1.0)) # Clip to ensure valid range
    plt.axis('off')
    plt.title(f"Sample {i+1}")

plt.suptitle(f'{num_eval_samples} Randomly Generated Frey Face Samples for Evaluation', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout
plt.show()

# For comparison, you might want to show some actual training images again
print("\nFor reference, a few examples from the training dataset:")
plt.figure(figsize=(10, 2))
for i in range(5):
    plt.subplot(1, 5, i + 1)
    img_display = (x_train[np.random.randint(0, len(x_train))] + 1.0) / 2.0 # Show random training images
    plt.imshow(np.squeeze(img_display))
    plt.axis('off')
plt.suptitle("Reference Training Images")
plt.show()
# Output:
#   Generating 20 images from random latent vectors...

#   Displaying randomly generated samples:

#   <Figure size 1000x880 with 20 Axes>
#   

#   For reference, a few examples from the training dataset:

#   <Figure size 1000x200 with 5 Axes>

"""
They look fairly similar, but there are problems. Specifically, the nose to mouth area struggles to have defining features and things get merged together. 
"""

def get_frey_data():
    try:
        path = get_file('frey_rawface.mat',
                        'https://www.dropbox.com/scl/fi/m70sh4ef39pvy01czc63r/frey_rawface.mat?rlkey=5v6meiap55z68ada2roxwxuql&dl=1',
                        cache_subdir='datasets')
        data = np.transpose(loadmat(path)['ff'])
        x_train_frey = np.reshape(data, (-1, 28, 20, 1))
        x_train_frey = x_train_frey.astype('float32')
        x_train_frey /= 255.0  # Normalize to [0, 1]
        x_train_frey = (x_train_frey * 2.0) - 1.0  # Normalize to [-1, 1]
        return x_train_frey
    except Exception as e:
        print(f"Error loading Frey dataset: {e}")
        return None

x_train_frey = get_frey_data()

IMG_H, IMG_W, IMG_C = x_train_frey.shape[1:]
FLAT_DIM = IMG_H * IMG_W * IMG_C
print(f"Frey dataset loaded: {x_train_frey.shape}")

# Display a few example images
plt.figure(figsize=(10, 2))
for i in range(5):
    plt.subplot(1, 5, i + 1)
    img_display = (x_train_frey[i] + 1.0) / 2.0 # De-normalize for display
    plt.imshow(np.squeeze(img_display))
    plt.axis('off')
plt.suptitle("Sample Frey Dataset Images")
plt.show()
# Output:
#   Frey dataset loaded: (1965, 28, 20, 1)

#   <Figure size 1000x200 with 5 Axes>

LATENT_DIM_AE = 2 # Bottleneck dimensionality

# Define Encoder
encoder_inputs = layers.Input(shape=(IMG_H, IMG_W, IMG_C), name='encoder_input')
x = layers.Flatten()(encoder_inputs)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dense(64, activation='relu')(x)
encoder_outputs = layers.Dense(LATENT_DIM_AE, activation=None, name='encoder_output')(x) # Linear activation for bottleneck
encoder = Model(encoder_inputs, encoder_outputs, name='encoder')
print("Encoder Summary:")
encoder.summary()

# Define Decoder
decoder_inputs = layers.Input(shape=(LATENT_DIM_AE,), name='decoder_input')
x = layers.Dense(64, activation='relu')(decoder_inputs)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dense(FLAT_DIM, activation='tanh')(x) # Tanh because input images are [-1, 1]
decoder_outputs = layers.Reshape((IMG_H, IMG_W, IMG_C), name='decoder_output')(x)
decoder = Model(decoder_inputs, decoder_outputs, name='decoder')
print("\nDecoder Summary:")
decoder.summary()

# Define Autoencoder
autoencoder_inputs = layers.Input(shape=(IMG_H, IMG_W, IMG_C), name='ae_input')
encoded = encoder(autoencoder_inputs)
decoded = decoder(encoded)
autoencoder = Model(autoencoder_inputs, decoded, name='autoencoder')
print("\nAutoencoder Summary:")
autoencoder.summary()

# Compile and Train Autoencoder
autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mse')

ae_epochs = 50 # Adjust as needed; might need more for good reconstruction
ae_batch_size = 32

print(f"\nTraining Autoencoder for {ae_epochs} epochs...")
history_ae = autoencoder.fit(x_train_frey, x_train_frey,
                                epochs=ae_epochs,
                                batch_size=ae_batch_size,
                                shuffle=True,
                                validation_split=0.1, # Optional: use a validation split
                                verbose=1) # Set to 1 or 2 for progress, 0 for silent

# Plot training loss
plt.figure()
plt.plot(history_ae.history['loss'], label='Training Loss')
if 'val_loss' in history_ae.history:
    plt.plot(history_ae.history['val_loss'], label='Validation Loss')
plt.title('Autoencoder Training Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.show()

# Display some original and reconstructed images
num_display = 5
random_indices = np.random.choice(len(x_train_frey), num_display, replace=False)
originals = x_train_frey[random_indices]
reconstructions = autoencoder.predict(originals, verbose=0)

plt.figure(figsize=(num_display * 2, 4))
for i in range(num_display):
    # Original
    plt.subplot(2, num_display, i + 1)
    plt.imshow(np.squeeze((originals[i] + 1.0) / 2.0))
    plt.title("Original")
    plt.axis('off')
    # Reconstruction
    plt.subplot(2, num_display, num_display + i + 1)
    plt.imshow(np.squeeze((reconstructions[i] + 1.0) / 2.0))
    plt.title("Recon.")
    plt.axis('off')
plt.suptitle("Autoencoder: Original vs. Reconstructed")
plt.show()

# Extract and store latent embeddings for the training set
print("\nExtracting latent embeddings from the training set...")
latent_embeddings_train = encoder.predict(x_train_frey, batch_size=ae_batch_size, verbose=0)
print(f"Shape of latent embeddings: {latent_embeddings_train.shape}") # Should be (num_frey_images, 2)

# Plot the 2D latent space
plt.figure(figsize=(8, 6))
plt.scatter(latent_embeddings_train[:, 0], latent_embeddings_train[:, 1], s=5, alpha=0.5)
plt.title('2D Latent Space of Frey Dataset (from Autoencoder)')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')
plt.grid(True)
plt.show()
# Output:
#   Encoder Summary:

#   [1mModel: "encoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ encoder_input ([38;5;33mInputLayer[0m)      â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m, [38;5;34m1[0m)      â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten_1 ([38;5;33mFlatten[0m)             â”‚ ([38;5;45mNone[0m, [38;5;34m560[0m)            â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_2 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)            â”‚        [38;5;34m71,808[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_3 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)             â”‚         [38;5;34m8,256[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ encoder_output ([38;5;33mDense[0m)          â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)              â”‚           [38;5;34m130[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m80,194[0m (313.26 KB)

#   [1m Trainable params: [0m[38;5;34m80,194[0m (313.26 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   Decoder Summary:

#   [1mModel: "decoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ decoder_input ([38;5;33mInputLayer[0m)      â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)              â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_4 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m64[0m)             â”‚           [38;5;34m192[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_5 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)            â”‚         [38;5;34m8,320[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_6 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m560[0m)            â”‚        [38;5;34m72,240[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ decoder_output ([38;5;33mReshape[0m)        â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m, [38;5;34m1[0m)      â”‚             [38;5;34m0[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m80,752[0m (315.44 KB)

#   [1m Trainable params: [0m[38;5;34m80,752[0m (315.44 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   Autoencoder Summary:

#   [1mModel: "autoencoder"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ ae_input ([38;5;33mInputLayer[0m)           â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m, [38;5;34m1[0m)      â”‚             [38;5;34m0[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ encoder ([38;5;33mFunctional[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)              â”‚        [38;5;34m80,194[0m â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ decoder ([38;5;33mFunctional[0m)            â”‚ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m20[0m, [38;5;34m1[0m)      â”‚        [38;5;34m80,752[0m â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m160,946[0m (628.70 KB)

#   [1m Trainable params: [0m[38;5;34m160,946[0m (628.70 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   Training Autoencoder for 50 epochs...

#   Epoch 1/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 3ms/step - loss: 0.0975 - val_loss: 0.0438

#   Epoch 2/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0371 - val_loss: 0.0387

#   Epoch 3/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0321 - val_loss: 0.0358

#   Epoch 4/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0307 - val_loss: 0.0351

#   Epoch 5/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0299 - val_loss: 0.0347

#   Epoch 6/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0292 - val_loss: 0.0365

#   Epoch 7/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0289 - val_loss: 0.0363

#   Epoch 8/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0290 - val_loss: 0.0360

#   Epoch 9/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0279 - val_loss: 0.0352

#   Epoch 10/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0277 - val_loss: 0.0359

#   Epoch 11/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0277 - val_loss: 0.0358

#   Epoch 12/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0268 - val_loss: 0.0354

#   Epoch 13/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0266 - val_loss: 0.0348

#   Epoch 14/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0256 - val_loss: 0.0353

#   Epoch 15/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0240 - val_loss: 0.0373

#   Epoch 16/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0244 - val_loss: 0.0361

#   Epoch 17/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0233 - val_loss: 0.0359

#   Epoch 18/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0234 - val_loss: 0.0372

#   Epoch 19/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0235 - val_loss: 0.0364

#   Epoch 20/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0230 - val_loss: 0.0375

#   Epoch 21/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0224 - val_loss: 0.0383

#   Epoch 22/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0227 - val_loss: 0.0379

#   Epoch 23/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0214 - val_loss: 0.0383

#   Epoch 24/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0217 - val_loss: 0.0372

#   Epoch 25/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 2ms/step - loss: 0.0211 - val_loss: 0.0383

#   Epoch 26/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0210 - val_loss: 0.0363

#   Epoch 27/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0209 - val_loss: 0.0374

#   Epoch 28/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0207 - val_loss: 0.0368

#   Epoch 29/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0203 - val_loss: 0.0374

#   Epoch 30/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 2ms/step - loss: 0.0197 - val_loss: 0.0380

#   Epoch 31/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0201 - val_loss: 0.0389

#   Epoch 32/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0202 - val_loss: 0.0405

#   Epoch 33/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0199 - val_loss: 0.0403

#   Epoch 34/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0194 - val_loss: 0.0386

#   Epoch 35/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0196 - val_loss: 0.0395

#   Epoch 36/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0186 - val_loss: 0.0393

#   Epoch 37/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0191 - val_loss: 0.0399

#   Epoch 38/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0188 - val_loss: 0.0390

#   Epoch 39/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0187 - val_loss: 0.0399

#   Epoch 40/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0191 - val_loss: 0.0416

#   Epoch 41/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0192 - val_loss: 0.0393

#   Epoch 42/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0183 - val_loss: 0.0396

#   Epoch 43/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0180 - val_loss: 0.0399

#   Epoch 44/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0183 - val_loss: 0.0396

#   Epoch 45/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0184 - val_loss: 0.0404

#   Epoch 46/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0181 - val_loss: 0.0398

#   Epoch 47/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0175 - val_loss: 0.0394

#   Epoch 48/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0181 - val_loss: 0.0399

#   Epoch 49/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0175 - val_loss: 0.0403

#   Epoch 50/50

#   [1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.0181 - val_loss: 0.0403

#   <Figure size 640x480 with 1 Axes>
#   <Figure size 1000x400 with 10 Axes>
#   

#   Extracting latent embeddings from the training set...

#   Shape of latent embeddings: (1965, 2)

#   <Figure size 800x600 with 1 Axes>

# Diffusion Hyperparameters (from example, can be tuned)
T_diffusion = 1000  # Number of timesteps

betas = np.linspace(1e-4, 0.02, T_diffusion, dtype=np.float32)

sigmas_diffusion = np.sqrt(betas)
alphas = 1.0 - betas
alphas_cumprod = np.cumprod(alphas, axis=-1)

sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)
# This was not square rooted in the original code
sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)


# Reverse Process Model (MLP for 2D latent space)
def build_reverse_process_model_mlp(latent_dim, time_embedding_dim, num_hidden_layers=3, hidden_layer_width=128):
    """Builds the MLP reverse process model for the latent space."""
    latent_input = layers.Input(shape=(latent_dim,), name='latent_input') # e.g., (2,)
    timestep_input = layers.Input(shape=(1,), name='timestep_input')     # Scalar timestep

    # Timestep embedding
    time_embed = layers.Embedding(input_dim=T_diffusion, output_dim=time_embedding_dim,
                                    embeddings_initializer='glorot_normal')(timestep_input)
    time_embed = layers.Flatten()(time_embed) # Shape from (batch, 1, D) to (batch, D)

    # Concatenate latent vector with time embedding
    merged_input = layers.Concatenate(axis=-1)([latent_input, time_embed])

    # MLP
    x = merged_input
    for _ in range(num_hidden_layers):
        x = layers.Dense(hidden_layer_width, activation='relu')(x)

    # Output predicted noise (same dimension as latent_input)
    output_noise = layers.Dense(latent_dim, activation=None)(x) # Linear activation

    model = Model(inputs=[latent_input, timestep_input], outputs=output_noise)
    return model

# Instantiate the model
latent_dim_for_diffusion = LATENT_DIM_AE # Should be 2
time_embedding_dim_diffusion = 128 # As suggested: "same size as your hidden layers"
hidden_mlp_width = 128

diffusion_model_mlp = build_reverse_process_model_mlp(
    latent_dim_for_diffusion,
    time_embedding_dim_diffusion,
    hidden_layer_width=hidden_mlp_width
)
print("Diffusion Model (MLP for Latents) Summary:")
diffusion_model_mlp.summary()

# Training Setup
diffusion_epochs = 150 # Adjust as needed
diffusion_batch_size = 128
diffusion_learning_rate = 3e-4 # Example uses 3e-4

optimizer_diffusion = keras.optimizers.Adam(learning_rate=diffusion_learning_rate)
loss_fn_diffusion = keras.losses.MeanSquaredError()

# Prepare dataset from latent embeddings
train_dataset_latents = tf.data.Dataset.from_tensor_slices(latent_embeddings_train)
train_dataset_latents = train_dataset_latents.shuffle(buffer_size=len(latent_embeddings_train)).batch(diffusion_batch_size)

print(f"\nTraining Diffusion Model on Latent Embeddings for {diffusion_epochs} epochs...")

# Custom Training Loop (adapted from frey_diffusion_example.py)
diffusion_loss_history = []
for epoch in range(diffusion_epochs):
    total_loss_epoch = 0

    pbar = tqdm(total=len(train_dataset_latents), desc=f"Epoch {epoch+1}/{diffusion_epochs}")
    for step, x0_batch_latents in enumerate(train_dataset_latents):
        # x0_batch_latents are the "clean" 2D latent vectors from AE
        current_batch_size = tf.shape(x0_batch_latents)[0]

        # Sample random timesteps t for this batch
        t_batch = np.random.randint(0, T_diffusion, size=current_batch_size)

        with tf.GradientTape() as tape:
            # Sample random noise $\epsilon \sim N(0,I)$
            noise_batch = tf.random.normal(shape=tf.shape(x0_batch_latents))

            # Get $\sqrt{\bar{\alpha}_t}$ and $\sqrt{1-\bar{\alpha}_t}$ for the sampled timesteps
            # These need to be gathered according to t_batch and reshaped for broadcasting
            sqrt_at_batch = tf.gather(sqrt_alphas_cumprod, t_batch)
            sqrt_one_minus_at_batch = tf.gather(sqrt_one_minus_alphas_cumprod, t_batch)

            # Reshape for broadcasting: (batch_size,) -> (batch_size, 1) for 2D latents
            sqrt_at_batch = tf.reshape(sqrt_at_batch, [current_batch_size, 1])
            sqrt_one_minus_at_batch = tf.reshape(sqrt_one_minus_at_batch, [current_batch_size, 1])

            # Construct noisy latents: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
            xt_batch_noisy_latents = sqrt_at_batch * x0_batch_latents + sqrt_one_minus_at_batch * noise_batch

            # Predict noise using the diffusion model
            # Ensure t_batch is correctly shaped for the model: (batch_size, 1)
            t_batch_reshaped_for_model = tf.reshape(t_batch, [current_batch_size, 1])
            predicted_noise_batch = diffusion_model_mlp([xt_batch_noisy_latents, t_batch_reshaped_for_model], training=True)

            # Calculate loss: MSE between actual noise and predicted noise
            loss_value = loss_fn_diffusion(noise_batch, predicted_noise_batch)

        # Compute gradients and update weights
        grads = tape.gradient(loss_value, diffusion_model_mlp.trainable_weights)
        optimizer_diffusion.apply_gradients(zip(grads, diffusion_model_mlp.trainable_weights))

        total_loss_epoch += loss_value.numpy()
        pbar.update(1)
        pbar.set_postfix_str(f"Loss: {loss_value.numpy():.4f}")

    pbar.close()
    avg_loss_epoch = total_loss_epoch / (step + 1)
    diffusion_loss_history.append(avg_loss_epoch)
    print(f"Epoch {epoch+1}, Average Loss: {avg_loss_epoch:.4f}")

# Plot diffusion model training loss
plt.figure()
plt.plot(diffusion_loss_history)
plt.title('Diffusion Model (on Latents) Training Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.grid(True)
plt.show()
# Output:
#   Diffusion Model (MLP for Latents) Summary:

#   [1mModel: "functional_1"[0m

#   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“

#   â”ƒ[1m [0m[1mLayer (type)       [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape     [0m[1m [0mâ”ƒ[1m [0m[1m   Param #[0m[1m [0mâ”ƒ[1m [0m[1mConnected to     [0m[1m [0mâ”ƒ

#   â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©

#   â”‚ timestep_input      â”‚ ([38;5;45mNone[0m, [38;5;34m1[0m)         â”‚          [38;5;34m0[0m â”‚ -                 â”‚

#   â”‚ ([38;5;33mInputLayer[0m)        â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ embedding_1         â”‚ ([38;5;45mNone[0m, [38;5;34m1[0m, [38;5;34m128[0m)    â”‚    [38;5;34m128,000[0m â”‚ timestep_input[[38;5;34m0[0mâ€¦ â”‚

#   â”‚ ([38;5;33mEmbedding[0m)         â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ latent_input        â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)         â”‚          [38;5;34m0[0m â”‚ -                 â”‚

#   â”‚ ([38;5;33mInputLayer[0m)        â”‚                   â”‚            â”‚                   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ flatten_3 ([38;5;33mFlatten[0m) â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚          [38;5;34m0[0m â”‚ embedding_1[[38;5;34m0[0m][[38;5;34m0[0m] â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ concatenate_1       â”‚ ([38;5;45mNone[0m, [38;5;34m130[0m)       â”‚          [38;5;34m0[0m â”‚ latent_input[[38;5;34m0[0m][[38;5;34mâ€¦[0m â”‚

#   â”‚ ([38;5;33mConcatenate[0m)       â”‚                   â”‚            â”‚ flatten_3[[38;5;34m0[0m][[38;5;34m0[0m]   â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_11 ([38;5;33mDense[0m)    â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚     [38;5;34m16,768[0m â”‚ concatenate_1[[38;5;34m0[0m]â€¦ â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_12 ([38;5;33mDense[0m)    â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚     [38;5;34m16,512[0m â”‚ dense_11[[38;5;34m0[0m][[38;5;34m0[0m]    â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_13 ([38;5;33mDense[0m)    â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)       â”‚     [38;5;34m16,512[0m â”‚ dense_12[[38;5;34m0[0m][[38;5;34m0[0m]    â”‚

#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

#   â”‚ dense_14 ([38;5;33mDense[0m)    â”‚ ([38;5;45mNone[0m, [38;5;34m2[0m)         â”‚        [38;5;34m258[0m â”‚ dense_13[[38;5;34m0[0m][[38;5;34m0[0m]    â”‚

#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#   [1m Total params: [0m[38;5;34m178,050[0m (695.51 KB)

#   [1m Trainable params: [0m[38;5;34m178,050[0m (695.51 KB)

#   [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)

#   

#   Training Diffusion Model on Latent Embeddings for 150 epochs...

#   Epoch 1/150:   0%|          | 0/16 [02:56<?, ?it/s]

#   2025-05-21 17:49:22.840244: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 1/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 39.30it/s, Loss: 0.7160]

#   Epoch 1, Average Loss: 0.9000

#   Epoch 2/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 51.43it/s, Loss: 0.6884]2025-05-21 17:49:23.152384: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 2/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 51.57it/s, Loss: 0.6884]

#   Epoch 2, Average Loss: 0.8082

#   Epoch 3/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 51.26it/s, Loss: 0.6672]

#   Epoch 3, Average Loss: 0.7215

#   Epoch 4/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.66it/s, Loss: 0.5183]2025-05-21 17:49:23.787722: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 4/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.82it/s, Loss: 0.5183]

#   Epoch 4, Average Loss: 0.6739

#   Epoch 5/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 51.49it/s, Loss: 0.6100]

#   Epoch 5, Average Loss: 0.6348

#   Epoch 6/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 42.09it/s, Loss: 0.7230]

#   Epoch 6, Average Loss: 0.6478

#   Epoch 7/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.72it/s, Loss: 0.4873]

#   Epoch 7, Average Loss: 0.6044

#   Epoch 8/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.98it/s, Loss: 0.6994]2025-05-21 17:49:25.144465: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 8/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.95it/s, Loss: 0.6994]

#   Epoch 8, Average Loss: 0.5993

#   Epoch 9/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.83it/s, Loss: 0.5525]

#   Epoch 9, Average Loss: 0.5999

#   Epoch 10/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.69it/s, Loss: 0.4500]

#   Epoch 10, Average Loss: 0.6249

#   Epoch 11/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.60it/s, Loss: 0.5908]

#   Epoch 11, Average Loss: 0.6160

#   Epoch 12/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.54it/s, Loss: 0.6659]

#   Epoch 12, Average Loss: 0.5700

#   Epoch 13/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 41.98it/s, Loss: 0.5271]

#   Epoch 13, Average Loss: 0.5625

#   Epoch 14/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.50it/s, Loss: 1.0294]

#   Epoch 14, Average Loss: 0.5551

#   Epoch 15/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.96it/s, Loss: 0.3769]

#   Epoch 15, Average Loss: 0.5327

#   Epoch 16/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.04it/s, Loss: 0.5343]2025-05-21 17:49:27.914019: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 16/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.52it/s, Loss: 0.5343]

#   Epoch 16, Average Loss: 0.5249

#   Epoch 17/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.60it/s, Loss: 0.4136]

#   Epoch 17, Average Loss: 0.4905

#   Epoch 18/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.02it/s, Loss: 0.5717]

#   Epoch 18, Average Loss: 0.4675

#   Epoch 19/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 44.56it/s, Loss: 0.3535]

#   Epoch 19, Average Loss: 0.4439

#   Epoch 20/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 41.97it/s, Loss: 0.4244]

#   Epoch 20, Average Loss: 0.4552

#   Epoch 21/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.67it/s, Loss: 0.3763]

#   Epoch 21, Average Loss: 0.4312

#   Epoch 22/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.30it/s, Loss: 0.5470]

#   Epoch 22, Average Loss: 0.4338

#   Epoch 23/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.37it/s, Loss: 0.2886]

#   Epoch 23, Average Loss: 0.4621

#   Epoch 24/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.85it/s, Loss: 0.6034]

#   Epoch 24, Average Loss: 0.4294

#   Epoch 25/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 41.61it/s, Loss: 0.3871]

#   Epoch 25, Average Loss: 0.4075

#   Epoch 26/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.63it/s, Loss: 0.4527]

#   Epoch 26, Average Loss: 0.4279

#   Epoch 27/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.83it/s, Loss: 0.3438]

#   Epoch 27, Average Loss: 0.4591

#   Epoch 28/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.52it/s, Loss: 0.4528]

#   Epoch 28, Average Loss: 0.4067

#   Epoch 29/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.79it/s, Loss: 0.3850]

#   Epoch 29, Average Loss: 0.4341

#   Epoch 30/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.88it/s, Loss: 0.4088]

#   Epoch 30, Average Loss: 0.4090

#   Epoch 31/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.85it/s, Loss: 0.3906]

#   Epoch 31, Average Loss: 0.4115

#   Epoch 32/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.52it/s, Loss: 0.3545]2025-05-21 17:49:33.509725: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 32/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 44.27it/s, Loss: 0.3545]

#   Epoch 32, Average Loss: 0.4246

#   Epoch 33/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.81it/s, Loss: 0.3578]

#   Epoch 33, Average Loss: 0.4065

#   Epoch 34/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.16it/s, Loss: 0.3910]

#   Epoch 34, Average Loss: 0.3926

#   Epoch 35/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.52it/s, Loss: 0.4152]

#   Epoch 35, Average Loss: 0.4239

#   Epoch 36/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.53it/s, Loss: 0.2703]

#   Epoch 36, Average Loss: 0.4144

#   Epoch 37/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 42.09it/s, Loss: 0.4236]

#   Epoch 37, Average Loss: 0.3830

#   Epoch 38/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.34it/s, Loss: 0.2783]

#   Epoch 38, Average Loss: 0.4031

#   Epoch 39/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.27it/s, Loss: 0.2495]

#   Epoch 39, Average Loss: 0.3964

#   Epoch 40/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.00it/s, Loss: 0.4339]

#   Epoch 40, Average Loss: 0.4100

#   Epoch 41/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.21it/s, Loss: 0.4018]

#   Epoch 41, Average Loss: 0.4197

#   Epoch 42/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 51.64it/s, Loss: 0.3645]

#   Epoch 42, Average Loss: 0.4037

#   Epoch 43/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.51it/s, Loss: 0.3681]

#   Epoch 43, Average Loss: 0.4095

#   Epoch 44/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 44.03it/s, Loss: 0.3531]

#   Epoch 44, Average Loss: 0.3959

#   Epoch 45/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.58it/s, Loss: 0.4992]

#   Epoch 45, Average Loss: 0.4112

#   Epoch 46/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.60it/s, Loss: 0.5125]

#   Epoch 46, Average Loss: 0.4138

#   Epoch 47/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.26it/s, Loss: 0.4363]

#   Epoch 47, Average Loss: 0.4004

#   Epoch 48/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 36.04it/s, Loss: 0.3828]

#   Epoch 48, Average Loss: 0.4163

#   Epoch 49/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 43.75it/s, Loss: 0.4909]

#   Epoch 49, Average Loss: 0.4298

#   Epoch 50/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.54it/s, Loss: 0.5631]

#   Epoch 50, Average Loss: 0.4060

#   Epoch 51/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.35it/s, Loss: 0.3476]

#   Epoch 51, Average Loss: 0.3820

#   Epoch 52/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.99it/s, Loss: 0.5081]

#   Epoch 52, Average Loss: 0.4185

#   Epoch 53/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.21it/s, Loss: 0.4797]

#   Epoch 53, Average Loss: 0.4036

#   Epoch 54/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.22it/s, Loss: 0.4559]

#   Epoch 54, Average Loss: 0.3973

#   Epoch 55/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 43.74it/s, Loss: 0.3942]

#   Epoch 55, Average Loss: 0.3943

#   Epoch 56/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.37it/s, Loss: 0.3547]

#   Epoch 56, Average Loss: 0.4072

#   Epoch 57/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.80it/s, Loss: 0.2724]

#   Epoch 57, Average Loss: 0.4034

#   Epoch 58/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.05it/s, Loss: 0.3535]

#   Epoch 58, Average Loss: 0.3872

#   Epoch 59/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.63it/s, Loss: 0.5095]

#   Epoch 59, Average Loss: 0.4212

#   Epoch 60/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.77it/s, Loss: 0.5114]

#   Epoch 60, Average Loss: 0.4049

#   Epoch 61/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.69it/s, Loss: 0.4536]

#   Epoch 61, Average Loss: 0.3999

#   Epoch 62/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.04it/s, Loss: 0.5538]

#   Epoch 62, Average Loss: 0.3996

#   Epoch 63/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.60it/s, Loss: 0.4508]

#   Epoch 63, Average Loss: 0.4057

#   Epoch 64/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.28it/s, Loss: 0.3124]2025-05-21 17:49:44.362138: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 64/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.98it/s, Loss: 0.3124]

#   Epoch 64, Average Loss: 0.3900

#   Epoch 65/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.86it/s, Loss: 0.3768]

#   Epoch 65, Average Loss: 0.3945

#   Epoch 66/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.14it/s, Loss: 0.5620]

#   Epoch 66, Average Loss: 0.4204

#   Epoch 67/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.38it/s, Loss: 0.3400]

#   Epoch 67, Average Loss: 0.4075

#   Epoch 68/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.44it/s, Loss: 0.4062]

#   Epoch 68, Average Loss: 0.3910

#   Epoch 69/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.49it/s, Loss: 0.4447]

#   Epoch 69, Average Loss: 0.4087

#   Epoch 70/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.20it/s, Loss: 0.3140]

#   Epoch 70, Average Loss: 0.4132

#   Epoch 71/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 52.10it/s, Loss: 0.3908]

#   Epoch 71, Average Loss: 0.4295

#   Epoch 72/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.28it/s, Loss: 0.5056]

#   Epoch 72, Average Loss: 0.4159

#   Epoch 73/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.97it/s, Loss: 0.5149]

#   Epoch 73, Average Loss: 0.3996

#   Epoch 74/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.21it/s, Loss: 0.4954]

#   Epoch 74, Average Loss: 0.4082

#   Epoch 75/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.01it/s, Loss: 0.4211]

#   Epoch 75, Average Loss: 0.4149

#   Epoch 76/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.23it/s, Loss: 0.4551]

#   Epoch 76, Average Loss: 0.4104

#   Epoch 77/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.13it/s, Loss: 0.3023]

#   Epoch 77, Average Loss: 0.3869

#   Epoch 78/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.54it/s, Loss: 0.2454]

#   Epoch 78, Average Loss: 0.3881

#   Epoch 79/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 51.61it/s, Loss: 0.2538]

#   Epoch 79, Average Loss: 0.3813

#   Epoch 80/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.10it/s, Loss: 0.3654]

#   Epoch 80, Average Loss: 0.3794

#   Epoch 81/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.64it/s, Loss: 0.4429]

#   Epoch 81, Average Loss: 0.3899

#   Epoch 82/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.48it/s, Loss: 0.4909]

#   Epoch 82, Average Loss: 0.4155

#   Epoch 83/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 44.92it/s, Loss: 0.4016]

#   Epoch 83, Average Loss: 0.3944

#   Epoch 84/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.95it/s, Loss: 0.2176]

#   Epoch 84, Average Loss: 0.3717

#   Epoch 85/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.16it/s, Loss: 0.2794]

#   Epoch 85, Average Loss: 0.3767

#   Epoch 86/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.52it/s, Loss: 0.4013]

#   Epoch 86, Average Loss: 0.4297

#   Epoch 87/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.97it/s, Loss: 0.3953]

#   Epoch 87, Average Loss: 0.4062

#   Epoch 88/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.49it/s, Loss: 0.4400]

#   Epoch 88, Average Loss: 0.4040

#   Epoch 89/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.49it/s, Loss: 0.3969]

#   Epoch 89, Average Loss: 0.3866

#   Epoch 90/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.75it/s, Loss: 0.3322]

#   Epoch 90, Average Loss: 0.4099

#   Epoch 91/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.35it/s, Loss: 0.1778]

#   Epoch 91, Average Loss: 0.3948

#   Epoch 92/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.75it/s, Loss: 0.5215]

#   Epoch 92, Average Loss: 0.4224

#   Epoch 93/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.26it/s, Loss: 0.5303]

#   Epoch 93, Average Loss: 0.3972

#   Epoch 94/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.10it/s, Loss: 0.6430]

#   Epoch 94, Average Loss: 0.3887

#   Epoch 95/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.04it/s, Loss: 0.4489]

#   Epoch 95, Average Loss: 0.4205

#   Epoch 96/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.19it/s, Loss: 0.3245]

#   Epoch 96, Average Loss: 0.4086

#   Epoch 97/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 43.30it/s, Loss: 0.2846]

#   Epoch 97, Average Loss: 0.4186

#   Epoch 98/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.32it/s, Loss: 0.2827]

#   Epoch 98, Average Loss: 0.4086

#   Epoch 99/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.60it/s, Loss: 0.4699]

#   Epoch 99, Average Loss: 0.4129

#   Epoch 100/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.79it/s, Loss: 0.3987]

#   Epoch 100, Average Loss: 0.3849

#   Epoch 101/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.20it/s, Loss: 0.3471]

#   Epoch 101, Average Loss: 0.3932

#   Epoch 102/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.59it/s, Loss: 0.2898]

#   Epoch 102, Average Loss: 0.4093

#   Epoch 103/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.69it/s, Loss: 0.4296]

#   Epoch 103, Average Loss: 0.4083

#   Epoch 104/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 44.58it/s, Loss: 0.3866]

#   Epoch 104, Average Loss: 0.3939

#   Epoch 105/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.29it/s, Loss: 0.4066]

#   Epoch 105, Average Loss: 0.4070

#   Epoch 106/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.01it/s, Loss: 0.4822]

#   Epoch 106, Average Loss: 0.3912

#   Epoch 107/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.17it/s, Loss: 0.3761]

#   Epoch 107, Average Loss: 0.3789

#   Epoch 108/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.20it/s, Loss: 0.2415]

#   Epoch 108, Average Loss: 0.3927

#   Epoch 109/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.39it/s, Loss: 0.3792]

#   Epoch 109, Average Loss: 0.3918

#   Epoch 110/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.48it/s, Loss: 0.5150]

#   Epoch 110, Average Loss: 0.3984

#   Epoch 111/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 43.78it/s, Loss: 0.4466]

#   Epoch 111, Average Loss: 0.3933

#   Epoch 112/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.74it/s, Loss: 0.4537]

#   Epoch 112, Average Loss: 0.3922

#   Epoch 113/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.13it/s, Loss: 0.3095]

#   Epoch 113, Average Loss: 0.3926

#   Epoch 114/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.31it/s, Loss: 0.4284]

#   Epoch 114, Average Loss: 0.3951

#   Epoch 115/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.71it/s, Loss: 0.3562]

#   Epoch 115, Average Loss: 0.3772

#   Epoch 116/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.47it/s, Loss: 0.4245]

#   Epoch 116, Average Loss: 0.4064

#   Epoch 117/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.10it/s, Loss: 0.3267]

#   Epoch 117, Average Loss: 0.3752

#   Epoch 118/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 41.19it/s, Loss: 0.2750]

#   Epoch 118, Average Loss: 0.4067

#   Epoch 119/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.73it/s, Loss: 0.3482]

#   Epoch 119, Average Loss: 0.3579

#   Epoch 120/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.77it/s, Loss: 0.4690]

#   Epoch 120, Average Loss: 0.3990

#   Epoch 121/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.26it/s, Loss: 0.2612]

#   Epoch 121, Average Loss: 0.4071

#   Epoch 122/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.61it/s, Loss: 0.3938]

#   Epoch 122, Average Loss: 0.3881

#   Epoch 123/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.92it/s, Loss: 0.2648]

#   Epoch 123, Average Loss: 0.3800

#   Epoch 124/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.08it/s, Loss: 0.3807]

#   Epoch 124, Average Loss: 0.3739

#   Epoch 125/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 42.94it/s, Loss: 0.5605]

#   Epoch 125, Average Loss: 0.4004

#   Epoch 126/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.35it/s, Loss: 0.4806]

#   Epoch 126, Average Loss: 0.3832

#   Epoch 127/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.73it/s, Loss: 0.3313]

#   Epoch 127, Average Loss: 0.3735

#   Epoch 128/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.07it/s, Loss: 0.3597]2025-05-21 17:50:05.665364: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

#   Epoch 128/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.98it/s, Loss: 0.3597]

#   Epoch 128, Average Loss: 0.3860

#   Epoch 129/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.96it/s, Loss: 0.4170]

#   Epoch 129, Average Loss: 0.3994

#   Epoch 130/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.60it/s, Loss: 0.2188]

#   Epoch 130, Average Loss: 0.3730

#   Epoch 131/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.69it/s, Loss: 0.3629]

#   Epoch 131, Average Loss: 0.4070

#   Epoch 132/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 41.20it/s, Loss: 0.3376]

#   Epoch 132, Average Loss: 0.3910

#   Epoch 133/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.43it/s, Loss: 0.2251]

#   Epoch 133, Average Loss: 0.3939

#   Epoch 134/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.50it/s, Loss: 0.5918]

#   Epoch 134, Average Loss: 0.4154

#   Epoch 135/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.10it/s, Loss: 0.2768]

#   Epoch 135, Average Loss: 0.3678

#   Epoch 136/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.61it/s, Loss: 0.4494]

#   Epoch 136, Average Loss: 0.3875

#   Epoch 137/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.39it/s, Loss: 0.2968]

#   Epoch 137, Average Loss: 0.4195

#   Epoch 138/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 46.64it/s, Loss: 0.3194]

#   Epoch 138, Average Loss: 0.3737

#   Epoch 139/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 44.46it/s, Loss: 0.4344]

#   Epoch 139, Average Loss: 0.3878

#   Epoch 140/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.18it/s, Loss: 0.4381]

#   Epoch 140, Average Loss: 0.3971

#   Epoch 141/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.47it/s, Loss: 0.2705]

#   Epoch 141, Average Loss: 0.3936

#   Epoch 142/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.08it/s, Loss: 0.4257]

#   Epoch 142, Average Loss: 0.4083

#   Epoch 143/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.57it/s, Loss: 0.2610]

#   Epoch 143, Average Loss: 0.3800

#   Epoch 144/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.53it/s, Loss: 0.5402]

#   Epoch 144, Average Loss: 0.4015

#   Epoch 145/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.01it/s, Loss: 0.4327]

#   Epoch 145, Average Loss: 0.3989

#   Epoch 146/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 45.05it/s, Loss: 0.2082]

#   Epoch 146, Average Loss: 0.3734

#   Epoch 147/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 48.52it/s, Loss: 0.5984]

#   Epoch 147, Average Loss: 0.3939

#   Epoch 148/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 47.95it/s, Loss: 0.2684]

#   Epoch 148, Average Loss: 0.4005

#   Epoch 149/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 51.46it/s, Loss: 0.3850]

#   Epoch 149, Average Loss: 0.4060

#   Epoch 150/150: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 50.14it/s, Loss: 0.4017]

#   Epoch 150, Average Loss: 0.4049

#   <Figure size 640x480 with 1 Axes>

def sample_latents_from_diffusion(model, num_samples, latent_dim, T, alphas_param, alphas_cumprod_param, sigmas_param):
    """Samples latent vectors from the trained diffusion model."""
    # Start with random noise $x_T \sim N(0,I)$
    x_t = tf.random.normal(shape=(num_samples, latent_dim))

    for t_val_rev in trange(T - 1, -1, -1, desc="Diffusion Sampling"): # Iterate t from T-1 down to 0
        t_batch_model = tf.ones((num_samples, 1), dtype=tf.int32) * t_val_rev

        # Sample noise z (unless t=0)
        z = tf.random.normal(shape=(num_samples, latent_dim)) if t_val_rev > 0 else tf.zeros((num_samples, latent_dim))

        # Predict noise $\epsilon_\theta(x_t, t)$
        eps = model.predict([x_t, t_batch_model], verbose=0)

        # Get constants for this timestep t
        alpha_t = alphas_param[t_val_rev]
        alpha_t_bar = alphas_cumprod_param[t_val_rev]
        sigma_t = sigmas_param[t_val_rev] # $\sigma_t = \sqrt{\beta_t}$ or $\sqrt{\tilde{\beta}_t}$ depending on DDPM/DDIM
                                            # Example notebook uses $\sigma_t = \sqrt{\beta_t}$

        # Reshape for broadcasting if necessary (not needed here as all are scalars for 2D)

        # DDPM sampling step formula:
        # $x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t)) + \sigma_t z$
        term1_coeff = 1.0 / np.sqrt(alpha_t)
        term2_coeff = (1.0 - alpha_t) / np.sqrt(1.0 - alpha_t_bar)

        x_t_minus_1 = term1_coeff * (x_t - term2_coeff * eps) + sigma_t * z
        x_t = x_t_minus_1

    return x_t.numpy() # Final $x_0$ samples

# Sample 1000 latent vectors
num_eval_samples_diffusion = 1000
print(f"\nSampling {num_eval_samples_diffusion} latent vectors using the trained diffusion model...")
sampled_latents_diffusion = sample_latents_from_diffusion(
    diffusion_model_mlp,
    num_eval_samples_diffusion,
    latent_dim_for_diffusion,
    T_diffusion,
    alphas, # Global alphas array
    alphas_cumprod, # Global alphas_cumprod array
    sigmas_diffusion # Global sigmas_diffusion array
)
print(f"Shape of sampled latents: {sampled_latents_diffusion.shape}")

# Plot training embeddings and sampled points
plt.figure(figsize=(10, 8))
plt.scatter(latent_embeddings_train[:, 0], latent_embeddings_train[:, 1],
            s=10, alpha=0.3, label='AE Training Embeddings (Originals)')
plt.scatter(sampled_latents_diffusion[:, 0], sampled_latents_diffusion[:, 1],
            s=10, alpha=0.5, color='red', label='Diffusion Sampled Latents')
plt.title('Comparison of AE Latent Space and Diffusion Model Samples')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')
plt.legend()
plt.grid(True)
plt.axis('equal') # Helps visualize distribution shape
plt.show()

# Decode the first 10 sampled latent vectors into images
num_decode_display = 10
first_10_sampled_latents = sampled_latents_diffusion[:num_decode_display]

print(f"\nDecoding the first {num_decode_display} sampled latent vectors into images...")
decoded_images_from_diffusion = decoder.predict(first_10_sampled_latents, verbose=0)

plt.figure(figsize=(num_decode_display * 1.5, 3)) # Adjusted for 10 images
for i in range(num_decode_display):
    plt.subplot(2, num_decode_display // 2, i + 1) # Assuming 2 rows
    img_display = (decoded_images_from_diffusion[i] + 1.0) / 2.0 # De-normalize
    plt.imshow(np.squeeze(np.clip(img_display, 0, 1)))
    plt.title(f"Sample {i+1}")
    plt.axis('off')
plt.suptitle("Images Decoded from Diffusion Model Latent Samples")
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()
# Output:
#   

#   Sampling 1000 latent vectors using the trained diffusion model...

#   Diffusion Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:48<00:00, 20.42it/s]

#   Shape of sampled latents: (1000, 2)

#   <Figure size 1000x800 with 1 Axes>
#   

#   Decoding the first 10 sampled latent vectors into images...

#   <Figure size 1500x300 with 10 Axes>

"""
The diffusion model appears to have learned the general distribution of the autoencoder's 2D latent space reasonably well. The scatter plot shows that the 1000 sampled red points largely overlap with the original blue AE training embeddings, capturing the overall shape and density, though with some diffusion samples extending slightly beyond the original training data's boundaries. The 10 decoded images are recognizable as blurry, grayscale faces, similar in style to the Frey dataset, albeit lacking fine detail due to the AE's 2D bottleneck. There's some evidence of mode preference rather than severe mode collapse; while not identical, several decoded faces share strong resemblances, suggesting the diffusion model might favor certain regions of the latent space, or that the 2D latent space itself limits the expressible diversity. But the dataset isn't super diverse to begin with, so doesn't affect outputs too much. It doesn't seem to generate the full nuanced variety of the entire training set, but it avoids collapsing to a single output.
"""



================================================
File: hw3/temp.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# **Part 2: Latent Diffusion Model on Frey Dataset**

In this part, we will build a latent diffusion model on the Frey dataset.
1.  Train an MLP autoencoder on Frey images to get a 2D latent space.
2.  Train a diffusion model on these 2D latent embeddings.
3.  Sample from the diffusion model in latent space, then decode these latents into images using the autoencoder's decoder.
"""

# General imports (some might have been imported in Part 1)
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
from tensorflow.keras.utils import get_file
from scipy.io import loadmat
import matplotlib.pyplot as plt
import matplotlib as mpl
# Ensure cmap is gray for images if not set globally before
if 'style' not in mpl.rcParams: # A simple check if some rcParams were already set
    mpl.rc('image', cmap='gray')
from tqdm.auto import tqdm, trange # For progress bars

"""
## Load Frey Dataset
We'll reuse the function from Part 1 or the example notebook to load the Frey dataset.
The data should be normalized to `[-1, 1]`.
"""

def get_frey_data():
    """Loads and preprocesses the Frey dataset.

    Returns:
        numpy.ndarray: Frey face images, normalized to [-1, 1]
                       with shape (N, 28, 20, 1).
    """
    try:
        path = get_file('frey_rawface.mat',
                        'https://www.dropbox.com/scl/fi/m70sh4ef39pvy01czc63r/frey_rawface.mat?rlkey=5v6meiap55z68ada2roxwxuql&dl=1',
                        cache_subdir='datasets')
        data = np.transpose(loadmat(path)['ff'])
        x_train_frey = np.reshape(data, (-1, 28, 20, 1))
        x_train_frey = x_train_frey.astype('float32')
        x_train_frey /= 255.0  # Normalize to [0, 1]
        x_train_frey = (x_train_frey * 2.0) - 1.0  # Normalize to [-1, 1]
        return x_train_frey
    except Exception as e:
        print(f"Error loading Frey dataset: {e}")
        return None

x_train_frey = get_frey_data()

if x_train_frey is not None:
    IMG_H, IMG_W, IMG_C = x_train_frey.shape[1:]
    FLAT_DIM = IMG_H * IMG_W * IMG_C
    print(f"Frey dataset loaded: {x_train_frey.shape}")

    # Display a few example images
    plt.figure(figsize=(10, 2))
    for i in range(5):
        plt.subplot(1, 5, i + 1)
        img_display = (x_train_frey[i] + 1.0) / 2.0 # De-normalize for display
        plt.imshow(np.squeeze(img_display))
        plt.axis('off')
    plt.suptitle("Sample Frey Dataset Images")
    plt.show()
else:
    print("Halting Part 2 due to dataset loading failure.")
    # In a real notebook, you'd probably stop execution here or handle x_train_frey being None

"""
## 2.1: Build an Autoencoder

Train an MLP autoencoder on the Frey dataset with a 2-dimensional bottleneck.
After training, extract and store the embedding vectors for the training images.
"""

if x_train_frey is not None:
    LATENT_DIM_AE = 2 # Bottleneck dimensionality

    # Define Encoder
    encoder_inputs = layers.Input(shape=(IMG_H, IMG_W, IMG_C), name='encoder_input')
    x = layers.Flatten()(encoder_inputs)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    encoder_outputs = layers.Dense(LATENT_DIM_AE, activation=None, name='encoder_output')(x) # Linear activation for bottleneck
    encoder = Model(encoder_inputs, encoder_outputs, name='encoder')
    print("Encoder Summary:")
    encoder.summary()

    # Define Decoder
    decoder_inputs = layers.Input(shape=(LATENT_DIM_AE,), name='decoder_input')
    x = layers.Dense(64, activation='relu')(decoder_inputs)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(FLAT_DIM, activation='tanh')(x) # Tanh because input images are [-1, 1]
    decoder_outputs = layers.Reshape((IMG_H, IMG_W, IMG_C), name='decoder_output')(x)
    decoder = Model(decoder_inputs, decoder_outputs, name='decoder')
    print("\nDecoder Summary:")
    decoder.summary()

    # Define Autoencoder
    autoencoder_inputs = layers.Input(shape=(IMG_H, IMG_W, IMG_C), name='ae_input')
    encoded = encoder(autoencoder_inputs)
    decoded = decoder(encoded)
    autoencoder = Model(autoencoder_inputs, decoded, name='autoencoder')
    print("\nAutoencoder Summary:")
    autoencoder.summary()

    # Compile and Train Autoencoder
    autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mse')

    ae_epochs = 50 # Adjust as needed; might need more for good reconstruction
    ae_batch_size = 32

    print(f"\nTraining Autoencoder for {ae_epochs} epochs...")
    history_ae = autoencoder.fit(x_train_frey, x_train_frey,
                                 epochs=ae_epochs,
                                 batch_size=ae_batch_size,
                                 shuffle=True,
                                 validation_split=0.1, # Optional: use a validation split
                                 verbose=1) # Set to 1 or 2 for progress, 0 for silent

    # Plot training loss
    plt.figure()
    plt.plot(history_ae.history['loss'], label='Training Loss')
    if 'val_loss' in history_ae.history:
        plt.plot(history_ae.history['val_loss'], label='Validation Loss')
    plt.title('Autoencoder Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.show()

    # Display some original and reconstructed images
    num_display = 5
    random_indices = np.random.choice(len(x_train_frey), num_display, replace=False)
    originals = x_train_frey[random_indices]
    reconstructions = autoencoder.predict(originals, verbose=0)

    plt.figure(figsize=(num_display * 2, 4))
    for i in range(num_display):
        # Original
        plt.subplot(2, num_display, i + 1)
        plt.imshow(np.squeeze((originals[i] + 1.0) / 2.0))
        plt.title("Original")
        plt.axis('off')
        # Reconstruction
        plt.subplot(2, num_display, num_display + i + 1)
        plt.imshow(np.squeeze((reconstructions[i] + 1.0) / 2.0))
        plt.title("Recon.")
        plt.axis('off')
    plt.suptitle("Autoencoder: Original vs. Reconstructed")
    plt.show()

    # Extract and store latent embeddings for the training set
    print("\nExtracting latent embeddings from the training set...")
    latent_embeddings_train = encoder.predict(x_train_frey, batch_size=ae_batch_size, verbose=0)
    print(f"Shape of latent embeddings: {latent_embeddings_train.shape}") # Should be (num_frey_images, 2)

    # Plot the 2D latent space
    plt.figure(figsize=(8, 6))
    plt.scatter(latent_embeddings_train[:, 0], latent_embeddings_train[:, 1], s=5, alpha=0.5)
    plt.title('2D Latent Space of Frey Dataset (from Autoencoder)')
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.grid(True)
    plt.show()

"""
## 2.2: Build and train the reverse process model

This model will operate on the 2D latent embeddings. It takes a (noisy) 2D embedding and a timestep `t`, and outputs a predicted 2D noise vector.
"""

if x_train_frey is not None and 'latent_embeddings_train' in locals():
    # Diffusion Hyperparameters (from example, can be tuned)
    T_diffusion = 1000  # Number of timesteps
    betas = np.linspace(1e-4, 0.02, T_diffusion)
    sigmas_diffusion = np.sqrt(betas) # $\sigma_t = \sqrt{\beta_t}$
    alphas = 1.0 - betas
    alphas_cumprod = np.cumprod(alphas, axis=-1) # $\bar{\alpha}_t$

    # For the forward process $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
    # we'll need sqrt_alphas_cumprod and sqrt_one_minus_alphas_cumprod
    sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)
    sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)


    # Reverse Process Model (MLP for 2D latent space)
    def build_reverse_process_model_mlp(latent_dim, time_embedding_dim, num_hidden_layers=3, hidden_layer_width=128):
        """Builds the MLP reverse process model for the latent space."""
        latent_input = layers.Input(shape=(latent_dim,), name='latent_input') # e.g., (2,)
        timestep_input = layers.Input(shape=(1,), name='timestep_input')     # Scalar timestep

        # Timestep embedding
        time_embed = layers.Embedding(input_dim=T_diffusion, output_dim=time_embedding_dim,
                                      embeddings_initializer='glorot_normal')(timestep_input)
        time_embed = layers.Flatten()(time_embed) # Shape from (batch, 1, D) to (batch, D)

        # Concatenate latent vector with time embedding
        merged_input = layers.Concatenate(axis=-1)([latent_input, time_embed])

        # MLP
        x = merged_input
        for _ in range(num_hidden_layers):
            x = layers.Dense(hidden_layer_width, activation='relu')(x)

        # Output predicted noise (same dimension as latent_input)
        output_noise = layers.Dense(latent_dim, activation=None)(x) # Linear activation

        model = Model(inputs=[latent_input, timestep_input], outputs=output_noise)
        return model

    # Instantiate the model
    latent_dim_for_diffusion = LATENT_DIM_AE # Should be 2
    time_embedding_dim_diffusion = 128 # As suggested: "same size as your hidden layers"
    hidden_mlp_width = 128

    diffusion_model_mlp = build_reverse_process_model_mlp(
        latent_dim_for_diffusion,
        time_embedding_dim_diffusion,
        hidden_layer_width=hidden_mlp_width
    )
    print("Diffusion Model (MLP for Latents) Summary:")
    diffusion_model_mlp.summary()

    # Training Setup
    diffusion_epochs = 150 # Adjust as needed
    diffusion_batch_size = 128
    diffusion_learning_rate = 3e-4 # Example uses 3e-4

    optimizer_diffusion = keras.optimizers.Adam(learning_rate=diffusion_learning_rate)
    loss_fn_diffusion = keras.losses.MeanSquaredError()

    # Prepare dataset from latent embeddings
    train_dataset_latents = tf.data.Dataset.from_tensor_slices(latent_embeddings_train)
    train_dataset_latents = train_dataset_latents.shuffle(buffer_size=len(latent_embeddings_train)).batch(diffusion_batch_size)

    print(f"\nTraining Diffusion Model on Latent Embeddings for {diffusion_epochs} epochs...")

    # Custom Training Loop (adapted from frey_diffusion_example.py)
    diffusion_loss_history = []
    for epoch in range(diffusion_epochs):
        total_loss_epoch = 0

        pbar = tqdm(total=len(train_dataset_latents), desc=f"Epoch {epoch+1}/{diffusion_epochs}")
        for step, x0_batch_latents in enumerate(train_dataset_latents):
            # x0_batch_latents are the "clean" 2D latent vectors from AE
            current_batch_size = tf.shape(x0_batch_latents)[0]

            # Sample random timesteps t for this batch
            t_batch = np.random.randint(0, T_diffusion, size=current_batch_size)

            with tf.GradientTape() as tape:
                # Sample random noise $\epsilon \sim N(0,I)$
                noise_batch = tf.random.normal(shape=tf.shape(x0_batch_latents))

                # Get $\sqrt{\bar{\alpha}_t}$ and $\sqrt{1-\bar{\alpha}_t}$ for the sampled timesteps
                # These need to be gathered according to t_batch and reshaped for broadcasting
                sqrt_at_batch = tf.gather(sqrt_alphas_cumprod, t_batch)
                sqrt_one_minus_at_batch = tf.gather(sqrt_one_minus_alphas_cumprod, t_batch)

                # Reshape for broadcasting: (batch_size,) -> (batch_size, 1) for 2D latents
                sqrt_at_batch = tf.reshape(sqrt_at_batch, [current_batch_size, 1])
                sqrt_one_minus_at_batch = tf.reshape(sqrt_one_minus_at_batch, [current_batch_size, 1])

                # Construct noisy latents: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
                xt_batch_noisy_latents = sqrt_at_batch * x0_batch_latents + sqrt_one_minus_at_batch * noise_batch

                # Predict noise using the diffusion model
                # Ensure t_batch is correctly shaped for the model: (batch_size, 1)
                t_batch_reshaped_for_model = tf.reshape(t_batch, [current_batch_size, 1])
                predicted_noise_batch = diffusion_model_mlp([xt_batch_noisy_latents, t_batch_reshaped_for_model], training=True)

                # Calculate loss: MSE between actual noise and predicted noise
                loss_value = loss_fn_diffusion(noise_batch, predicted_noise_batch)

            # Compute gradients and update weights
            grads = tape.gradient(loss_value, diffusion_model_mlp.trainable_weights)
            optimizer_diffusion.apply_gradients(zip(grads, diffusion_model_mlp.trainable_weights))

            total_loss_epoch += loss_value.numpy()
            pbar.update(1)
            pbar.set_postfix_str(f"Loss: {loss_value.numpy():.4f}")

        pbar.close()
        avg_loss_epoch = total_loss_epoch / (step + 1)
        diffusion_loss_history.append(avg_loss_epoch)
        print(f"Epoch {epoch+1}, Average Loss: {avg_loss_epoch:.4f}")

    # Plot diffusion model training loss
    plt.figure()
    plt.plot(diffusion_loss_history)
    plt.title('Diffusion Model (on Latents) Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.grid(True)
    plt.show()

"""
## 2.3: Evaluate the results

- Sample 1000 random 2D latent vectors using the diffusion sampling process.
- Plot the training embedding vectors and your sampled 2D points on top.
- Decode the first 10 sampled latent vectors into images.
- Discuss results.
"""

if x_train_frey is not None and 'diffusion_model_mlp' in locals() and 'decoder' in locals():

    def sample_latents_from_diffusion(model, num_samples, latent_dim, T, alphas_param, alphas_cumprod_param, sigmas_param):
        """Samples latent vectors from the trained diffusion model."""
        # Start with random noise $x_T \sim N(0,I)$
        x_t = tf.random.normal(shape=(num_samples, latent_dim))

        for t_val_rev in trange(T - 1, -1, -1, desc="Diffusion Sampling"): # Iterate t from T-1 down to 0
            t_batch_model = tf.ones((num_samples, 1), dtype=tf.int32) * t_val_rev

            # Sample noise z (unless t=0)
            z = tf.random.normal(shape=(num_samples, latent_dim)) if t_val_rev > 0 else tf.zeros((num_samples, latent_dim))

            # Predict noise $\epsilon_\theta(x_t, t)$
            eps = model.predict([x_t, t_batch_model], verbose=0)

            # Get constants for this timestep t
            alpha_t = alphas_param[t_val_rev]
            alpha_t_bar = alphas_cumprod_param[t_val_rev]
            sigma_t = sigmas_param[t_val_rev] # $\sigma_t = \sqrt{\beta_t}$ or $\sqrt{\tilde{\beta}_t}$ depending on DDPM/DDIM
                                              # Example notebook uses $\sigma_t = \sqrt{\beta_t}$

            # Reshape for broadcasting if necessary (not needed here as all are scalars for 2D)

            # DDPM sampling step formula:
            # $x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t)) + \sigma_t z$
            term1_coeff = 1.0 / np.sqrt(alpha_t)
            term2_coeff = (1.0 - alpha_t) / np.sqrt(1.0 - alpha_t_bar)

            x_t_minus_1 = term1_coeff * (x_t - term2_coeff * eps) + sigma_t * z
            x_t = x_t_minus_1

        return x_t.numpy() # Final $x_0$ samples

    # Sample 1000 latent vectors
    num_eval_samples_diffusion = 1000
    print(f"\nSampling {num_eval_samples_diffusion} latent vectors using the trained diffusion model...")
    sampled_latents_diffusion = sample_latents_from_diffusion(
        diffusion_model_mlp,
        num_eval_samples_diffusion,
        latent_dim_for_diffusion,
        T_diffusion,
        alphas, # Global alphas array
        alphas_cumprod, # Global alphas_cumprod array
        sigmas_diffusion # Global sigmas_diffusion array
    )
    print(f"Shape of sampled latents: {sampled_latents_diffusion.shape}")

    # Plot training embeddings and sampled points
    plt.figure(figsize=(10, 8))
    plt.scatter(latent_embeddings_train[:, 0], latent_embeddings_train[:, 1],
                s=10, alpha=0.3, label='AE Training Embeddings (Originals)')
    plt.scatter(sampled_latents_diffusion[:, 0], sampled_latents_diffusion[:, 1],
                s=10, alpha=0.5, color='red', label='Diffusion Sampled Latents')
    plt.title('Comparison of AE Latent Space and Diffusion Model Samples')
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.legend()
    plt.grid(True)
    plt.axis('equal') # Helps visualize distribution shape
    plt.show()

    # Decode the first 10 sampled latent vectors into images
    num_decode_display = 10
    first_10_sampled_latents = sampled_latents_diffusion[:num_decode_display]

    print(f"\nDecoding the first {num_decode_display} sampled latent vectors into images...")
    decoded_images_from_diffusion = decoder.predict(first_10_sampled_latents, verbose=0)

    plt.figure(figsize=(num_decode_display * 1.5, 3)) # Adjusted for 10 images
    for i in range(num_decode_display):
        plt.subplot(2, num_decode_display // 2, i + 1) # Assuming 2 rows
        img_display = (decoded_images_from_diffusion[i] + 1.0) / 2.0 # De-normalize
        plt.imshow(np.squeeze(np.clip(img_display, 0, 1)))
        plt.title(f"Sample {i+1}")
        plt.axis('off')
    plt.suptitle("Images Decoded from Diffusion Model Latent Samples")
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

"""
### Discussion of Latent Diffusion Results

*(Student to fill in based on the plots and images generated above):*

1.  **Accuracy of Latent Space Distribution Learning:**
    *   Compare the scatter plot of the original AE training embeddings with the scatter plot of the 1000 points sampled by the diffusion model.
    *   Does the distribution of the red (diffusion-sampled) points closely match the distribution of the blue (original AE) points in terms of shape, density, and coverage?
    *   Are there areas where the AE embeddings exist but the diffusion model failed to sample (gaps)? Are there areas where the diffusion model samples points outside the original distribution?

2.  **Quality and Similarity of Decoded Images:**
    *   Examine the 10 images decoded from the latent vectors sampled by the diffusion model.
    *   Do these images appear similar to the training images from the Frey dataset in terms of being recognizable faces?
    *   How is their quality compared to, for example, direct reconstructions from the autoencoder (if you recall them)? Are they sharp, blurry, or do they have artifacts?

3.  **Evidence of Mode Collapse (in Latent Diffusion):**
    *   Looking at the 10 decoded images (and potentially more if you generate them), do they show good variety, or do many of them look very similar?
    *   Relate this to the scatter plot of sampled latents. If the sampled latents are clumped in only a few regions of the learned AE manifold, this would lead to low variety in decoded images.
    *   Does the diffusion model seem capable of generating latent vectors that would decode into the full range of face types present in the Frey dataset (as captured by the AE's latent space)? Or does it seem to prefer certain "modes" or regions of the latent space?

**Overall Assessment:** How well did the latent diffusion model perform in learning and sampling from the autoencoder's latent space?
"""



================================================
File: hw3/temp.py
================================================
# %% [markdown]
# # **Part 2: Latent Diffusion Model on Frey Dataset**
#
# In this part, we will build a latent diffusion model on the Frey dataset.
# 1.  Train an MLP autoencoder on Frey images to get a 2D latent space.
# 2.  Train a diffusion model on these 2D latent embeddings.
# 3.  Sample from the diffusion model in latent space, then decode these latents into images using the autoencoder's decoder.

# %%
# General imports (some might have been imported in Part 1)
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
from tensorflow.keras.utils import get_file
from scipy.io import loadmat
import matplotlib.pyplot as plt
import matplotlib as mpl
# Ensure cmap is gray for images if not set globally before
if 'style' not in mpl.rcParams: # A simple check if some rcParams were already set
    mpl.rc('image', cmap='gray')
from tqdm.auto import tqdm, trange # For progress bars

# %% [markdown]
# ## Load Frey Dataset
# We'll reuse the function from Part 1 or the example notebook to load the Frey dataset.
# The data should be normalized to `[-1, 1]`.

# %%
def get_frey_data():
    """Loads and preprocesses the Frey dataset.

    Returns:
        numpy.ndarray: Frey face images, normalized to [-1, 1]
                       with shape (N, 28, 20, 1).
    """
    try:
        path = get_file('frey_rawface.mat',
                        'https://www.dropbox.com/scl/fi/m70sh4ef39pvy01czc63r/frey_rawface.mat?rlkey=5v6meiap55z68ada2roxwxuql&dl=1',
                        cache_subdir='datasets')
        data = np.transpose(loadmat(path)['ff'])
        x_train_frey = np.reshape(data, (-1, 28, 20, 1))
        x_train_frey = x_train_frey.astype('float32')
        x_train_frey /= 255.0  # Normalize to [0, 1]
        x_train_frey = (x_train_frey * 2.0) - 1.0  # Normalize to [-1, 1]
        return x_train_frey
    except Exception as e:
        print(f"Error loading Frey dataset: {e}")
        return None

x_train_frey = get_frey_data()

if x_train_frey is not None:
    IMG_H, IMG_W, IMG_C = x_train_frey.shape[1:]
    FLAT_DIM = IMG_H * IMG_W * IMG_C
    print(f"Frey dataset loaded: {x_train_frey.shape}")

    # Display a few example images
    plt.figure(figsize=(10, 2))
    for i in range(5):
        plt.subplot(1, 5, i + 1)
        img_display = (x_train_frey[i] + 1.0) / 2.0 # De-normalize for display
        plt.imshow(np.squeeze(img_display))
        plt.axis('off')
    plt.suptitle("Sample Frey Dataset Images")
    plt.show()
else:
    print("Halting Part 2 due to dataset loading failure.")
    # In a real notebook, you'd probably stop execution here or handle x_train_frey being None

# %% [markdown]
# ## 2.1: Build an Autoencoder
#
# Train an MLP autoencoder on the Frey dataset with a 2-dimensional bottleneck.
# After training, extract and store the embedding vectors for the training images.

# %%
if x_train_frey is not None:
    LATENT_DIM_AE = 2 # Bottleneck dimensionality

    # Define Encoder
    encoder_inputs = layers.Input(shape=(IMG_H, IMG_W, IMG_C), name='encoder_input')
    x = layers.Flatten()(encoder_inputs)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    encoder_outputs = layers.Dense(LATENT_DIM_AE, activation=None, name='encoder_output')(x) # Linear activation for bottleneck
    encoder = Model(encoder_inputs, encoder_outputs, name='encoder')
    print("Encoder Summary:")
    encoder.summary()

    # Define Decoder
    decoder_inputs = layers.Input(shape=(LATENT_DIM_AE,), name='decoder_input')
    x = layers.Dense(64, activation='relu')(decoder_inputs)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(FLAT_DIM, activation='tanh')(x) # Tanh because input images are [-1, 1]
    decoder_outputs = layers.Reshape((IMG_H, IMG_W, IMG_C), name='decoder_output')(x)
    decoder = Model(decoder_inputs, decoder_outputs, name='decoder')
    print("\nDecoder Summary:")
    decoder.summary()

    # Define Autoencoder
    autoencoder_inputs = layers.Input(shape=(IMG_H, IMG_W, IMG_C), name='ae_input')
    encoded = encoder(autoencoder_inputs)
    decoded = decoder(encoded)
    autoencoder = Model(autoencoder_inputs, decoded, name='autoencoder')
    print("\nAutoencoder Summary:")
    autoencoder.summary()

    # Compile and Train Autoencoder
    autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mse')

    ae_epochs = 50 # Adjust as needed; might need more for good reconstruction
    ae_batch_size = 32

    print(f"\nTraining Autoencoder for {ae_epochs} epochs...")
    history_ae = autoencoder.fit(x_train_frey, x_train_frey,
                                 epochs=ae_epochs,
                                 batch_size=ae_batch_size,
                                 shuffle=True,
                                 validation_split=0.1, # Optional: use a validation split
                                 verbose=1) # Set to 1 or 2 for progress, 0 for silent

    # Plot training loss
    plt.figure()
    plt.plot(history_ae.history['loss'], label='Training Loss')
    if 'val_loss' in history_ae.history:
        plt.plot(history_ae.history['val_loss'], label='Validation Loss')
    plt.title('Autoencoder Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.show()

    # Display some original and reconstructed images
    num_display = 5
    random_indices = np.random.choice(len(x_train_frey), num_display, replace=False)
    originals = x_train_frey[random_indices]
    reconstructions = autoencoder.predict(originals, verbose=0)

    plt.figure(figsize=(num_display * 2, 4))
    for i in range(num_display):
        # Original
        plt.subplot(2, num_display, i + 1)
        plt.imshow(np.squeeze((originals[i] + 1.0) / 2.0))
        plt.title("Original")
        plt.axis('off')
        # Reconstruction
        plt.subplot(2, num_display, num_display + i + 1)
        plt.imshow(np.squeeze((reconstructions[i] + 1.0) / 2.0))
        plt.title("Recon.")
        plt.axis('off')
    plt.suptitle("Autoencoder: Original vs. Reconstructed")
    plt.show()

    # Extract and store latent embeddings for the training set
    print("\nExtracting latent embeddings from the training set...")
    latent_embeddings_train = encoder.predict(x_train_frey, batch_size=ae_batch_size, verbose=0)
    print(f"Shape of latent embeddings: {latent_embeddings_train.shape}") # Should be (num_frey_images, 2)

    # Plot the 2D latent space
    plt.figure(figsize=(8, 6))
    plt.scatter(latent_embeddings_train[:, 0], latent_embeddings_train[:, 1], s=5, alpha=0.5)
    plt.title('2D Latent Space of Frey Dataset (from Autoencoder)')
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.grid(True)
    plt.show()

# %% [markdown]
# ## 2.2: Build and train the reverse process model
#
# This model will operate on the 2D latent embeddings. It takes a (noisy) 2D embedding and a timestep `t`, and outputs a predicted 2D noise vector.

# %%
if x_train_frey is not None and 'latent_embeddings_train' in locals():
    # Diffusion Hyperparameters (from example, can be tuned)
    T_diffusion = 1000  # Number of timesteps
    betas = np.linspace(1e-4, 0.02, T_diffusion)
    sigmas_diffusion = np.sqrt(betas) # $\sigma_t = \sqrt{\beta_t}$
    alphas = 1.0 - betas
    alphas_cumprod = np.cumprod(alphas, axis=-1) # $\bar{\alpha}_t$

    # For the forward process $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
    # we'll need sqrt_alphas_cumprod and sqrt_one_minus_alphas_cumprod
    sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)
    sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)


    # Reverse Process Model (MLP for 2D latent space)
    def build_reverse_process_model_mlp(latent_dim, time_embedding_dim, num_hidden_layers=3, hidden_layer_width=128):
        """Builds the MLP reverse process model for the latent space."""
        latent_input = layers.Input(shape=(latent_dim,), name='latent_input') # e.g., (2,)
        timestep_input = layers.Input(shape=(1,), name='timestep_input')     # Scalar timestep

        # Timestep embedding
        time_embed = layers.Embedding(input_dim=T_diffusion, output_dim=time_embedding_dim,
                                      embeddings_initializer='glorot_normal')(timestep_input)
        time_embed = layers.Flatten()(time_embed) # Shape from (batch, 1, D) to (batch, D)

        # Concatenate latent vector with time embedding
        merged_input = layers.Concatenate(axis=-1)([latent_input, time_embed])

        # MLP
        x = merged_input
        for _ in range(num_hidden_layers):
            x = layers.Dense(hidden_layer_width, activation='relu')(x)

        # Output predicted noise (same dimension as latent_input)
        output_noise = layers.Dense(latent_dim, activation=None)(x) # Linear activation

        model = Model(inputs=[latent_input, timestep_input], outputs=output_noise)
        return model

    # Instantiate the model
    latent_dim_for_diffusion = LATENT_DIM_AE # Should be 2
    time_embedding_dim_diffusion = 128 # As suggested: "same size as your hidden layers"
    hidden_mlp_width = 128

    diffusion_model_mlp = build_reverse_process_model_mlp(
        latent_dim_for_diffusion,
        time_embedding_dim_diffusion,
        hidden_layer_width=hidden_mlp_width
    )
    print("Diffusion Model (MLP for Latents) Summary:")
    diffusion_model_mlp.summary()

    # Training Setup
    diffusion_epochs = 150 # Adjust as needed
    diffusion_batch_size = 128
    diffusion_learning_rate = 3e-4 # Example uses 3e-4

    optimizer_diffusion = keras.optimizers.Adam(learning_rate=diffusion_learning_rate)
    loss_fn_diffusion = keras.losses.MeanSquaredError()

    # Prepare dataset from latent embeddings
    train_dataset_latents = tf.data.Dataset.from_tensor_slices(latent_embeddings_train)
    train_dataset_latents = train_dataset_latents.shuffle(buffer_size=len(latent_embeddings_train)).batch(diffusion_batch_size)

    print(f"\nTraining Diffusion Model on Latent Embeddings for {diffusion_epochs} epochs...")

    # Custom Training Loop (adapted from frey_diffusion_example.py)
    diffusion_loss_history = []
    for epoch in range(diffusion_epochs):
        total_loss_epoch = 0

        pbar = tqdm(total=len(train_dataset_latents), desc=f"Epoch {epoch+1}/{diffusion_epochs}")
        for step, x0_batch_latents in enumerate(train_dataset_latents):
            # x0_batch_latents are the "clean" 2D latent vectors from AE
            current_batch_size = tf.shape(x0_batch_latents)[0]

            # Sample random timesteps t for this batch
            t_batch = np.random.randint(0, T_diffusion, size=current_batch_size)

            with tf.GradientTape() as tape:
                # Sample random noise $\epsilon \sim N(0,I)$
                noise_batch = tf.random.normal(shape=tf.shape(x0_batch_latents))

                # Get $\sqrt{\bar{\alpha}_t}$ and $\sqrt{1-\bar{\alpha}_t}$ for the sampled timesteps
                # These need to be gathered according to t_batch and reshaped for broadcasting
                sqrt_at_batch = tf.gather(sqrt_alphas_cumprod, t_batch)
                sqrt_one_minus_at_batch = tf.gather(sqrt_one_minus_alphas_cumprod, t_batch)

                # Reshape for broadcasting: (batch_size,) -> (batch_size, 1) for 2D latents
                sqrt_at_batch = tf.reshape(sqrt_at_batch, [current_batch_size, 1])
                sqrt_one_minus_at_batch = tf.reshape(sqrt_one_minus_at_batch, [current_batch_size, 1])

                # Construct noisy latents: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
                xt_batch_noisy_latents = sqrt_at_batch * x0_batch_latents + sqrt_one_minus_at_batch * noise_batch

                # Predict noise using the diffusion model
                # Ensure t_batch is correctly shaped for the model: (batch_size, 1)
                t_batch_reshaped_for_model = tf.reshape(t_batch, [current_batch_size, 1])
                predicted_noise_batch = diffusion_model_mlp([xt_batch_noisy_latents, t_batch_reshaped_for_model], training=True)

                # Calculate loss: MSE between actual noise and predicted noise
                loss_value = loss_fn_diffusion(noise_batch, predicted_noise_batch)

            # Compute gradients and update weights
            grads = tape.gradient(loss_value, diffusion_model_mlp.trainable_weights)
            optimizer_diffusion.apply_gradients(zip(grads, diffusion_model_mlp.trainable_weights))

            total_loss_epoch += loss_value.numpy()
            pbar.update(1)
            pbar.set_postfix_str(f"Loss: {loss_value.numpy():.4f}")

        pbar.close()
        avg_loss_epoch = total_loss_epoch / (step + 1)
        diffusion_loss_history.append(avg_loss_epoch)
        print(f"Epoch {epoch+1}, Average Loss: {avg_loss_epoch:.4f}")

    # Plot diffusion model training loss
    plt.figure()
    plt.plot(diffusion_loss_history)
    plt.title('Diffusion Model (on Latents) Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.grid(True)
    plt.show()

# %% [markdown]
# ## 2.3: Evaluate the results
#
# - Sample 1000 random 2D latent vectors using the diffusion sampling process.
# - Plot the training embedding vectors and your sampled 2D points on top.
# - Decode the first 10 sampled latent vectors into images.
# - Discuss results.

# %%
if x_train_frey is not None and 'diffusion_model_mlp' in locals() and 'decoder' in locals():

    def sample_latents_from_diffusion(model, num_samples, latent_dim, T, alphas_param, alphas_cumprod_param, sigmas_param):
        """Samples latent vectors from the trained diffusion model."""
        # Start with random noise $x_T \sim N(0,I)$
        x_t = tf.random.normal(shape=(num_samples, latent_dim))

        for t_val_rev in trange(T - 1, -1, -1, desc="Diffusion Sampling"): # Iterate t from T-1 down to 0
            t_batch_model = tf.ones((num_samples, 1), dtype=tf.int32) * t_val_rev

            # Sample noise z (unless t=0)
            z = tf.random.normal(shape=(num_samples, latent_dim)) if t_val_rev > 0 else tf.zeros((num_samples, latent_dim))

            # Predict noise $\epsilon_\theta(x_t, t)$
            eps = model.predict([x_t, t_batch_model], verbose=0)

            # Get constants for this timestep t
            alpha_t = alphas_param[t_val_rev]
            alpha_t_bar = alphas_cumprod_param[t_val_rev]
            sigma_t = sigmas_param[t_val_rev] # $\sigma_t = \sqrt{\beta_t}$ or $\sqrt{\tilde{\beta}_t}$ depending on DDPM/DDIM
                                              # Example notebook uses $\sigma_t = \sqrt{\beta_t}$

            # Reshape for broadcasting if necessary (not needed here as all are scalars for 2D)

            # DDPM sampling step formula:
            # $x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t)) + \sigma_t z$
            term1_coeff = 1.0 / np.sqrt(alpha_t)
            term2_coeff = (1.0 - alpha_t) / np.sqrt(1.0 - alpha_t_bar)

            x_t_minus_1 = term1_coeff * (x_t - term2_coeff * eps) + sigma_t * z
            x_t = x_t_minus_1

        return x_t.numpy() # Final $x_0$ samples

    # Sample 1000 latent vectors
    num_eval_samples_diffusion = 1000
    print(f"\nSampling {num_eval_samples_diffusion} latent vectors using the trained diffusion model...")
    sampled_latents_diffusion = sample_latents_from_diffusion(
        diffusion_model_mlp,
        num_eval_samples_diffusion,
        latent_dim_for_diffusion,
        T_diffusion,
        alphas, # Global alphas array
        alphas_cumprod, # Global alphas_cumprod array
        sigmas_diffusion # Global sigmas_diffusion array
    )
    print(f"Shape of sampled latents: {sampled_latents_diffusion.shape}")

    # Plot training embeddings and sampled points
    plt.figure(figsize=(10, 8))
    plt.scatter(latent_embeddings_train[:, 0], latent_embeddings_train[:, 1],
                s=10, alpha=0.3, label='AE Training Embeddings (Originals)')
    plt.scatter(sampled_latents_diffusion[:, 0], sampled_latents_diffusion[:, 1],
                s=10, alpha=0.5, color='red', label='Diffusion Sampled Latents')
    plt.title('Comparison of AE Latent Space and Diffusion Model Samples')
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.legend()
    plt.grid(True)
    plt.axis('equal') # Helps visualize distribution shape
    plt.show()

    # Decode the first 10 sampled latent vectors into images
    num_decode_display = 10
    first_10_sampled_latents = sampled_latents_diffusion[:num_decode_display]

    print(f"\nDecoding the first {num_decode_display} sampled latent vectors into images...")
    decoded_images_from_diffusion = decoder.predict(first_10_sampled_latents, verbose=0)

    plt.figure(figsize=(num_decode_display * 1.5, 3)) # Adjusted for 10 images
    for i in range(num_decode_display):
        plt.subplot(2, num_decode_display // 2, i + 1) # Assuming 2 rows
        img_display = (decoded_images_from_diffusion[i] + 1.0) / 2.0 # De-normalize
        plt.imshow(np.squeeze(np.clip(img_display, 0, 1)))
        plt.title(f"Sample {i+1}")
        plt.axis('off')
    plt.suptitle("Images Decoded from Diffusion Model Latent Samples")
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# %% [markdown]
# ### Discussion of Latent Diffusion Results
#
# *(Student to fill in based on the plots and images generated above):*
#
# 1.  **Accuracy of Latent Space Distribution Learning:**
#     *   Compare the scatter plot of the original AE training embeddings with the scatter plot of the 1000 points sampled by the diffusion model.
#     *   Does the distribution of the red (diffusion-sampled) points closely match the distribution of the blue (original AE) points in terms of shape, density, and coverage?
#     *   Are there areas where the AE embeddings exist but the diffusion model failed to sample (gaps)? Are there areas where the diffusion model samples points outside the original distribution?
#
# 2.  **Quality and Similarity of Decoded Images:**
#     *   Examine the 10 images decoded from the latent vectors sampled by the diffusion model.
#     *   Do these images appear similar to the training images from the Frey dataset in terms of being recognizable faces?
#     *   How is their quality compared to, for example, direct reconstructions from the autoencoder (if you recall them)? Are they sharp, blurry, or do they have artifacts?
#
# 3.  **Evidence of Mode Collapse (in Latent Diffusion):**
#     *   Looking at the 10 decoded images (and potentially more if you generate them), do they show good variety, or do many of them look very similar?
#     *   Relate this to the scatter plot of sampled latents. If the sampled latents are clumped in only a few regions of the learned AE manifold, this would lead to low variety in decoded images.
#     *   Does the diffusion model seem capable of generating latent vectors that would decode into the full range of face types present in the Frey dataset (as captured by the AE's latent space)? Or does it seem to prefer certain "modes" or regions of the latent space?
#
# **Overall Assessment:** How well did the latent diffusion model perform in learning and sampling from the autoencoder's latent space?



================================================
File: hw4/hw4_gym_examples.py
================================================
# %% [markdown]
# ## Gymnasium examples
#
# Gymnasium is a Python module that provides many classic example environments for reinforcement learning tasks.  This notebook shows how to create and run an environment.

# %%
import gymnasium as gym

# %%
from matplotlib import pyplot as plt
from ipywidgets import interact, IntSlider

# %%
def show_video(images,captions):
  """ Show a sequence of images as an interactive plot. """
  def f(i):
    plt.imshow(images[i])
    plt.title(captions[i])
  interact(f, i=IntSlider(min=0, max=len(images)-1, step=1, value=0))

# %% [markdown]
# ### Frozen Lake
#
# The [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment consists of a 4x4 map:
#
#     SFFF
#     FHFH
#     FFFH
#     HFFG
#
# Where S is the start, G is the goal, F is a free space and H is a hole.  When you create the environment, pass the argument â€œis_slippery=Falseâ€ so that the environment is deterministic and you will always move in the desired direction.
#
# The sixteen states are numbered 0 to 15 in row-major order with 0 being the start and 15 being the goal.
#
# There are four possible actions: left (0), down (1), right (2), and up (3).
#
# A reward of +1 is given for reaching the goal state; all other states have zero reward.
#

# %%
# Configuration parameters for the whole setup
seed = 1234
env = gym.make("FrozenLake-v1",is_slippery=False,render_mode='rgb_array')
env.reset(seed=seed)
env.action_space.seed(seed)

# %%
# reset the environment back to the initial state
state, _ = env.reset()

renders = []
captions = []

# get the initial state
renders.append(env.render())
captions.append(f'state: {state} action: reward: done:')

while True:
  # sample a random action
  action = env.action_space.sample()

  # apply the action to get the next state, reward, and done flag
  state, reward, done, _, _ = env.step(action)

  renders.append(env.render())
  captions.append(f'state: {state} action:{action} reward:{reward} done:{done}')

  if done:
      break
show_video(renders,captions)

# %% [markdown]
# ### Cart Pole environment
#
# The [CartPole-v1 environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) consists of a pole on a cart which can move left and right.  The goal is to keep the pole from falling down.
#
# The state consists of four continuous values: the cart position, cart velocity, pole angle, and pole angular velocity.
#
# There are two possible actions: left (0) and right (1).
#
# A reward of +1 is given for each step taken, and the game ends when the pole angle > 12 degrees, the absolute cart position > 2.4, or the episode length is greater than 500 steps.

# %%
seed = 1234
env = gym.make("CartPole-v1",render_mode='rgb_array')
env.reset(seed=seed)
env.action_space.seed(seed)

# %%
# reset the environment back to the initial state
state, _ = env.reset(seed=seed)

renders = []
captions = []

# get the initial state
renders.append(env.render())
captions.append(f'state: {state} action: reward: done:')

while True:
  # sample a random action
  action = env.action_space.sample()

  # apply the action to get the next state, reward, and done flag
  state, reward, done, _, _ = env.step(action)

  renders.append(env.render())
  captions.append(f'state: {state} action:{action} reward:{reward} done:{done}')

  if done:
      break
show_video(renders,captions)

# %%






================================================
File: hw4/instructions.md
================================================
# **Homework 4: Reinforcement Learning**

In this homework you will build and experiment with deep Q-learning for reinforcement learning on a couple classic RL problems.

Your implementation should use the Keras module in Tensorflow (import tensorflow.keras) and Gymnasium (import gymnasium as gym).

# **Part 1: Table-based Q-learning (Frozen Lake)**

In this part you will write code to learn a Q-table for the Frozen Lake (FrozenLake-v1) environment.  Note that you can do this part entirely in Numpy without Keras / Tensorflow.

**To replicate my results exactly, set the random seeds to 1234\.**

The [frozen lake environment](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) consists of a 4x4 map:

SFFF
FHFH
FFFH
HFFG

Where S is the start, G is the goal, F is a free space and H is a hole.  When you create the environment, pass the argument `is_slippery=False` so that the environment is deterministic and you will always move in the desired direction.

The state is an integer from 0 to 15 indicating the current grid cell.  The sixteen grid cells are numbered 0 to 15 in row-major order with 0 being the start and 15 being the goal.

There are four possible actions: left (0), down (1), right (2), and up (3).

A reward of \+1 is given for reaching the goal state; all other states have zero reward.

Here is pseudo-code for how to learn the Q-table using Bellman updates:

1. Initialize a 16x4 matrix of zeros to represent the Q-table Q(*s*,*a*).
2. Reset the environment (s \= env.reset()) to get the initial state *s*.
3. Repeat for num\_step steps:
4.     Sample an action *a* using an *epsilon-greedy* strategy
5.     Apply the action to receive a new state *sâ€™* and a reward *r*.
6.     Calculate the expected cumulative payoff *y* as
   *y \= r*                                              if the game is over or
   *y* \= *r* \+ *gamma* \* max~a~ Q(*sâ€™*,*a*)     otherwise
7.     Update Q using Q(*s*,*a*) \= Q(*s*,*a*) \+ lr\*(*y*\-Q(*s*,*a*))
8.     If the game is over, reset the environment (s \= env.reset()); otherwise, update the current state with *s* \= *sâ€™*.

For the epsilon-greedy strategy, I used epsilon=exp(-decay\_rate \* step) with decay\_rate=0.0001.  For the other parameters I used gamma \= 0.99, num\_steps=2000, and lr=.01.

Now test the policy learned by the Q-table on the environment as follows:

1. Reset the environment to get initial state *s*.
2. Choose the next action *a* as the action with maximum utility: max~a~ Q(*s*,*a*)
3. Apply the action *a* to get the next state *sâ€™*.
4. Render the environment (env.render())
5. Set *s* \= *sâ€™.*
6. Repeat 2-5 until done, or a maximum number of steps is reached.

I set the maximum number of steps to 10,000 â€“ if this limit is reached, then the trial is marked as failed.  If you have properly learned the Q-table, you should reach the goal state every time.

Now learn a Q-table for the slippery frozen lake (is\_slippery=True).   How often does the resulting policy succeed over 1000 trials?  Now learn the Q-table again, but use 200,000 iterations and a decay\_rate of 0.00001.  Your success rate should have improved \-- why?

# **Part 2: n-step Deep Q-Learning (Cart Pole)**

In this part you will implement n-step deep Q-learning to solve the CartPole-v1 environment.  Since we will be training a neural network to learn the Q-function, you will need to use Keras and Tensorflow in this part.

The [cart pole environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) consists of a pole on a cart which can move left and right.  The goal is to keep the pole from falling down.

The state consists of four continuous values: the cart position, cart velocity, pole angle, and pole angular velocity.

There are two possible actions: left (0) and right (1).

A reward of \+1 is given for each step taken, and the game ends when the pole angle \> 12 degrees, the absolute cart position \> 2.4, or the episode length is greater than 500 steps.

Your goal is to learn a policy which can keep the pole aloft for an average of at least 200 steps over 100 trials.

Your Q-function approximator should be a multi-layer perceptron with a single hidden layer with ReLU activation.  The input to the network is the state, and the output is a Q-value for each action.  Thus the input vector size is the size of the state (four) the output vector size is the number of actions (two).

Our approach here is to run through a complete episode of the environment before updating the Q-network.  Starting from the initialization state, you will play through the game using an epsilon-greedy policy until the â€œdoneâ€ flag is raised.  Along the way you will accumulate lists for the previous state, action taken, reward received, and next state.  At the end of the episode, you calculate the discounted cumulative reward at each state.  Finally, you will use the action taken and discounted cumulative reward received at each step to calculate a batch of losses and update the neural network.  This process is repeated for some number of episodes.

Here is pseudo-code to implement n-step Q-learning:

1. Create a neural network to represent the Q-function Q(*s*,*a*).
2. Initialize step counter to 1\.
3. Repeat for num\_episodes episodes:
4.     Reset the environment (s \= env.reset()) to get the initial state *s*.
5.     Create empty lists for *s*, *a*, and *r*.
6.     While not done:
7.         Sample an action *a* using an *epsilon-greedy* strategy.
8.         Apply the action to receive a new state *sâ€™* and a reward *r*.
9.         Append *s*, *a*, and *r* to their lists.
10.         Set *s* \= *sâ€™*
11.         Increment the step counter.
12.     Let *n* be the number of steps in the episode.
13.     Calculate the cumulative payoff *y* at each step as:
    *y*(*n*) *\= r*(*n*)
    *y*(*i*) \= *r*(*i*) \+ *gamma* \* *y*(i+1)     for *i* \= *n*\-1,*n*\-2,...,1
14.     Divide each *y*(*i*) by the maximum possible cumulative reward .  (This normalization helps stabilize training.  Note that the maximum possible per-step reward is *R*max \= 1.)
15.     For each action *a*(*i*), calculate the loss *L(i) \=* (*y*(*i*) \- *Q*(*s*(*i*),*a*(*i*)))2
16.     Calculate the average loss over all *L(i)*.
17.     Calculate the gradient of the average loss and update the model.

For the epsilon-greedy strategy, I used epsilon=exp(-decay\_rate \* step) with decay\_rate=0.00001, where step is the global step count.  For the other parameters I used gamma \= 0.99, num\_episodes \= 3000, and lr=.001 with the Adam optimizer.  I used a hidden layer size of 32 in the Q-network.

**Note: Make sure that you do not include the calculation of y(i) in the calculation of the gradient.  You should not turn on GradientTape until after you have calculated the y(i) values.**

It is helpful to print the number of steps in the episode at each iteration and the current epsilon value to monitor the progress during training.

Once the network is trained, calculate how often the learned policy succeeds (reaches \>200 steps) over 100 episodes, and the average number of steps per episode.

What happens if you increase the decay\_rate to 0.001 and set num\_episodes \= 100?  Why?



================================================
File: hw4/main.ipynb
================================================
# Jupyter notebook converted to Python script.

import gymnasium as gym
import numpy as np
import tensorflow.keras
import random
from matplotlib import pyplot as plt
from ipywidgets import interact, IntSlider
import math
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tqdm import tqdm

"""
## Helper Functions

We'll define a helper function to visualize the agent's performance and functions to handle training and evaluation to keep the code modular.
"""

def show_video(images, captions):
  """ Show a sequence of images as an interactive plot. """
  if not images:
      print("No images to display.")
      return
  def f(i):
    plt.imshow(images[i])
    plt.title(captions[i])
    plt.axis('off')
    plt.show()
  interact(f, i=IntSlider(min=0, max=len(images)-1, step=1, value=0))

def train_q_table(env, num_steps, lr, gamma, decay_rate, seed):
    """ Trains a Q-table for the given environment. """
    # Set seeds for reproducibility
    random.seed(seed)
    np.random.seed(seed)
    env.action_space.seed(seed)

    # Initialize Q-table
    q_table = np.zeros((env.observation_space.n, env.action_space.n))

    # Reset environment, seeding it for a reproducible training sequence
    state, _ = env.reset(seed=seed)

    # Training loop
    for step in range(num_steps):
        # Epsilon-greedy action selection
        epsilon = math.exp(-decay_rate * step)
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # Explore
        else:
            action = np.argmax(q_table[state, :])  # Exploit

        # Take action
        next_state, reward, done, _, _ = env.step(action)

        # Calculate target y based on Bellman equation
        if done:
            y = reward
        else:
            y = reward + gamma * np.max(q_table[next_state, :])

        # Update Q-table
        q_table[state, action] = q_table[state, action] + lr * (y - q_table[state, action])

        # Update state
        if done:
            state, _ = env.reset() # Reset for the next episode
        else:
            state = next_state

    return q_table

def evaluate_policy(env, q_table, num_trials, seed):
    """ Evaluates a Q-table policy over a number of trials. """
    success_count = 0
    max_steps_per_trial = 100  # Prevent infinite loops in non-optimal policies

    # Seed the environment's RNG to make the sequence of trials reproducible.
    # This reset also provides the starting state for the first trial.
    state, _ = env.reset(seed=seed)
    env.action_space.seed(seed)

    for i in range(num_trials):
        # For subsequent trials, just reset the state to the beginning.
        # This will use the same, seeded RNG to produce the next episode in the sequence.
        if i > 0:
            state, _ = env.reset()

        done = False
        steps = 0
        while not done and steps < max_steps_per_trial:
            action = np.argmax(q_table[state, :])
            state, reward, done, _, _ = env.step(action)
            steps += 1

        if done and reward == 1:
            success_count += 1

    return success_count / num_trials

"""
## Non-Slippery Frozen Lake

First, we'll learn a Q-table for the deterministic version (`is_slippery=False`). In this environment, the agent's actions always succeed as intended.
"""

# Global seed for the notebook
seed = 1234

# Environment setup
env_non_slippery = gym.make("FrozenLake-v1", is_slippery=False, render_mode='rgb_array')

"""
### Q-Learning Algorithm

We implement the Q-learning algorithm as described in the pseudo-code.
"""

# Q-learning parameters
num_steps = 2000
lr = 0.01
gamma = 0.99
decay_rate = 0.0001

# Train the Q-table
q_table_non_slippery = train_q_table(
    env=env_non_slippery,
    num_steps=num_steps,
    lr=lr,
    gamma=gamma,
    decay_rate=decay_rate,
    seed=seed
)

print("Q-table learned for non-slippery environment.")
# Output:
#   Q-table learned for non-slippery environment.


"""
### Testing the Learned Policy

Now we test the policy learned by the Q-table. Since the environment is deterministic and the Q-table is well-trained, the agent should find the optimal path to the goal every time.
"""

# Test the policy
state, _ = env_non_slippery.reset(seed=seed)
done = False
max_test_steps = 10000
renders = []
captions = []
step_count = 0

# Initial state
renders.append(env_non_slippery.render())
captions.append(f'Step: {step_count} State: {state} Action: N/A Reward: N/A')

while not done and step_count < max_test_steps:
    step_count += 1
    # Choose action with max utility from the Q-table
    action = np.argmax(q_table_non_slippery[state, :])

    # Apply action
    next_state, reward, done, _, _ = env_non_slippery.step(action)

    # Render and store frame for video
    renders.append(env_non_slippery.render())
    captions.append(f'Step: {step_count} State: {next_state} Action: {action} Reward: {reward}')

    # Update state
    state = next_state

if done and reward == 1:
    print(f"Success! Reached the goal in {step_count} steps.")
else:
    print(f"Failure. Did not reach the goal after {step_count} steps.")

# Display the successful run
show_video(renders, captions)
env_non_slippery.close()
# Output:
#   Success! Reached the goal in 6 steps.

#   interactive(children=(IntSlider(value=0, description='i', max=6), Output()), _dom_classes=('widget-interact',)â€¦

"""
## Slippery Frozen Lake

Now we will learn a Q-table for the slippery version of Frozen Lake (`is_slippery=True`), where actions do not always have the intended effect. This makes the problem significantly harder.
"""

# Environment setup for the slippery version
env_slippery = gym.make("FrozenLake-v1", is_slippery=True, render_mode='rgb_array')

"""
### Training and Evaluation (2,000 steps)

First, we train with the same parameters as the non-slippery case (2,000 steps). We expect the performance to be poor due to the environment's stochasticity and the short training time.
"""

# Train with 2,000 steps
q_table_slippery_2k = train_q_table(
    env=env_slippery,
    num_steps=2000,
    lr=0.01,
    gamma=0.99,
    decay_rate=0.0001,
    seed=seed
)

# Evaluate the policy over 1000 trials
success_rate_2k = evaluate_policy(env_slippery, q_table_slippery_2k, num_trials=1000, seed=seed)

print(f"Success rate over 1000 trials (2,000 training steps): {success_rate_2k:.2%}")
# Output:
#   Success rate over 1000 trials (2,000 training steps): 0.00%


"""
### Training and Evaluation (200,000 steps)

To improve performance, we drastically increase the number of training steps to 200,000 and use a slower epsilon decay rate to encourage more exploration.
"""

# Train with 200,000 steps and a slower decay rate
num_steps_200k = 200000
decay_rate_slower = 0.00001

q_table_slippery_200k = train_q_table(
    env=env_slippery,
    num_steps=num_steps_200k,
    lr=0.01,
    gamma=0.99,
    decay_rate=decay_rate_slower,
    seed=seed
)

# Evaluate the new policy
success_rate_200k = evaluate_policy(env_slippery, q_table_slippery_200k, num_trials=1000, seed=seed)
env_slippery.close()

print(f"Success rate over 1000 trials (200,000 training steps): {success_rate_200k:.2%}")
# Output:
#   Success rate over 1000 trials (200,000 training steps): 71.80%


"""
### Why did the success rate improve?

The success rate improved significantly for two main reasons:

1.  **More Training Steps:** The slippery environment is stochastic, meaning the same action in the same state can lead to different next states. A small number of training steps (like 2,000) is insufficient for the agent to experience the full range of possible outcomes for each state-action pair. By increasing the training steps to 200,000, the agent gathers a much larger and more representative sample of transitions. This allows the Q-values to converge more closely to their true expected values, which properly account for the environment's randomness.

2.  **Slower Epsilon Decay:** The decay rate for epsilon was reduced from 0.0001 to 0.00001. A slower decay means that the `epsilon` value stays higher for longer, forcing the agent to perform more random (exploratory) actions throughout the extended training period. In a stochastic environment, thorough exploration is critical to avoid settling on a suboptimal policy that might seem good based on early, lucky outcomes. The prolonged exploration ensures the agent discovers more robust paths to the goal that are less susceptible to the environment's slipperiness.

In summary, the combination of significantly more experience and a more patient exploration strategy allowed the agent to build a much more accurate and robust model of the stochastic environment, leading to a higher success rate.
"""

"""
# Part 2: n-step Deep Q-Learning (Cart Pole)

In this part, we implement an n-step Deep Q-Learning agent to solve the CartPole-v1 environment. We will use a Keras neural network to approximate the Q-function. The agent will be trained by running full episodes and then performing a batch update on the network using the collected experience.
"""

"""
## Setting up the Environment and Model

First, we set the seed for reproducibility across all relevant libraries. Then we define a function to create our Q-network. The network will be a simple multi-layer perceptron (MLP) with one hidden layer, as specified in the instructions.
"""

# Configuration parameters
seed = 1234
gamma = 0.99  # Discount factor for past rewards

# Set seeds
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# Create the environment
env = gym.make("CartPole-v1", render_mode='rgb_array')
env.reset(seed=seed)
env.action_space.seed(seed)

num_actions = env.action_space.n
state_shape = env.observation_space.shape

def create_q_model():
    """Creates a Keras model for the Q-network."""
    model = keras.Sequential([
        layers.InputLayer(input_shape=state_shape),
        layers.Dense(32, activation="relu"),
        layers.Dense(num_actions, activation="linear") # Linear activation for Q-values
    ])
    return model

"""
## N-step Q-Learning Implementation

Here is the core of the n-step Q-learning algorithm. We will run for a specified number of episodes. In each episode, we collect states, actions, and rewards. At the end of the episode, we calculate the discounted cumulative rewards (also called returns) and use them as targets to update our Q-network in a single batch.
"""

def train_dql(num_episodes, decay_rate, learning_rate):
    """Trains a Deep Q-Learning model on the CartPole environment."""
    q_network = create_q_model()
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    mse_loss = tf.keras.losses.MeanSquaredError()

    global_step_counter = 0
    episode_rewards_history = []

    # Maximum possible reward for normalization, as per instructions.
    # The environment caps episodes at 500 steps.
    max_possible_reward = 500.0

    print("Starting training...")
    pbar = tqdm(range(num_episodes))
    for episode in pbar:
        # ---- 1. Collect experience by playing one episode ----
        state, _ = env.reset()
        episode_states = []
        episode_actions = []
        episode_rewards = []
        done = False

        while not done:
            global_step_counter += 1

            # Epsilon-greedy action selection
            epsilon = math.exp(-decay_rate * global_step_counter)
            if random.uniform(0, 1) < epsilon:
                action = env.action_space.sample()  # Explore
            else:
                q_values = q_network(tf.expand_dims(state, 0))
                action = tf.argmax(q_values[0]).numpy()  # Exploit

            # Apply action
            next_state, reward, done, _, _ = env.step(action)

            # Store experience
            episode_states.append(state)
            episode_actions.append(action)
            episode_rewards.append(reward)

            state = next_state

        episode_rewards_history.append(len(episode_rewards))

        # ---- 2. Calculate discounted cumulative rewards (returns) ----
        n_steps = len(episode_rewards)
        returns = np.zeros_like(episode_rewards, dtype=np.float32)
        running_return = 0.0
        # Iterate backwards from the last step
        for i in reversed(range(n_steps)):
            running_return = episode_rewards[i] + gamma * running_return
            returns[i] = running_return

        # ---- 3. Normalize returns ----
        returns_normalized = returns / max_possible_reward

        # ---- 4. Prepare data for batch update ----
        states_tensor = tf.convert_to_tensor(episode_states, dtype=tf.float32)
        actions_tensor = tf.convert_to_tensor(episode_actions, dtype=tf.int32)
        returns_tensor = tf.convert_to_tensor(returns_normalized, dtype=tf.float32)

        # Create indices for tf.gather_nd to select the Q-values of actions taken
        action_indices = tf.stack([tf.range(n_steps, dtype=tf.int32), actions_tensor], axis=1)

        # ---- 5. Update the Q-network ----
        with tf.GradientTape() as tape:
            # Predict Q-values for all actions for the states in the episode
            all_q_values = q_network(states_tensor)
            # Select the Q-values for the actions that were actually taken
            action_q_values = tf.gather_nd(all_q_values, action_indices)
            # Calculate the loss between the predicted Q-values and the calculated returns
            loss = mse_loss(returns_tensor, action_q_values)

        # Calculate gradients and update the model
        grads = tape.gradient(loss, q_network.trainable_variables)
        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))

        # Print progress
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards_history[-100:])
            pbar.set_description(f"Episode {episode + 1}/{num_episodes}, Avg Reward (last 100): {avg_reward:.2f}, Epsilon: {epsilon:.4f}")

    print("\nTraining finished.")
    return q_network, episode_rewards_history

def evaluate_policy_dql(env, q_network, num_trials=100):
    """Evaluates the performance of a trained DQL policy."""
    print(f"\nEvaluating policy over {num_trials} trials...")
    total_steps = 0
    success_count = 0
    all_steps = []

    for _ in range(num_trials):
        state, _ = env.reset()
        done = False
        steps = 0
        while not done:
            q_values = q_network(tf.expand_dims(state, 0))
            action = tf.argmax(q_values[0]).numpy()
            state, _, done, _, _ = env.step(action)
            steps += 1

        all_steps.append(steps)
        total_steps += steps
        if steps >= 200:
            success_count += 1

    avg_steps = total_steps / num_trials
    success_rate = success_count / num_trials

    print(f"Average steps per episode: {avg_steps:.2f}")
    print(f"Success rate (>= 200 steps): {success_rate:.2%}")
    return avg_steps, success_rate

"""
### Experiment 1: Standard Training

We train for 3000 episodes with a slow epsilon decay rate. This gives the agent ample time to explore the environment and learn a robust policy.
"""

# Experiment 1: Parameters from instructions
num_episodes_exp1 = 3000
decay_rate_exp1 = 0.00001
learning_rate_exp1 = 0.001

trained_model_exp1, _ = train_dql(num_episodes_exp1, decay_rate_exp1, learning_rate_exp1)
evaluate_policy_dql(env, trained_model_exp1)
# Output:
#   /Users/bshowell/Desktop/school/spring 24-25/587/.venv/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.

#     warnings.warn(

#   Starting training...

#   Episode 3000/3000, Avg Reward (last 100): 113.36, Epsilon: 0.1610: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [01:31<00:00, 32.95it/s]

#   

#   Training finished.

#   

#   Evaluating policy over 100 trials...

#   Average steps per episode: 269.60

#   Success rate (>= 200 steps): 96.00%

#   (269.6, 0.96)

"""
### Experiment 2: Rapid Decay and Short Training

Now, we drastically change the parameters. We train for only 100 episodes and use a much faster epsilon decay rate.
"""

# Experiment 2: Modified parameters
num_episodes_exp2 = 100
decay_rate_exp2 = 0.001
learning_rate_exp2 = 0.001

trained_model_exp2, _ = train_dql(num_episodes_exp2, decay_rate_exp2, learning_rate_exp2)
evaluate_policy_dql(env, trained_model_exp2)

env.close()
# Output:
#   Starting training...

#   Episode 100/100, Avg Reward (last 100): 18.09, Epsilon: 0.1638: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 74.03it/s]

#   

#   Training finished.

#   

#   Evaluating policy over 100 trials...

#   Average steps per episode: 15.80

#   Success rate (>= 200 steps): 0.00%


"""
### Why did the performance drop so drastically in Experiment 2?

The performance plummeted in the second experiment due to two interconnected factors:

1.  **Premature Exploitation:** The `decay_rate` was increased from `0.00001` to `0.001`. This causes the exploration probability, `epsilon`, to decrease very rapidly. The agent stops taking random actions and starts exploiting its learned knowledge far too early. Since it has only been trained for a few episodes, its "knowledge" is based on a tiny, unrepresentative sample of the environment. It likely latches onto a poor, short-sighted strategy and never explores enough to find the better, long-term solution required to balance the pole for 200+ steps.

2.  **Insufficient Training Data:** Training for only 100 episodes is not enough time for the neural network to learn the complex dynamics of the CartPole environment. In the early stages, episodes are very short as the agent's policy is essentially random. With only 100 short episodes, the total number of (state, action, reward) samples collected is extremely small. The Q-network cannot generalize from such sparse data and fails to learn a meaningful policy.

In essence, the second experiment combines the worst of both worlds: it forces the agent to commit to a strategy before it has had a chance to explore (due to rapid epsilon decay) and it doesn't provide enough experience for any strategy it learns to be a good one (due to the low number of episodes). The first experiment succeeded because the long training duration and slow decay rate allowed for a healthy balance between exploration and exploitation, which is crucial for effective reinforcement learning.
"""



================================================
File: hw4/temp.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Part 2: n-step Deep Q-Learning (Cart Pole)

In this part, we implement an n-step Deep Q-Learning agent to solve the CartPole-v1 environment. We will use a Keras neural network to approximate the Q-function. The agent will be trained by running full episodes and then performing a batch update on the network using the collected experience.
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import gymnasium as gym
import math
from tqdm import tqdm

"""
## Setting up the Environment and Model

First, we set the seed for reproducibility across all relevant libraries. Then we define a function to create our Q-network. The network will be a simple multi-layer perceptron (MLP) with one hidden layer, as specified in the instructions.
"""

# Configuration parameters
seed = 1234
gamma = 0.99  # Discount factor for past rewards

# Set seeds
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# Create the environment
env = gym.make("CartPole-v1", render_mode='rgb_array')
env.reset(seed=seed)
env.action_space.seed(seed)

num_actions = env.action_space.n
state_shape = env.observation_space.shape

def create_q_model():
    """Creates a Keras model for the Q-network."""
    model = keras.Sequential([
        layers.InputLayer(input_shape=state_shape),
        layers.Dense(32, activation="relu"),
        layers.Dense(num_actions, activation="linear") # Linear activation for Q-values
    ])
    return model

"""
## N-step Q-Learning Implementation

Here is the core of the n-step Q-learning algorithm. We will run for a specified number of episodes. In each episode, we collect states, actions, and rewards. At the end of the episode, we calculate the discounted cumulative rewards (also called returns) and use them as targets to update our Q-network in a single batch.
"""

def train_dql(num_episodes, decay_rate, learning_rate):
    """Trains a Deep Q-Learning model on the CartPole environment."""
    q_network = create_q_model()
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    mse_loss = tf.keras.losses.MeanSquaredError()

    global_step_counter = 0
    episode_rewards_history = []

    # Maximum possible reward for normalization, as per instructions.
    # The environment caps episodes at 500 steps.
    max_possible_reward = 500.0

    print("Starting training...")
    pbar = tqdm(range(num_episodes))
    for episode in pbar:
        # ---- 1. Collect experience by playing one episode ----
        state, _ = env.reset()
        episode_states = []
        episode_actions = []
        episode_rewards = []
        done = False

        while not done:
            global_step_counter += 1

            # Epsilon-greedy action selection
            epsilon = math.exp(-decay_rate * global_step_counter)
            if random.uniform(0, 1) < epsilon:
                action = env.action_space.sample()  # Explore
            else:
                q_values = q_network(tf.expand_dims(state, 0))
                action = tf.argmax(q_values[0]).numpy()  # Exploit

            # Apply action
            next_state, reward, done, _, _ = env.step(action)

            # Store experience
            episode_states.append(state)
            episode_actions.append(action)
            episode_rewards.append(reward)

            state = next_state

        episode_rewards_history.append(len(episode_rewards))

        # ---- 2. Calculate discounted cumulative rewards (returns) ----
        n_steps = len(episode_rewards)
        returns = np.zeros_like(episode_rewards, dtype=np.float32)
        running_return = 0.0
        # Iterate backwards from the last step
        for i in reversed(range(n_steps)):
            running_return = episode_rewards[i] + gamma * running_return
            returns[i] = running_return

        # ---- 3. Normalize returns ----
        returns_normalized = returns / max_possible_reward

        # ---- 4. Prepare data for batch update ----
        states_tensor = tf.convert_to_tensor(episode_states, dtype=tf.float32)
        actions_tensor = tf.convert_to_tensor(episode_actions, dtype=tf.int32)
        returns_tensor = tf.convert_to_tensor(returns_normalized, dtype=tf.float32)

        # Create indices for tf.gather_nd to select the Q-values of actions taken
        action_indices = tf.stack([tf.range(n_steps, dtype=tf.int32), actions_tensor], axis=1)

        # ---- 5. Update the Q-network ----
        with tf.GradientTape() as tape:
            # Predict Q-values for all actions for the states in the episode
            all_q_values = q_network(states_tensor)
            # Select the Q-values for the actions that were actually taken
            action_q_values = tf.gather_nd(all_q_values, action_indices)
            # Calculate the loss between the predicted Q-values and the calculated returns
            loss = mse_loss(returns_tensor, action_q_values)

        # Calculate gradients and update the model
        grads = tape.gradient(loss, q_network.trainable_variables)
        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))

        # Print progress
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards_history[-100:])
            pbar.set_description(f"Episode {episode + 1}/{num_episodes}, Avg Reward (last 100): {avg_reward:.2f}, Epsilon: {epsilon:.4f}")

    print("\nTraining finished.")
    return q_network, episode_rewards_history

def evaluate_policy_dql(env, q_network, num_trials=100):
    """Evaluates the performance of a trained DQL policy."""
    print(f"\nEvaluating policy over {num_trials} trials...")
    total_steps = 0
    success_count = 0
    all_steps = []

    for _ in range(num_trials):
        state, _ = env.reset()
        done = False
        steps = 0
        while not done:
            q_values = q_network(tf.expand_dims(state, 0))
            action = tf.argmax(q_values[0]).numpy()
            state, _, done, _, _ = env.step(action)
            steps += 1

        all_steps.append(steps)
        total_steps += steps
        if steps >= 200:
            success_count += 1

    avg_steps = total_steps / num_trials
    success_rate = success_count / num_trials

    print(f"Average steps per episode: {avg_steps:.2f}")
    print(f"Success rate (>= 200 steps): {success_rate:.2%}")
    return avg_steps, success_rate

"""
### Experiment 1: Standard Training

We train for 3000 episodes with a slow epsilon decay rate. This gives the agent ample time to explore the environment and learn a robust policy.
"""

# Experiment 1: Parameters from instructions
num_episodes_exp1 = 3000
decay_rate_exp1 = 0.00001
learning_rate_exp1 = 0.001

trained_model_exp1, _ = train_dql(num_episodes_exp1, decay_rate_exp1, learning_rate_exp1)
evaluate_policy_dql(env, trained_model_exp1)

"""
### Experiment 2: Rapid Decay and Short Training

Now, we drastically change the parameters. We train for only 100 episodes and use a much faster epsilon decay rate.
"""

# Experiment 2: Modified parameters
num_episodes_exp2 = 100
decay_rate_exp2 = 0.001
learning_rate_exp2 = 0.001

trained_model_exp2, _ = train_dql(num_episodes_exp2, decay_rate_exp2, learning_rate_exp2)
evaluate_policy_dql(env, trained_model_exp2)

env.close()

"""
### Why did the performance drop so drastically in Experiment 2?

The performance plummeted in the second experiment due to two interconnected factors:

1.  **Premature Exploitation:** The `decay_rate` was increased from `0.00001` to `0.001`. This causes the exploration probability, `epsilon`, to decrease very rapidly. The agent stops taking random actions and starts exploiting its learned knowledge far too early. Since it has only been trained for a few episodes, its "knowledge" is based on a tiny, unrepresentative sample of the environment. It likely latches onto a poor, short-sighted strategy and never explores enough to find the better, long-term solution required to balance the pole for 200+ steps.

2.  **Insufficient Training Data:** Training for only 100 episodes is not enough time for the neural network to learn the complex dynamics of the CartPole environment. In the early stages, episodes are very short as the agent's policy is essentially random. With only 100 short episodes, the total number of (state, action, reward) samples collected is extremely small. The Q-network cannot generalize from such sparse data and fails to learn a meaningful policy.

In essence, the second experiment combines the worst of both worlds: it forces the agent to commit to a strategy before it has had a chance to explore (due to rapid epsilon decay) and it doesn't provide enough experience for any strategy it learns to be a good one (due to the low number of episodes). The first experiment succeeded because the long training duration and slow decay rate allowed for a healthy balance between exploration and exploitation, which is crucial for effective reinforcement learning.
"""



================================================
File: hw4/temp.py
================================================
# %% [markdown]
# # Part 2: n-step Deep Q-Learning (Cart Pole)
#
# In this part, we implement an n-step Deep Q-Learning agent to solve the CartPole-v1 environment. We will use a Keras neural network to approximate the Q-function. The agent will be trained by running full episodes and then performing a batch update on the network using the collected experience.

# %%
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import gymnasium as gym
import math
from tqdm import tqdm

# %% [markdown]
# ## Setting up the Environment and Model
#
# First, we set the seed for reproducibility across all relevant libraries. Then we define a function to create our Q-network. The network will be a simple multi-layer perceptron (MLP) with one hidden layer, as specified in the instructions.

# %%
# Configuration parameters
seed = 1234
gamma = 0.99  # Discount factor for past rewards

# Set seeds
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# Create the environment
env = gym.make("CartPole-v1", render_mode='rgb_array')
env.reset(seed=seed)
env.action_space.seed(seed)

num_actions = env.action_space.n
state_shape = env.observation_space.shape

def create_q_model():
    """Creates a Keras model for the Q-network."""
    model = keras.Sequential([
        layers.InputLayer(input_shape=state_shape),
        layers.Dense(32, activation="relu"),
        layers.Dense(num_actions, activation="linear") # Linear activation for Q-values
    ])
    return model

# %% [markdown]
# ## N-step Q-Learning Implementation
#
# Here is the core of the n-step Q-learning algorithm. We will run for a specified number of episodes. In each episode, we collect states, actions, and rewards. At the end of the episode, we calculate the discounted cumulative rewards (also called returns) and use them as targets to update our Q-network in a single batch.

# %%
def train_dql(num_episodes, decay_rate, learning_rate):
    """Trains a Deep Q-Learning model on the CartPole environment."""
    q_network = create_q_model()
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    mse_loss = tf.keras.losses.MeanSquaredError()

    global_step_counter = 0
    episode_rewards_history = []

    # Maximum possible reward for normalization, as per instructions.
    # The environment caps episodes at 500 steps.
    max_possible_reward = 500.0

    print("Starting training...")
    pbar = tqdm(range(num_episodes))
    for episode in pbar:
        # ---- 1. Collect experience by playing one episode ----
        state, _ = env.reset()
        episode_states = []
        episode_actions = []
        episode_rewards = []
        done = False

        while not done:
            global_step_counter += 1

            # Epsilon-greedy action selection
            epsilon = math.exp(-decay_rate * global_step_counter)
            if random.uniform(0, 1) < epsilon:
                action = env.action_space.sample()  # Explore
            else:
                q_values = q_network(tf.expand_dims(state, 0))
                action = tf.argmax(q_values[0]).numpy()  # Exploit

            # Apply action
            next_state, reward, done, _, _ = env.step(action)

            # Store experience
            episode_states.append(state)
            episode_actions.append(action)
            episode_rewards.append(reward)

            state = next_state

        episode_rewards_history.append(len(episode_rewards))

        # ---- 2. Calculate discounted cumulative rewards (returns) ----
        n_steps = len(episode_rewards)
        returns = np.zeros_like(episode_rewards, dtype=np.float32)
        running_return = 0.0
        # Iterate backwards from the last step
        for i in reversed(range(n_steps)):
            running_return = episode_rewards[i] + gamma * running_return
            returns[i] = running_return

        # ---- 3. Normalize returns ----
        returns_normalized = returns / max_possible_reward

        # ---- 4. Prepare data for batch update ----
        states_tensor = tf.convert_to_tensor(episode_states, dtype=tf.float32)
        actions_tensor = tf.convert_to_tensor(episode_actions, dtype=tf.int32)
        returns_tensor = tf.convert_to_tensor(returns_normalized, dtype=tf.float32)

        # Create indices for tf.gather_nd to select the Q-values of actions taken
        action_indices = tf.stack([tf.range(n_steps, dtype=tf.int32), actions_tensor], axis=1)

        # ---- 5. Update the Q-network ----
        with tf.GradientTape() as tape:
            # Predict Q-values for all actions for the states in the episode
            all_q_values = q_network(states_tensor)
            # Select the Q-values for the actions that were actually taken
            action_q_values = tf.gather_nd(all_q_values, action_indices)
            # Calculate the loss between the predicted Q-values and the calculated returns
            loss = mse_loss(returns_tensor, action_q_values)

        # Calculate gradients and update the model
        grads = tape.gradient(loss, q_network.trainable_variables)
        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))

        # Print progress
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards_history[-100:])
            pbar.set_description(f"Episode {episode + 1}/{num_episodes}, Avg Reward (last 100): {avg_reward:.2f}, Epsilon: {epsilon:.4f}")

    print("\nTraining finished.")
    return q_network, episode_rewards_history

def evaluate_policy_dql(env, q_network, num_trials=100):
    """Evaluates the performance of a trained DQL policy."""
    print(f"\nEvaluating policy over {num_trials} trials...")
    total_steps = 0
    success_count = 0
    all_steps = []

    for _ in range(num_trials):
        state, _ = env.reset()
        done = False
        steps = 0
        while not done:
            q_values = q_network(tf.expand_dims(state, 0))
            action = tf.argmax(q_values[0]).numpy()
            state, _, done, _, _ = env.step(action)
            steps += 1

        all_steps.append(steps)
        total_steps += steps
        if steps >= 200:
            success_count += 1

    avg_steps = total_steps / num_trials
    success_rate = success_count / num_trials

    print(f"Average steps per episode: {avg_steps:.2f}")
    print(f"Success rate (>= 200 steps): {success_rate:.2%}")
    return avg_steps, success_rate

# %% [markdown]
# ### Experiment 1: Standard Training
#
# We train for 3000 episodes with a slow epsilon decay rate. This gives the agent ample time to explore the environment and learn a robust policy.

# %%
# Experiment 1: Parameters from instructions
num_episodes_exp1 = 3000
decay_rate_exp1 = 0.00001
learning_rate_exp1 = 0.001

trained_model_exp1, _ = train_dql(num_episodes_exp1, decay_rate_exp1, learning_rate_exp1)
evaluate_policy_dql(env, trained_model_exp1)

# %% [markdown]
# ### Experiment 2: Rapid Decay and Short Training
#
# Now, we drastically change the parameters. We train for only 100 episodes and use a much faster epsilon decay rate.

# %%
# Experiment 2: Modified parameters
num_episodes_exp2 = 100
decay_rate_exp2 = 0.001
learning_rate_exp2 = 0.001

trained_model_exp2, _ = train_dql(num_episodes_exp2, decay_rate_exp2, learning_rate_exp2)
evaluate_policy_dql(env, trained_model_exp2)

env.close()

# %% [markdown]
# ### Why did the performance drop so drastically in Experiment 2?
#
# The performance plummeted in the second experiment due to two interconnected factors:
#
# 1.  **Premature Exploitation:** The `decay_rate` was increased from `0.00001` to `0.001`. This causes the exploration probability, `epsilon`, to decrease very rapidly. The agent stops taking random actions and starts exploiting its learned knowledge far too early. Since it has only been trained for a few episodes, its "knowledge" is based on a tiny, unrepresentative sample of the environment. It likely latches onto a poor, short-sighted strategy and never explores enough to find the better, long-term solution required to balance the pole for 200+ steps.
#
# 2.  **Insufficient Training Data:** Training for only 100 episodes is not enough time for the neural network to learn the complex dynamics of the CartPole environment. In the early stages, episodes are very short as the agent's policy is essentially random. With only 100 short episodes, the total number of (state, action, reward) samples collected is extremely small. The Q-network cannot generalize from such sparse data and fails to learn a meaningful policy.
#
# In essence, the second experiment combines the worst of both worlds: it forces the agent to commit to a strategy before it has had a chance to explore (due to rapid epsilon decay) and it doesn't provide enough experience for any strategy it learns to be a good one (due to the low number of episodes). The first experiment succeeded because the long training duration and slow decay rate allowed for a healthy balance between exploration and exploitation, which is crucial for effective reinforcement learning.



================================================
File: paper/.DS_Store
================================================
[Non-text file]





================================================
File: project/notes.md
================================================
- change of scope
  - Instead of customizing a fully personalized local assistant, which I now don't want to do because sourcing the data for that (a bunch of facts about myself) would be hard, I think I will train it on basic facts about myself pertaining to interview and related stuff. For me this is basic interview things about myself, and code projects that I've written.
  - Basic facts include but not limited to:
    - name
    - goal
    - school
    - work experience
    - common interview questions
  - My code includes:
    - My github repos



================================================
File: project/proposal.md
================================================
# Project Proposal: On-Device Personalized Agent using Fine-tuned Qwen3-0.6B

Brandon Howell

## 1. Introduction

<ins>Scope:</ins> This project aims to create a personalized AI assistant capable of running directly on an Android smartphone. The core involves fine-tuning a small, large language model (LLM), specifically Qwen3-0.6B[^qwen], with personal user data and other Android domain specific information. Subsequently, this personalized model will be integrated into an experimental Android agent framework (potentially a fork of Gosling[^gosling]) to enable it to perform tasks by interacting with other applications on my device.

[^qwen]: https://huggingface.co/Qwen/Qwen3-0.6B
[^gosling]: https://github.com/block/gosling

<ins>Problem:</ins> Current large language models most people are familiar with and use, while powerful, are typically generic and operate in the cloud, which raises privacy concerns and lacks more personalization. Running large models locally on resource-constrained devices like smartphones is computationally challenging. Enabling these models to interact with the device's applications and data in an automated fashion (i.e., act as agents) presents significant technical hurdles in the mobile environment. This project addresses the need for a private, personalized, and capable AI assistant that lives entirely on a user's device.

<ins>Interest & Non-Triviality:</ins> This project is interesting because it tackles the intersection of several cutting-edge AI domains that I've been following the news of closely for a while now:
- <ins>On-Device AI:</ins> Deploying capable LLMs on mobile phones pushes the boundaries of model optimization and efficient inference while preserving privacy.
- <ins>LLM Personalization:</ins> Fine-tuning with personal data explores methods to make AI truly tailored to an individual user's context, habits, and preferences.
- <ins>Agentic AI on Mobile:</ins> Integrating an LLM with device automation frameworks like Gosling explores the practical challenges and potential of creating autonomous agents in the complex and varied Android ecosystem.
- <ins>Parameter-Efficient Fine-Tuning (PEFT):</ins> Experimenting with techniques like adapters offers insights into efficient model adaptation for resource-constrained environments.

The combination of fine-tuning a very recent small LLM, deploying it locally, integrating personal context, and building agentic capabilities makes this project ambitious and non-trivial, offering ample room for experimentation and learning.

![Mobile LLM Agent](preprints-145259-g005.png)[^mobileagentimage]

[^mobileagentimage]: https://www.preprints.org/frontend/picture/ms_xml/manuscript/3da31779a6c7c196419c841cee5292cf/preprints-145259-g005.png

## 2. Related Work

The development of personalized, on-device AI agents draws upon research in several areas:

- <ins>Small Language Models (SLMs):</ins> The trend towards smaller, yet capable, language models is necessary for on-device deployment. Models like Phi-3 (Microsoft), Gemma (Google), and the chosen Qwen3-0.6B (Alibaba) represent efforts to achieve high performance with significantly fewer parameters than models like GPT-3/4. Their smaller size makes them candidates for mobile inference. The Qwen3 series specifically notes optimization for resource-constrained environments[^qwen3blog].
- <ins>On-Device LLM Inference:</ins> Techniques for efficiently running LLMs on mobile devices are critical. This includes model quantization (reducing the precision of model weights), optimized inference engines (like `llama.cpp`, MediaPipe LLM Inference, ONNX Runtime), and leveraging mobile NPUs/GPUs[^efficiency].
- <ins>LLM Fine-Tuning for Personalization:</ins> Adapting pre-trained LLMs to specific domains or user data is a common practice. While full fine-tuning modifies all parameters, **Parameter-Efficient Fine-Tuning (PEFT)** methods have gained prominence. Techniques like Adapter Tuning (Houlsby et al., 2019[^adapterspaper]), LoRA (Hu et al., 2021[^lora]), and QLoRA (Dettmers et al., 2023[^qlora]) allow adaptation by training only a small number of additional parameters. This is particularly relevant for on-device scenarios, reducing computational cost and storage requirements for personalized models.
![Adapters Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c855ba-814f-4f95-9b1b-c97a46eb2f42_1646x880.png)[^adaptersimage]
- <ins>LLM-Powered Agents:</ins> Frameworks enabling LLMs to use tools, plan, and execute tasks have emerged (e.g., LangChain, AutoGPT). Applying these concepts to mobile environments involves interacting with apps and system services. Projects like Gosling from Block[^gosling] specifically explore agentic capabilities on Android, using platform features like Accessibility Services or custom inter-app communication protocols (like their proposed "mobile MCP") to automate tasks. This relates to broader research on autonomous agents and tool augmentation for LLMs.

[^qwen3blog]: https://qwenlm.github.io/blog/qwen3/
[^efficiency]: https://arxiv.org/abs/2312.03863
[^adapterspaper]: https://arxiv.org/abs/1902.00751
[^lora]: https://arxiv.org/abs/2106.09685
[^qlora]: https://arxiv.org/abs/2305.14314
[^adaptersimage]: https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters

## 3. Method

This project will be developed in two main phases:

**Phase 1: Personalized On-Device LLM Fine-tuning**

1. <ins>Model Selection:</ins> The core model will be Qwen3-0.6B, chosen for its small size and state-of-the-art performance within its parameter class[^qwen].
2. <ins>Data Collection & Preparation:</ins> A personal dataset will be curated, which may include (but is not limited to):
   - Calendar entries (structure, common events, timings).
   - Anonymized or synthesized communication logs (SMS/email snippets reflecting personal style, common contacts).
   - Personal notes or documents (FAQs about oneself, preferences, routines).
   - App usage patterns (e.g., frequently used apps for specific tasks).
   - Location habits (e.g., common places like "home", "work", "gym").
   - The format will likely be structured prompts and desired responses (instruction fine-tuning format).
3. <ins>Fine-tuning Implementation:</ins> Two fine-tuning approaches will be explored using libraries like Hugging Face `transformers` and `peft`:
   - <ins>Full Fine-tuning:</ins> Update all weights of the Qwen3-0.6B model on the curated personal dataset.
   - <ins>Adapter-based Fine-tuning (PEFT):</ins> Freeze the base Qwen3-0.6B model and train lightweight adapter modules[^adapterspaper][^lora][^qlora].
4. <ins>On-Device Deployment:</ins> The fine-tuned models (both versions) will be prepared for on-device execution. This will likely involve:
   - Quantization (e.g., to 4-bit using GGUF format).
   - Using an on-device inference engine compatible with Qwen models and Android, such as `llama.cpp`'s Android bindings or potentially adapting other frameworks if necessary.
   - Building a simple Android application wrapper to load the model and allow text-based interaction for initial testing.

**Phase 2: Agentic Integration with Gosling**

1. <ins>Framework Setup:</ins> Fork the Gosling Android agent repository. Familiarize with its architecture, particularly how it invokes LLMs and interacts with device capabilities (Accessibility Services, Intents, potential "mobile MCP").
2. <ins>Model Integration:</ins> Modify the forked Gosling code to use the personalized, locally deployed Qwen3-0.6B model (likely the PEFT version due to efficiency) as its reasoning engine. This will involve adapting the LLM API calls within Gosling to interface with the chosen on-device inference engine.
3. <ins>Tool Definition & Use:</ins> Explore Gosling's mechanisms for defining and using tools. This might involve:
    - Leveraging existing Gosling capabilities triggered via natural language prompts.
    - Potentially implementing a simple custom "mobile MCP" provider app (as shown in the Gosling README example) to expose a new capability (e.g., retrieving a piece of personal info directly).
4. <ins>Task Automation:</ins> Define and test simple, multi-step tasks that require the agent to use the personalized LLM and interact with other apps via Gosling's capabilities.

## 4. Evaluation

Success will be evaluated based on experimentation depth and insights gained, rather than solely on flawless execution.

**Phase 1 Evaluation (Personalized LLM):**

- <ins>Dataset:</ins> A held-out set of prompts based on the personal data domain (e.g., questions about schedule, preferences, communication style).
- <ins>Metrics:</ins>
  - <ins>Qualitative Assessment:</ins> Subjective evaluation of the model's responses for personalization (does it know my context?), accuracy (correct information), coherence, and tone compared to the base Qwen3 model.
  - <ins>Quantitative Assessment (Exploratory):</ins>
    - Perplexity on the held-out personal dataset.
    - Resource Usage: Measure model storage size (base vs. full fine-tune vs. base + adapter), inference latency, and peak memory usage on the Android device for both fine-tuning approaches.
  - <ins>Comparison:</ins> Directly compare the effectiveness and resource trade-offs between the fully fine-tuned model and the adapter-based model.

**Phase 2 Evaluation (Agentic Capabilities):**

- <ins>Dataset:</ins> A set of defined, multi-step tasks requiring interaction with device features or apps.
- <ins>Metrics:</ins>
  - <ins>Task Completion Rate:</ins> Percentage of tasks successfully completed by the agent.
  - <ins>Qualitative Assessment:</ins> Evaluate the agent's planning ability, tool usage effectiveness, error handling, and overall usefulness for the defined tasks. Does the personalization from Phase 1 demonstrably improve task execution (e.g., using correct contact names, understanding implicit context)?
  - <ins>Efficiency (Observational):</ins> Note the time taken and number of steps/interactions required for task completion.

## 5. Milestones

- <ins>Week 6:</ins> Download Qwen model and test its current capabilities, begin making fine tuning dataset
- <ins>Week 7:</ins> Fine tune and test new model, evaulate success, where new model succeeds that old model didn't
- <ins>Week 8:</ins> Assuming above has gone well, fork Gosling and begin working on Agent integration
- <ins>Week 9:</ins> Combine fine-tuned Qwen3 with Gosling to automate tasks, test and get quantitative results of success
- <ins>Week 10:</ins> Wrap up any last features, test, and evaluations and finish report

## 6. Task Assignments

I'm working solo for this project since there was some miscommunication with groups, so don't have to worry about splitting up tasks.




================================================
File: project/.DS_Store
================================================
[Non-text file]


