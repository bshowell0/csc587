Directory structure:
└── jonathanventura-3d-pano-inpainting/
    ├── README.md
    ├── LICENSE
    ├── data/
    │   ├── credits.txt
    │   └── data.txt
    ├── depth-estimation/
    │   ├── Dockerfile
    │   ├── run_360monodepth.sh
    │   └── 360monodepth/
    │       ├── README.md
    │       ├── LICENSE
    │       ├── BoostingMonocularDepth/
    │       │   ├── README.md
    │       │   ├── Boostmonoculardepth.ipynb
    │       │   ├── demo.py
    │       │   ├── LICENSE
    │       │   ├── requirements.txt
    │       │   ├── run.py
    │       │   ├── utils.py
    │       │   ├── .DS_Store
    │       │   ├── .gitignore
    │       │   ├── dataset_prepare/
    │       │   │   ├── create_crops.m
    │       │   │   ├── generatecrops.m
    │       │   │   ├── ibims1_prepare.m
    │       │   │   ├── ibims1_selected.mat
    │       │   │   ├── mergenet_dataset_prepare.md
    │       │   │   └── midas/
    │       │   │       ├── run.py
    │       │   │       ├── utils.py
    │       │   │       └── models/
    │       │   │           ├── base_model.py
    │       │   │           ├── blocks.py
    │       │   │           ├── midas_net.py
    │       │   │           └── transforms.py
    │       │   ├── evaluation/
    │       │   │   ├── D3R.m
    │       │   │   ├── evaluatedataset.m
    │       │   │   ├── extractD3Rpoints.m
    │       │   │   ├── index2index.m
    │       │   │   └── ord.m
    │       │   ├── figures/
    │       │   ├── inputs/
    │       │   ├── midas/
    │       │   │   ├── utils.py
    │       │   │   └── models/
    │       │   │       ├── base_model.py
    │       │   │       ├── blocks.py
    │       │   │       ├── midas_net.py
    │       │   │       └── transforms.py
    │       │   ├── pix2pix/
    │       │   │   ├── test.py
    │       │   │   ├── train.py
    │       │   │   ├── .DS_Store
    │       │   │   ├── data/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── base_dataset.py
    │       │   │   │   ├── depthmerge_dataset.py
    │       │   │   │   └── image_folder.py
    │       │   │   ├── models/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── base_model.py
    │       │   │   │   ├── base_model_hg.py
    │       │   │   │   ├── networks.py
    │       │   │   │   └── pix2pix4depth_model.py
    │       │   │   ├── options/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── base_options.py
    │       │   │   │   ├── test_options.py
    │       │   │   │   └── train_options.py
    │       │   │   └── util/
    │       │   │       ├── __init__.py
    │       │   │       ├── get_data.py
    │       │   │       ├── guidedfilter.py
    │       │   │       ├── html.py
    │       │   │       ├── image_pool.py
    │       │   │       ├── util.py
    │       │   │       └── visualizer.py
    │       │   └── structuredrl/
    │       │       └── models/
    │       │           ├── DepthNet.py
    │       │           ├── networks.py
    │       │           ├── resnet.py
    │       │           └── syncbn/
    │       │               ├── README.md
    │       │               ├── LICENSE
    │       │               ├── make_ext.sh
    │       │               ├── requirements.txt
    │       │               ├── test.py
    │       │               └── modules/
    │       │                   ├── __init__.py
    │       │                   ├── functional/
    │       │                   │   ├── __init__.py
    │       │                   │   ├── syncbn.py
    │       │                   │   └── _syncbn/
    │       │                   │       ├── __init__.py
    │       │                   │       ├── build.py
    │       │                   │       ├── _ext/
    │       │                   │       │   ├── __init__.py
    │       │                   │       │   └── syncbn/
    │       │                   │       │       └── __init__.py
    │       │                   │       └── src/
    │       │                   │           ├── common.h
    │       │                   │           ├── syncbn.cpp
    │       │                   │           ├── syncbn.cu
    │       │                   │           ├── syncbn.cu.h
    │       │                   │           └── syncbn.h
    │       │                   └── nn/
    │       │                       ├── __init__.py
    │       │                       └── syncbn.py
    │       ├── code/
    │       │   ├── cpp/
    │       │   │   ├── README.md
    │       │   │   ├── CMakeConfig.txt
    │       │   │   ├── CMakeLists.txt
    │       │   │   ├── main_depthmapstitch.cpp
    │       │   │   ├── 3rd_party/
    │       │   │   │   ├── readme.md
    │       │   │   │   └── pfm_io.hpp
    │       │   │   ├── include/
    │       │   │   │   ├── data_imitator.hpp
    │       │   │   │   ├── data_io.hpp
    │       │   │   │   ├── depthmap_stitcher.hpp
    │       │   │   │   ├── depthmap_stitcher_enum.hpp
    │       │   │   │   ├── depthmap_stitcher_group.hpp
    │       │   │   │   ├── depthmap_utility.hpp
    │       │   │   │   ├── python_binding.hpp
    │       │   │   │   └── timer.hpp
    │       │   │   ├── python/
    │       │   │   │   ├── README.md
    │       │   │   │   ├── LICENSE.txt
    │       │   │   │   ├── MANIFEST.in
    │       │   │   │   ├── pyproject.toml
    │       │   │   │   ├── reinstall.bat
    │       │   │   │   ├── reinstall.sh
    │       │   │   │   ├── setup.cfg
    │       │   │   │   ├── setup.py
    │       │   │   │   └── instaOmniDepth/
    │       │   │   │       ├── __init__.py
    │       │   │   │       └── depthmapAlignModule.cpp
    │       │   │   ├── src/
    │       │   │   │   ├── data_imitator.cpp
    │       │   │   │   ├── data_io.cpp
    │       │   │   │   ├── depthmap_stitcher.cpp
    │       │   │   │   ├── depthmap_stitcher_enum.cpp
    │       │   │   │   ├── depthmap_stitcher_group.cpp
    │       │   │   │   ├── depthmap_utility.cpp
    │       │   │   │   ├── EigenSolvers.cpp
    │       │   │   │   └── python_binding.cpp
    │       │   │   └── test/
    │       │   │       └── depth_stitch_test.cpp
    │       │   └── python/
    │       │       ├── requirements.txt
    │       │       └── src/
    │       │           ├── main.py
    │       │           └── utility/
    │       │               ├── __init__.py
    │       │               ├── blending.py
    │       │               ├── cam_models.py
    │       │               ├── data_mocker.py
    │       │               ├── depth_stitch.py
    │       │               ├── depthmap_align.py
    │       │               ├── depthmap_utils.py
    │       │               ├── fs_utility.py
    │       │               ├── gnomonic_projection.py
    │       │               ├── image_io.py
    │       │               ├── logger.py
    │       │               ├── metrics.py
    │       │               ├── plot_figure.py
    │       │               ├── pointcloud_utils.py
    │       │               ├── polygon.py
    │       │               ├── projection_icosahedron.py
    │       │               ├── serialization.py
    │       │               ├── spherical_coordinates.py
    │       │               └── subimage.py
    │       ├── data/
    │       │   ├── erp_00_data.txt
    │       │   └── erp_00/
    │       │       └── 0001_depth.dpt
    │       └── imgs/
    ├── docs/
    │   ├── demo.html
    │   ├── index.html
    │   ├── index_copy.html
    │   ├── links.html
    │   ├── main.css
    │   ├── meshopt_decoder.js
    │   ├── renderer-uv.html
    │   ├── renderer.html
    │   ├── study.html
    │   └── assets/
    │       └── mesh_thumbnails/
    ├── inpainting/
    │   ├── README.md
    │   ├── argument.yml
    │   ├── argument_p2m.yml
    │   ├── bilateral_filtering.py
    │   ├── boostmonodepth_utils.py
    │   ├── Dockerfile
    │   ├── DOCUMENTATION.md
    │   ├── download.sh
    │   ├── LICENSE
    │   ├── main.py
    │   ├── mesh.py
    │   ├── mesh_tools.py
    │   ├── networks.py
    │   ├── pano2mesh.py
    │   ├── run_3d_photo_inpainting.sh
    │   ├── run_pano2mesh.sh
    │   ├── time.py
    │   ├── utils.py
    │   ├── MiDaS/
    │   │   ├── MiDaS_utils.py
    │   │   ├── monodepth_net.py
    │   │   └── run.py
    │   └── pano/
    │       └── credits.txt
    ├── mesh/
    │   ├── estimate_scale_histogram.py
    │   └── subdivide.py
    └── results/
        └── README.md

================================================
FILE: README.md
================================================
<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown "reference style" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->
# 3D Panorama Inpainting

<!-- ABOUT THE PROJECT -->

![image](docs/assets/model.png)

3D Pano Inpainting: Building a VR Environment from a Single Input Panorama<br>
Shivam Asija, Edward Du, Nam Nguyen, Stefanie Zollmann, Jonathan Ventura<br>
2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)

Abstract:
Creating 360-degree 3D content is challenging because it requires either a multi-camera rig or a collection of many images taken from different perspectives. Our approach aims to generate a 360◦ VR scene from a single panoramic image using a learning-based inpainting method adapted for panoramic content. We introduce a pipeline capable of transforming an equirectangular panoramic RGB image into a complete 360◦ 3D virtual reality scene represented as a textured mesh, which is easily rendered on a VR headset using standard graphics rendering pipelines. We qualitatively evaluate our results on a synthetic dataset consisting of 360 panoramas in indoor scenes.


<!-- GETTING STARTED -->

## Getting Started

To get a local copy of the project up and running on your machine, follow these simple steps:

### Prerequisites

Before attempting to build this project, make sure you have [Docker Engine](https://docs.docker.com/engine/install/) installed on your machine.

### Installation

Clone the repository
```
git clone https://github.com/jonathanventura/3d-pano-inpainting.git
cd 3d-pano-inpainting
```

### Processing panoramic images

2. Place your panoramic images in the ```data``` directory.  The images should be in equirectangular format and the width of each image should be twice the height.  You will also need to update the filenames in ```data/data.txt```.
3. Execute the depth estimation step
```
sh depth-estimation/run_360monodepth.sh
```   
5. Execute the meshing and inpainting step
```
sh inpainting/run_3d_photo_inpainting.sh
```
6. The results are placed in the ```results``` directory.
7. Re-scale the mesh according to the known height of the camera off of the ground:
```
python mesh/estimate_scale_histogram.py <input mesh> <output mesh> [--camera_height <height>]
```

#### Notes ####

The inpainting code will resize the images to a fixed maximum side length determined by the ```longer_side_len``` parameter in ```inpainting/argument.yml```.

### Running the renderer in a web browser ###

Copy the resulting ```.glb``` file into ```docs/assets``` and update the path in ```docs/renderer.html``` accordingly.

To start the renderer you can use
```
python -m http.server
```

and then navigate to ```localhost:8000/renderer.html```.

<!-- LICENSE -->
<!-- https://choosealicense.com/ -->

## License

This project is distributed under the terms of the  MIT license. See [LICENSE](LICENSE) for details and more information.



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Jonathan Ventura

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: data/credits.txt
================================================
0001_rgb.jpg 
The Replica Dataset: A Digital Replica of Indoor Spaces
https://github.com/facebookresearch/Replica-Dataset

360_0265.png
Nam Nguyen
Creative Commons Attribution-Share Alike 4.0 International

360_0277.png
Nam Nguyen
Creative Commons Attribution-Share Alike 4.0 International

ampleben_dorfkirche_panorama.png
DerMische
Creative Commons Attribution-Share Alike 4.0 International
https://commons.wikimedia.org/wiki/File:Ampleben_Dorfkirche_Panorama.jpg

ascheberg_kirchplatz_panorama.png
DerMische
Creative Commons Attribution-Share Alike 4.0 International
https://commons.wikimedia.org/wiki/File:Ascheberg_Kirchplatz_Panorama.jpg

basel_martinsgasse_panorama.png
DerMische
Creative Commons Attribution-Share Alike 4.0 International
https://commons.wikimedia.org/wiki/File:Basel_Martinsgasse_Panorama.jpg

basel_stapfelberg_panorama.png
DerMische
Creative Commons Attribution-Share Alike 4.0 International
https://commons.wikimedia.org/wiki/File:Basel_Stapfelberg_Panorama.jpg

braunschweig_altstadt_panorama.png
DerMische
Creative Commons Attribution-Share Alike 4.0 International
https://commons.wikimedia.org/wiki/File:Braunschweig_Altstadt_Panorama.jpg

soissons_cathedral_interior.png
Diliff
Creative Commons Attribution-Share Alike 3.0 Unported license
https://en.wikipedia.org/wiki/File:Soissons_Cathedral_Interior_360x180,_Picardy,_France_-_Diliff.jpg




================================================
FILE: data/data.txt
================================================
/monodepth/data/ampleben_dorfkirche_panorama.png None
/monodepth/data/ascheberg_kirchplatz_panorama.png None
/monodepth/data/basel_martinsgasse_panorama.png None
/monodepth/data/basel_stapfelberg_panorama.png None
/monodepth/data/braunschweig_altstadt_panorama.png None
/monodepth/data/soissons_cathedral_interior.png None
/monodepth/data/360_0265.png None
/monodepth/data/360_0277.png None
/monodepth/data/001_rgb.jpg None


================================================
FILE: depth-estimation/Dockerfile
================================================
# build docker image: docker build -f Dockerfile -t mzy22/monodepth:v1.0 .
# create docker container with image: docker run --name=mzy22_monodepth -e COLUMNS=300 --mount type=bind,source="$(pwd)",target=/monodepth_dev -it --gpus all mzy22/monodepth:v1.0

# ubuntu 20.04/cuda
FROM nvidia/cuda:12.3.0-devel-ubuntu20.04
# FROM alpine:3.4

#-- setup building environment 
RUN apt-get update

# 1) set up cpp code building dependent 3rd party libraries
ARG DEBIAN_FRONTEND=noninteractive
ENV TZ=Europe/London

RUN apt-get install --no-install-recommends \
        build-essential \
         cmake \
         libeigen3-dev \
         libgoogle-glog-dev \
         libgtest-dev \
         libopencv-dev \
         libceres-dev \
         python3-pybind11 \
         git \
         wget \
         libboost1.71-dev  -y

# 2) set up python module's build environment
RUN apt install --no-install-recommends \
        libpython3-dev \
        python3-pip -y

# 3) set up python run environment
# Put everything in some subfolder
WORKDIR "/monodepth"
COPY 360monodepth ./

# Setup BoostingMonocularDepth
#RUN cd ./BoostingMonocularDepth/pix2pix/ && mkdir -p checkpoints/mergemodel
# Midas weights
RUN wget https://github.com/isl-org/MiDaS/releases/download/v2_1/model-f6b98070.pt -O ./BoostingMonocularDepth/midas/model.pt
# Merge net weights
#RUN wget -P ./BoostingMonocularDepth/pix2pix/checkpoints/mergemodel https://www.sfu.ca/~yagiz/CVPR21/latest_net_G.pth

RUN pip3 install -r ./code/python/requirements.txt

#-- build python cpp module
# 1) build the cpp project
RUN cd ./code/cpp && mkdir build && cd build && cmake ..  -DCMAKE_BUILD_TYPE=Release && make -j

# 2) build & install python module
RUN cd ./code/cpp/python/ && python3 ./setup.py build && python3 ./setup.py bdist_wheel && pip3 install dist/instaOmniDepth-0.1.0-cp38-cp38-linux_x86_64.whl

RUN chmod -R a+rwX /monodepth

ENV HF_HOME /monodepth/cache



================================================
FILE: depth-estimation/run_360monodepth.sh
================================================
docker build -t 360monodepth depth-estimation/.
docker run -u $(id -u):$(id -g) -e USER=$USER -e XDG_CACHE_HOME=$XDG_CACHE_HOME -it --rm -e CUDA_VISIBLE_DEVICES=0 -v ./data:/monodepth/data -v ./results:/monodepth/results -v $XDG_CACHE_HOME/huggingface/hub:/cache 360monodepth sh -c "cd /monodepth/code/python/src; python3 main.py --expname test_experiment --blending_method frustum --grid_size 8x7"




================================================
FILE: depth-estimation/360monodepth/README.md
================================================
# 360MonoDepth
### [Paper](https://arxiv.org/abs/2111.15669) | [Project Page](https://manurare.github.io/360monodepth/)

This is the code for [360MonoDepth: High-Resolution 360° Monocular Depth Estimation](https://arxiv.org/abs/2111.15669)
 

 [Manuel Rey-Area](https://manurare.github.io/)\*,
 [Mingze Yuan](https://yuanmingze.github.io/)\*,
 [Christian Richardt](https://richardt.name/) <br>
 University of Bath  
  \*denotes equal contribution  
 __CVPR 2022__
<img src='imgs/pipeline.jpg'/>

## Setup

Tested with Python >= 3.8


Dependencies for C++ code:
 * Ceres 2.0.0
 * Eigen 3.3.9
 * Glog 0.5.0
 * Gflags 2.2.2
 * GTest 1.10.0
 * OpenCV 4.2.0
 * Boost 1.75.0
 * pybind11 2.8.1

Dependencies for python are in ```code/python/requirements.txt```


#### With Docker 
We recommend Docker to run 360MonoDepth to avoid problems with dependencies.
```
docker build -t 360monodepth .
docker run -it --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 360monodepth sh -c "cd /monodepth/code/python/src; python3 main.py --expname test_experiment --blending_method all --grid_size 8x7"
```

#### Without Docker 
1. We need to create a conda environment with python 3.8 and build the C++ targets

 
```
conda create -n 360monodepth python=3.8
conda activate 360monodepth
pip install -r code/python/requirements.txt
```

2. Build ```pybind11``` in ```code/cpp/3rd_party``` first (or ```apt-get install python3-pybind11```). Then, modify ```cmakeconfig``` and ```code/cpp/python/setup.py``` to add own paths to libraries/includes

```
cd code/cpp
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ../
make -j8
```
3. Copy all the dependent DLL/so files to ```code/cpp/python/[dll,so]```. For example, in Linux ```code/cpp/python/so``` should contain the following dynamic libraries: ```libamd.so.2, libcholmod.so.3, libglog.so, libm.so.6, libsuitesparseconfig.so.5, libblas.so.3, libcolamd.so.2, libglog.so.0, libopencv_core.so.4.2, libtbb.so.2, libcamd.so.2, libcxsparse.so.3, libgomp.so.1, libopencv_imgproc.so.4.2, libccolamd.so.2, libgflags.so.2.2, liblapack.so.3, libquadmath.so.0, libceres.so.2, libgfortran.so.5, libmetis.so.5, libspqr.so.2```

```
cd code/cpp/python
python setup.py build
python setup.py bdist_wheel
pip install code/cpp/python/dist/instaOmniDepth-0.1.0-cp38-cp38-linux_x86_64.whl
```

*OPTIONAL*: To add support for BoostingMonocularDepth
```
git submodule update --init
```
And download the required weights as indicated in their [README](https://github.com/compphoto/BoostingMonocularDepth#setup). 

## Running code
Always execute this command per new instance of shell. 

```
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/conda/envs/360monodepth/lib/python3.8/site-packages/instaOmniDepth
```

Now, we can run the code!

```
cd code/python/src
python main.py --expname test_experiment --blending_method all --grid_size 8x7
```

### Data
Data must be provided with the ```--data``` flag and must be a ```.txt``` file with the following structure:

```
/path/to/dataset/filename00_rgb.[png,jpg] /path/to/dataset/filename00_depth.dpt
/path/to/dataset/filename01_rgb.[png,jpg] /path/to/dataset/filename01_depth.dpt
		.				          .
		.				          .
		.				          .
```
An example can be found at ```data/erp_00_data.txt```.
In case of using data without GT, ```None``` should be written in the second column. 
## Citation

```
@inproceedings{reyarea2021360monodepth,
	title={{360MonoDepth}: High-Resolution 360{\deg} Monocular Depth Estimation},
	author={Manuel Rey-Area and Mingze Yuan and Christian Richardt},
	booktitle={CVPR},
	year={2022}}
```



================================================
FILE: depth-estimation/360monodepth/LICENSE
================================================
MIT License

Copyright (c) 2022 manurare

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/README.md
================================================
### **(NEW!)** [Boost Your Own depth](https://github.com/compphoto/BoostYourOwnDepth) with our new repo

We present a stand-alone implementation of our [Merging Operator](#method). This new repo allows using any pair of monocular depth estimations in our double estimation. This includes using separate networks for base and high-res estimations, using networks not supported by this repo (such as [Midas-v3](https://github.com/isl-org/MiDaS)), or using manually edited depth maps for artistic use. This will also be useful for scientists developing CNN-based MDE as a way to quickly apply double estimation to their own network. For more details please take a look [here](https://github.com/compphoto/BoostYourOwnDepth).

| Input | Original result | After manual editing of base|
|----|------------|------------|
|![patchselection](./figures/lunch_rgb.jpg)|![patchselection](./figures/lunch_orig.png)|![patchselection](./figures/lunch_edited.png)|


### **(NEW!)** [LeRes][2] is now supported within our method.

Here is a visualization of the improvement gained using [LeRes][2] instead of [MiDas][1].
|RGB | Our method using [MiDaS][1] | Our method using [LeRes][2] (**NEW**!) |
|----|------------|-----------|
|![patchselection](./inputs/sample2.jpg)|![Patchexpand](./figures/sample2_midas.png)|![Patchexpand](./figures/sample2_leres.jpg)|



### (NEW!) Maximum resolution can be set for a faster run time.

Use **\--max_res** as input argument for run.py in combination with **--Final** to set a limit on the resolution of the results that our method generates.

We provide this parameter as a trade-off between run-time and resolution. Using this reduces the run-time if only a result up to *specific-megapixel* is needed.

This parameter sets a limit on the bigger dimension of the result in term of pixels (while keeping aspect ratio). For example, to generate results with a bigger dimension size up to 2000 pixels use the following:   

```python
python run.py --Final --max_res 2000 --data_dir PATH_TO_INPUT --output_dir PATH_TO_RESULT --depthNet 0
```


### Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging 

> S. Mahdi H. Miangoleh\*, Sebastian Dille\*, Long Mai, Sylvain Paris, Yağız Aksoy.
> [Main pdf](http://yaksoy.github.io/papers/CVPR21-HighResDepth.pdf),
> [Supplementary pdf](http://yaksoy.github.io/papers/CVPR21-HighResDepth-Supp.pdf),
> [Project Page](http://yaksoy.github.io/highresdepth/).

[![video](./figures/video_thumbnail.jpg)](https://www.youtube.com/watch?v=lDeI17pHlqo)

We propose a method that can generate highly detailed high-resolution depth estimations from a single image. Our method is based on optimizing the performance of a pre-trained network by merging estimations in different resolutions and different patches to generate a high-resolution estimate. 

Try our model easily on Colab : [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/compphoto/BoostingMonocularDepth/blob/main/Boostmonoculardepth.ipynb)



### Change log:

* (**NEW!**) Now you can set the maximum resolution of the results to reduce runtime.  
* **(NEW!)** Our method implementation using [LeReS][2] is now available. [July 2021]
* A Quick overview of the method is now presented in README.md. [July 2021]
* [Google Colaboratory notebook](./Boostmonoculardepth.ipynb) is now available.  [June 2021]   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/compphoto/BoostingMonocularDepth/blob/main/Boostmonoculardepth.ipynb)
* Merge net training dataset generation [instructions](./dataset_prepare/mergenet_dataset_prepare.md) is now available. [June 2021] 
* Bug fix. [June 2021, July 2021]



## Method

We use **existing** *monocular depth estimation* networks to generate highly detailed estimations **without re-training**.

We achieve our results by getting several estimations at different resolutions. We then merge these into a structurally consistent high-resolution depth map followed by a local boosting to enhance the results and generate our final result.

![Overview](./figures/overview.png)



##### Observations

Monocular depth estimation uses contextual cues such as occlusions or the relative sizes of objects to estimate the structure of the scene.

We will use a pre-trained [MiDas-v2][1] here, but our analysis with the [SGR][2] network also supports our claims.

When we feed the image to the network at different resolutions, some interesting patterns arise.
At lower resolutions, many details in the scene are missing, such as birds in this example. At high resolutions, however, we start to see inconsistent overall structure, and this flat board gets significantly less flat. The advantage is that the network is able to generate high frequency details.
This shows that there is a trade-off between structural consistency and high-frequency details with respect to input resolution.

![Observations](./figures/observation.png)



##### Explanations

We explain this behavior through two properties of convolutional neural networks: limited **receptive field** size and **network capacity**.
The lack of high frequency details in low resolutions are due to a limited network capacity. A small network that generates the structure of a complex scene cannot also generate fine details.

The loss of structure at high resolutions comes from a limited receptive field size. The receptive field is the region around a pixel that contributes to the estimation at that pixel. It is set by the network configuration and training resolution, and effectively gets smaller as resolution increases. At a low resolution, every pixel can see the edges of the board, so the network judges that this is a flat wall. At a high resolution, however, some pixels do not receive any contextual information. This results in large structural inconsistencies.

![Explanations](./figures/explanation.png)

##### Best resolution search 

For any given image, we determine the highest resolution that will result in a consistent structure by making sure that every pixel has contextual information. For this purpose, we need the distribution of contextual cues in the image. We approximate contextual cues with a simple edge map. 

The resolution where every pixel is at most a half receptive field size away from context edges is called R_0.
When we increase the resolution any further, structural inconsistencies will arise but more details will be generated. When 20% of the pixels do not receive any context, we call this resolution R_20.
**Note that R_0 and R_20 depend on the image content!**

![Resolutionsearch](./figures/ressearch.png)

##### Double Estimation

We are still able to go beyond R0 by merging the high-frequency details in the R20 resolution onto the structure of the base resolution. We call this **Double Estimation**.
We train an image-to-image translation network to merge the low-resolution depth range of the base with the high-resolution details of R_20. It does so without inheriting the structural inconsistencies of the high-res input. This way, we go beyond R_0 and generate more details by using R_20 as our high-resolution input. In fact, the network is so robust against low-frequency artifacts that we can even use R_20 as our high-resolution input.

![merge](./figures/merge.png)

##### Local Boosting

Note that R20 is *bounded by the smoothest regions* in the image, while there are image patches that could support a higher resolution.
We choose candidate patches by tiling the image and discarding all patches without useful details (step1). The leftover patches are expanded until their edge density matches that of the image(step2). Finally, we merge a double estimation for each patch onto our R20 results and generate our final results (step3).


|Step 1: Tile and discard | Step 2: Expand | Step 3: Merge|
|-------------------------|----------------|--------------|
|![patchselection](./figures/patchselection.gif)|![Patchexpand](./figures/patchexpand.gif)|![Patchexpand](./figures/patchmerge.gif)|

## Setup

We Provided the implementation of our method using [MiDas-v2][1], [LeReS][2] and [SGRnet][3] as the base. Note that [MiDas-v2][1] and [SGRnet][3] estimate inverse depth while [LeReS][2] estimates depth. 

### Environments
Our mergenet model is trained using torch 0.4.1 and python 3.7 and is tested with torch<=1.8.

Download our mergenet model weights from [here](https://sfu.ca/~yagiz/CVPR21/latest_net_G.pth) and put it in 
> .\pix2pix\checkpoints\mergemodel\latest_net_G.pth

To use [MiDas-v2][1] or [LeReS][2] as base:
Install dependancies as following:
```sh
conda install pytorch torchvision opencv cudatoolkit=10.2 -c pytorch
conda install matplotlib
conda install scipy
conda install scikit-image
```
For MiDaS-v2, download the model weights from [MiDas-v2][1] and put it in 
> ./midas/model.pt

```sh
activate the environment
python run.py --Final --data_dir PATH_TO_INPUT --output_dir PATH_TO_RESULT --depthNet 0
```

For LeReS, download the model weights from [LeReS][2] (Resnext101) and put it in root:
> ./res101.pth

```sh
activate the environment
python run.py --Final --data_dir PATH_TO_INPUT --output_dir PATH_TO_RESULT --depthNet 2
```

To use [SGRnet][3] as base:
Install dependencies as following:
```sh
conda install pytorch=0.4.1 cuda92 -c pytorch
conda install torchvision
conda install matplotlib
conda install scikit-image
pip install opencv-python
```
Follow the official [SGRnet][3] repository to compile the syncbn module in ./structuredrl/models/syncbn.
Download the model weights from [SGRnet][3] and put it in 
> ./structuredrl/model.pth.tar

```sh
activate the environment
python run.py --Final --data_dir PATH_TO_INPUT --output_dir PATH_TO_RESULT --depthNet 1
```

Different input arguments can be used to generate R0 and R20 results as discussed in the paper. 

```python
python run.py --R0 --data_dir PATH_TO_INPUT --output_dir PATH_TO_RESULT --depthNet #[0,1 or 2]
python run.py --R20 --data_dir PATH_TO_INPUT --output_dir PATH_TO_RESULT --depthNet #[0,1 or 2]
```

To generate the results with *CV.INFERNO* colormap use **--colorize_results** like the sample below:

```python
python run.py --colorize_results --Final --data_dir PATH_TO_INPUT --output_dir PATH_TO_RESULT --depthNet #[0,1 or 2]
```

### Evaluation
Fill in the needed variables in the following matlab file and run:
>./evaluation/evaluatedataset.m

* **estimation_path** : path to estimated disparity maps
* **gt_depth_path** : path to gt depth/disparity maps
* **dataset_disp_gttype** : (true) if ground truth data is disparity and (false) if gt depth data is depth.
* **evaluation_matfile_save_dir** : directory to save the evalution results as .mat file. 
* **superpixel_scale** : scale parameter to run the superpixels on scaled version of the ground truth images to accelarate the evaluation. use 1 for small gt images.


### Training

Navigate to [dataset preparation instructions](./dataset_prepare/mergenet_dataset_prepare.md) to download and prepare the training dataset. 

```sh
python ./pix2pix/train.py --dataroot DATASETDIR --name mergemodeltrain --model pix2pix4depth --no_flip --no_dropout
```
```sh
python ./pix2pix/test.py --dataroot DATASETDIR --name mergemodeleval --model pix2pix4depth --no_flip --no_dropout
```


## Citation

This implementation is provided for academic use only. Please cite our paper if you use this code or any of the models.
```
@INPROCEEDINGS{Miangoleh2021Boosting,
author={S. Mahdi H. Miangoleh and Sebastian Dille and Long Mai and Sylvain Paris and Ya\u{g}{\i}z Aksoy},
title={Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging},
journal={Proc. CVPR},
year={2021},
}
```

## Credits

The "Merge model" code skeleton (./pix2pix folder) was adapted from the [pytorch-CycleGAN-and-pix2pix][4] repository. 

For MiDaS, LeReS and SGR inferences we used the scripts and models from [MiDas-v2][1], [LeReS][2] and [SGRnet][3] respectively (./midas, ./lib and ./structuredrl folders). 

Thanks to [k-washi](https://github.com/k-washi) for providing us with a Google Colaboratory notebook implementation.

[1]: https://github.com/intel-isl/MiDaS/tree/v2
[2]: https://github.com/aim-uofa/AdelaiDepth/tree/main/LeReS
[3]: https://github.com/KexianHust/Structure-Guided-Ranking-Loss
[4]: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix




================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/Boostmonoculardepth.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
#Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging

This Colab Notebook contains an inference on the implementation from https://github.com/compphoto/BoostingMonocularDepth.

It supports using [MiDas-v2][1] and [LeRes][2] but not [SGRnet][3].


### Instructions
0. Ensure using a GPU by setting "Runtime/change runtime type" to GPU
1. Run code section 1
2. Put your test images inside /content/inputs
3. Run code section 2 :  it will download the needed model weights for both [MiDas-v2][1] and [LeRes][2]. 

5. Run code section 3
4. Run code section "Ours with MiDas" or "Ours with LeRes" to generate the results 
3. Results will be generated in /content/outputs_midas or /content/outputs_leres folders. 


[1]: https://github.com/intel-isl/MiDaS/tree/v2
[2]: https://github.com/aim-uofa/AdelaiDepth/tree/main/LeReS
[3]: https://github.com/KexianHust/Structure-Guided-Ranking-Loss
"""

# Code section 1

!mkdir -p inputs
!mkdir -p outputs_midas
!mkdir -p outputs_leres

# Code section 2


# Clone git repo
!git clone https://github.com/compphoto/BoostingMonocularDepth.git

!wget https://sfu.ca/~yagiz/CVPR21/latest_net_G.pth
#!gdown https://drive.google.com/u/0/uc?id=1cU2y-kMbt0Sf00Ns4CN2oO9qPJ8BensP&export=download

# Downloading merge model weights
!mkdir -p /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/
!mv latest_net_G.pth /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/

# Downloading Midas weights
!wget https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21-f6b98070.pt
!mv midas_v21-f6b98070.pt /content/BoostingMonocularDepth/midas/model.pt

# # Downloading LeRes weights
!wget https://cloudstor.aarnet.edu.au/plus/s/lTIJF4vrvHCAI31/download
!mv download /content/BoostingMonocularDepth/res101.pth

# Code section 3
%cd BoostingMonocularDepth/

"""
> After execution of Code section 3 you can run the following code sections multiple times to generate results but **do not run the previous code sections**. If you did so by mistake use "Runtime/ Reset factory runtime" and then start from step 0.
"""

# Running the method using MiDas
!python run.py --Final --data_dir /content/inputs --output_dir  /content/outputs_midas/ --depthNet 0

# Running the method using LeRes
!python run.py --Final --data_dir /content/inputs --output_dir  /content/outputs_leres/ --depthNet 2



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/demo.py
================================================
from operator import getitem
from torchvision.transforms import Compose
import torch
import cv2
import numpy as np
import warnings
warnings.simplefilter('ignore', np.RankWarning)
import gradio as gr
import os
import gdown
# OUR
from utils import ImageandPatchs, generatemask, getGF_fromintegral, calculateprocessingres, rgb2gray,\
    applyGridpatch

# MIDAS
import midas.utils
from midas.models.midas_net import MidasNet
from midas.models.transforms import Resize, NormalizeImage, PrepareForNet

# PIX2PIX : MERGE NET
from pix2pix.options.test_options import TestOptions
from pix2pix.models.pix2pix4depth_model import Pix2Pix4DepthModel
#
## Download model wieghts
# Mergenet model
os.system("mkdir -p ./pix2pix/checkpoints/mergemodel/")
url = "https://drive.google.com/u/0/uc?id=1cU2y-kMbt0Sf00Ns4CN2oO9qPJ8BensP&export=download"
output = "./pix2pix/checkpoints/mergemodel/"
gdown.download(url, output, quiet=False)

url = "https://drive.google.com/uc?id=1nqW_Hwj86kslfsXR7EnXpEWdO2csz1cC"
output = "./midas/"
gdown.download(url, output, quiet=False)

#
# select device
device = torch.device("cpu")
print("device: %s" % device)

print("nvidia:", torch.cuda.device_count())

whole_size_threshold = 3000  # R_max from the paper
GPU_threshold = 1600 - 32 # Limit for the GPU (NVIDIA RTX 2080), can be adjusted
scale_threshold = 3  # Allows up-scaling with a scale up to 3

opt = TestOptions().parse()
opt.gpu_ids = []
global pix2pixmodel
pix2pixmodel = Pix2Pix4DepthModel(opt)
pix2pixmodel.save_dir = './pix2pix/checkpoints/mergemodel'
pix2pixmodel.load_networks('latest')
pix2pixmodel.netG.to(device)
pix2pixmodel.device = device
pix2pixmodel.eval()

midas_model_path = "midas/model.pt"
global midasmodel
midasmodel = MidasNet(midas_model_path, non_negative=True)
midasmodel.to(device)
midasmodel.eval()


mask_org = generatemask((3000, 3000))


def estimatemidas(img, msize):
    # MiDas -v2 forward pass script adapted from https://github.com/intel-isl/MiDaS/tree/v2

    transform = Compose(
        [
            Resize(
                msize,
                msize,
                resize_target=None,
                keep_aspect_ratio=True,
                ensure_multiple_of=32,
                resize_method="upper_bound",
                image_interpolation_method=cv2.INTER_CUBIC,
            ),
            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            PrepareForNet(),
        ]
    )

    img_input = transform({"image": img})["image"]

    # Forward pass
    with torch.no_grad():
        sample = torch.from_numpy(img_input).to(device).unsqueeze(0)
        prediction = midasmodel.forward(sample)

    prediction = prediction.squeeze().cpu().numpy()
    prediction = cv2.resize(prediction, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)

    # Normalization
    depth_min = prediction.min()
    depth_max = prediction.max()

    if depth_max - depth_min > np.finfo("float").eps:
        prediction = (prediction - depth_min) / (depth_max - depth_min)
    else:
        prediction = 0

    return prediction

# Generate a single-input depth estimation
def singleestimate(img, msize, net_type):
    if msize > GPU_threshold:
        print(" \t \t DEBUG| GPU THRESHOLD REACHED", msize, '--->', GPU_threshold)
        msize = GPU_threshold

    return estimatemidas(img, msize)


def doubleestimate(img, size1, size2, pix2pixsize, net_type):
    # Generate the low resolution estimation
    estimate1 = singleestimate(img, size1, net_type)
    # Resize to the inference size of merge network.
    estimate1 = cv2.resize(estimate1, (pix2pixsize, pix2pixsize), interpolation=cv2.INTER_CUBIC)

    # Generate the high resolution estimation
    estimate2 = singleestimate(img, size2, net_type)
    # Resize to the inference size of merge network.
    estimate2 = cv2.resize(estimate2, (pix2pixsize, pix2pixsize), interpolation=cv2.INTER_CUBIC)

    # Inference on the merge model
    pix2pixmodel.set_input(estimate1, estimate2)
    pix2pixmodel.test()
    visuals = pix2pixmodel.get_current_visuals()
    prediction_mapped = visuals['fake_B']
    prediction_mapped = (prediction_mapped+1)/2
    prediction_mapped = (prediction_mapped - torch.min(prediction_mapped)) / (
                torch.max(prediction_mapped) - torch.min(prediction_mapped))
    prediction_mapped = prediction_mapped.squeeze().cpu().numpy()

    return prediction_mapped


def adaptiveselection(integral_grad, patch_bound_list, gf, factor):
    patchlist = {}
    count = 0
    height, width = integral_grad.shape

    search_step = int(32 / factor)

    # Go through all patches
    for c in range(len(patch_bound_list)):
        # Get patch
        bbox = patch_bound_list[str(c)]['rect']

        # Compute the amount of gradients present in the patch from the integral image.
        cgf = getGF_fromintegral(integral_grad, bbox) / (bbox[2] * bbox[3])

        # Check if patching is beneficial by comparing the gradient density of the patch to
        # the gradient density of the whole image
        if cgf >= gf:
            bbox_test = bbox.copy()
            patchlist[str(count)] = {}

            # Enlarge each patch until the gradient density of the patch is equal
            # to the whole image gradient density
            while True:

                bbox_test[0] = bbox_test[0] - int(search_step / 2)
                bbox_test[1] = bbox_test[1] - int(search_step / 2)

                bbox_test[2] = bbox_test[2] + search_step
                bbox_test[3] = bbox_test[3] + search_step

                # Check if we are still within the image
                if bbox_test[0] < 0 or bbox_test[1] < 0 or bbox_test[1] + bbox_test[3] >= height \
                        or bbox_test[0] + bbox_test[2] >= width:
                    break

                # Compare gradient density
                cgf = getGF_fromintegral(integral_grad, bbox_test) / (bbox_test[2] * bbox_test[3])
                if cgf < gf:
                    break
                bbox = bbox_test.copy()

            # Add patch to selected patches
            patchlist[str(count)]['rect'] = bbox
            patchlist[str(count)]['size'] = bbox[2]
            count = count + 1

    # Return selected patches
    return patchlist


def generatepatchs(img, base_size, factor):
    # Compute the gradients as a proxy of the contextual cues.
    img_gray = rgb2gray(img)
    whole_grad = np.abs(cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)) + \
                 np.abs(cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3))

    threshold = whole_grad[whole_grad > 0].mean()
    whole_grad[whole_grad < threshold] = 0

    # We use the integral image to speed-up the evaluation of the amount of gradients for each patch.
    gf = whole_grad.sum() / len(whole_grad.reshape(-1))
    grad_integral_image = cv2.integral(whole_grad)

    # Variables are selected such that the initial patch size would be the receptive field size
    # and the stride is set to 1/3 of the receptive field size.
    blsize = int(round(base_size / 2))
    stride = int(round(blsize * 0.75))

    # Get initial Grid
    patch_bound_list = applyGridpatch(blsize, stride, img, [0, 0, 0, 0])

    # Refine initial Grid of patches by discarding the flat (in terms of gradients of the rgb image) ones. Refine
    # each patch size to ensure that there will be enough depth cues for the network to generate a consistent depth map.
    print("Selecting patchs ...")
    patch_bound_list = adaptiveselection(grad_integral_image, patch_bound_list, gf, factor)

    # Sort the patch list to make sure the merging operation will be done with the correct order: starting from biggest
    # patch
    patchset = sorted(patch_bound_list.items(), key=lambda x: getitem(x[1], 'size'), reverse=True)
    return patchset


def generatedepth(img, type="Final"):
    mask = mask_org.copy()
    print(type)
    if type == "Final" or type == "R20":
        r_threshold_value = 0.2
    elif type == "R0":
        r_threshold_value = 0
    else:
        return np.zeros_like(img), "Please select on of the Model Types"

    print(type,r_threshold_value)
    img = (img / 255.0).astype("float32")
    input_resolution = img.shape

    # Find the best input resolution R-x. The resolution search described in section 5-double estimation of the
    # main paper and section B of the supplementary material.
    whole_image_optimal_size, patch_scale = calculateprocessingres(img, 384,
                                                                   r_threshold_value, scale_threshold,
                                                                   whole_size_threshold)

    print('\t wholeImage being processed in :', whole_image_optimal_size)

    # Generate the base estimate using the double estimation.
    whole_estimate = doubleestimate(img, 384, whole_image_optimal_size, 1024, 0)


    if type == "R0" or type == "R20":
        result = cv2.resize(whole_estimate, (input_resolution[1], input_resolution[0]),
                   interpolation=cv2.INTER_CUBIC)
        result = (result * 255).astype('uint8')
        result_colored = cv2.applyColorMap(result, cv2.COLORMAP_INFERNO)
        result_colored = cv2.cvtColor(result_colored, cv2.COLOR_RGB2BGR)

        return result_colored, "Completed"

    factor = max(min(1, 4 * patch_scale * whole_image_optimal_size / whole_size_threshold), 0.2)

    print('Adjust factor is:', 1 / factor)

    # Compute the target resolution.
    if img.shape[0] > img.shape[1]:
        a = 2 * whole_image_optimal_size
        b = round(2 * whole_image_optimal_size * img.shape[1] / img.shape[0])
    else:
        a = round(2 * whole_image_optimal_size * img.shape[0] / img.shape[1])
        b = 2 * whole_image_optimal_size

    img = cv2.resize(img, (round(b / factor), round(a / factor)), interpolation=cv2.INTER_CUBIC)
    print('Target resolution: ', img.shape)

    # Extract selected patches for local refinement
    base_size = 384 * 2
    patchset = generatepatchs(img, base_size, factor)

    # Computing a scale in case user prompted to generate the results as the same resolution of the input.
    # Notice that our method output resolution is independent of the input resolution and this parameter will only
    # enable a scaling operation during the local patch merge implementation to generate results with the same
    # resolution as the input.
    mergein_scale = input_resolution[0] / img.shape[0]

    imageandpatchs = ImageandPatchs("", "temp.png", patchset, img, mergein_scale)
    whole_estimate_resized = cv2.resize(whole_estimate, (round(img.shape[1] * mergein_scale),
                                                         round(img.shape[0] * mergein_scale)),
                                        interpolation=cv2.INTER_CUBIC)
    imageandpatchs.set_base_estimate(whole_estimate_resized.copy())
    imageandpatchs.set_updated_estimate(whole_estimate_resized.copy())

    print('\t Resulted depthmap res will be :', whole_estimate_resized.shape[:2])
    print('patchs to process: ' + str(len(imageandpatchs)))

    # Enumerate through all patches, generate their estimations and refining the base estimate.
    for patch_ind in range(len(imageandpatchs)):
        # Get patch information
        patch = imageandpatchs[patch_ind]  # patch object
        patch_rgb = patch['patch_rgb']  # rgb patch
        patch_whole_estimate_base = patch['patch_whole_estimate_base']  # corresponding patch from base
        rect = patch['rect']  # patch size and location
        patch_id = patch['id']  # patch ID
        org_size = patch_whole_estimate_base.shape  # the original size from the unscaled input
        print('\t processing patch', patch_ind, '|', rect)
        # We apply double estimation for patches. The high resolution value is fixed to twice the receptive
        # field size of the network for patches to accelerate the process.
        patch_estimation = doubleestimate(patch_rgb, 384, int(384*2),
                                          1024, 0)

        patch_estimation = cv2.resize(patch_estimation, (1024, 1024),
                                      interpolation=cv2.INTER_CUBIC)
        patch_whole_estimate_base = cv2.resize(patch_whole_estimate_base, (1024, 1024),
                                               interpolation=cv2.INTER_CUBIC)

        # Merging the patch estimation into the base estimate using our merge network:
        # We feed the patch estimation and the same region from the updated base estimate to the merge network
        # to generate the target estimate for the corresponding region.
        pix2pixmodel.set_input(patch_whole_estimate_base, patch_estimation)

        # Run merging network
        pix2pixmodel.test()
        visuals = pix2pixmodel.get_current_visuals()

        prediction_mapped = visuals['fake_B']
        prediction_mapped = (prediction_mapped + 1) / 2
        prediction_mapped = prediction_mapped.squeeze().cpu().numpy()

        mapped = prediction_mapped

        # We use a simple linear polynomial to make sure the result of the merge network would match the values of
        # base estimate
        p_coef = np.polyfit(mapped.reshape(-1), patch_whole_estimate_base.reshape(-1), deg=1)
        merged = np.polyval(p_coef, mapped.reshape(-1)).reshape(mapped.shape)

        merged = cv2.resize(merged, (org_size[1], org_size[0]), interpolation=cv2.INTER_CUBIC)

        # Get patch size and location
        w1 = rect[0]
        h1 = rect[1]
        w2 = w1 + rect[2]
        h2 = h1 + rect[3]

        # To speed up the implementation, we only generate the Gaussian mask once with a sufficiently large size
        # and resize it to our needed size while merging the patches.
        if mask.shape != org_size:
            mask = cv2.resize(mask_org, (org_size[1], org_size[0]), interpolation=cv2.INTER_LINEAR)

        tobemergedto = imageandpatchs.estimation_updated_image

        # Update the whole estimation:
        # We use a simple Gaussian mask to blend the merged patch region with the base estimate to ensure seamless
        # blending at the boundaries of the patch region.
        tobemergedto[h1:h2, w1:w2] = np.multiply(tobemergedto[h1:h2, w1:w2], 1 - mask) + np.multiply(merged, mask)
        imageandpatchs.set_updated_estimate(tobemergedto)

    result = (imageandpatchs.estimation_updated_image * 255).astype('uint8')
    result_colored = cv2.applyColorMap(result,cv2.COLORMAP_INFERNO)
    result_colored = cv2.cvtColor(result_colored,cv2.COLOR_RGB2BGR)
    return result_colored, "Completed"



title = "Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging"
description = "To use this demo, simply upload your image and click submit. Note that it might take a few minutes for the results to be generated."
article = "<p style='text-align: center'><a href='http://yaksoy.github.io/highresdepth/'> Project page</a> | <a href='https://github.com/compphoto/BoostingMonocularDepth'>Github Repo</a></p>"

gr.Interface(
    generatedepth,
    [gr.inputs.Image(type="numpy", label="Input")],
    [gr.outputs.Image(type="numpy", label="Output"), gr.outputs.Textbox(label=":")],
    title=title,
    description=description,
    article=article,
    examples=[["inputs/sample1.png"],
              ["inputs/sample2.jpg"]]
    ).launch(debug=True,share=True)


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/LICENSE
================================================
Copyright 2021, Seyed Mahdi Hosseini Miangoleh, Sebastian Dille, Computational Photography Laboratory. All rights reserved.

This software is for academic use only. A redistribution of this 
software, with or  without modifications, has to be for academic 
use only, while giving the appropriate credit to the original 
authors of the software. The methods implemented as a part of 
this software may be covered under patents or patent applications.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR ''AS IS'' AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/requirements.txt
================================================
torch==1.2
torchvision
cudnnenv
gradio
matplotlib
opencv-python
scikit-image
scipy
gdown




================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/run.py
================================================
from operator import getitem
from torchvision.transforms import Compose
from torchvision.transforms import transforms

# OUR
from utils import ImageandPatchs, ImageDataset, generatemask, getGF_fromintegral, calculateprocessingres, rgb2gray,\
    applyGridpatch

# MIDAS
import midas.utils
from midas.models.midas_net import MidasNet
from midas.models.transforms import Resize, NormalizeImage, PrepareForNet

#AdelaiDepth
from lib.multi_depth_model_woauxi import RelDepthModel
from lib.net_tools import strip_prefix_if_present

# PIX2PIX : MERGE NET
from pix2pix.options.test_options import TestOptions
from pix2pix.models.pix2pix4depth_model import Pix2Pix4DepthModel

import time
import os
import torch
import cv2
import numpy as np
import argparse
import warnings
warnings.simplefilter('ignore', np.RankWarning)

# select device
device = torch.device("cuda")
print("device: %s" % device)

# Global variables
pix2pixmodel = None
midasmodel = None
srlnet = None
leresmodel = None
factor = None
whole_size_threshold = 3000  # R_max from the paper
GPU_threshold = 1600 - 32 # Limit for the GPU (NVIDIA RTX 2080), can be adjusted 

# MAIN PART OF OUR METHOD
def run(dataset, option):

    # Load merge network
    opt = TestOptions().parse()
    global pix2pixmodel
    pix2pixmodel = Pix2Pix4DepthModel(opt)
    pix2pixmodel.save_dir = './pix2pix/checkpoints/mergemodel'
    pix2pixmodel.load_networks('latest')
    pix2pixmodel.eval()

    # Decide which depth estimation network to load
    if option.depthNet == 0:
        midas_model_path = "midas/model.pt"
        global midasmodel
        midasmodel = MidasNet(midas_model_path, non_negative=True)
        midasmodel.to(device)
        midasmodel.eval()
    elif option.depthNet == 1:
        global srlnet
        srlnet = DepthNet.DepthNet()
        srlnet = torch.nn.DataParallel(srlnet, device_ids=[0]).cuda()
        checkpoint = torch.load('structuredrl/model.pth.tar')
        srlnet.load_state_dict(checkpoint['state_dict'])
        srlnet.eval()
    elif option.depthNet == 2:
        global leresmodel
        leres_model_path = "res101.pth"
        checkpoint = torch.load(leres_model_path)
        leresmodel = RelDepthModel(backbone='resnext101')
        leresmodel.load_state_dict(strip_prefix_if_present(checkpoint['depth_model'], "module."),
                                    strict=True)
        del checkpoint
        torch.cuda.empty_cache()
        leresmodel.to(device)
        leresmodel.eval()

    # Generating required directories
    result_dir = option.output_dir
    os.makedirs(result_dir, exist_ok=True)

    if option.savewholeest:
        whole_est_outputpath = option.output_dir + '_wholeimage'
        os.makedirs(whole_est_outputpath, exist_ok=True)

    if option.savepatchs:
        patchped_est_outputpath = option.output_dir + '_patchest'
        os.makedirs(patchped_est_outputpath, exist_ok=True)

    # Generate mask used to smoothly blend the local pathc estimations to the base estimate.
    # It is arbitrarily large to avoid artifacts during rescaling for each crop.
    mask_org = generatemask((3000, 3000))
    mask = mask_org.copy()

    # Value x of R_x defined in the section 5 of the main paper.
    r_threshold_value = 0.2
    if option.R0:
        r_threshold_value = 0
    elif option.R20:
        r_threshold_value = 0.2

    # Go through all images in input directory
    print("start processing")
    for image_ind, images in enumerate(dataset):
        print('processing image', image_ind, ':', images.name)

        # Load image from dataset
        img = images.rgb_image
        input_resolution = img.shape

        scale_threshold = 3  # Allows up-scaling with a scale up to 3

        # Find the best input resolution R-x. The resolution search described in section 5-double estimation of the main paper and section B of the
        # supplementary material.
        whole_image_optimal_size, patch_scale = calculateprocessingres(img, option.net_receptive_field_size,
                                                                       r_threshold_value, scale_threshold,
                                                                       whole_size_threshold)

        print('\t wholeImage being processed in :', whole_image_optimal_size)

        # Generate the base estimate using the double estimation.
        whole_estimate = doubleestimate(img, option.net_receptive_field_size, whole_image_optimal_size,
                                        option.pix2pixsize, option.depthNet)
        if option.R0 or option.R20:
            path = os.path.join(result_dir, images.name)
            if option.output_resolution == 1:
                midas.utils.write_depth(path, cv2.resize(whole_estimate, (input_resolution[1], input_resolution[0]),
                                                         interpolation=cv2.INTER_CUBIC),
                                        bits=2, colored=option.colorize_results)
            else:
                midas.utils.write_depth(path, whole_estimate, bits=2, colored=option.colorize_results)
            continue

        # Output double estimation if required
        if option.savewholeest:
            path = os.path.join(whole_est_outputpath, images.name)
            if option.output_resolution == 1:
                midas.utils.write_depth(path,
                                        cv2.resize(whole_estimate, (input_resolution[1], input_resolution[0]),
                                                   interpolation=cv2.INTER_CUBIC), bits=2,
                                        colored=option.colorize_results)
            else:
                midas.utils.write_depth(path, whole_estimate, bits=2, colored=option.colorize_results)

        # Compute the multiplier described in section 6 of the main paper to make sure our initial patch can select
        # small high-density regions of the image.
        global factor
        factor = max(min(1, 4 * patch_scale * whole_image_optimal_size / whole_size_threshold), 0.2)
        print('Adjust factor is:', 1/factor)

        # Check if Local boosting is beneficial.
        if option.max_res < whole_image_optimal_size:
            print("No Local boosting. Specified Max Res is smaller than R20")
            path = os.path.join(result_dir, images.name)
            if option.output_resolution == 1:
                midas.utils.write_depth(path,
                                        cv2.resize(whole_estimate,
                                                   (input_resolution[1], input_resolution[0]),
                                                   interpolation=cv2.INTER_CUBIC), bits=2,
                                        colored=option.colorize_results)
            else:
                midas.utils.write_depth(path, whole_estimate, bits=2,
                                        colored=option.colorize_results)
            continue

        # Compute the default target resolution.
        if img.shape[0] > img.shape[1]:
            a = 2 * whole_image_optimal_size
            b = round(2 * whole_image_optimal_size * img.shape[1] / img.shape[0])
        else:
            a = round(2 * whole_image_optimal_size * img.shape[0] / img.shape[1])
            b = 2 * whole_image_optimal_size
        b = int(round(b / factor))
        a = int(round(a / factor))

        # recompute a, b and saturate to max res.
        if max(a,b) > option.max_res:
            print('Default Res is higher than max-res: Reducing final resolution')
            if img.shape[0] > img.shape[1]:
                a = option.max_res
                b = round(option.max_res * img.shape[1] / img.shape[0])
            else:
                a = round(option.max_res * img.shape[0] / img.shape[1])
                b = option.max_res
            b = int(b)
            a = int(a)

        img = cv2.resize(img, (b, a), interpolation=cv2.INTER_CUBIC)

        # Extract selected patches for local refinement
        base_size = option.net_receptive_field_size*2
        patchset = generatepatchs(img, base_size)

        print('Target resolution: ', img.shape)

        # Computing a scale in case user prompted to generate the results as the same resolution of the input.
        # Notice that our method output resolution is independent of the input resolution and this parameter will only
        # enable a scaling operation during the local patch merge implementation to generate results with the same resolution
        # as the input.
        if option.output_resolution == 1:
            mergein_scale = input_resolution[0] / img.shape[0]
            print('Dynamicly change merged-in resolution; scale:', mergein_scale)
        else:
            mergein_scale = 1

        imageandpatchs = ImageandPatchs(option.data_dir, images.name, patchset, img, mergein_scale)
        whole_estimate_resized = cv2.resize(whole_estimate, (round(img.shape[1]*mergein_scale),
                                            round(img.shape[0]*mergein_scale)), interpolation=cv2.INTER_CUBIC)
        imageandpatchs.set_base_estimate(whole_estimate_resized.copy())
        imageandpatchs.set_updated_estimate(whole_estimate_resized.copy())

        print('\t Resulted depthmap res will be :', whole_estimate_resized.shape[:2])
        print('patchs to process: '+str(len(imageandpatchs)))

        # Enumerate through all patches, generate their estimations and refining the base estimate.
        for patch_ind in range(len(imageandpatchs)):
            
            # Get patch information
            patch = imageandpatchs[patch_ind] # patch object
            patch_rgb = patch['patch_rgb'] # rgb patch
            patch_whole_estimate_base = patch['patch_whole_estimate_base'] # corresponding patch from base
            rect = patch['rect'] # patch size and location
            patch_id = patch['id'] # patch ID
            org_size = patch_whole_estimate_base.shape # the original size from the unscaled input
            print('\t processing patch', patch_ind, '|', rect)

            # We apply double estimation for patches. The high resolution value is fixed to twice the receptive
            # field size of the network for patches to accelerate the process.
            patch_estimation = doubleestimate(patch_rgb, option.net_receptive_field_size, option.patch_netsize,
                                              option.pix2pixsize, option.depthNet)

            # Output patch estimation if required
            if option.savepatchs:
                path = os.path.join(patchped_est_outputpath, imageandpatchs.name + '_{:04}'.format(patch_id))
                midas.utils.write_depth(path, patch_estimation, bits=2, colored=option.colorize_results)

            patch_estimation = cv2.resize(patch_estimation, (option.pix2pixsize, option.pix2pixsize),
                                          interpolation=cv2.INTER_CUBIC)

            patch_whole_estimate_base = cv2.resize(patch_whole_estimate_base, (option.pix2pixsize, option.pix2pixsize),
                                                   interpolation=cv2.INTER_CUBIC)

            # Merging the patch estimation into the base estimate using our merge network:
            # We feed the patch estimation and the same region from the updated base estimate to the merge network
            # to generate the target estimate for the corresponding region.
            pix2pixmodel.set_input(patch_whole_estimate_base, patch_estimation)

            # Run merging network
            pix2pixmodel.test()
            visuals = pix2pixmodel.get_current_visuals()

            prediction_mapped = visuals['fake_B']
            prediction_mapped = (prediction_mapped+1)/2
            prediction_mapped = prediction_mapped.squeeze().cpu().numpy()

            mapped = prediction_mapped

            # We use a simple linear polynomial to make sure the result of the merge network would match the values of
            # base estimate
            p_coef = np.polyfit(mapped.reshape(-1), patch_whole_estimate_base.reshape(-1), deg=1)
            merged = np.polyval(p_coef, mapped.reshape(-1)).reshape(mapped.shape)

            merged = cv2.resize(merged, (org_size[1],org_size[0]), interpolation=cv2.INTER_CUBIC)

            # Get patch size and location
            w1 = rect[0]
            h1 = rect[1]
            w2 = w1 + rect[2]
            h2 = h1 + rect[3]

            # To speed up the implementation, we only generate the Gaussian mask once with a sufficiently large size
            # and resize it to our needed size while merging the patches.
            if mask.shape != org_size:
                mask = cv2.resize(mask_org, (org_size[1],org_size[0]), interpolation=cv2.INTER_LINEAR)

            tobemergedto = imageandpatchs.estimation_updated_image

            # Update the whole estimation:
            # We use a simple Gaussian mask to blend the merged patch region with the base estimate to ensure seamless
            # blending at the boundaries of the patch region.
            tobemergedto[h1:h2, w1:w2] = np.multiply(tobemergedto[h1:h2, w1:w2], 1 - mask) + np.multiply(merged, mask)
            imageandpatchs.set_updated_estimate(tobemergedto)

        # Output the result
        path = os.path.join(result_dir, imageandpatchs.name)
        if option.output_resolution == 1:
            midas.utils.write_depth(path,
                                    cv2.resize(imageandpatchs.estimation_updated_image,
                                               (input_resolution[1], input_resolution[0]),
                                               interpolation=cv2.INTER_CUBIC), bits=2, colored=option.colorize_results)
        else:
            midas.utils.write_depth(path, imageandpatchs.estimation_updated_image, bits=2, colored=option.colorize_results)

    print("finished")


# Generating local patches to perform the local refinement described in section 6 of the main paper.
def generatepatchs(img, base_size):
    
    # Compute the gradients as a proxy of the contextual cues.
    img_gray = rgb2gray(img)
    whole_grad = np.abs(cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)) +\
        np.abs(cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3))

    threshold = whole_grad[whole_grad > 0].mean()
    whole_grad[whole_grad < threshold] = 0

    # We use the integral image to speed-up the evaluation of the amount of gradients for each patch.
    gf = whole_grad.sum()/len(whole_grad.reshape(-1))
    grad_integral_image = cv2.integral(whole_grad)

    # Variables are selected such that the initial patch size would be the receptive field size
    # and the stride is set to 1/3 of the receptive field size.
    blsize = int(round(base_size/2))
    stride = int(round(blsize*0.75))

    # Get initial Grid
    patch_bound_list = applyGridpatch(blsize, stride, img, [0, 0, 0, 0])

    # Refine initial Grid of patches by discarding the flat (in terms of gradients of the rgb image) ones. Refine
    # each patch size to ensure that there will be enough depth cues for the network to generate a consistent depth map.
    print("Selecting patchs ...")
    patch_bound_list = adaptiveselection(grad_integral_image, patch_bound_list, gf)

    # Sort the patch list to make sure the merging operation will be done with the correct order: starting from biggest
    # patch
    patchset = sorted(patch_bound_list.items(), key=lambda x: getitem(x[1], 'size'), reverse=True)
    return patchset


# Adaptively select patches
def adaptiveselection(integral_grad, patch_bound_list, gf):
    patchlist = {}
    count = 0
    height, width = integral_grad.shape

    search_step = int(32/factor)

    # Go through all patches
    for c in range(len(patch_bound_list)):
        # Get patch
        bbox = patch_bound_list[str(c)]['rect']

        # Compute the amount of gradients present in the patch from the integral image.
        cgf = getGF_fromintegral(integral_grad, bbox)/(bbox[2]*bbox[3])

        # Check if patching is beneficial by comparing the gradient density of the patch to
        # the gradient density of the whole image
        if cgf >= gf:
            bbox_test = bbox.copy()
            patchlist[str(count)] = {}

            # Enlarge each patch until the gradient density of the patch is equal
            # to the whole image gradient density
            while True:

                bbox_test[0] = bbox_test[0] - int(search_step/2)
                bbox_test[1] = bbox_test[1] - int(search_step/2)

                bbox_test[2] = bbox_test[2] + search_step
                bbox_test[3] = bbox_test[3] + search_step

                # Check if we are still within the image
                if bbox_test[0] < 0 or bbox_test[1] < 0 or bbox_test[1] + bbox_test[3] >= height \
                        or bbox_test[0] + bbox_test[2] >= width:
                    break

                # Compare gradient density
                cgf = getGF_fromintegral(integral_grad, bbox_test)/(bbox_test[2]*bbox_test[3])
                if cgf < gf:
                    break
                bbox = bbox_test.copy()

            # Add patch to selected patches
            patchlist[str(count)]['rect'] = bbox
            patchlist[str(count)]['size'] = bbox[2]
            count = count + 1
    
    # Return selected patches
    return patchlist


# Generate a double-input depth estimation
def doubleestimate(img, size1, size2, pix2pixsize, net_type):
    # Generate the low resolution estimation
    estimate1 = singleestimate(img, size1, net_type)
    # Resize to the inference size of merge network.
    estimate1 = cv2.resize(estimate1, (pix2pixsize, pix2pixsize), interpolation=cv2.INTER_CUBIC)

    # Generate the high resolution estimation
    estimate2 = singleestimate(img, size2, net_type)
    # Resize to the inference size of merge network.
    estimate2 = cv2.resize(estimate2, (pix2pixsize, pix2pixsize), interpolation=cv2.INTER_CUBIC)

    # Inference on the merge model
    pix2pixmodel.set_input(estimate1, estimate2)
    pix2pixmodel.test()
    visuals = pix2pixmodel.get_current_visuals()
    prediction_mapped = visuals['fake_B']
    prediction_mapped = (prediction_mapped+1)/2
    prediction_mapped = (prediction_mapped - torch.min(prediction_mapped)) / (
                torch.max(prediction_mapped) - torch.min(prediction_mapped))
    prediction_mapped = prediction_mapped.squeeze().cpu().numpy()

    return prediction_mapped


# Generate a single-input depth estimation
def singleestimate(img, msize, net_type):
    if msize > GPU_threshold:
        print(" \t \t DEBUG| GPU THRESHOLD REACHED", msize, '--->', GPU_threshold)
        msize = GPU_threshold

    if net_type == 0:
        return estimatemidas(img, msize)
    elif net_type == 1:
        return estimatesrl(img, msize)
    elif net_type == 2:
        return estimateleres(img, msize)


# Inference on SGRNet
def estimatesrl(img, msize):
    # SGRNet forward pass script adapted from https://github.com/KexianHust/Structure-Guided-Ranking-Loss
    img_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    img_resized = cv2.resize(img, (msize, msize), interpolation=cv2.INTER_CUBIC).astype('float32')
    tensor_img = img_transform(img_resized)

    # Forward pass
    input_img = torch.autograd.Variable(tensor_img.cuda().unsqueeze(0), volatile=True)
    with torch.no_grad():
        output = srlnet(input_img)

    # Normalization
    depth = output.squeeze().cpu().data.numpy()
    min_d, max_d = depth.min(), depth.max()
    depth_norm = (depth - min_d) / (max_d - min_d)

    depth_norm = cv2.resize(depth_norm, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)

    return depth_norm

# Inference on MiDas-v2
def estimatemidas(img, msize):
    # MiDas -v2 forward pass script adapted from https://github.com/intel-isl/MiDaS/tree/v2

    transform = Compose(
        [
            Resize(
                msize,
                msize,
                resize_target=None,
                keep_aspect_ratio=True,
                ensure_multiple_of=32,
                resize_method="upper_bound",
                image_interpolation_method=cv2.INTER_CUBIC,
            ),
            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            PrepareForNet(),
        ]
    )

    img_input = transform({"image": img})["image"]

    # Forward pass
    with torch.no_grad():
        sample = torch.from_numpy(img_input).to(device).unsqueeze(0)
        prediction = midasmodel.forward(sample)

    prediction = prediction.squeeze().cpu().numpy()
    prediction = cv2.resize(prediction, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)

    # Normalization
    depth_min = prediction.min()
    depth_max = prediction.max()

    if depth_max - depth_min > np.finfo("float").eps:
        prediction = (prediction - depth_min) / (depth_max - depth_min)
    else:
        prediction = 0

    return prediction


def scale_torch(img):
    """
    Scale the image and output it in torch.tensor.
    :param img: input rgb is in shape [H, W, C], input depth/disp is in shape [H, W]
    :param scale: the scale factor. float
    :return: img. [C, H, W]
    """
    if len(img.shape) == 2:
        img = img[np.newaxis, :, :]
    if img.shape[2] == 3:
        transform = transforms.Compose([transforms.ToTensor(),
		                                transforms.Normalize((0.485, 0.456, 0.406) , (0.229, 0.224, 0.225) )])
        img = transform(img.astype(np.float32))
    else:
        img = img.astype(np.float32)
        img = torch.from_numpy(img)
    return img

# Inference on LeRes
def estimateleres(img, msize):
    # LeReS forward pass script adapted from https://github.com/aim-uofa/AdelaiDepth/tree/main/LeReS

    rgb_c = img[:, :, ::-1].copy()
    A_resize = cv2.resize(rgb_c, (msize, msize))
    img_torch = scale_torch(A_resize)[None, :, :, :]

    # Forward pass
    with torch.no_grad():
        prediction = leresmodel.inference(img_torch)

    prediction = prediction.squeeze().cpu().numpy()
    prediction = cv2.resize(prediction, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)

    return prediction


if __name__ == "__main__":
    # Adding necessary input arguments
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--data_dir', type=str, required=True, help='input files directory '
                                                                    'Images can be .png .jpg .tiff')
    parser.add_argument('--output_dir', type=str, required=True, help='result dir. result depth will be png.'
                                                                      ' vides are JMPG as avi')
    parser.add_argument('--savepatchs', type=int, default=0, required=False,
                        help='Activate to save the patch estimations')
    parser.add_argument('--savewholeest', type=int, default=0, required=False,
                        help='Activate to save the base estimations')
    parser.add_argument('--output_resolution', type=int, default=1, required=False,
                        help='0 for results in maximum resolution 1 for resize to input size')
    parser.add_argument('--net_receptive_field_size', type=int, required=False)  # Do not set the value here
    parser.add_argument('--pix2pixsize', type=int, default=1024, required=False)  # Do not change it
    parser.add_argument('--depthNet', type=int, default=0, required=False,
                        help='use to select different base depth networks 0:midas 1:strurturedRL 2:LeRes')
    parser.add_argument('--colorize_results', action='store_true')
    parser.add_argument('--R0', action='store_true')
    parser.add_argument('--R20', action='store_true')
    parser.add_argument('--Final', action='store_true')
    parser.add_argument('--max_res', type=float, default=np.inf)

    # Check for required input
    option_, _ = parser.parse_known_args()
    print(option_)
    if int(option_.R0) + int(option_.R20) + int(option_.Final) == 0:
        assert False, 'Please activate one of the [R0, R20, Final] options using --[R0]'
    elif int(option_.R0) + int(option_.R20) + int(option_.Final) > 1:
        assert False, 'Please activate only ONE of the [R0, R20, Final] options'

    if option_.depthNet == 1:
        from structuredrl.models import DepthNet

    # Setting each networks receptive field and setting the patch estimation resolution to twice the receptive
    # field size to speed up the local refinement as described in the section 6 of the main paper.
    if option_.depthNet == 0:
        option_.net_receptive_field_size = 384
        option_.patch_netsize = 2*option_.net_receptive_field_size
    elif option_.depthNet == 1:
        option_.net_receptive_field_size = 448
        option_.patch_netsize = 2*option_.net_receptive_field_size
    elif option_.depthNet == 2:
        option_.net_receptive_field_size = 448
        option_.patch_netsize = 2 * option_.net_receptive_field_size
    else:
        assert False, 'depthNet can only be 0,1 or 2'

    # Create dataset from input images
    dataset_ = ImageDataset(option_.data_dir, 'test')

    # Run pipeline
    run(dataset_, option_)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/utils.py
================================================
import os
import cv2
import numpy as np
import math
import matplotlib.pyplot as plt
import skimage.measure

# miscellaneous function for reading, writing and processing rgb and depth images.


def resizewithpool(img, size):
    i_size = img.shape[0]
    n = int(np.floor(i_size/size))

    out = skimage.measure.block_reduce(img, (n, n), np.max)
    return out


def showimage(img):
    plt.imshow(img)
    plt.colorbar()
    plt.show()


def read_image(path):
    img = cv2.imread(path)
    if img.ndim == 2:
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0
    return img


def generatemask(size):
    # Generates a Guassian mask
    mask = np.zeros(size, dtype=np.float32)
    sigma = int(size[0]/16)
    k_size = int(2 * np.ceil(2 * int(size[0]/16)) + 1)
    mask[int(0.15*size[0]):size[0] - int(0.15*size[0]), int(0.15*size[1]): size[1] - int(0.15*size[1])] = 1
    mask = cv2.GaussianBlur(mask, (int(k_size), int(k_size)), sigma)
    mask = (mask - mask.min()) / (mask.max() - mask.min())
    mask = mask.astype(np.float32)
    return mask


def impatch(image, rect):
    # Extract the given patch pixels from a given image.
    w1 = rect[0]
    h1 = rect[1]
    w2 = w1 + rect[2]
    h2 = h1 + rect[3]
    image_patch = image[h1:h2, w1:w2]
    return image_patch


def getGF_fromintegral(integralimage, rect):
    # Computes the gradient density of a given patch from the gradient integral image.
    x1 = rect[1]
    x2 = rect[1]+rect[3]
    y1 = rect[0]
    y2 = rect[0]+rect[2]
    value = integralimage[x2, y2]-integralimage[x1, y2]-integralimage[x2, y1]+integralimage[x1, y1]
    return value


def rgb2gray(rgb):
    # Converts rgb to gray
    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])


def calculateprocessingres(img, basesize, confidence=0.1, scale_threshold=3, whole_size_threshold=3000):
    # Returns the R_x resolution described in section 5 of the main paper.

    # Parameters:
    #    img :input rgb image
    #    basesize : size the dilation kernel which is equal to receptive field of the network.
    #    confidence: value of x in R_x; allowed percentage of pixels that are not getting any contextual cue.
    #    scale_threshold: maximum allowed upscaling on the input image ; it has been set to 3.
    #    whole_size_threshold: maximum allowed resolution. (R_max from section 6 of the main paper)

    # Returns:
    #    outputsize_scale*speed_scale :The computed R_x resolution
    #    patch_scale: K parameter from section 6 of the paper

    # speed scale parameter is to process every image in a smaller size to accelerate the R_x resolution search
    speed_scale = 32
    image_dim = int(min(img.shape[0:2]))

    gray = rgb2gray(img)
    grad = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)) + np.abs(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3))
    grad = cv2.resize(grad, (image_dim, image_dim), cv2.INTER_AREA)

    # thresholding the gradient map to generate the edge-map as a proxy of the contextual cues
    m = grad.min()
    M = grad.max()
    middle = m + (0.4 * (M - m))
    grad[grad < middle] = 0
    grad[grad >= middle] = 1

    # dilation kernel with size of the receptive field
    kernel = np.ones((int(basesize/speed_scale), int(basesize/speed_scale)), np.float)
    # dilation kernel with size of the a quarter of receptive field used to compute k
    # as described in section 6 of main paper
    kernel2 = np.ones((int(basesize / (4*speed_scale)), int(basesize / (4*speed_scale))), np.float)

    # Output resolution limit set by the whole_size_threshold and scale_threshold.
    threshold = min(whole_size_threshold, scale_threshold * max(img.shape[:2]))

    outputsize_scale = basesize / speed_scale
    for p_size in range(int(basesize/speed_scale), int(threshold/speed_scale), int(basesize / (2*speed_scale))):
        grad_resized = resizewithpool(grad, p_size)
        grad_resized = cv2.resize(grad_resized, (p_size, p_size), cv2.INTER_NEAREST)
        grad_resized[grad_resized >= 0.5] = 1
        grad_resized[grad_resized < 0.5] = 0

        dilated = cv2.dilate(grad_resized, kernel, iterations=1)
        meanvalue = (1-dilated).mean()
        if meanvalue > confidence:
            break
        else:
            outputsize_scale = p_size

    grad_region = cv2.dilate(grad_resized, kernel2, iterations=1)
    patch_scale = grad_region.mean()

    return int(outputsize_scale*speed_scale), patch_scale


def applyGridpatch(blsize, stride, img, box):
    # Extract a simple grid patch.
    counter1 = 0
    patch_bound_list = {}
    for k in range(blsize, img.shape[1] - blsize, stride):
        for j in range(blsize, img.shape[0] - blsize, stride):
            patch_bound_list[str(counter1)] = {}
            patchbounds = [j - blsize, k - blsize, j - blsize + 2 * blsize, k - blsize + 2 * blsize]
            patch_bound = [box[0] + patchbounds[1], box[1] + patchbounds[0], patchbounds[3] - patchbounds[1],
                           patchbounds[2] - patchbounds[0]]
            patch_bound_list[str(counter1)]['rect'] = patch_bound
            patch_bound_list[str(counter1)]['size'] = patch_bound[2]
            counter1 = counter1 + 1
    return patch_bound_list


class Images:
    def __init__(self, root_dir, files, index):
        self.root_dir = root_dir
        name = files[index]
        self.rgb_image = read_image(os.path.join(self.root_dir, name))
        name = name.replace(".jpg", "")
        name = name.replace(".png", "")
        name = name.replace(".jpeg", "")
        self.name = name


class ImageandPatchs:
    def __init__(self, root_dir, name, patchsinfo, rgb_image, scale=1):
        self.root_dir = root_dir
        self.patchsinfo = patchsinfo
        self.name = name
        self.patchs = patchsinfo
        self.scale = scale

        self.rgb_image = cv2.resize(rgb_image, (round(rgb_image.shape[1]*scale), round(rgb_image.shape[0]*scale)),
                                    interpolation=cv2.INTER_CUBIC)

        self.do_have_estimate = False
        self.estimation_updated_image = None
        self.estimation_base_image = None

    def __len__(self):
        return len(self.patchs)

    def set_base_estimate(self, est):
        self.estimation_base_image = est
        if self.estimation_updated_image is not None:
            self.do_have_estimate = True

    def set_updated_estimate(self, est):
        self.estimation_updated_image = est
        if self.estimation_base_image is not None:
            self.do_have_estimate = True

    def __getitem__(self, index):
        patch_id = int(self.patchs[index][0])
        rect = np.array(self.patchs[index][1]['rect'])
        msize = self.patchs[index][1]['size']

        ## applying scale to rect:
        rect = np.round(rect * self.scale)
        rect = rect.astype('int')
        msize = round(msize * self.scale)

        patch_rgb = impatch(self.rgb_image, rect)
        if self.do_have_estimate:
            patch_whole_estimate_base = impatch(self.estimation_base_image, rect)
            patch_whole_estimate_updated = impatch(self.estimation_updated_image, rect)
            return {'patch_rgb': patch_rgb, 'patch_whole_estimate_base': patch_whole_estimate_base,
                    'patch_whole_estimate_updated': patch_whole_estimate_updated, 'rect': rect,
                    'size': msize, 'id': patch_id}
        else:
            return {'patch_rgb': patch_rgb, 'rect': rect, 'size': msize, 'id': patch_id}


class ImageDataset:
    def __init__(self, root_dir, subsetname):
        self.dataset_dir = root_dir
        self.subsetname = subsetname
        self.rgb_image_dir = root_dir
        self.files = sorted(os.listdir(self.rgb_image_dir))

    def __len__(self):
        return len(self.files)

    def __getitem__(self, index):
        return Images(self.rgb_image_dir, self.files, index)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/.DS_Store
================================================
[Non-text file]


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/.gitignore
================================================
/.idea
/pix2pix/checkpoints
/midas/model-f46da743.pt
/midas/model.pt
/structuredrl/model.pth.tar
dataset_prepare/midas/model.pt
/results
/result
/__pycache__
*/*.pyc
*/**/*.pyc
*/**/**/*.pyc
*/**/**/**/*.pyc
*/**/**/**/**/*.pyc
*/*.so*
*/**/*.so*
*/**/*.dylib*
*/**/__pycache__


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/create_crops.m
================================================
clc
clear
%%

generatecrops('middleburry',1800/2,100,'train',0)
generatecrops('middleburry',1800/2,100,'test',0)

generatecrops('middleburry',1400/2,150,'train',100)
generatecrops('middleburry',1400/2,150,'test',100)

generatecrops('middleburry',1000/2,250,'train',200)
generatecrops('middleburry',1000/2,250,'test',200)


generatecrops('ibims1',480/2,70,'train',0)
generatecrops('ibims1',360/2,80,'train',100)
generatecrops('ibims1',280/2,50,'train',200)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/generatecrops.m
================================================
function generatecrops(dataset, blsize, stride, subsetname,counter_offset)

root_dir = '';

imgdir = sprintf("%s/%s/rgb",root_dir,dataset); 
est_lq_dir = sprintf("%s/%s/whole_low_est",root_dir,dataset);
est_hq_dir = sprintf("%s/%s/whole_high_est",root_dir,dataset);

img_result_dir = sprintf("%s/mergenetdataset/%s/rgb",root_dir,subsetname);
outer_result_dir = sprintf("%s/mergenetdataset/%s/outer",root_dir,subsetname);
gt_fake_result_dir = sprintf("%s/mergenetdataset/%s/gtfake",root_dir,subsetname);

mkdir(img_result_dir);
mkdir(outer_result_dir);
mkdir(gt_fake_result_dir);


imglist = dir(sprintf('%s/*.png',imgdir));

if strcmp(dataset,'middleburry')
    test_set_size = 2;
else
    test_set_size = 0;
end
if strcmp(subsetname,'train')
    i_start = 0;
    i_end = -test_set_size;
else
    i_start = numel(imglist)-test_set_size;
    i_end = 0;
end

for i=1+i_start:numel(imglist)+i_end
    samplename = erase(imglist(i).name,'.png');

    img = im2uint16(imread(sprintf('%s/%s.png',imgdir,samplename)));
    
    est_lq = im2uint16(imread(sprintf('%s/%s.png',est_lq_dir,samplename)));
    est_hq = im2uint16(imread(sprintf('%s/%s.png',est_hq_dir,samplename)));
    
    counter1 = counter_offset;
    for k = blsize:stride:size(img,2)-blsize
        counter2 = counter_offset;
        for j = blsize:stride:size(img,1)-blsize

            cropbounds = [j-blsize+1,k-blsize+1,j-blsize+2*blsize,k-blsize+2*blsize];
            sample_img = img(cropbounds(1):cropbounds(3),cropbounds(2):cropbounds(4),:);
            sample_hq_est = est_hq(cropbounds(1):cropbounds(3),cropbounds(2):cropbounds(4),:);
            sample_lq_est = est_lq(cropbounds(1):cropbounds(3),cropbounds(2):cropbounds(4),:);
       
            
            if size(sample_hq_est) == [2*blsize 2*blsize]
                
                sample_img = imresize(sample_img,[672,672]);
                sample_hq_est = imresize(sample_hq_est,[672,672]);
                sample_lq_est = imresize(sample_lq_est,[672,672]);

                imwrite(sample_img,sprintf('%s/%s_&%d_%d&.png',img_result_dir,samplename,counter1,counter2)) 
                imwrite(sample_hq_est,sprintf('%s/%s_&%d_%d&.png',gt_fake_result_dir,samplename,counter1,counter2)) 
                imwrite(sample_lq_est,sprintf('%s/%s_&%d_%d&.png',outer_result_dir,samplename,counter1,counter2))
                fprintf('(%d/%d) %s - [%d-%d] \n',i,numel(imglist),samplename,counter1,counter2);
            else
                fprintf('(%d/%d) %s - Size Issue: Not cropped! \n',i,numel(imglist),samplename)    
            end
            counter2 = counter2 + 1;
        end
        counter1 = counter1 + 1;
    end
       
end

end





================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/ibims1_prepare.m
================================================
clc
clear
%%

load('ibims1_selected.mat');

root_dir = '';

ibims1_all_files = dir(sprintf('%s/ibims1/rgb/*.png',root_dir)); 

for i=1:numel(ibims1_all_files)
    current_name = ibims1_all_files(i).name;
    if ismember(current_name,files)
        fprintf('%d - %s \n',i,current_name)
    else
        delete(fullfile(ibims1_all_files(i).folder,ibims1_all_files(i).name))
    end
end


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/ibims1_selected.mat
================================================
[Non-text file]


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/mergenet_dataset_prepare.md
================================================
## Merge net training dataset preparation
### Download the datasets
Download rgb images of Ibims1-core-raw from [Ibims1 official webpage](https://www.bgu.tum.de/en/lmf/research/datasets/ibims-1/).

Download rgb images of Middleburry2014 "10 evaluation training sets with GT" + "13 additional datasets with GT
" from [Middleburry-2014 official webpage](https://vision.middlebury.edu/stereo/data/scenes2014/) . 

### Folder structure
Folder structure should be as below:

```bash
root_dir to datasets. 
|----middleburry
|    |----rgb
|         |----{rgb images.*}
|----ibims1
|    |----rgb
|         |----{rgb images.*}
```

### Setup

Set the "root_dir" parameter in bash
```bash
root_dir=''
```
Also, edit the root_dir parameter inside "./dataset_prepare/ibims1_prepare.m" and ""./dataset_prepare/generatecrops.m""exactly the same as the $root_dir parameter you've used above.


#### Step 1 : Remove not selected images from ibims1 dataset
```bash
cd ./dataset_prepare
## current dir : ./dataset_prepare
ibims1_prepare.m
```

#### Step 2 : Generate whole-image estimations

Download the midas weights from [MiDas-v2](https://github.com/intel-isl/MiDaS/tree/v2) and put it in 
> ./mergnet_dataset_prepare/midas/model.pt

Use the same python envirmenment as the one instructed in [Main method instruction](/README.md) under using [MiDas-v2](https://github.com/intel-isl/MiDaS/tree/v2) as base section. 

Run the following commands to generate estimations:
```python
cd ./midas/
## current dir : ./dataset_prepare/midas
python run.py --res 384 --input_dir $root_dir/ibims1/rgb --output_dir $root_dir/ibims1/whole_low_est
python run.py --res 672 --input_dir $root_dir/ibims1/rgb --output_dir $root_dir/ibims1/whole_high_est
python run.py --res 384 --input_dir $root_dir/middleburry/rgb --output_dir $root_dir/middleburry/whole_low_est
python run.py --res 672 --input_dir $root_dir/middleburry/rgb --output_dir $root_dir/middleburry/whole_high_est

```

#### Step 3 : Generate rgb, proxy ground truth and low resolution estimations of the patches
```bash
cd .. 
## current dir : ./dataset_prepare
create_crops.m
```
Allow ~20 minutes for the script execution to finish. 

#### Step 4 : Generate the patch estimations for high res input of the network
```python
cd ./midas/
## current dir : ./dataset_prepare/midas
python run.py --res 672 --input_dir $root_dir/mergenetdataset/train/rgb --output_dir $root_dir/mergenetdataset/train/inner
python run.py --res 672 --input_dir $root_dir/mergenetdataset/test/rgb --output_dir $root_dir/mergenetdataset/test/inner
```

Dataset is complete and is located at "$root_dir/mergenetdataset"



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/midas/run.py
================================================
"""Compute depth maps for images in the input folder.
"""
import os
import glob
import torch
import utils
import cv2
import argparse

from torchvision.transforms import Compose
from models.midas_net import MidasNet
from models.transforms import Resize, NormalizeImage, PrepareForNet


def run(input_path, output_path, model_path, process_res):
    """Run MonoDepthNN to compute depth maps.
    Args:
        input_path (str): path to input folder
        output_path (str): path to output folder
        model_path (str): path to saved model
    """
    print("initialize")

    # select device
    device = torch.device("cuda")
    print("device: %s" % device)

    # load network
    model = MidasNet(model_path, non_negative=True)

    transform = Compose(
        [
            Resize(
                process_res,
                process_res,
                resize_target=None,
                keep_aspect_ratio=True,
                ensure_multiple_of=32,
                resize_method="upper_bound",
                image_interpolation_method=cv2.INTER_CUBIC,
            ),
            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            PrepareForNet(),
        ]
    )

    model.to(device)
    model.eval()

    # get input
    img_names = glob.glob(os.path.join(input_path, "*"))
    num_images = len(img_names)

    # create output folder
    os.makedirs(output_path, exist_ok=True)

    print("start processing")

    for ind, img_name in enumerate(img_names):

        print("  processing {} ({}/{})".format(img_name, ind + 1, num_images))

        # input

        img = utils.read_image(img_name)
        img_input = transform({"image": img})["image"]

        # compute
        with torch.no_grad():
            sample = torch.from_numpy(img_input).to(device).unsqueeze(0)
            prediction = model.forward(sample)
            prediction = (
                torch.nn.functional.interpolate(
                    prediction.unsqueeze(1),
                    size=img.shape[:2],
                    mode="bicubic",
                    align_corners=False,
                )
                .squeeze()
                .cpu()
                .numpy()
            )

        # output
        filename = os.path.join(
            output_path, os.path.splitext(os.path.basename(img_name))[0]
        )
        utils.write_depth(filename, prediction, bits=2)

    print("finished")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--input_dir', required=True, type=str)
    parser.add_argument('--output_dir', required=True, type=str)
    parser.add_argument('--res', required=True, type=int)
    args = parser.parse_args()

    MODEL_PATH = "model.pt"

    # set torch options
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True

    # compute depth maps
    run(args.input_dir, args.output_dir, MODEL_PATH, args.res)


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/midas/utils.py
================================================
"""Utils for monoDepth.
"""
import sys
import re
import numpy as np
import cv2
import torch


def read_pfm(path):
    """Read pfm file.

    Args:
        path (str): path to file

    Returns:
        tuple: (data, scale)
    """
    with open(path, "rb") as file:

        color = None
        width = None
        height = None
        scale = None
        endian = None

        header = file.readline().rstrip()
        if header.decode("ascii") == "PF":
            color = True
        elif header.decode("ascii") == "Pf":
            color = False
        else:
            raise Exception("Not a PFM file: " + path)

        dim_match = re.match(r"^(\d+)\s(\d+)\s$", file.readline().decode("ascii"))
        if dim_match:
            width, height = list(map(int, dim_match.groups()))
        else:
            raise Exception("Malformed PFM header.")

        scale = float(file.readline().decode("ascii").rstrip())
        if scale < 0:
            # little-endian
            endian = "<"
            scale = -scale
        else:
            # big-endian
            endian = ">"

        data = np.fromfile(file, endian + "f")
        shape = (height, width, 3) if color else (height, width)

        data = np.reshape(data, shape)
        data = np.flipud(data)

        return data, scale


def write_pfm(path, image, scale=1):
    """Write pfm file.

    Args:
        path (str): pathto file
        image (array): data
        scale (int, optional): Scale. Defaults to 1.
    """

    with open(path, "wb") as file:
        color = None

        if image.dtype.name != "float32":
            raise Exception("Image dtype must be float32.")

        image = np.flipud(image)

        if len(image.shape) == 3 and image.shape[2] == 3:  # color image
            color = True
        elif (
            len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1
        ):  # greyscale
            color = False
        else:
            raise Exception("Image must have H x W x 3, H x W x 1 or H x W dimensions.")

        file.write("PF\n" if color else "Pf\n".encode())
        file.write("%d %d\n".encode() % (image.shape[1], image.shape[0]))

        endian = image.dtype.byteorder

        if endian == "<" or endian == "=" and sys.byteorder == "little":
            scale = -scale

        file.write("%f\n".encode() % scale)

        image.tofile(file)


def read_image(path):
    """Read image and output RGB image (0-1).

    Args:
        path (str): path to file

    Returns:
        array: RGB image (0-1)
    """
    img = cv2.imread(path)

    if img.ndim == 2:
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0

    return img


def resize_image(img):
    """Resize image and make it fit for network.

    Args:
        img (array): image

    Returns:
        tensor: data ready for network
    """
    height_orig = img.shape[0]
    width_orig = img.shape[1]

    if width_orig > height_orig:
        scale = width_orig / 384
    else:
        scale = height_orig / 384

    height = (np.ceil(height_orig / scale / 32) * 32).astype(int)
    width = (np.ceil(width_orig / scale / 32) * 32).astype(int)

    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)

    img_resized = (
        torch.from_numpy(np.transpose(img_resized, (2, 0, 1))).contiguous().float()
    )
    img_resized = img_resized.unsqueeze(0)

    return img_resized


def resize_depth(depth, width, height):
    """Resize depth map and bring to CPU (numpy).

    Args:
        depth (tensor): depth
        width (int): image width
        height (int): image height

    Returns:
        array: processed depth
    """
    depth = torch.squeeze(depth[0, :, :, :]).to("cpu")

    depth_resized = cv2.resize(
        depth.numpy(), (width, height), interpolation=cv2.INTER_CUBIC
    )

    return depth_resized

def write_depth(path, depth, bits=1):
    """Write depth map to pfm and png file.

    Args:
        path (str): filepath without extension
        depth (array): depth
    """
    # write_pfm(path + ".pfm", depth.astype(np.float32))

    depth_min = depth.min()
    depth_max = depth.max()

    max_val = (2**(8*bits))-1
    # if depth_max>max_val:
    #     print('Warning: Depth being clipped')
    #
    # if depth_max - depth_min > np.finfo("float").eps:
    #     out = depth
    #     out [depth > max_val] = max_val
    # else:
    #     out = 0

    if depth_max - depth_min > np.finfo("float").eps:
        out = max_val * (depth - depth_min) / (depth_max - depth_min)
    else:
        out = 0

    if bits == 1:
        cv2.imwrite(path+'.png', out.astype("uint8"))
    elif bits == 2:
        cv2.imwrite(path+'.png', out.astype("uint16"))

    return



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/midas/models/base_model.py
================================================
import torch
import torch.nn as nn


class BaseModel(torch.nn.Module):
    def load(self, path):
        """Load model from file.

        Args:
            path (str): file path
        """
        parameters = torch.load(path)

        if "optimizer" in parameters:
            parameters = parameters["model"]

        self.load_state_dict(parameters)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/midas/models/blocks.py
================================================
import torch
import torch.nn as nn


def _make_encoder(features, use_pretrained):
    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)
    scratch = _make_scratch([256, 512, 1024, 2048], features)

    return pretrained, scratch


def _make_resnet_backbone(resnet):
    pretrained = nn.Module()
    pretrained.layer1 = nn.Sequential(
        resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1
    )

    pretrained.layer2 = resnet.layer2
    pretrained.layer3 = resnet.layer3
    pretrained.layer4 = resnet.layer4

    return pretrained


def _make_pretrained_resnext101_wsl(use_pretrained):
    resnet = torch.hub.load("facebookresearch/WSL-Images:main", "resnext101_32x8d_wsl")
    return _make_resnet_backbone(resnet)


def _make_scratch(in_shape, out_shape):
    scratch = nn.Module()

    scratch.layer1_rn = nn.Conv2d(
        in_shape[0], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    scratch.layer2_rn = nn.Conv2d(
        in_shape[1], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    scratch.layer3_rn = nn.Conv2d(
        in_shape[2], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    scratch.layer4_rn = nn.Conv2d(
        in_shape[3], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    return scratch


class Interpolate(nn.Module):
    """Interpolation module.
    """

    def __init__(self, scale_factor, mode):
        """Init.

        Args:
            scale_factor (float): scaling
            mode (str): interpolation mode
        """
        super(Interpolate, self).__init__()

        self.interp = nn.functional.interpolate
        self.scale_factor = scale_factor
        self.mode = mode

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input

        Returns:
            tensor: interpolated data
        """

        x = self.interp(
            x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False
        )

        return x


class ResidualConvUnit(nn.Module):
    """Residual convolution module.
    """

    def __init__(self, features):
        """Init.

        Args:
            features (int): number of features
        """
        super().__init__()

        self.conv1 = nn.Conv2d(
            features, features, kernel_size=3, stride=1, padding=1, bias=True
        )

        self.conv2 = nn.Conv2d(
            features, features, kernel_size=3, stride=1, padding=1, bias=True
        )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input

        Returns:
            tensor: output
        """
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)

        return out + x


class FeatureFusionBlock(nn.Module):
    """Feature fusion block.
    """

    def __init__(self, features):
        """Init.

        Args:
            features (int): number of features
        """
        super(FeatureFusionBlock, self).__init__()

        self.resConfUnit1 = ResidualConvUnit(features)
        self.resConfUnit2 = ResidualConvUnit(features)

    def forward(self, *xs):
        """Forward pass.

        Returns:
            tensor: output
        """
        output = xs[0]

        if len(xs) == 2:
            output += self.resConfUnit1(xs[1])

        output = self.resConfUnit2(output)

        output = nn.functional.interpolate(
            output, scale_factor=2, mode="bilinear", align_corners=True
        )

        return output



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/midas/models/midas_net.py
================================================
"""MidashNet: Network for monocular depth estimation trained by mixing several datasets.
This file contains code that is adapted from
https://github.com/thomasjpfan/pytorch_refinenet/blob/master/pytorch_refinenet/refinenet/refinenet_4cascade.py
"""
import torch
import torch.nn as nn

from models.base_model import BaseModel
from models.blocks import FeatureFusionBlock, Interpolate, _make_encoder


class MidasNet(BaseModel):
    """Network for monocular depth estimation.
    """

    def __init__(self, path=None, features=256, non_negative=True):
        """Init.

        Args:
            path (str, optional): Path to saved model. Defaults to None.
            features (int, optional): Number of features. Defaults to 256.
            backbone (str, optional): Backbone network for encoder. Defaults to resnet50
        """
        print("Loading weights: ", path)

        super(MidasNet, self).__init__()

        use_pretrained = False if path else True

        self.pretrained, self.scratch = _make_encoder(features, use_pretrained)

        self.scratch.refinenet4 = FeatureFusionBlock(features)
        self.scratch.refinenet3 = FeatureFusionBlock(features)
        self.scratch.refinenet2 = FeatureFusionBlock(features)
        self.scratch.refinenet1 = FeatureFusionBlock(features)

        self.scratch.output_conv = nn.Sequential(
            nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),
            Interpolate(scale_factor=2, mode="bilinear"),
            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),
            nn.ReLU(True) if non_negative else nn.Identity(),
        )

        if path:
            self.load(path)

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input data (image)

        Returns:
            tensor: depth
        """

        layer_1 = self.pretrained.layer1(x)
        layer_2 = self.pretrained.layer2(layer_1)
        layer_3 = self.pretrained.layer3(layer_2)
        layer_4 = self.pretrained.layer4(layer_3)

        layer_1_rn = self.scratch.layer1_rn(layer_1)
        layer_2_rn = self.scratch.layer2_rn(layer_2)
        layer_3_rn = self.scratch.layer3_rn(layer_3)
        layer_4_rn = self.scratch.layer4_rn(layer_4)

        path_4 = self.scratch.refinenet4(layer_4_rn)
        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)
        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)
        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)

        out = self.scratch.output_conv(path_1)

        return torch.squeeze(out, dim=1)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/dataset_prepare/midas/models/transforms.py
================================================
import numpy as np
import cv2
import math


def apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):
    """Rezise the sample to ensure the given size. Keeps aspect ratio.

    Args:
        sample (dict): sample
        size (tuple): image size

    Returns:
        tuple: new size
    """
    shape = list(sample["disparity"].shape)

    if shape[0] >= size[0] and shape[1] >= size[1]:
        return sample

    scale = [0, 0]
    scale[0] = size[0] / shape[0]
    scale[1] = size[1] / shape[1]

    scale = max(scale)

    shape[0] = math.ceil(scale * shape[0])
    shape[1] = math.ceil(scale * shape[1])

    # resize
    sample["image"] = cv2.resize(
        sample["image"], tuple(shape[::-1]), interpolation=image_interpolation_method
    )

    sample["disparity"] = cv2.resize(
        sample["disparity"], tuple(shape[::-1]), interpolation=cv2.INTER_NEAREST
    )
    sample["mask"] = cv2.resize(
        sample["mask"].astype(np.float32),
        tuple(shape[::-1]),
        interpolation=cv2.INTER_NEAREST,
    )
    sample["mask"] = sample["mask"].astype(bool)

    return tuple(shape)


class Resize(object):
    """Resize sample to given size (width, height).
    """

    def __init__(
        self,
        width,
        height,
        resize_target=True,
        keep_aspect_ratio=False,
        ensure_multiple_of=1,
        resize_method="lower_bound",
        image_interpolation_method=cv2.INTER_AREA,
    ):
        """Init.

        Args:
            width (int): desired output width
            height (int): desired output height
            resize_target (bool, optional):
                True: Resize the full sample (image, mask, target).
                False: Resize image only.
                Defaults to True.
            keep_aspect_ratio (bool, optional):
                True: Keep the aspect ratio of the input sample.
                Output sample might not have the given width and height, and
                resize behaviour depends on the parameter 'resize_method'.
                Defaults to False.
            ensure_multiple_of (int, optional):
                Output width and height is constrained to be multiple of this parameter.
                Defaults to 1.
            resize_method (str, optional):
                "lower_bound": Output will be at least as large as the given size.
                "upper_bound": Output will be at max as large as the given size. (Output size might be smaller than given size.)
                "minimal": Scale as least as possible.  (Output size might be smaller than given size.)
                Defaults to "lower_bound".
        """
        self.__width = width
        self.__height = height

        self.__resize_target = resize_target
        self.__keep_aspect_ratio = keep_aspect_ratio
        self.__multiple_of = ensure_multiple_of
        self.__resize_method = resize_method
        self.__image_interpolation_method = image_interpolation_method

    def constrain_to_multiple_of(self, x, min_val=0, max_val=None):
        y = (np.round(x / self.__multiple_of) * self.__multiple_of).astype(int)

        if max_val is not None and y > max_val:
            y = (np.floor(x / self.__multiple_of) * self.__multiple_of).astype(int)

        if y < min_val:
            y = (np.ceil(x / self.__multiple_of) * self.__multiple_of).astype(int)

        return y

    def get_size(self, width, height):
        # determine new height and width
        scale_height = self.__height / height
        scale_width = self.__width / width

        if self.__keep_aspect_ratio:
            if self.__resize_method == "lower_bound":
                # scale such that output size is lower bound
                if scale_width > scale_height:
                    # fit width
                    scale_height = scale_width
                else:
                    # fit height
                    scale_width = scale_height
            elif self.__resize_method == "upper_bound":
                # scale such that output size is upper bound
                if scale_width < scale_height:
                    # fit width
                    scale_height = scale_width
                else:
                    # fit height
                    scale_width = scale_height
            elif self.__resize_method == "minimal":
                # scale as least as possbile
                if abs(1 - scale_width) < abs(1 - scale_height):
                    # fit width
                    scale_height = scale_width
                else:
                    # fit height
                    scale_width = scale_height
            else:
                raise ValueError(
                    f"resize_method {self.__resize_method} not implemented"
                )

        if self.__resize_method == "lower_bound":
            new_height = self.constrain_to_multiple_of(
                scale_height * height, min_val=self.__height
            )
            new_width = self.constrain_to_multiple_of(
                scale_width * width, min_val=self.__width
            )
        elif self.__resize_method == "upper_bound":
            new_height = self.constrain_to_multiple_of(
                scale_height * height, max_val=self.__height
            )
            new_width = self.constrain_to_multiple_of(
                scale_width * width, max_val=self.__width
            )
        elif self.__resize_method == "minimal":
            new_height = self.constrain_to_multiple_of(scale_height * height)
            new_width = self.constrain_to_multiple_of(scale_width * width)
        else:
            raise ValueError(f"resize_method {self.__resize_method} not implemented")

        return (new_width, new_height)

    def __call__(self, sample):
        width, height = self.get_size(
            sample["image"].shape[1], sample["image"].shape[0]
        )

        # resize sample
        sample["image"] = cv2.resize(
            sample["image"],
            (width, height),
            interpolation=self.__image_interpolation_method,
        )

        if self.__resize_target:
            if "disparity" in sample:
                sample["disparity"] = cv2.resize(
                    sample["disparity"],
                    (width, height),
                    interpolation=cv2.INTER_NEAREST,
                )

            if "depth" in sample:
                sample["depth"] = cv2.resize(
                    sample["depth"], (width, height), interpolation=cv2.INTER_NEAREST
                )

            sample["mask"] = cv2.resize(
                sample["mask"].astype(np.float32),
                (width, height),
                interpolation=cv2.INTER_NEAREST,
            )
            sample["mask"] = sample["mask"].astype(bool)

        return sample


class NormalizeImage(object):
    """Normlize image by given mean and std.
    """

    def __init__(self, mean, std):
        self.__mean = mean
        self.__std = std

    def __call__(self, sample):
        sample["image"] = (sample["image"] - self.__mean) / self.__std

        return sample


class PrepareForNet(object):
    """Prepare sample for usage as network input.
    """

    def __init__(self):
        pass

    def __call__(self, sample):
        image = np.transpose(sample["image"], (2, 0, 1))
        sample["image"] = np.ascontiguousarray(image).astype(np.float32)

        if "mask" in sample:
            sample["mask"] = sample["mask"].astype(np.float32)
            sample["mask"] = np.ascontiguousarray(sample["mask"])

        if "disparity" in sample:
            disparity = sample["disparity"].astype(np.float32)
            sample["disparity"] = np.ascontiguousarray(disparity)

        if "depth" in sample:
            depth = sample["depth"].astype(np.float32)
            sample["depth"] = np.ascontiguousarray(depth)

        return sample



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/evaluation/D3R.m
================================================
function [confidence,error] = D3R(gt,depth_est,center_points,point_pairs,freq_ratio,d3r_ratio)
% Inputs
% gt: ground truth.
% depth_est: estimation.
% center_points: superpixel center location from extractD3Rpoints function.
% point_pairs: point pair relations from extractD3Rpoints function.
% freq_ratio: ratio used to extract high frequnecies depth changes ... setting to zero means all frequencies must be taken into account.
% d3r_ratio: metric ratio.

% Outputs
% error: some of the errors
% confidence: number of the point pairs used to compute the error. (error value should be normalized according to the confidence to be comparable)


gt(isnan(gt))=0;
inflamed_gt =gt;% imadjust(gt);
same_ratio_gt = 1+d3r_ratio;
same_ratio_est = 1+d3r_ratio;
error = 0;
confidence = 0;
for i=1:length(center_points)
    neighbours = point_pairs{i};
    if isempty(neighbours)
        continue;
    end
    for j = neighbours'
        j_neighbours = point_pairs{j};
        j_neighbours(j_neighbours == i) = [];
        if isempty(j_neighbours)
             point_pairs{j} = [];
        else
             point_pairs{j} = j_neighbours;
        end
        index1=center_points(i);
        index2=center_points(j); 
        if(gt(index1)~=0 && gt(index2)~=0) % error in GT
             if ord(gt(index1),gt(index2),1+freq_ratio)~=0
                er = abs(ord(inflamed_gt(index1),inflamed_gt(index2),same_ratio_gt) - ord(depth_est(index1),depth_est(index2),same_ratio_est));
                error = error + er;
                confidence = confidence + 1;
             end
        end
    end
end

end



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/evaluation/evaluatedataset.m
================================================
clc;
clear

%% 
evaluation_name = 'testEvaluation';

%% Fill in the needed path and flags for evaluation
estimation_path = '';
gt_depth_path = '';
evaluation_matfile_save_dir = './';
dataset_disp_gttype = true; %% (True) for gt disparity (False) for gt depth 
superpixel_scale = 1; % use 0.2 for middleburry and 1 for ibims1 and NYU %Used for rescaling the image before extracting superpixel centers for D3R error metric. smaller scale for high res images results in a faster evaluation.

%%

imglist = dir(fullfile(gt_depth_path,'*.png'));
fprintf('Estimation path: %s\nGT path: %s\nGT type:%d (0:depth 1:disparity)\nTotal number of images: %d \n',estimation_path,gt_depth_path,dataset_disp_gttype,numel(imglist))

for img=1:numel(imglist)
    imagename = imglist(img).name;
    if dataset_disp_gttype
        gt_disp = im2double(imread(fullfile(gt_depth_path,sprintf('%s',imagename))));
        gt_disp(gt_disp==0)=nan;
        min_gt_disp = min(gt_disp(:));
        gt_depth = 1./gt_disp;
        gt_disp = rescale(gt_disp,0,1);
        gt_depth = rescale(gt_depth,0,1);
    else
        gt_depth = im2double(imread(fullfile(gt_depth_path,sprintf('%s',imagename))));
        gt_depth(gt_depth==0)=nan;
        gt_disp = 1./gt_depth;
        gt_disp = gt_disp / max(gt_disp(:));
        gt_depth(gt_disp>1)=nan;
        gt_disp(gt_disp>1)=nan;
        min_gt_disp = min(gt_disp(:));
        gt_disp = rescale(gt_disp,0,1);
        gt_depth = rescale(gt_depth,0,1);
    end
        
    estimate_disp = im2double(imread(fullfile(estimation_path,sprintf('%s',imagename))));
    estimate_disp_ = rescale(estimate_disp,min_gt_disp,1);
    estimate_depth = 1./estimate_disp_;
    estimate_depth = rescale(estimate_depth,0,1);
    
    gt_small=imresize(gt_disp,superpixel_scale,'nearest');
    samples=5000;
    [centers,neightbouring_rel,random_rel]=extractD3Rpoints(gt_small,samples);
    [sub_x,sub_y] = ind2sub(size(gt_small),centers);
         
    sub_x = sub_x/superpixel_scale;
    sub_y = sub_y/superpixel_scale;
    centers = sub2ind(size(gt_depth),sub_x,sub_y);  
        
    hf_ratio = 0.1;
    d3r_ratio = 0.01;
     
    [confidence,error]=D3R(gt_depth, estimate_depth, centers, neightbouring_rel,hf_ratio,d3r_ratio);
    d3r_error(img)=error/confidence;
    
    hf_ratio = 0;
    d3r_ratio = 0.03;
    [confidence,error]=D3R(gt_depth, estimate_depth, centers, random_rel,hf_ratio,d3r_ratio); 
    ord_error(img)=error/confidence; 
    
    mask = ones(size(gt_depth));
    mask(isnan(gt_disp))=0;

    gt_disp(mask==0)=0;
    estimate_disp(mask==0)=0;
    rmse_error(img) = sqrt(immse(gt_disp,estimate_disp));
    
    mask(gt_depth==0)=0;
    mask(estimate_depth==0)=0;
    
    gt_depth = gt_depth(:);
    estimate_depth = estimate_depth(:);
    
    gt_depth(mask(:)==0) = [];
    estimate_depth(mask(:)==0) = [];
     
    thresh = max(gt_depth./estimate_depth, estimate_depth./gt_depth);
    thresh_1_25_error(img) = length(find(thresh>1.25)) / numel(thresh(:));
    thresh_1_25_2_error(img) = length(find(thresh>1.25^2)) / numel(thresh(:));
    thresh_1_25_3_error(img) = length(find(thresh>1.25^3)) / numel(thresh(:));
    log_10(img) = mean(abs(log10(gt_depth) - log10(estimate_depth)),'all');
    abs_rel(img) = mean(abs(gt_depth - estimate_depth)./ gt_depth,'all');
    sq_rel(img) = mean(((gt_depth - estimate_depth).^2)./ gt_depth,'all');
    
    fprintf('(%d/%d) - %s\n\t D3R:%0.4f RMSE:%0.4f ABSREL:%0.4f\n',img,numel(imglist),imagename,d3r_error(img),rmse_error(img),abs_rel(img))
end

save(fullfile(evaluation_matfile_save_dir,sprintf('evaluation_%s.mat',evaluation_name)),'ord_error','d3r_error','rmse_error','thresh_1_25_error','thresh_1_25_2_error','thresh_1_25_3_error','log_10','abs_rel','sq_rel');

%%
fprintf('ORD: %0.4f \n',mean(ord_error(:),'omitnan'))
fprintf('D3R: %0.4f \n',mean(d3r_error(:),'omitnan'))
fprintf('RMSE: %0.4f \n',mean(rmse_error(:),'omitnan'))
fprintf('THRESH 1.25: %0.4f \n',mean(thresh_1_25_error(:),'omitnan'))
fprintf('THRESH 1.25^2: %0.4f \n',mean(thresh_1_25_2_error(:),'omitnan'))
fprintf('THRESH 1.25^3: %0.4f \n',mean(thresh_1_25_3_error(:),'omitnan'))
fprintf('LOG10: %0.4f \n',mean(log_10(:),'omitnan'))
fprintf('ABS REL: %0.4f \n',mean(abs_rel(:),'omitnan'))
fprintf('SQ REL: %0.4f \n',mean(sq_rel(:),'omitnan'))



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/evaluation/extractD3Rpoints.m
================================================
function [centers,neightbouring_rel,random_rel]=extractD3Rpoints(gt,samples)
% Inputs
% gt: ground truth image to exctact the superpixels from
% samples: expected number of samples (actual number of samples might be slightly different)

% Outputs:
% centers: superpixel centers to be used within D3R function
% neightbouring_rel: neighbouring point pair relations to be used within D3R function
% random_rel:  randomly selected point pair relations to be used within D3R function

gt(isnan(gt))=0;
[L,NumLabels]=superpixels(gt,samples,'Compactness',20);

idx = label2idx(L);
[height, width, ~] = size(gt);
neightbouring_rel = cell(NumLabels,1);    
random_rel = cell(NumLabels,1);
centers = zeros(NumLabels,1);
for i = 1:NumLabels
    mask  = L == i;
    neightbouring_rel{i} = unique(L(bwdist(mask ,'euclidean')==1));
    random_rel{i} = randi([1,NumLabels],3,1);   
    a = idx{i};
    center = computeCenter(a,height);
    centers(i) = center;
end

end

function center = computeCenter(cluster,imgheight)
sumx=0;
sumy=0;
for i=1:length(cluster)
    [x,y] = index2index(cluster(i),imgheight);
    sumx = sumx + x;
    sumy = sumy + y;
end
center = floor(sumx/length(cluster)) + (floor(sumy/length(cluster))-1)*imgheight;
end


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/evaluation/index2index.m
================================================
function [x,y] = index2index(index,rows)
x=mod(index,rows);
y=floor(index/rows)+1;
if x == 0
    x = rows;
    y = y-1; 
end  
end


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/evaluation/ord.m
================================================
function out = ord(val1,val2,delta)
ratio = (val1+eps)/(val2+eps);
    if ratio > delta
        out = 1;
    elseif ratio < 1/delta
        out = -1;
    else
        out = 0;
    end
end




================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/midas/utils.py
================================================
"""Utils for monoDepth.
"""
import sys
import re
import numpy as np
import cv2
import torch


def read_pfm(path):
    """Read pfm file.

    Args:
        path (str): path to file

    Returns:
        tuple: (data, scale)
    """
    with open(path, "rb") as file:

        color = None
        width = None
        height = None
        scale = None
        endian = None

        header = file.readline().rstrip()
        if header.decode("ascii") == "PF":
            color = True
        elif header.decode("ascii") == "Pf":
            color = False
        else:
            raise Exception("Not a PFM file: " + path)

        dim_match = re.match(r"^(\d+)\s(\d+)\s$", file.readline().decode("ascii"))
        if dim_match:
            width, height = list(map(int, dim_match.groups()))
        else:
            raise Exception("Malformed PFM header.")

        scale = float(file.readline().decode("ascii").rstrip())
        if scale < 0:
            # little-endian
            endian = "<"
            scale = -scale
        else:
            # big-endian
            endian = ">"

        data = np.fromfile(file, endian + "f")
        shape = (height, width, 3) if color else (height, width)

        data = np.reshape(data, shape)
        data = np.flipud(data)

        return data, scale


def write_pfm(path, image, scale=1):
    """Write pfm file.

    Args:
        path (str): pathto file
        image (array): data
        scale (int, optional): Scale. Defaults to 1.
    """

    with open(path, "wb") as file:
        color = None

        if image.dtype.name != "float32":
            raise Exception("Image dtype must be float32.")

        image = np.flipud(image)

        if len(image.shape) == 3 and image.shape[2] == 3:  # color image
            color = True
        elif (
            len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1
        ):  # greyscale
            color = False
        else:
            raise Exception("Image must have H x W x 3, H x W x 1 or H x W dimensions.")

        file.write("PF\n" if color else "Pf\n".encode())
        file.write("%d %d\n".encode() % (image.shape[1], image.shape[0]))

        endian = image.dtype.byteorder

        if endian == "<" or endian == "=" and sys.byteorder == "little":
            scale = -scale

        file.write("%f\n".encode() % scale)

        image.tofile(file)


def read_image(path):
    """Read image and output RGB image (0-1).

    Args:
        path (str): path to file

    Returns:
        array: RGB image (0-1)
    """
    img = cv2.imread(path)

    if img.ndim == 2:
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0

    return img


def resize_image(img):
    """Resize image and make it fit for network.

    Args:
        img (array): image

    Returns:
        tensor: data ready for network
    """
    height_orig = img.shape[0]
    width_orig = img.shape[1]

    if width_orig > height_orig:
        scale = width_orig / 384
    else:
        scale = height_orig / 384

    height = (np.ceil(height_orig / scale / 32) * 32).astype(int)
    width = (np.ceil(width_orig / scale / 32) * 32).astype(int)

    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)

    img_resized = (
        torch.from_numpy(np.transpose(img_resized, (2, 0, 1))).contiguous().float()
    )
    img_resized = img_resized.unsqueeze(0)

    return img_resized


def resize_depth(depth, width, height):
    """Resize depth map and bring to CPU (numpy).

    Args:
        depth (tensor): depth
        width (int): image width
        height (int): image height

    Returns:
        array: processed depth
    """
    depth = torch.squeeze(depth[0, :, :, :]).to("cpu")

    depth_resized = cv2.resize(
        depth.numpy(), (width, height), interpolation=cv2.INTER_CUBIC
    )

    return depth_resized

def write_depth(path, depth, bits=1 , colored=False):
    """Write depth map to pfm and png file.

    Args:
        path (str): filepath without extension
        depth (array): depth
    """
    # write_pfm(path + ".pfm", depth.astype(np.float32))
    if colored == True:
        bits = 1

    depth_min = depth.min()
    depth_max = depth.max()

    max_val = (2**(8*bits))-1
    # if depth_max>max_val:
    #     print('Warning: Depth being clipped')
    #
    # if depth_max - depth_min > np.finfo("float").eps:
    #     out = depth
    #     out [depth > max_val] = max_val
    # else:
    #     out = 0

    if depth_max - depth_min > np.finfo("float").eps:
        out = max_val * (depth - depth_min) / (depth_max - depth_min)
    else:
        out = 0

    if bits == 1 or colored:
        out = out.astype("uint8")
        if colored:
            out = cv2.applyColorMap(out,cv2.COLORMAP_INFERNO)
        cv2.imwrite(path+'.png', out)
    elif bits == 2:
        cv2.imwrite(path+'.png', out.astype("uint16"))

    return



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/midas/models/base_model.py
================================================
import torch
import torch.nn as nn


class BaseModel(torch.nn.Module):
    def load(self, path):
        """Load model from file.

        Args:
            path (str): file path
        """
        parameters = torch.load(path)

        if "optimizer" in parameters:
            parameters = parameters["model"]

        self.load_state_dict(parameters)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/midas/models/blocks.py
================================================
import torch
import torch.nn as nn


def _make_encoder(features, use_pretrained):
    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)
    scratch = _make_scratch([256, 512, 1024, 2048], features)

    return pretrained, scratch


def _make_resnet_backbone(resnet):
    pretrained = nn.Module()
    pretrained.layer1 = nn.Sequential(
        resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1
    )

    pretrained.layer2 = resnet.layer2
    pretrained.layer3 = resnet.layer3
    pretrained.layer4 = resnet.layer4

    return pretrained


def _make_pretrained_resnext101_wsl(use_pretrained):
    resnet = torch.hub.load("facebookresearch/WSL-Images:main", "resnext101_32x8d_wsl")
    return _make_resnet_backbone(resnet)


def _make_scratch(in_shape, out_shape):
    scratch = nn.Module()

    scratch.layer1_rn = nn.Conv2d(
        in_shape[0], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    scratch.layer2_rn = nn.Conv2d(
        in_shape[1], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    scratch.layer3_rn = nn.Conv2d(
        in_shape[2], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    scratch.layer4_rn = nn.Conv2d(
        in_shape[3], out_shape, kernel_size=3, stride=1, padding=1, bias=False
    )
    return scratch


class Interpolate(nn.Module):
    """Interpolation module.
    """

    def __init__(self, scale_factor, mode):
        """Init.

        Args:
            scale_factor (float): scaling
            mode (str): interpolation mode
        """
        super(Interpolate, self).__init__()

        self.interp = nn.functional.interpolate
        self.scale_factor = scale_factor
        self.mode = mode

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input

        Returns:
            tensor: interpolated data
        """

        x = self.interp(
            x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False
        )

        return x


class ResidualConvUnit(nn.Module):
    """Residual convolution module.
    """

    def __init__(self, features):
        """Init.

        Args:
            features (int): number of features
        """
        super().__init__()

        self.conv1 = nn.Conv2d(
            features, features, kernel_size=3, stride=1, padding=1, bias=True
        )

        self.conv2 = nn.Conv2d(
            features, features, kernel_size=3, stride=1, padding=1, bias=True
        )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input

        Returns:
            tensor: output
        """
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)

        return out + x


class FeatureFusionBlock(nn.Module):
    """Feature fusion block.
    """

    def __init__(self, features):
        """Init.

        Args:
            features (int): number of features
        """
        super(FeatureFusionBlock, self).__init__()

        self.resConfUnit1 = ResidualConvUnit(features)
        self.resConfUnit2 = ResidualConvUnit(features)

    def forward(self, *xs):
        """Forward pass.

        Returns:
            tensor: output
        """
        output = xs[0]

        if len(xs) == 2:
            output += self.resConfUnit1(xs[1])

        output = self.resConfUnit2(output)

        output = nn.functional.interpolate(
            output, scale_factor=2, mode="bilinear", align_corners=True
        )

        return output



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/midas/models/midas_net.py
================================================
"""MidashNet: Network for monocular depth estimation trained by mixing several datasets.
This file contains code that is adapted from
https://github.com/thomasjpfan/pytorch_refinenet/blob/master/pytorch_refinenet/refinenet/refinenet_4cascade.py
"""
import torch
import torch.nn as nn

from midas.models.base_model import BaseModel
from midas.models.blocks import FeatureFusionBlock, Interpolate, _make_encoder


class MidasNet(BaseModel):
    """Network for monocular depth estimation.
    """

    def __init__(self, path=None, features=256, non_negative=True):
        """Init.

        Args:
            path (str, optional): Path to saved model. Defaults to None.
            features (int, optional): Number of features. Defaults to 256.
            backbone (str, optional): Backbone network for encoder. Defaults to resnet50
        """
        print("Loading weights: ", path)

        super(MidasNet, self).__init__()

        use_pretrained = False if path else True

        self.pretrained, self.scratch = _make_encoder(features, use_pretrained)

        self.scratch.refinenet4 = FeatureFusionBlock(features)
        self.scratch.refinenet3 = FeatureFusionBlock(features)
        self.scratch.refinenet2 = FeatureFusionBlock(features)
        self.scratch.refinenet1 = FeatureFusionBlock(features)

        self.scratch.output_conv = nn.Sequential(
            nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),
            Interpolate(scale_factor=2, mode="bilinear"),
            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),
            nn.ReLU(True) if non_negative else nn.Identity(),
        )

        if path:
            self.load(path)

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input data (image)

        Returns:
            tensor: depth
        """

        layer_1 = self.pretrained.layer1(x)
        layer_2 = self.pretrained.layer2(layer_1)
        layer_3 = self.pretrained.layer3(layer_2)
        layer_4 = self.pretrained.layer4(layer_3)

        layer_1_rn = self.scratch.layer1_rn(layer_1)
        layer_2_rn = self.scratch.layer2_rn(layer_2)
        layer_3_rn = self.scratch.layer3_rn(layer_3)
        layer_4_rn = self.scratch.layer4_rn(layer_4)

        path_4 = self.scratch.refinenet4(layer_4_rn)
        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)
        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)
        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)

        out = self.scratch.output_conv(path_1)

        return torch.squeeze(out, dim=1)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/midas/models/transforms.py
================================================
import numpy as np
import cv2
import math


def apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):
    """Rezise the sample to ensure the given size. Keeps aspect ratio.

    Args:
        sample (dict): sample
        size (tuple): image size

    Returns:
        tuple: new size
    """
    shape = list(sample["disparity"].shape)

    if shape[0] >= size[0] and shape[1] >= size[1]:
        return sample

    scale = [0, 0]
    scale[0] = size[0] / shape[0]
    scale[1] = size[1] / shape[1]

    scale = max(scale)

    shape[0] = math.ceil(scale * shape[0])
    shape[1] = math.ceil(scale * shape[1])

    # resize
    sample["image"] = cv2.resize(
        sample["image"], tuple(shape[::-1]), interpolation=image_interpolation_method
    )

    sample["disparity"] = cv2.resize(
        sample["disparity"], tuple(shape[::-1]), interpolation=cv2.INTER_NEAREST
    )
    sample["mask"] = cv2.resize(
        sample["mask"].astype(np.float32),
        tuple(shape[::-1]),
        interpolation=cv2.INTER_NEAREST,
    )
    sample["mask"] = sample["mask"].astype(bool)

    return tuple(shape)


class Resize(object):
    """Resize sample to given size (width, height).
    """

    def __init__(
        self,
        width,
        height,
        resize_target=True,
        keep_aspect_ratio=False,
        ensure_multiple_of=1,
        resize_method="lower_bound",
        image_interpolation_method=cv2.INTER_AREA,
    ):
        """Init.

        Args:
            width (int): desired output width
            height (int): desired output height
            resize_target (bool, optional):
                True: Resize the full sample (image, mask, target).
                False: Resize image only.
                Defaults to True.
            keep_aspect_ratio (bool, optional):
                True: Keep the aspect ratio of the input sample.
                Output sample might not have the given width and height, and
                resize behaviour depends on the parameter 'resize_method'.
                Defaults to False.
            ensure_multiple_of (int, optional):
                Output width and height is constrained to be multiple of this parameter.
                Defaults to 1.
            resize_method (str, optional):
                "lower_bound": Output will be at least as large as the given size.
                "upper_bound": Output will be at max as large as the given size. (Output size might be smaller than given size.)
                "minimal": Scale as least as possible.  (Output size might be smaller than given size.)
                Defaults to "lower_bound".
        """
        self.__width = width
        self.__height = height

        self.__resize_target = resize_target
        self.__keep_aspect_ratio = keep_aspect_ratio
        self.__multiple_of = ensure_multiple_of
        self.__resize_method = resize_method
        self.__image_interpolation_method = image_interpolation_method

    def constrain_to_multiple_of(self, x, min_val=0, max_val=None):
        y = (np.round(x / self.__multiple_of) * self.__multiple_of).astype(int)

        if max_val is not None and y > max_val:
            y = (np.floor(x / self.__multiple_of) * self.__multiple_of).astype(int)

        if y < min_val:
            y = (np.ceil(x / self.__multiple_of) * self.__multiple_of).astype(int)

        return y

    def get_size(self, width, height):
        # determine new height and width
        scale_height = self.__height / height
        scale_width = self.__width / width

        if self.__keep_aspect_ratio:
            if self.__resize_method == "lower_bound":
                # scale such that output size is lower bound
                if scale_width > scale_height:
                    # fit width
                    scale_height = scale_width
                else:
                    # fit height
                    scale_width = scale_height
            elif self.__resize_method == "upper_bound":
                # scale such that output size is upper bound
                if scale_width < scale_height:
                    # fit width
                    scale_height = scale_width
                else:
                    # fit height
                    scale_width = scale_height
            elif self.__resize_method == "minimal":
                # scale as least as possbile
                if abs(1 - scale_width) < abs(1 - scale_height):
                    # fit width
                    scale_height = scale_width
                else:
                    # fit height
                    scale_width = scale_height
            else:
                raise ValueError(
                    f"resize_method {self.__resize_method} not implemented"
                )

        if self.__resize_method == "lower_bound":
            new_height = self.constrain_to_multiple_of(
                scale_height * height, min_val=self.__height
            )
            new_width = self.constrain_to_multiple_of(
                scale_width * width, min_val=self.__width
            )
        elif self.__resize_method == "upper_bound":
            new_height = self.constrain_to_multiple_of(
                scale_height * height, max_val=self.__height
            )
            new_width = self.constrain_to_multiple_of(
                scale_width * width, max_val=self.__width
            )
        elif self.__resize_method == "minimal":
            new_height = self.constrain_to_multiple_of(scale_height * height)
            new_width = self.constrain_to_multiple_of(scale_width * width)
        else:
            raise ValueError(f"resize_method {self.__resize_method} not implemented")

        return (new_width, new_height)

    def __call__(self, sample):
        width, height = self.get_size(
            sample["image"].shape[1], sample["image"].shape[0]
        )

        # resize sample
        sample["image"] = cv2.resize(
            sample["image"],
            (width, height),
            interpolation=self.__image_interpolation_method,
        )

        if self.__resize_target:
            if "disparity" in sample:
                sample["disparity"] = cv2.resize(
                    sample["disparity"],
                    (width, height),
                    interpolation=cv2.INTER_NEAREST,
                )

            if "depth" in sample:
                sample["depth"] = cv2.resize(
                    sample["depth"], (width, height), interpolation=cv2.INTER_NEAREST
                )

            sample["mask"] = cv2.resize(
                sample["mask"].astype(np.float32),
                (width, height),
                interpolation=cv2.INTER_NEAREST,
            )
            sample["mask"] = sample["mask"].astype(bool)

        return sample


class NormalizeImage(object):
    """Normlize image by given mean and std.
    """

    def __init__(self, mean, std):
        self.__mean = mean
        self.__std = std

    def __call__(self, sample):
        sample["image"] = (sample["image"] - self.__mean) / self.__std

        return sample


class PrepareForNet(object):
    """Prepare sample for usage as network input.
    """

    def __init__(self):
        pass

    def __call__(self, sample):
        image = np.transpose(sample["image"], (2, 0, 1))
        sample["image"] = np.ascontiguousarray(image).astype(np.float32)

        if "mask" in sample:
            sample["mask"] = sample["mask"].astype(np.float32)
            sample["mask"] = np.ascontiguousarray(sample["mask"])

        if "disparity" in sample:
            disparity = sample["disparity"].astype(np.float32)
            sample["disparity"] = np.ascontiguousarray(disparity)

        if "depth" in sample:
            depth = sample["depth"].astype(np.float32)
            sample["depth"] = np.ascontiguousarray(depth)

        return sample



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/test.py
================================================
"""General-purpose test script for image-to-image translation.

Once you have trained your model with train.py, you can use this script to test the model.
It will load a saved model from '--checkpoints_dir' and save the results to '--results_dir'.

It first creates model and dataset given the option. It will hard-code some parameters.
It then runs inference for '--num_test' images and save results to an HTML file.

Example (You need to train models first or download pre-trained models from our website):
    Test a CycleGAN model (both sides):
        python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan

    Test a CycleGAN model (one side only):
        python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout

    The option '--model test' is used for generating CycleGAN results only for one side.
    This option will automatically set '--dataset_mode single', which only loads the images from one set.
    On the contrary, using '--model cycle_gan' requires loading and generating results in both directions,
    which is sometimes unnecessary. The results will be saved at ./results/.
    Use '--results_dir <directory_path_to_save_result>' to specify the results directory.

    Test a pix2pix model:
        python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA

See options/base_options.py and options/test_options.py for more test options.
See training and test tips at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md
See frequently asked questions at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md
"""
import os
from options.test_options import TestOptions
from data import create_dataset
from models import create_model
from util.visualizer import save_images
from util import html
from PIL import Image
import numpy as np
import torch
from util.guidedfilter import GuidedFilter

if __name__ == '__main__':
    opt = TestOptions().parse()  # get test options
    # hard-code some parameters for test
    opt.num_threads = 0   # test code only supports num_threads = 1
    opt.batch_size = 1    # test code only supports batch_size = 1
    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.
    opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.
    opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.
    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
    model = create_model(opt)      # create a model given opt.model and other options
    model.setup(opt)               # regular setup: load and print networks; create schedulers
    # create a website
    web_dir = os.path.join(opt.results_dir, opt.name, '{}_{}'.format(opt.phase, opt.epoch))  # define the website directory
    if opt.load_iter > 0:  # load_iter is 0 by default
        web_dir = '{:s}_iter{:d}'.format(web_dir, opt.load_iter)
    print('creating web directory', web_dir)
    # webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.epoch))
    # test with eval mode. This only affects layers like batchnorm and dropout.
    # For [pix2pix]: we use batchnorm and dropout in the original pix2pix. You can experiment it with and without eval() mode.
    # For [CycleGAN]: It should not affect CycleGAN as CycleGAN uses instancenorm without dropout.
    normalize_coef = np.float32(2 ** (16))

    model.eval()
    for i, data in enumerate(dataset):
        model.set_input_train(data)  # unpack data from data loader
        model.test()           # run inference
        visuals = model.get_current_visuals()  # get image results
        img_path = model.get_image_paths()     # get image paths
        filename = os.path.basename(img_path[0])
        print('processing (%04d)-th image... %s' % (i, filename))

        inner = visuals['inner']
        inner = inner.cpu()
        inner = torch.squeeze(inner)
        inner = inner.numpy()
        inner = (inner + 1) / 2

        out = visuals['fake_B']
        out = out.cpu()
        out = torch.squeeze(out)
        out = out.numpy()
        out = (out+1)/2

        # out = GuidedFilter(inner, out, 32, 0).smooth.astype('float32')
        out = GuidedFilter(inner, out, 64, 0).smooth.astype('float32')

        out = out * (normalize_coef - 1)
        out = out.astype('uint16')
        out = Image.fromarray(out)
        out = out.convert('I;16')
        # out = out.resize(input_size)

        save_dirname = os.path.join('results','mahdi_pix2pix_unet_l1_basic','test_latest')
        if not os.path.exists(save_dirname):
            os.makedirs(save_dirname)
        out.save(os.path.join(save_dirname, filename))
        # save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)
    # webpage.save()  # save the HTML



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/train.py
================================================
"""General-purpose training script for image-to-image translation.

This script works for various models (with option '--model': e.g., pix2pix, cyclegan, colorization) and
different datasets (with option '--dataset_mode': e.g., aligned, unaligned, single, colorization).
You need to specify the dataset ('--dataroot'), experiment name ('--name'), and model ('--model').

It first creates model, dataset, and visualizer given the option.
It then does standard network training. During the training, it also visualize/save the images, print/save the loss plot, and save models.
The script supports continue/resume training. Use '--continue_train' to resume your previous training.

Example:
    Train a CycleGAN model:
        python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan
    Train a pix2pix model:
        python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA

See options/base_options.py and options/train_options.py for more training options.
See training and test tips at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md
See frequently asked questions at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md
"""
import time
from options.train_options import TrainOptions
from data import create_dataset
from models import create_model
from util.visualizer import Visualizer

if __name__ == '__main__':
    opt = TrainOptions().parse()   # get training options
    # opt.serial_batches = True
    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
    dataset_size = len(dataset)    # get the number of images in the dataset.
    print('The number of training images = %d' % dataset_size)

    model = create_model(opt)      # create a model given opt.model and other options
    model.setup(opt)               # regular setup: load and print networks; create schedulers
    visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots

    for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>
        epoch_start_time = time.time()  # timer for entire epoch
        iter_data_time = time.time()    # timer for data loading per iteration
        epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch
        visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch
        model.update_learning_rate()    # update learning rates in the beginning of every epoch.
        for i, data in enumerate(dataset):  # inner loop within one epoch
            iter_start_time = time.time()  # timer for computation per iteration

            epoch_iter += opt.batch_size
            model.set_input_train(data)         # unpack data from dataset and apply preprocessing
            model.optimize_parameters()   # calculate loss functions, get gradients, update network weights

            if epoch_iter == dataset_size:
                model.compute_visuals()
                visualizer.display_current_results(model.get_current_visuals(), epoch, True)

            if epoch_iter % 500 == 0 or epoch_iter == dataset_size:    # print training losses and save logging information to the disk
                losses = model.get_current_losses()
                t_data = iter_start_time - iter_data_time
                t_comp = (time.time() - iter_start_time) / opt.batch_size
                visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)


        if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs
            print('saving the model at the end of epoch %d' % epoch)
            model.save_networks('latest')
            model.save_networks(epoch)

        print('End of epoch %d / %d \t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/.DS_Store
================================================
[Non-text file]


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/data/__init__.py
================================================
"""This package includes all the modules related to data loading and preprocessing

 To add a custom dataset class called 'dummy', you need to add a file called 'dummy_dataset.py' and define a subclass 'DummyDataset' inherited from BaseDataset.
 You need to implement four functions:
    -- <__init__>:                      initialize the class, first call BaseDataset.__init__(self, opt).
    -- <__len__>:                       return the size of dataset.
    -- <__getitem__>:                   get a data point from data loader.
    -- <modify_commandline_options>:    (optionally) add dataset-specific options and set default options.

Now you can use the dataset class by specifying flag '--dataset_mode dummy'.
See our template dataset class 'template_dataset.py' for more details.
"""
import importlib
import torch.utils.data
from pix2pix.data.base_dataset import BaseDataset


def find_dataset_using_name(dataset_name):
    """Import the module "data/[dataset_name]_dataset.py".

    In the file, the class called DatasetNameDataset() will
    be instantiated. It has to be a subclass of BaseDataset,
    and it is case-insensitive.
    """
    dataset_filename = "pix2pix.data." + dataset_name + "_dataset"
    datasetlib = importlib.import_module(dataset_filename)

    dataset = None
    target_dataset_name = dataset_name.replace('_', '') + 'dataset'
    for name, cls in datasetlib.__dict__.items():
        if name.lower() == target_dataset_name.lower() \
           and issubclass(cls, BaseDataset):
            dataset = cls

    if dataset is None:
        raise NotImplementedError("In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase." % (dataset_filename, target_dataset_name))

    return dataset


def get_option_setter(dataset_name):
    """Return the static method <modify_commandline_options> of the dataset class."""
    dataset_class = find_dataset_using_name(dataset_name)
    return dataset_class.modify_commandline_options


def create_dataset(opt):
    """Create a dataset given the option.

    This function wraps the class CustomDatasetDataLoader.
        This is the main interface between this package and 'train.py'/'test.py'

    Example:
        >>> from data import create_dataset
        >>> dataset = create_dataset(opt)
    """
    data_loader = CustomDatasetDataLoader(opt)
    dataset = data_loader.load_data()
    return dataset


class CustomDatasetDataLoader():
    """Wrapper class of Dataset class that performs multi-threaded data loading"""

    def __init__(self, opt):
        """Initialize this class

        Step 1: create a dataset instance given the name [dataset_mode]
        Step 2: create a multi-threaded data loader.
        """
        self.opt = opt
        dataset_class = find_dataset_using_name(opt.dataset_mode)
        self.dataset = dataset_class(opt)
        print("dataset [%s] was created" % type(self.dataset).__name__)
        self.dataloader = torch.utils.data.DataLoader(
            self.dataset,
            batch_size=opt.batch_size,
            shuffle=not opt.serial_batches,
            num_workers=int(opt.num_threads))

    def load_data(self):
        return self

    def __len__(self):
        """Return the number of data in the dataset"""
        return min(len(self.dataset), self.opt.max_dataset_size)

    def __iter__(self):
        """Return a batch of data"""
        for i, data in enumerate(self.dataloader):
            if i * self.opt.batch_size >= self.opt.max_dataset_size:
                break
            yield data



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/data/base_dataset.py
================================================
"""This module implements an abstract base class (ABC) 'BaseDataset' for datasets.

It also includes common transformation functions (e.g., get_transform, __scale_width), which can be later used in subclasses.
"""
import random
import numpy as np
import torch.utils.data as data
from PIL import Image
import torchvision.transforms as transforms
from abc import ABC, abstractmethod


class BaseDataset(data.Dataset, ABC):
    """This class is an abstract base class (ABC) for datasets.

    To create a subclass, you need to implement the following four functions:
    -- <__init__>:                      initialize the class, first call BaseDataset.__init__(self, opt).
    -- <__len__>:                       return the size of dataset.
    -- <__getitem__>:                   get a data point.
    -- <modify_commandline_options>:    (optionally) add dataset-specific options and set default options.
    """

    def __init__(self, opt):
        """Initialize the class; save the options in the class

        Parameters:
            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions
        """
        self.opt = opt
        self.root = opt.dataroot

    @staticmethod
    def modify_commandline_options(parser, is_train):
        """Add new dataset-specific options, and rewrite default values for existing options.

        Parameters:
            parser          -- original option parser
            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.

        Returns:
            the modified parser.
        """
        return parser

    @abstractmethod
    def __len__(self):
        """Return the total number of images in the dataset."""
        return 0

    @abstractmethod
    def __getitem__(self, index):
        """Return a data point and its metadata information.

        Parameters:
            index - - a random integer for data indexing

        Returns:
            a dictionary of data with their names. It ususally contains the data itself and its metadata information.
        """
        pass


def get_params(opt, size):
    w, h = size
    new_h = h
    new_w = w
    if opt.preprocess == 'resize_and_crop':
        new_h = new_w = opt.load_size
    elif opt.preprocess == 'scale_width_and_crop':
        new_w = opt.load_size
        new_h = opt.load_size * h // w

    x = random.randint(0, np.maximum(0, new_w - opt.crop_size))
    y = random.randint(0, np.maximum(0, new_h - opt.crop_size))

    flip = random.random() > 0.5

    return {'crop_pos': (x, y), 'flip': flip}


def get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True):
    transform_list = []
    if grayscale:
        transform_list.append(transforms.Grayscale(1))
    if 'resize' in opt.preprocess:
        osize = [opt.load_size, opt.load_size]
        transform_list.append(transforms.Resize(osize, method))
    elif 'scale_width' in opt.preprocess:
        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, opt.crop_size, method)))

    if 'crop' in opt.preprocess:
        if params is None:
            transform_list.append(transforms.RandomCrop(opt.crop_size))
        else:
            transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))

    if opt.preprocess == 'none':
        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))

    if not opt.no_flip:
        if params is None:
            transform_list.append(transforms.RandomHorizontalFlip())
        elif params['flip']:
            transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))

    if convert:
        transform_list += [transforms.ToTensor()]
        if grayscale:
            transform_list += [transforms.Normalize((0.5,), (0.5,))]
        else:
            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)


def __make_power_2(img, base, method=Image.BICUBIC):
    ow, oh = img.size
    h = int(round(oh / base) * base)
    w = int(round(ow / base) * base)
    if h == oh and w == ow:
        return img

    __print_size_warning(ow, oh, w, h)
    return img.resize((w, h), method)


def __scale_width(img, target_size, crop_size, method=Image.BICUBIC):
    ow, oh = img.size
    if ow == target_size and oh >= crop_size:
        return img
    w = target_size
    h = int(max(target_size * oh / ow, crop_size))
    return img.resize((w, h), method)


def __crop(img, pos, size):
    ow, oh = img.size
    x1, y1 = pos
    tw = th = size
    if (ow > tw or oh > th):
        return img.crop((x1, y1, x1 + tw, y1 + th))
    return img


def __flip(img, flip):
    if flip:
        return img.transpose(Image.FLIP_LEFT_RIGHT)
    return img


def __print_size_warning(ow, oh, w, h):
    """Print warning information about image size(only print once)"""
    if not hasattr(__print_size_warning, 'has_printed'):
        print("The image size needs to be a multiple of 4. "
              "The loaded image size was (%d, %d), so it was adjusted to "
              "(%d, %d). This adjustment will be done to all images "
              "whose sizes are not multiples of 4" % (ow, oh, w, h))
        __print_size_warning.has_printed = True



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/data/depthmerge_dataset.py
================================================
from pix2pix.data.base_dataset import BaseDataset
from pix2pix.data.image_folder import make_dataset
from pix2pix.util.guidedfilter import GuidedFilter

import numpy as np
import os
import torch
from PIL import Image


def normalize(img):
    img = img * 2
    img = img - 1
    return img


def normalize01(img):
    return (img - torch.min(img)) / (torch.max(img)-torch.min(img))


class DepthMergeDataset(BaseDataset):
    def __init__(self, opt):
        BaseDataset.__init__(self, opt)
        self.dir_outer = os.path.join(opt.dataroot, opt.phase, 'outer')
        self.dir_inner = os.path.join(opt.dataroot, opt.phase, 'inner')
        self.dir_gtfake = os.path.join(opt.dataroot, opt.phase, 'gtfake')

        self.outer_paths = sorted(make_dataset(self.dir_outer, opt.max_dataset_size))
        self.inner_paths = sorted(make_dataset(self.dir_inner, opt.max_dataset_size))
        self.gtfake_paths = sorted(make_dataset(self.dir_gtfake, opt.max_dataset_size))

        self.dataset_size = len(self.outer_paths)

        if opt.phase == 'train':
            self.isTrain = True
        else:
            self.isTrain = False

    def __getitem__(self, index):
        normalize_coef = np.float32(2 ** 16)

        data_outer = Image.open(self.outer_paths[index % self.dataset_size])  # needs to be a tensor
        data_outer = np.array(data_outer, dtype=np.float32)
        data_outer = data_outer / normalize_coef

        data_inner = Image.open(self.inner_paths[index % self.dataset_size])  # needs to be a tensor
        data_inner = np.array(data_inner, dtype=np.float32)
        data_inner = data_inner / normalize_coef

        if self.isTrain:
            data_gtfake = Image.open(self.gtfake_paths[index % self.dataset_size])  # needs to be a tensor
            data_gtfake = np.array(data_gtfake, dtype=np.float32)
            data_gtfake = data_gtfake / normalize_coef

            data_inner = GuidedFilter(data_gtfake, data_inner, 64, 0.00000001).smooth.astype('float32')
            data_outer = GuidedFilter(data_outer, data_gtfake, 64, 0.00000001).smooth.astype('float32')

        data_outer = torch.from_numpy(data_outer)
        data_outer = torch.unsqueeze(data_outer, 0)
        data_outer = normalize01(data_outer)
        data_outer = normalize(data_outer)

        data_inner = torch.from_numpy(data_inner)
        data_inner = torch.unsqueeze(data_inner, 0)
        data_inner = normalize01(data_inner)
        data_inner = normalize(data_inner)

        if self.isTrain:
            data_gtfake = torch.from_numpy(data_gtfake)
            data_gtfake = torch.unsqueeze(data_gtfake, 0)
            data_gtfake = normalize01(data_gtfake)
            data_gtfake = normalize(data_gtfake)

        image_path = self.outer_paths[index % self.dataset_size]
        if self.isTrain:
            return {'data_inner': data_inner, 'data_outer': data_outer,
                    'data_gtfake': data_gtfake, 'image_path': image_path}
        else:
            return {'data_inner': data_inner, 'data_outer': data_outer, 'image_path': image_path}

    def __len__(self):
        """Return the total number of images."""
        return self.dataset_size



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/data/image_folder.py
================================================
"""A modified image folder class

We modify the official PyTorch image folder (https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py)
so that this class can load images from both current directory and its subdirectories.
"""

import torch.utils.data as data

from PIL import Image
import os

IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG',
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',
    '.tif', '.TIF', '.tiff', '.TIFF',
]


def is_image_file(filename):
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)


def make_dataset(dir, max_dataset_size=float("inf")):
    images = []
    assert os.path.isdir(dir), '%s is not a valid directory' % dir

    for root, _, fnames in sorted(os.walk(dir)):
        for fname in fnames:
            if is_image_file(fname):
                path = os.path.join(root, fname)
                images.append(path)
    return images[:min(max_dataset_size, len(images))]


def default_loader(path):
    return Image.open(path).convert('RGB')


class ImageFolder(data.Dataset):

    def __init__(self, root, transform=None, return_paths=False,
                 loader=default_loader):
        imgs = make_dataset(root)
        if len(imgs) == 0:
            raise(RuntimeError("Found 0 images in: " + root + "\n"
                               "Supported image extensions are: " + ",".join(IMG_EXTENSIONS)))

        self.root = root
        self.imgs = imgs
        self.transform = transform
        self.return_paths = return_paths
        self.loader = loader

    def __getitem__(self, index):
        path = self.imgs[index]
        img = self.loader(path)
        if self.transform is not None:
            img = self.transform(img)
        if self.return_paths:
            return img, path
        else:
            return img

    def __len__(self):
        return len(self.imgs)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/models/__init__.py
================================================
"""This package contains modules related to objective functions, optimizations, and network architectures.

To add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.
You need to implement the following five functions:
    -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).
    -- <set_input>:                     unpack data from dataset and apply preprocessing.
    -- <forward>:                       produce intermediate results.
    -- <optimize_parameters>:           calculate loss, gradients, and update network weights.
    -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.

In the function <__init__>, you need to define four lists:
    -- self.loss_names (str list):          specify the training losses that you want to plot and save.
    -- self.model_names (str list):         define networks used in our training.
    -- self.visual_names (str list):        specify the images that you want to display and save.
    -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an usage.

Now you can use the model class by specifying flag '--model dummy'.
See our template model class 'template_model.py' for more details.
"""

import importlib
from pix2pix.models.base_model import BaseModel


def find_model_using_name(model_name):
    """Import the module "models/[model_name]_model.py".

    In the file, the class called DatasetNameModel() will
    be instantiated. It has to be a subclass of BaseModel,
    and it is case-insensitive.
    """
    model_filename = "pix2pix.models." + model_name + "_model"
    modellib = importlib.import_module(model_filename)
    model = None
    target_model_name = model_name.replace('_', '') + 'model'
    for name, cls in modellib.__dict__.items():
        if name.lower() == target_model_name.lower() \
           and issubclass(cls, BaseModel):
            model = cls

    if model is None:
        print("In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase." % (model_filename, target_model_name))
        exit(0)

    return model


def get_option_setter(model_name):
    """Return the static method <modify_commandline_options> of the model class."""
    model_class = find_model_using_name(model_name)
    return model_class.modify_commandline_options


def create_model(opt):
    """Create a model given the option.

    This function warps the class CustomDatasetDataLoader.
    This is the main interface between this package and 'train.py'/'test.py'

    Example:
        >>> from models import create_model
        >>> model = create_model(opt)
    """
    model = find_model_using_name(opt.model)
    instance = model(opt)
    print("model [%s] was created" % type(instance).__name__)
    return instance



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/models/base_model.py
================================================
import os
import torch
from collections import OrderedDict
from abc import ABC, abstractmethod
from . import networks


class BaseModel(ABC):
    """This class is an abstract base class (ABC) for models.
    To create a subclass, you need to implement the following five functions:
        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).
        -- <set_input>:                     unpack data from dataset and apply preprocessing.
        -- <forward>:                       produce intermediate results.
        -- <optimize_parameters>:           calculate losses, gradients, and update network weights.
        -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.
    """

    def __init__(self, opt):
        """Initialize the BaseModel class.

        Parameters:
            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions

        When creating your custom class, you need to implement your own initialization.
        In this function, you should first call <BaseModel.__init__(self, opt)>
        Then, you need to define four lists:
            -- self.loss_names (str list):          specify the training losses that you want to plot and save.
            -- self.model_names (str list):         define networks used in our training.
            -- self.visual_names (str list):        specify the images that you want to display and save.
            -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.
        """
        self.opt = opt
        self.gpu_ids = opt.gpu_ids
        self.isTrain = opt.isTrain
        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')  # get device name: CPU or GPU
        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)  # save all the checkpoints to save_dir
        if opt.preprocess != 'scale_width':  # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.
            torch.backends.cudnn.benchmark = True
        self.loss_names = []
        self.model_names = []
        self.visual_names = []
        self.optimizers = []
        self.image_paths = []
        self.metric = 0  # used for learning rate policy 'plateau'

    @staticmethod
    def modify_commandline_options(parser, is_train):
        """Add new model-specific options, and rewrite default values for existing options.

        Parameters:
            parser          -- original option parser
            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.

        Returns:
            the modified parser.
        """
        return parser

    @abstractmethod
    def set_input(self, input):
        """Unpack input data from the dataloader and perform necessary pre-processing steps.

        Parameters:
            input (dict): includes the data itself and its metadata information.
        """
        pass

    @abstractmethod
    def forward(self):
        """Run forward pass; called by both functions <optimize_parameters> and <test>."""
        pass

    @abstractmethod
    def optimize_parameters(self):
        """Calculate losses, gradients, and update network weights; called in every training iteration"""
        pass

    def setup(self, opt):
        """Load and print networks; create schedulers

        Parameters:
            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions
        """
        if self.isTrain:
            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]
        if not self.isTrain or opt.continue_train:
            load_suffix = 'iter_%d' % opt.load_iter if opt.load_iter > 0 else opt.epoch
            self.load_networks(load_suffix)
        self.print_networks(opt.verbose)

    def eval(self):
        """Make models eval mode during test time"""
        for name in self.model_names:
            if isinstance(name, str):
                net = getattr(self, 'net' + name)
                net.eval()

    def test(self):
        """Forward function used in test time.

        This function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop
        It also calls <compute_visuals> to produce additional visualization results
        """
        with torch.no_grad():
            self.forward()
            self.compute_visuals()

    def compute_visuals(self):
        """Calculate additional output images for visdom and HTML visualization"""
        pass

    def get_image_paths(self):
        """ Return image paths that are used to load current data"""
        return self.image_paths

    def update_learning_rate(self):
        """Update learning rates for all the networks; called at the end of every epoch"""
        old_lr = self.optimizers[0].param_groups[0]['lr']
        for scheduler in self.schedulers:
            if self.opt.lr_policy == 'plateau':
                scheduler.step(self.metric)
            else:
                scheduler.step()

        lr = self.optimizers[0].param_groups[0]['lr']
        print('learning rate %.7f -> %.7f' % (old_lr, lr))

    def get_current_visuals(self):
        """Return visualization images. train.py will display these images with visdom, and save the images to a HTML"""
        visual_ret = OrderedDict()
        for name in self.visual_names:
            if isinstance(name, str):
                visual_ret[name] = getattr(self, name)
        return visual_ret

    def get_current_losses(self):
        """Return traning losses / errors. train.py will print out these errors on console, and save them to a file"""
        errors_ret = OrderedDict()
        for name in self.loss_names:
            if isinstance(name, str):
                errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number
        return errors_ret

    def save_networks(self, epoch):
        """Save all the networks to the disk.

        Parameters:
            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)
        """
        for name in self.model_names:
            if isinstance(name, str):
                save_filename = '%s_net_%s.pth' % (epoch, name)
                save_path = os.path.join(self.save_dir, save_filename)
                net = getattr(self, 'net' + name)

                if len(self.gpu_ids) > 0 and torch.cuda.is_available():
                    torch.save(net.module.cpu().state_dict(), save_path)
                    net.cuda(self.gpu_ids[0])
                else:
                    torch.save(net.cpu().state_dict(), save_path)

    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):
        """Fix InstanceNorm checkpoints incompatibility (prior to 0.4)"""
        key = keys[i]
        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer
            if module.__class__.__name__.startswith('InstanceNorm') and \
                    (key == 'running_mean' or key == 'running_var'):
                if getattr(module, key) is None:
                    state_dict.pop('.'.join(keys))
            if module.__class__.__name__.startswith('InstanceNorm') and \
               (key == 'num_batches_tracked'):
                state_dict.pop('.'.join(keys))
        else:
            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)

    def load_networks(self, epoch):
        """Load all the networks from the disk.

        Parameters:
            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)
        """
        for name in self.model_names:
            if isinstance(name, str):
                load_filename = '%s_net_%s.pth' % (epoch, name)
                load_path = os.path.join(self.save_dir, load_filename)
                net = getattr(self, 'net' + name)
                if isinstance(net, torch.nn.DataParallel):
                    net = net.module
                print('loading the model from %s' % load_path)
                # if you are using PyTorch newer than 0.4 (e.g., built from
                # GitHub source), you can remove str() on self.device
                state_dict = torch.load(load_path, map_location=str(self.device))
                if hasattr(state_dict, '_metadata'):
                    del state_dict._metadata

                # patch InstanceNorm checkpoints prior to 0.4
                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop
                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
                net.load_state_dict(state_dict)

    def print_networks(self, verbose):
        """Print the total number of parameters in the network and (if verbose) network architecture

        Parameters:
            verbose (bool) -- if verbose: print the network architecture
        """
        print('---------- Networks initialized -------------')
        for name in self.model_names:
            if isinstance(name, str):
                net = getattr(self, 'net' + name)
                num_params = 0
                for param in net.parameters():
                    num_params += param.numel()
                if verbose:
                    print(net)
                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))
        print('-----------------------------------------------')

    def set_requires_grad(self, nets, requires_grad=False):
        """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
        Parameters:
            nets (network list)   -- a list of networks
            requires_grad (bool)  -- whether the networks require gradients or not
        """
        if not isinstance(nets, list):
            nets = [nets]
        for net in nets:
            if net is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/models/base_model_hg.py
================================================
import os
import torch

class BaseModelHG():
    def name(self):
        return 'BaseModel'

    def initialize(self, opt):
        self.opt = opt
        self.gpu_ids = opt.gpu_ids
        self.isTrain = opt.isTrain
        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor
        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)

    def set_input(self, input):
        self.input = input

    def forward(self):
        pass

    # used in test time, no backprop
    def test(self):
        pass

    def get_image_paths(self):
        pass

    def optimize_parameters(self):
        pass

    def get_current_visuals(self):
        return self.input

    def get_current_errors(self):
        return {}

    def save(self, label):
        pass

    # helper saving function that can be used by subclasses
    def save_network(self, network, network_label, epoch_label, gpu_ids):
        save_filename = '_%s_net_%s.pth' % (epoch_label, network_label)
        save_path = os.path.join(self.save_dir, save_filename)
        torch.save(network.cpu().state_dict(), save_path)
        if len(gpu_ids) and torch.cuda.is_available():
            network.cuda(device_id=gpu_ids[0])

    # helper loading function that can be used by subclasses
    def load_network(self, network, network_label, epoch_label):
        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)
        save_path = os.path.join(self.save_dir, save_filename)
        print(save_path)
        model = torch.load(save_path)
        return model
        # network.load_state_dict(torch.load(save_path))

    def update_learning_rate():
        pass



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/models/networks.py
================================================
import torch
import torch.nn as nn
from torch.nn import init
import functools
from torch.optim import lr_scheduler


###############################################################################
# Helper Functions
###############################################################################


class Identity(nn.Module):
    def forward(self, x):
        return x


def get_norm_layer(norm_type='instance'):
    """Return a normalization layer

    Parameters:
        norm_type (str) -- the name of the normalization layer: batch | instance | none

    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).
    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.
    """
    if norm_type == 'batch':
        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)
    elif norm_type == 'instance':
        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)
    elif norm_type == 'none':
        def norm_layer(x): return Identity()
    else:
        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)
    return norm_layer


def get_scheduler(optimizer, opt):
    """Return a learning rate scheduler

    Parameters:
        optimizer          -- the optimizer of the network
        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　
                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine

    For 'linear', we keep the same learning rate for the first <opt.n_epochs> epochs
    and linearly decay the rate to zero over the next <opt.n_epochs_decay> epochs.
    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.
    See https://pytorch.org/docs/stable/optim.html for more details.
    """
    if opt.lr_policy == 'linear':
        def lambda_rule(epoch):
            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.n_epochs) / float(opt.n_epochs_decay + 1)
            return lr_l
        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)
    elif opt.lr_policy == 'step':
        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)
    elif opt.lr_policy == 'plateau':
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)
    elif opt.lr_policy == 'cosine':
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.n_epochs, eta_min=0)
    else:
        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)
    return scheduler


def init_weights(net, init_type='normal', init_gain=0.02):
    """Initialize network weights.

    Parameters:
        net (network)   -- network to be initialized
        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal
        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.

    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might
    work better for some applications. Feel free to try yourself.
    """
    def init_func(m):  # define the initialization function
        classname = m.__class__.__name__
        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):
            if init_type == 'normal':
                init.normal_(m.weight.data, 0.0, init_gain)
            elif init_type == 'xavier':
                init.xavier_normal_(m.weight.data, gain=init_gain)
            elif init_type == 'kaiming':
                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
            elif init_type == 'orthogonal':
                init.orthogonal_(m.weight.data, gain=init_gain)
            else:
                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)
            if hasattr(m, 'bias') and m.bias is not None:
                init.constant_(m.bias.data, 0.0)
        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.
            init.normal_(m.weight.data, 1.0, init_gain)
            init.constant_(m.bias.data, 0.0)

    print('initialize network with %s' % init_type)
    net.apply(init_func)  # apply the initialization function <init_func>


def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):
    """Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights
    Parameters:
        net (network)      -- the network to be initialized
        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal
        gain (float)       -- scaling factor for normal, xavier and orthogonal.
        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2

    Return an initialized network.
    """
    if len(gpu_ids) > 0:
        assert(torch.cuda.is_available())
        net.to(gpu_ids[0])
        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs
    init_weights(net, init_type, init_gain=init_gain)
    return net


def define_G(input_nc, output_nc, ngf, netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):
    """Create a generator

    Parameters:
        input_nc (int) -- the number of channels in input images
        output_nc (int) -- the number of channels in output images
        ngf (int) -- the number of filters in the last conv layer
        netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128
        norm (str) -- the name of normalization layers used in the network: batch | instance | none
        use_dropout (bool) -- if use dropout layers.
        init_type (str)    -- the name of our initialization method.
        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.
        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2

    Returns a generator

    Our current implementation provides two types of generators:
        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)
        The original U-Net paper: https://arxiv.org/abs/1505.04597

        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)
        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.
        We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).


    The generator has been initialized by <init_net>. It uses RELU for non-linearity.
    """
    net = None
    norm_layer = get_norm_layer(norm_type=norm)

    if netG == 'resnet_9blocks':
        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)
    elif netG == 'resnet_6blocks':
        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)
    elif netG == 'resnet_12blocks':
        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=12)
    elif netG == 'unet_128':
        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    elif netG == 'unet_256':
        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    elif netG == 'unet_672':
        net = UnetGenerator(input_nc, output_nc, 5, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    elif netG == 'unet_960':
        net = UnetGenerator(input_nc, output_nc, 6, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    elif netG == 'unet_1024':
        net = UnetGenerator(input_nc, output_nc, 10, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    else:
        raise NotImplementedError('Generator model name [%s] is not recognized' % netG)
    return init_net(net, init_type, init_gain, gpu_ids)


def define_D(input_nc, ndf, netD, n_layers_D=3, norm='batch', init_type='normal', init_gain=0.02, gpu_ids=[]):
    """Create a discriminator

    Parameters:
        input_nc (int)     -- the number of channels in input images
        ndf (int)          -- the number of filters in the first conv layer
        netD (str)         -- the architecture's name: basic | n_layers | pixel
        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'
        norm (str)         -- the type of normalization layers used in the network.
        init_type (str)    -- the name of the initialization method.
        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.
        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2

    Returns a discriminator

    Our current implementation provides three types of discriminators:
        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.
        It can classify whether 70×70 overlapping patches are real or fake.
        Such a patch-level discriminator architecture has fewer parameters
        than a full-image discriminator and can work on arbitrarily-sized images
        in a fully convolutional fashion.

        [n_layers]: With this mode, you can specify the number of conv layers in the discriminator
        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)

        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.
        It encourages greater color diversity but has no effect on spatial statistics.

    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.
    """
    net = None
    norm_layer = get_norm_layer(norm_type=norm)

    if netD == 'basic':  # default PatchGAN classifier
        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)
    elif netD == 'n_layers':  # more options
        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)
    elif netD == 'pixel':     # classify if each pixel is real or fake
        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)
    else:
        raise NotImplementedError('Discriminator model name [%s] is not recognized' % netD)
    return init_net(net, init_type, init_gain, gpu_ids)


##############################################################################
# Classes
##############################################################################
class GANLoss(nn.Module):
    """Define different GAN objectives.

    The GANLoss class abstracts away the need to create the target label tensor
    that has the same size as the input.
    """

    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):
        """ Initialize the GANLoss class.

        Parameters:
            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.
            target_real_label (bool) - - label for a real image
            target_fake_label (bool) - - label of a fake image

        Note: Do not use sigmoid as the last layer of Discriminator.
        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.
        """
        super(GANLoss, self).__init__()
        self.register_buffer('real_label', torch.tensor(target_real_label))
        self.register_buffer('fake_label', torch.tensor(target_fake_label))
        self.gan_mode = gan_mode
        if gan_mode == 'lsgan':
            self.loss = nn.MSELoss()
        elif gan_mode == 'vanilla':
            self.loss = nn.BCEWithLogitsLoss()
        elif gan_mode in ['wgangp']:
            self.loss = None
        else:
            raise NotImplementedError('gan mode %s not implemented' % gan_mode)

    def get_target_tensor(self, prediction, target_is_real):
        """Create label tensors with the same size as the input.

        Parameters:
            prediction (tensor) - - tpyically the prediction from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images

        Returns:
            A label tensor filled with ground truth label, and with the size of the input
        """

        if target_is_real:
            target_tensor = self.real_label
        else:
            target_tensor = self.fake_label
        return target_tensor.expand_as(prediction)

    def __call__(self, prediction, target_is_real):
        """Calculate loss given Discriminator's output and grount truth labels.

        Parameters:
            prediction (tensor) - - tpyically the prediction output from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images

        Returns:
            the calculated loss.
        """
        if self.gan_mode in ['lsgan', 'vanilla']:
            target_tensor = self.get_target_tensor(prediction, target_is_real)
            loss = self.loss(prediction, target_tensor)
        elif self.gan_mode == 'wgangp':
            if target_is_real:
                loss = -prediction.mean()
            else:
                loss = prediction.mean()
        return loss


def cal_gradient_penalty(netD, real_data, fake_data, device, type='mixed', constant=1.0, lambda_gp=10.0):
    """Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028

    Arguments:
        netD (network)              -- discriminator network
        real_data (tensor array)    -- real images
        fake_data (tensor array)    -- generated images from the generator
        device (str)                -- GPU / CPU: from torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')
        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].
        constant (float)            -- the constant used in formula ( ||gradient||_2 - constant)^2
        lambda_gp (float)           -- weight for this loss

    Returns the gradient penalty loss
    """
    if lambda_gp > 0.0:
        if type == 'real':   # either use real images, fake images, or a linear interpolation of two.
            interpolatesv = real_data
        elif type == 'fake':
            interpolatesv = fake_data
        elif type == 'mixed':
            alpha = torch.rand(real_data.shape[0], 1, device=device)
            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)
            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)
        else:
            raise NotImplementedError('{} not implemented'.format(type))
        interpolatesv.requires_grad_(True)
        disc_interpolates = netD(interpolatesv)
        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,
                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),
                                        create_graph=True, retain_graph=True, only_inputs=True)
        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data
        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps
        return gradient_penalty, gradients
    else:
        return 0.0, None


class ResnetGenerator(nn.Module):
    """Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.

    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)
    """

    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):
        """Construct a Resnet-based generator

        Parameters:
            input_nc (int)      -- the number of channels in input images
            output_nc (int)     -- the number of channels in output images
            ngf (int)           -- the number of filters in the last conv layer
            norm_layer          -- normalization layer
            use_dropout (bool)  -- if use dropout layers
            n_blocks (int)      -- the number of ResNet blocks
            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero
        """
        assert(n_blocks >= 0)
        super(ResnetGenerator, self).__init__()
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        model = [nn.ReflectionPad2d(3),
                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),
                 norm_layer(ngf),
                 nn.ReLU(True)]

        n_downsampling = 2
        for i in range(n_downsampling):  # add downsampling layers
            mult = 2 ** i
            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),
                      norm_layer(ngf * mult * 2),
                      nn.ReLU(True)]

        mult = 2 ** n_downsampling
        for i in range(n_blocks):       # add ResNet blocks

            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]

        for i in range(n_downsampling):  # add upsampling layers
            mult = 2 ** (n_downsampling - i)
            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),
                                         kernel_size=3, stride=2,
                                         padding=1, output_padding=1,
                                         bias=use_bias),
                      norm_layer(int(ngf * mult / 2)),
                      nn.ReLU(True)]
        model += [nn.ReflectionPad2d(3)]
        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]
        model += [nn.Tanh()]

        self.model = nn.Sequential(*model)

    def forward(self, input):
        """Standard forward"""
        return self.model(input)


class ResnetBlock(nn.Module):
    """Define a Resnet block"""

    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):
        """Initialize the Resnet block

        A resnet block is a conv block with skip connections
        We construct a conv block with build_conv_block function,
        and implement skip connections in <forward> function.
        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf
        """
        super(ResnetBlock, self).__init__()
        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)

    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):
        """Construct a convolutional block.

        Parameters:
            dim (int)           -- the number of channels in the conv layer.
            padding_type (str)  -- the name of padding layer: reflect | replicate | zero
            norm_layer          -- normalization layer
            use_dropout (bool)  -- if use dropout layers.
            use_bias (bool)     -- if the conv layer uses bias or not

        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))
        """
        conv_block = []
        p = 0
        if padding_type == 'reflect':
            conv_block += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            conv_block += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]
        if use_dropout:
            conv_block += [nn.Dropout(0.5)]

        p = 0
        if padding_type == 'reflect':
            conv_block += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            conv_block += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)
        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]

        return nn.Sequential(*conv_block)

    def forward(self, x):
        """Forward function (with skip connections)"""
        out = x + self.conv_block(x)  # add skip connections
        return out


class UnetGenerator(nn.Module):
    """Create a Unet-based generator"""

    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):
        """Construct a Unet generator
        Parameters:
            input_nc (int)  -- the number of channels in input images
            output_nc (int) -- the number of channels in output images
            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,
                                image of size 128x128 will become of size 1x1 # at the bottleneck
            ngf (int)       -- the number of filters in the last conv layer
            norm_layer      -- normalization layer

        We construct the U-Net from the innermost layer to the outermost layer.
        It is a recursive process.
        """
        super(UnetGenerator, self).__init__()
        # construct unet structure
        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer
        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters
            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)
        # gradually reduce the number of filters from ngf * 8 to ngf
        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)
        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)
        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)
        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer

    def forward(self, input):
        """Standard forward"""
        return self.model(input)


class UnetSkipConnectionBlock(nn.Module):
    """Defines the Unet submodule with skip connection.
        X -------------------identity----------------------
        |-- downsampling -- |submodule| -- upsampling --|
    """

    def __init__(self, outer_nc, inner_nc, input_nc=None,
                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):
        """Construct a Unet submodule with skip connections.

        Parameters:
            outer_nc (int) -- the number of filters in the outer conv layer
            inner_nc (int) -- the number of filters in the inner conv layer
            input_nc (int) -- the number of channels in input images/features
            submodule (UnetSkipConnectionBlock) -- previously defined submodules
            outermost (bool)    -- if this module is the outermost module
            innermost (bool)    -- if this module is the innermost module
            norm_layer          -- normalization layer
            use_dropout (bool)  -- if use dropout layers.
        """
        super(UnetSkipConnectionBlock, self).__init__()
        self.outermost = outermost
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d
        if input_nc is None:
            input_nc = outer_nc
        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,
                             stride=2, padding=1, bias=use_bias)
        downrelu = nn.LeakyReLU(0.2, True)
        downnorm = norm_layer(inner_nc)
        uprelu = nn.ReLU(True)
        upnorm = norm_layer(outer_nc)

        if outermost:
            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,
                                        kernel_size=4, stride=2,
                                        padding=1)
            down = [downconv]
            up = [uprelu, upconv, nn.Tanh()]
            model = down + [submodule] + up
        elif innermost:
            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,
                                        kernel_size=4, stride=2,
                                        padding=1, bias=use_bias)
            down = [downrelu, downconv]
            up = [uprelu, upconv, upnorm]
            model = down + up
        else:
            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,
                                        kernel_size=4, stride=2,
                                        padding=1, bias=use_bias)
            down = [downrelu, downconv, downnorm]
            up = [uprelu, upconv, upnorm]

            if use_dropout:
                model = down + [submodule] + up + [nn.Dropout(0.5)]
            else:
                model = down + [submodule] + up

        self.model = nn.Sequential(*model)

    def forward(self, x):
        if self.outermost:
            return self.model(x)
        else:   # add skip connections
            return torch.cat([x, self.model(x)], 1)


class NLayerDiscriminator(nn.Module):
    """Defines a PatchGAN discriminator"""

    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):
        """Construct a PatchGAN discriminator

        Parameters:
            input_nc (int)  -- the number of channels in input images
            ndf (int)       -- the number of filters in the last conv layer
            n_layers (int)  -- the number of conv layers in the discriminator
            norm_layer      -- normalization layer
        """
        super(NLayerDiscriminator, self).__init__()
        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        kw = 4
        padw = 1
        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]
        nf_mult = 1
        nf_mult_prev = 1
        for n in range(1, n_layers):  # gradually increase the number of filters
            nf_mult_prev = nf_mult
            nf_mult = min(2 ** n, 8)
            sequence += [
                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),
                norm_layer(ndf * nf_mult),
                nn.LeakyReLU(0.2, True)
            ]

        nf_mult_prev = nf_mult
        nf_mult = min(2 ** n_layers, 8)
        sequence += [
            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),
            norm_layer(ndf * nf_mult),
            nn.LeakyReLU(0.2, True)
        ]

        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map
        self.model = nn.Sequential(*sequence)

    def forward(self, input):
        """Standard forward."""
        return self.model(input)


class PixelDiscriminator(nn.Module):
    """Defines a 1x1 PatchGAN discriminator (pixelGAN)"""

    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):
        """Construct a 1x1 PatchGAN discriminator

        Parameters:
            input_nc (int)  -- the number of channels in input images
            ndf (int)       -- the number of filters in the last conv layer
            norm_layer      -- normalization layer
        """
        super(PixelDiscriminator, self).__init__()
        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        self.net = [
            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),
            norm_layer(ndf * 2),
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]

        self.net = nn.Sequential(*self.net)

    def forward(self, input):
        """Standard forward."""
        return self.net(input)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/models/pix2pix4depth_model.py
================================================
import torch
from .base_model import BaseModel
from . import networks


class Pix2Pix4DepthModel(BaseModel):
    """ This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.

    The model training requires '--dataset_mode aligned' dataset.
    By default, it uses a '--netG unet256' U-Net generator,
    a '--netD basic' discriminator (PatchGAN),
    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).

    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf
    """
    @staticmethod
    def modify_commandline_options(parser, is_train=True):
        """Add new dataset-specific options, and rewrite default values for existing options.

        Parameters:
            parser          -- original option parser
            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.

        Returns:
            the modified parser.

        For pix2pix, we do not use image buffer
        The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1
        By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.
        """
        # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)
        parser.set_defaults(input_nc=2,output_nc=1,norm='none', netG='unet_1024', dataset_mode='depthmerge')
        if is_train:
            parser.set_defaults(pool_size=0, gan_mode='vanilla',)
            parser.add_argument('--lambda_L1', type=float, default=1000, help='weight for L1 loss')
        return parser

    def __init__(self, opt):
        """Initialize the pix2pix class.

        Parameters:
            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions
        """
        BaseModel.__init__(self, opt)
        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>

        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']
        # self.loss_names = ['G_L1']

        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>
        if self.isTrain:
            self.visual_names = ['outer','inner', 'fake_B', 'real_B']
        else:
            self.visual_names = ['fake_B']

        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>
        if self.isTrain:
            self.model_names = ['G','D']
        else:  # during test time, only load G
            self.model_names = ['G']

        # define networks (both generator and discriminator)
        self.netG = networks.define_G(opt.input_nc, opt.output_nc, 64, 'unet_1024', 'none',
                                      False, 'normal', 0.02, self.gpu_ids)

        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc
            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,
                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)

        if self.isTrain:
            # define loss functions
            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)
            self.criterionL1 = torch.nn.L1Loss()
            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.
            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=1e-4, betas=(opt.beta1, 0.999))
            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=2e-06, betas=(opt.beta1, 0.999))
            self.optimizers.append(self.optimizer_G)
            self.optimizers.append(self.optimizer_D)

    def set_input_train(self, input):
        self.outer = input['data_outer'].to(self.device)
        self.outer = torch.nn.functional.interpolate(self.outer,(1024,1024),mode='bilinear',align_corners=False)

        self.inner = input['data_inner'].to(self.device)
        self.inner = torch.nn.functional.interpolate(self.inner,(1024,1024),mode='bilinear',align_corners=False)

        self.image_paths = input['image_path']

        if self.isTrain:
            self.gtfake = input['data_gtfake'].to(self.device)
            self.gtfake = torch.nn.functional.interpolate(self.gtfake, (1024, 1024), mode='bilinear', align_corners=False)
            self.real_B = self.gtfake

        self.real_A = torch.cat((self.outer, self.inner), 1)

    def set_input(self, outer, inner):
        inner = torch.from_numpy(inner).unsqueeze(0).unsqueeze(0)
        outer = torch.from_numpy(outer).unsqueeze(0).unsqueeze(0)

        inner = (inner - torch.min(inner))/(torch.max(inner)-torch.min(inner))
        outer = (outer - torch.min(outer))/(torch.max(outer)-torch.min(outer))

        inner = self.normalize(inner)
        outer = self.normalize(outer)

        self.real_A = torch.cat((outer, inner), 1).to(self.device)


    def normalize(self, input):
        input = input * 2
        input = input - 1
        return input

    def forward(self):
        """Run forward pass; called by both functions <optimize_parameters> and <test>."""
        self.fake_B = self.netG(self.real_A)  # G(A)

    def backward_D(self):
        """Calculate GAN loss for the discriminator"""
        # Fake; stop backprop to the generator by detaching fake_B
        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator
        pred_fake = self.netD(fake_AB.detach())
        self.loss_D_fake = self.criterionGAN(pred_fake, False)
        # Real
        real_AB = torch.cat((self.real_A, self.real_B), 1)
        pred_real = self.netD(real_AB)
        self.loss_D_real = self.criterionGAN(pred_real, True)
        # combine loss and calculate gradients
        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5
        self.loss_D.backward()

    def backward_G(self):
        """Calculate GAN and L1 loss for the generator"""
        # First, G(A) should fake the discriminator
        fake_AB = torch.cat((self.real_A, self.fake_B), 1)
        pred_fake = self.netD(fake_AB)
        self.loss_G_GAN = self.criterionGAN(pred_fake, True)
        # Second, G(A) = B
        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1
        # combine loss and calculate gradients
        self.loss_G = self.loss_G_L1 + self.loss_G_GAN
        self.loss_G.backward()

    def optimize_parameters(self):
        self.forward()                   # compute fake images: G(A)
        # update D
        self.set_requires_grad(self.netD, True)  # enable backprop for D
        self.optimizer_D.zero_grad()     # set D's gradients to zero
        self.backward_D()                # calculate gradients for D
        self.optimizer_D.step()          # update D's weights
        # update G
        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G
        self.optimizer_G.zero_grad()        # set G's gradients to zero
        self.backward_G()                   # calculate graidents for G
        self.optimizer_G.step()             # udpate G's weights


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/options/__init__.py
================================================
"""This package options includes option modules: training options, test options, and basic options (used in both training and test)."""



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/options/base_options.py
================================================
import argparse
import os
from pix2pix.util import util
import torch
import pix2pix.models
import pix2pix.data
import numpy as np

class BaseOptions():
    """This class defines options used during both training and test time.

    It also implements several helper functions such as parsing, printing, and saving the options.
    It also gathers additional options defined in <modify_commandline_options> functions in both dataset class and model class.
    """

    def __init__(self):
        """Reset the class; indicates the class hasn't been initailized"""
        self.initialized = False

    def initialize(self, parser):
        """Define the common options that are used in both training and test."""
        # basic parameters
        parser.add_argument('--dataroot', help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')
        parser.add_argument('--name', type=str, default='void', help='mahdi_unet_new, scaled_unet')
        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')
        parser.add_argument('--checkpoints_dir', type=str, default='./pix2pix/checkpoints', help='models are saved here')
        # model parameters
        parser.add_argument('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')
        parser.add_argument('--input_nc', type=int, default=2, help='# of input image channels: 3 for RGB and 1 for grayscale')
        parser.add_argument('--output_nc', type=int, default=1, help='# of output image channels: 3 for RGB and 1 for grayscale')
        parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')
        parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')
        parser.add_argument('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')
        parser.add_argument('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')
        parser.add_argument('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')
        parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')
        parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')
        parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')
        parser.add_argument('--no_dropout', action='store_true', help='no dropout for the generator')
        # dataset parameters
        parser.add_argument('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')
        parser.add_argument('--direction', type=str, default='AtoB', help='AtoB or BtoA')
        parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')
        parser.add_argument('--num_threads', default=4, type=int, help='# threads for loading data')
        parser.add_argument('--batch_size', type=int, default=1, help='input batch size')
        parser.add_argument('--load_size', type=int, default=672, help='scale images to this size')
        parser.add_argument('--crop_size', type=int, default=672, help='then crop to this size')
        parser.add_argument('--max_dataset_size', type=int, default=10000, help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')
        parser.add_argument('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')
        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')
        parser.add_argument('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')
        # additional parameters
        parser.add_argument('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')
        parser.add_argument('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')
        parser.add_argument('--verbose', action='store_true', help='if specified, print more debugging information')
        parser.add_argument('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')

        parser.add_argument('--data_dir', type=str, required=False,
                            help='input files directory images can be .png .jpg .tiff')
        parser.add_argument('--output_dir', type=str, required=False,
                            help='result dir. result depth will be png. vides are JMPG as avi')
        parser.add_argument('--savecrops', type=int, required=False)
        parser.add_argument('--savewholeest', type=int, required=False)
        parser.add_argument('--output_resolution', type=int, required=False,
                            help='0 for no restriction 1 for resize to input size')
        parser.add_argument('--net_receptive_field_size', type=int, required=False)
        parser.add_argument('--pix2pixsize', type=int, required=False)
        parser.add_argument('--generatevideo', type=int, required=False)
        parser.add_argument('--depthNet', type=int, required=False, help='0: midas 1:strurturedRL')
        parser.add_argument('--R0', action='store_true')
        parser.add_argument('--R20', action='store_true')
        parser.add_argument('--Final', action='store_true')
        parser.add_argument('--colorize_results', action='store_true')
        parser.add_argument('--max_res', type=float, default=np.inf)

        self.initialized = True
        return parser

    def gather_options(self):
        """Initialize our parser with basic options(only once).
        Add additional model-specific and dataset-specific options.
        These options are defined in the <modify_commandline_options> function
        in model and dataset classes.
        """
        if not self.initialized:  # check if it has been initialized
            parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
            parser = self.initialize(parser)

        # get the basic options
        opt, _ = parser.parse_known_args()

        # modify model-related parser options
        model_name = opt.model
        model_option_setter = pix2pix.models.get_option_setter(model_name)
        parser = model_option_setter(parser, self.isTrain)
        opt, _ = parser.parse_known_args()  # parse again with new defaults

        # modify dataset-related parser options
        dataset_name = opt.dataset_mode
        dataset_option_setter = pix2pix.data.get_option_setter(dataset_name)
        parser = dataset_option_setter(parser, self.isTrain)

        # save and return the parser
        self.parser = parser
        return parser.parse_args()

    def print_options(self, opt):
        """Print and save options

        It will print both current options and default values(if different).
        It will save options into a text file / [checkpoints_dir] / opt.txt
        """
        message = ''
        message += '----------------- Options ---------------\n'
        for k, v in sorted(vars(opt).items()):
            comment = ''
            default = self.parser.get_default(k)
            if v != default:
                comment = '\t[default: %s]' % str(default)
            message += '{:>25}: {:<30}{}\n'.format(str(k), str(v), comment)
        message += '----------------- End -------------------'
        print(message)

        # save to the disk
        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)
        util.mkdirs(expr_dir)
        file_name = os.path.join(expr_dir, '{}_opt.txt'.format(opt.phase))
        with open(file_name, 'wt') as opt_file:
            opt_file.write(message)
            opt_file.write('\n')

    def parse(self):
        """Parse our options, create checkpoints directory suffix, and set up gpu device."""
        opt = self.gather_options()
        opt.isTrain = self.isTrain   # train or test

        # process opt.suffix
        if opt.suffix:
            suffix = ('_' + opt.suffix.format(**vars(opt))) if opt.suffix != '' else ''
            opt.name = opt.name + suffix

        self.print_options(opt)

        # set gpu ids
        str_ids = opt.gpu_ids.split(',')
        opt.gpu_ids = []
        for str_id in str_ids:
            id = int(str_id)
            if id >= 0:
                opt.gpu_ids.append(id)
        if len(opt.gpu_ids) > 0:
            torch.cuda.set_device(opt.gpu_ids[0])

        self.opt = opt
        return self.opt



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/options/test_options.py
================================================
from .base_options import BaseOptions


class TestOptions(BaseOptions):
    """This class includes test options.

    It also includes shared options defined in BaseOptions.
    """

    def initialize(self, parser):
        parser = BaseOptions.initialize(self, parser)  # define shared options
        parser.add_argument('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')
        parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')
        # Dropout and Batchnorm has different behavioir during training and test.
        parser.add_argument('--eval', action='store_true', help='use eval mode during test time.')
        parser.add_argument('--num_test', type=int, default=50, help='how many test images to run')
        # rewrite devalue values
        parser.set_defaults(model='pix2pix4depth')
        # To avoid cropping, the load_size should be the same as crop_size
        parser.set_defaults(load_size=parser.get_default('crop_size'))
        self.isTrain = False
        return parser



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/options/train_options.py
================================================
from .base_options import BaseOptions


class TrainOptions(BaseOptions):
    """This class includes training options.

    It also includes shared options defined in BaseOptions.
    """

    def initialize(self, parser):
        parser = BaseOptions.initialize(self, parser)
        # visdom and HTML visualization parameters
        parser.add_argument('--display_freq', type=int, default=2500, help='frequency of showing training results on screen')
        parser.add_argument('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')
        parser.add_argument('--display_id', type=int, default=1, help='window id of the web display')
        parser.add_argument('--display_server', type=str, default="http://localhost", help='visdom server of the web display')
        parser.add_argument('--display_env', type=str, default='main', help='visdom display environment name (default is "main")')
        parser.add_argument('--display_port', type=int, default=8097, help='visdom port of the web display')
        parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')
        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')
        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')
        # network saving and loading parameters
        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')
        parser.add_argument('--save_epoch_freq', type=int, default=10, help='frequency of saving checkpoints at the end of epochs')
        parser.add_argument('--save_by_iter', action='store_true', help='whether saves model by iteration')
        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')
        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')
        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')
        # training parameters
        parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')
        parser.add_argument('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')
        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')
        parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')
        parser.add_argument('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')
        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')
        parser.add_argument('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')
        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')

        self.isTrain = True
        return parser



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/util/__init__.py
================================================
"""This package includes a miscellaneous collection of useful helper functions."""



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/util/get_data.py
================================================
from __future__ import print_function
import os
import tarfile
import requests
from warnings import warn
from zipfile import ZipFile
from bs4 import BeautifulSoup
from os.path import abspath, isdir, join, basename


class GetData(object):
    """A Python script for downloading CycleGAN or pix2pix datasets.

    Parameters:
        technique (str) -- One of: 'cyclegan' or 'pix2pix'.
        verbose (bool)  -- If True, print additional information.

    Examples:
        >>> from util.get_data import GetData
        >>> gd = GetData(technique='cyclegan')
        >>> new_data_path = gd.get(save_path='./datasets')  # options will be displayed.

    Alternatively, You can use bash scripts: 'scripts/download_pix2pix_model.sh'
    and 'scripts/download_cyclegan_model.sh'.
    """

    def __init__(self, technique='cyclegan', verbose=True):
        url_dict = {
            'pix2pix': 'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/',
            'cyclegan': 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets'
        }
        self.url = url_dict.get(technique.lower())
        self._verbose = verbose

    def _print(self, text):
        if self._verbose:
            print(text)

    @staticmethod
    def _get_options(r):
        soup = BeautifulSoup(r.text, 'lxml')
        options = [h.text for h in soup.find_all('a', href=True)
                   if h.text.endswith(('.zip', 'tar.gz'))]
        return options

    def _present_options(self):
        r = requests.get(self.url)
        options = self._get_options(r)
        print('Options:\n')
        for i, o in enumerate(options):
            print("{0}: {1}".format(i, o))
        choice = input("\nPlease enter the number of the "
                       "dataset above you wish to download:")
        return options[int(choice)]

    def _download_data(self, dataset_url, save_path):
        if not isdir(save_path):
            os.makedirs(save_path)

        base = basename(dataset_url)
        temp_save_path = join(save_path, base)

        with open(temp_save_path, "wb") as f:
            r = requests.get(dataset_url)
            f.write(r.content)

        if base.endswith('.tar.gz'):
            obj = tarfile.open(temp_save_path)
        elif base.endswith('.zip'):
            obj = ZipFile(temp_save_path, 'r')
        else:
            raise ValueError("Unknown File Type: {0}.".format(base))

        self._print("Unpacking Data...")
        obj.extractall(save_path)
        obj.close()
        os.remove(temp_save_path)

    def get(self, save_path, dataset=None):
        """

        Download a dataset.

        Parameters:
            save_path (str) -- A directory to save the data to.
            dataset (str)   -- (optional). A specific dataset to download.
                            Note: this must include the file extension.
                            If None, options will be presented for you
                            to choose from.

        Returns:
            save_path_full (str) -- the absolute path to the downloaded data.

        """
        if dataset is None:
            selected_dataset = self._present_options()
        else:
            selected_dataset = dataset

        save_path_full = join(save_path, selected_dataset.split('.')[0])

        if isdir(save_path_full):
            warn("\n'{0}' already exists. Voiding Download.".format(
                save_path_full))
        else:
            self._print('Downloading Data...')
            url = "{0}/{1}".format(self.url, selected_dataset)
            self._download_data(url, save_path=save_path)

        return abspath(save_path_full)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/util/guidedfilter.py
================================================
import numpy as np

class GuidedFilter():
    def __init__(self, source, reference, r=64, eps= 0.05**2):
        self.source = source;
        self.reference = reference;
        self.r = r
        self.eps = eps

        self.smooth = self.guidedfilter(self.source,self.reference,self.r,self.eps)

    def boxfilter(self,img, r):
        (rows, cols) = img.shape
        imDst = np.zeros_like(img)

        imCum = np.cumsum(img, 0)
        imDst[0 : r+1, :] = imCum[r : 2*r+1, :]
        imDst[r+1 : rows-r, :] = imCum[2*r+1 : rows, :] - imCum[0 : rows-2*r-1, :]
        imDst[rows-r: rows, :] = np.tile(imCum[rows-1, :], [r, 1]) - imCum[rows-2*r-1 : rows-r-1, :]

        imCum = np.cumsum(imDst, 1)
        imDst[:, 0 : r+1] = imCum[:, r : 2*r+1]
        imDst[:, r+1 : cols-r] = imCum[:, 2*r+1 : cols] - imCum[:, 0 : cols-2*r-1]
        imDst[:, cols-r: cols] = np.tile(imCum[:, cols-1], [r, 1]).T - imCum[:, cols-2*r-1 : cols-r-1]

        return imDst

    def guidedfilter(self,I, p, r, eps):
        (rows, cols) = I.shape
        N = self.boxfilter(np.ones([rows, cols]), r)

        meanI = self.boxfilter(I, r) / N
        meanP = self.boxfilter(p, r) / N
        meanIp = self.boxfilter(I * p, r) / N
        covIp = meanIp - meanI * meanP

        meanII = self.boxfilter(I * I, r) / N
        varI = meanII - meanI * meanI

        a = covIp / (varI + eps)
        b = meanP - a * meanI

        meanA = self.boxfilter(a, r) / N
        meanB = self.boxfilter(b, r) / N

        q = meanA * I + meanB
        return q


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/util/html.py
================================================
import dominate
from dominate.tags import meta, h3, table, tr, td, p, a, img, br
import os


class HTML:
    """This HTML class allows us to save images and write texts into a single HTML file.

     It consists of functions such as <add_header> (add a text header to the HTML file),
     <add_images> (add a row of images to the HTML file), and <save> (save the HTML to the disk).
     It is based on Python library 'dominate', a Python library for creating and manipulating HTML documents using a DOM API.
    """

    def __init__(self, web_dir, title, refresh=0):
        """Initialize the HTML classes

        Parameters:
            web_dir (str) -- a directory that stores the webpage. HTML file will be created at <web_dir>/index.html; images will be saved at <web_dir/images/
            title (str)   -- the webpage name
            refresh (int) -- how often the website refresh itself; if 0; no refreshing
        """
        self.title = title
        self.web_dir = web_dir
        self.img_dir = os.path.join(self.web_dir, 'images')
        if not os.path.exists(self.web_dir):
            os.makedirs(self.web_dir)
        if not os.path.exists(self.img_dir):
            os.makedirs(self.img_dir)

        self.doc = dominate.document(title=title)
        if refresh > 0:
            with self.doc.head:
                meta(http_equiv="refresh", content=str(refresh))

    def get_image_dir(self):
        """Return the directory that stores images"""
        return self.img_dir

    def add_header(self, text):
        """Insert a header to the HTML file

        Parameters:
            text (str) -- the header text
        """
        with self.doc:
            h3(text)

    def add_images(self, ims, txts, links, width=400):
        """add images to the HTML file

        Parameters:
            ims (str list)   -- a list of image paths
            txts (str list)  -- a list of image names shown on the website
            links (str list) --  a list of hyperref links; when you click an image, it will redirect you to a new page
        """
        self.t = table(border=1, style="table-layout: fixed;")  # Insert a table
        self.doc.add(self.t)
        with self.t:
            with tr():
                for im, txt, link in zip(ims, txts, links):
                    with td(style="word-wrap: break-word;", halign="center", valign="top"):
                        with p():
                            with a(href=os.path.join('images', link)):
                                img(style="width:%dpx" % width, src=os.path.join('images', im))
                            br()
                            p(txt)

    def save(self):
        """save the current content to the HMTL file"""
        html_file = '%s/index.html' % self.web_dir
        f = open(html_file, 'wt')
        f.write(self.doc.render())
        f.close()


if __name__ == '__main__':  # we show an example usage here.
    html = HTML('web/', 'test_html')
    html.add_header('hello world')

    ims, txts, links = [], [], []
    for n in range(4):
        ims.append('image_%d.png' % n)
        txts.append('text_%d' % n)
        links.append('image_%d.png' % n)
    html.add_images(ims, txts, links)
    html.save()



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/util/image_pool.py
================================================
import random
import torch


class ImagePool():
    """This class implements an image buffer that stores previously generated images.

    This buffer enables us to update discriminators using a history of generated images
    rather than the ones produced by the latest generators.
    """

    def __init__(self, pool_size):
        """Initialize the ImagePool class

        Parameters:
            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created
        """
        self.pool_size = pool_size
        if self.pool_size > 0:  # create an empty pool
            self.num_imgs = 0
            self.images = []

    def query(self, images):
        """Return an image from the pool.

        Parameters:
            images: the latest generated images from the generator

        Returns images from the buffer.

        By 50/100, the buffer will return input images.
        By 50/100, the buffer will return images previously stored in the buffer,
        and insert the current images to the buffer.
        """
        if self.pool_size == 0:  # if the buffer size is 0, do nothing
            return images
        return_images = []
        for image in images:
            image = torch.unsqueeze(image.data, 0)
            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer
                self.num_imgs = self.num_imgs + 1
                self.images.append(image)
                return_images.append(image)
            else:
                p = random.uniform(0, 1)
                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer
                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive
                    tmp = self.images[random_id].clone()
                    self.images[random_id] = image
                    return_images.append(tmp)
                else:       # by another 50% chance, the buffer will return the current image
                    return_images.append(image)
        return_images = torch.cat(return_images, 0)   # collect all the images and return
        return return_images



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/util/util.py
================================================
"""This module contains simple helper functions """
from __future__ import print_function
import torch
import numpy as np
from PIL import Image
import os


def tensor2im(input_image, imtype=np.uint16):
    """"Converts a Tensor array into a numpy image array.

    Parameters:
        input_image (tensor) --  the input image tensor array
        imtype (type)        --  the desired type of the converted numpy array
    """
    if not isinstance(input_image, np.ndarray):
        if isinstance(input_image, torch.Tensor):  # get the data from a variable
            image_tensor = input_image.data
        else:
            return input_image
        image_numpy = torch.squeeze(image_tensor).cpu().numpy()  # convert it into a numpy array
        image_numpy = (image_numpy + 1) / 2.0 * (2**16-1) #
    else:  # if it is a numpy array, do nothing
        image_numpy = input_image
    return image_numpy.astype(imtype)


def diagnose_network(net, name='network'):
    """Calculate and print the mean of average absolute(gradients)

    Parameters:
        net (torch network) -- Torch network
        name (str) -- the name of the network
    """
    mean = 0.0
    count = 0
    for param in net.parameters():
        if param.grad is not None:
            mean += torch.mean(torch.abs(param.grad.data))
            count += 1
    if count > 0:
        mean = mean / count
    print(name)
    print(mean)


def save_image(image_numpy, image_path, aspect_ratio=1.0):
    """Save a numpy image to the disk

    Parameters:
        image_numpy (numpy array) -- input numpy array
        image_path (str)          -- the path of the image
    """
    image_pil = Image.fromarray(image_numpy)

    image_pil = image_pil.convert('I;16')

    # image_pil = Image.fromarray(image_numpy)
    # h, w, _ = image_numpy.shape
    #
    # if aspect_ratio > 1.0:
    #     image_pil = image_pil.resize((h, int(w * aspect_ratio)), Image.BICUBIC)
    # if aspect_ratio < 1.0:
    #     image_pil = image_pil.resize((int(h / aspect_ratio), w), Image.BICUBIC)

    image_pil.save(image_path)


def print_numpy(x, val=True, shp=False):
    """Print the mean, min, max, median, std, and size of a numpy array

    Parameters:
        val (bool) -- if print the values of the numpy array
        shp (bool) -- if print the shape of the numpy array
    """
    x = x.astype(np.float64)
    if shp:
        print('shape,', x.shape)
    if val:
        x = x.flatten()
        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (
            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))


def mkdirs(paths):
    """create empty directories if they don't exist

    Parameters:
        paths (str list) -- a list of directory paths
    """
    if isinstance(paths, list) and not isinstance(paths, str):
        for path in paths:
            mkdir(path)
    else:
        mkdir(paths)


def mkdir(path):
    """create a single empty directory if it didn't exist

    Parameters:
        path (str) -- a single directory path
    """
    if not os.path.exists(path):
        os.makedirs(path)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/pix2pix/util/visualizer.py
================================================
import numpy as np
import os
import sys
import ntpath
import time
from . import util, html
from subprocess import Popen, PIPE
import torch


if sys.version_info[0] == 2:
    VisdomExceptionBase = Exception
else:
    VisdomExceptionBase = ConnectionError


def save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256):
    """Save images to the disk.

    Parameters:
        webpage (the HTML class) -- the HTML webpage class that stores these imaegs (see html.py for more details)
        visuals (OrderedDict)    -- an ordered dictionary that stores (name, images (either tensor or numpy) ) pairs
        image_path (str)         -- the string is used to create image paths
        aspect_ratio (float)     -- the aspect ratio of saved images
        width (int)              -- the images will be resized to width x width

    This function will save images stored in 'visuals' to the HTML file specified by 'webpage'.
    """
    image_dir = webpage.get_image_dir()
    short_path = ntpath.basename(image_path[0])
    name = os.path.splitext(short_path)[0]

    webpage.add_header(name)
    ims, txts, links = [], [], []

    for label, im_data in visuals.items():
        im = util.tensor2im(im_data)
        image_name = '%s_%s.png' % (name, label)
        save_path = os.path.join(image_dir, image_name)
        util.save_image(im, save_path, aspect_ratio=aspect_ratio)
        ims.append(image_name)
        txts.append(label)
        links.append(image_name)
    webpage.add_images(ims, txts, links, width=width)


class Visualizer():
    """This class includes several functions that can display/save images and print/save logging information.

    It uses a Python library 'visdom' for display, and a Python library 'dominate' (wrapped in 'HTML') for creating HTML files with images.
    """

    def __init__(self, opt):
        """Initialize the Visualizer class

        Parameters:
            opt -- stores all the experiment flags; needs to be a subclass of BaseOptions
        Step 1: Cache the training/test options
        Step 2: connect to a visdom server
        Step 3: create an HTML object for saveing HTML filters
        Step 4: create a logging file to store training losses
        """
        self.opt = opt  # cache the option
        self.display_id = opt.display_id
        self.use_html = opt.isTrain and not opt.no_html
        self.win_size = opt.display_winsize
        self.name = opt.name
        self.port = opt.display_port
        self.saved = False

        if self.use_html:  # create an HTML object at <checkpoints_dir>/web/; images will be saved under <checkpoints_dir>/web/images/
            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, 'web')
            self.img_dir = os.path.join(self.web_dir, 'images')
            print('create web directory %s...' % self.web_dir)
            util.mkdirs([self.web_dir, self.img_dir])
        # create a logging file to store training losses
        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, 'loss_log.txt')
        with open(self.log_name, "a") as log_file:
            now = time.strftime("%c")
            log_file.write('================ Training Loss (%s) ================\n' % now)

    def reset(self):
        """Reset the self.saved status"""
        self.saved = False

    def create_visdom_connections(self):
        """If the program could not connect to Visdom server, this function will start a new server at port < self.port > """
        cmd = sys.executable + ' -m visdom.server -p %d &>/dev/null &' % self.port
        print('\n\nCould not connect to Visdom server. \n Trying to start a server....')
        print('Command: %s' % cmd)
        Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)

    def display_current_results(self, visuals, epoch, save_result):
        """Display current results on visdom; save current results to an HTML file.

        Parameters:
            visuals (OrderedDict) - - dictionary of images to display or save
            epoch (int) - - the current epoch
            save_result (bool) - - if save the current results to an HTML file
        """
        if self.use_html and (save_result or not self.saved):  # save images to an HTML file if they haven't been saved.
            self.saved = True
            # save images to the disk
            for label, image in visuals.items():
                image_numpy = util.tensor2im(image)
                img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))
                util.save_image(image_numpy, img_path)

            # update website
            webpage = html.HTML(self.web_dir, 'Experiment name = %s' % self.name, refresh=1)
            for n in range(epoch, 0, -1):
                webpage.add_header('epoch [%d]' % n)
                ims, txts, links = [], [], []

                for label, image_numpy in visuals.items():
                    # image_numpy = util.tensor2im(image)
                    img_path = 'epoch%.3d_%s.png' % (n, label)
                    ims.append(img_path)
                    txts.append(label)
                    links.append(img_path)
                webpage.add_images(ims, txts, links, width=self.win_size)
            webpage.save()

    # def plot_current_losses(self, epoch, counter_ratio, losses):
        # """display the current losses on visdom display: dictionary of error labels and values
        #
        # Parameters:
        #     epoch (int)           -- current epoch
        #     counter_ratio (float) -- progress (percentage) in the current epoch, between 0 to 1
        #     losses (OrderedDict)  -- training losses stored in the format of (name, float) pairs
        # """
        # if not hasattr(self, 'plot_data'):
        #     self.plot_data = {'X': [], 'Y': [], 'legend': list(losses.keys())}
        # self.plot_data['X'].append(epoch + counter_ratio)
        # self.plot_data['Y'].append([losses[k] for k in self.plot_data['legend']])
        # try:
        #     self.vis.line(
        #         X=np.stack([np.array(self.plot_data['X'])] * len(self.plot_data['legend']), 1),
        #         Y=np.array(self.plot_data['Y']),
        #         opts={
        #             'title': self.name + ' loss over time',
        #             'legend': self.plot_data['legend'],
        #             'xlabel': 'epoch',
        #             'ylabel': 'loss'},
        #         win=self.display_id)
        # except VisdomExceptionBase:
        #     self.create_visdom_connections()

    # losses: same format as |losses| of plot_current_losses
    def print_current_losses(self, epoch, iters, losses, t_comp, t_data):
        """print current losses on console; also save the losses to the disk

        Parameters:
            epoch (int) -- current epoch
            iters (int) -- current training iteration during this epoch (reset to 0 at the end of every epoch)
            losses (OrderedDict) -- training losses stored in the format of (name, float) pairs
            t_comp (float) -- computational time per data point (normalized by batch_size)
            t_data (float) -- data loading time per data point (normalized by batch_size)
        """
        message = '(epoch: %d, iters: %d, time: %.3f, data: %.3f) ' % (epoch, iters, t_comp, t_data)
        for k, v in losses.items():
            message += '%s: %.3f ' % (k, v)

        print(message)  # print the message
        with open(self.log_name, "a") as log_file:
            log_file.write('%s\n' % message)  # save the message



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/DepthNet.py
================================================
#!/usr/bin/env python2
# coding: utf-8

'''
Author: Ke Xian
Email: kexian@hust.edu.cn
Date: 2019/04/09
'''

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init

import sys
sys.path.append('./structuredrl/models/syncbn')
from modules import nn as NN


from . import resnet

from .networks import *

class Decoder(nn.Module):
    def __init__(self, inchannels = [256, 512, 1024, 2048], midchannels = [256, 256, 256, 512], upfactors = [2,2,2,2], outchannels = 1):
        super(Decoder, self).__init__()
        self.inchannels = inchannels
        self.midchannels = midchannels
        self.upfactors = upfactors
        self.outchannels = outchannels

        self.conv = FTB(inchannels=self.inchannels[3], midchannels=self.midchannels[3])
        self.conv1 = nn.Conv2d(in_channels=self.midchannels[3], out_channels=self.midchannels[2], kernel_size=3, padding=1, stride=1, bias=True)
        self.upsample = nn.Upsample(scale_factor=self.upfactors[3], mode='bilinear', align_corners=True)

        self.ffm2 = FFM(inchannels=self.inchannels[2], midchannels=self.midchannels[2], outchannels = self.midchannels[2], upfactor=self.upfactors[2])
        self.ffm1 = FFM(inchannels=self.inchannels[1], midchannels=self.midchannels[1], outchannels = self.midchannels[1], upfactor=self.upfactors[1])
        self.ffm0 = FFM(inchannels=self.inchannels[0], midchannels=self.midchannels[0], outchannels = self.midchannels[0], upfactor=self.upfactors[0])

        self.outconv = AO(inchannels=self.inchannels[0], outchannels=self.outchannels, upfactor=2)

        self._init_params()

    def _init_params(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                #init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.ConvTranspose2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                #init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, NN.BatchNorm2d): #NN.BatchNorm2d
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.01)
                if m.bias is not None:
                    init.constant_(m.bias, 0)

    def forward(self, features):
        _,_,h,w = features[3].size()
        x = self.conv(features[3])
        x = self.conv1(x)
        x = self.upsample(x)

        x = self.ffm2(features[2], x)
        x = self.ffm1(features[1], x)
        x = self.ffm0(features[0], x)

        #-----------------------------------------
        x = self.outconv(x)

        return x

class DepthNet(nn.Module):
    __factory = {
        18: resnet.resnet18,
        34: resnet.resnet34,
        50: resnet.resnet50,
        101: resnet.resnet101,
        152: resnet.resnet152
    }
    def __init__(self,
                backbone='resnet',
                depth=50,
                pretrained=True,
                inchannels=[256, 512, 1024, 2048],
                midchannels=[256, 256, 256, 512],
                upfactors=[2, 2, 2, 2],
                outchannels=1):
        super(DepthNet, self).__init__()
        self.backbone = backbone
        self.depth = depth
        self.pretrained = pretrained
        self.inchannels = inchannels
        self.midchannels = midchannels
        self.upfactors = upfactors
        self.outchannels = outchannels

        # Build model
        if self.depth not in DepthNet.__factory:
            raise KeyError("Unsupported depth:", self.depth)
        self.encoder = DepthNet.__factory[depth](pretrained=pretrained)

        self.decoder = Decoder(inchannels=self.inchannels, midchannels=self.midchannels, upfactors=self.upfactors, outchannels=self.outchannels)

    def forward(self, x):
        x = self.encoder(x) # 1/4, 1/8, 1/16, 1/32
        x = self.decoder(x)

        return x

if __name__ == '__main__':
    net = DepthNet(depth=50, pretrained=True)
    print(net)
    inputs = torch.ones(4,3,128,128)
    out = net(inputs)
    print(out.size())



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/networks.py
================================================
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
'''
Author: Ke Xian
Email: kexian@hust.edu.cn
Date: 2019/04/09
'''

import torch
import torch.nn as nn
import torch.nn.init as init
import sys
sys.path.append('./structuredrl/models/syncbn')
import modules.nn as NN

# ==============================================================================================================

class FTB(nn.Module):
    def __init__(self, inchannels, midchannels=512):
        super(FTB, self).__init__()
        self.in1 = inchannels
        self.mid = midchannels

        self.conv1 = nn.Conv2d(in_channels=self.in1, out_channels=self.mid, kernel_size=3, padding=1, stride=1, bias=True)
        # NN.BatchNorm2d
        self.conv_branch = nn.Sequential(nn.ReLU(inplace=True),\
                                         nn.Conv2d(in_channels=self.mid, out_channels=self.mid, kernel_size=3, padding=1, stride=1, bias=True),\
                                         NN.BatchNorm2d(num_features=self.mid),\
                                         nn.ReLU(inplace=True),\
                                         nn.Conv2d(in_channels=self.mid, out_channels= self.mid, kernel_size=3, padding=1, stride=1, bias=True))
        self.relu = nn.ReLU(inplace=True)

        self.init_params()

    def forward(self, x):
        x = self.conv1(x)
        x = x + self.conv_branch(x)
        x = self.relu(x)

        return x

    def init_params(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                # init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.ConvTranspose2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                # init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, NN.BatchNorm2d):  #NN.BatchNorm2d
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.01)
                if m.bias is not None:
                    init.constant_(m.bias, 0)


class FFM(nn.Module):
    def __init__(self, inchannels, midchannels, outchannels, upfactor=2):
        super(FFM, self).__init__()
        self.inchannels = inchannels
        self.midchannels = midchannels
        self.outchannels = outchannels
        self.upfactor = upfactor

        self.ftb1 = FTB(inchannels=self.inchannels, midchannels=self.midchannels)
        self.ftb2 = FTB(inchannels=self.midchannels, midchannels=self.outchannels)

        self.upsample = nn.Upsample(scale_factor=self.upfactor, mode='bilinear', align_corners=True)

        self.init_params()

    def forward(self, low_x, high_x):
        x = self.ftb1(low_x)
        x = x + high_x
        x = self.ftb2(x)
        x = self.upsample(x)

        return x

    def init_params(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                #init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.ConvTranspose2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                #init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, NN.BatchNorm2d): #NN.Batchnorm2d
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.01)
                if m.bias is not None:
                    init.constant_(m.bias, 0)


class AO(nn.Module):
    # Adaptive output module
    def __init__(self, inchannels, outchannels, upfactor=2):
        super(AO, self).__init__()
        self.inchannels = inchannels
        self.outchannels = outchannels
        self.upfactor = upfactor

        self.adapt_conv = nn.Sequential(nn.Conv2d(in_channels=self.inchannels, out_channels=int(self.inchannels/2), kernel_size=3, padding=1, stride=1, bias=True),\
                                  NN.BatchNorm2d(num_features=int(self.inchannels/2)),\
                                  nn.ReLU(inplace=True),\
                                  nn.Conv2d(in_channels=int(self.inchannels/2), out_channels=self.outchannels, kernel_size=3, padding=1, stride=1, bias=True),\
                                       nn.Upsample(scale_factor=self.upfactor, mode='bilinear', align_corners=True))

        self.init_params()

    def forward(self, x):
        x = self.adapt_conv(x)
        return x

    def init_params(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                #init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.ConvTranspose2d):
                #init.kaiming_normal_(m.weight, mode='fan_out')
                init.normal_(m.weight, std=0.01)
                #init.xavier_normal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, NN.BatchNorm2d): #NN.Batchnorm2d
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.01)
                if m.bias is not None:
                    init.constant_(m.bias, 0)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/resnet.py
================================================
import torch.nn as nn
import math
import torch.utils.model_zoo as model_zoo
import torchvision

import sys
sys.path.append('./structuredrl/models/syncbn')
from modules import nn as NN


__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
           'resnet152']


model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = NN.BatchNorm2d(planes) #NN.BatchNorm2d
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = NN.BatchNorm2d(planes) #NN.BatchNorm2d
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = NN.BatchNorm2d(planes) #NN.BatchNorm2d
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = NN.BatchNorm2d(planes) #NN.BatchNorm2d
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = NN.BatchNorm2d(planes * self.expansion) #NN.BatchNorm2d
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes=1000):
        self.inplanes = 64
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = NN.BatchNorm2d(64)  #NN.BatchNorm2d
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        #self.avgpool = nn.AvgPool2d(7, stride=1)
        #self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                NN.BatchNorm2d(planes * block.expansion), #NN.BatchNorm2d
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        features = []

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        features.append(x)
        x = self.layer2(x)
        features.append(x)
        x = self.layer3(x)
        features.append(x)
        x = self.layer4(x)
        features.append(x)

        return features


def resnet18(pretrained=True, **kwargs):
    """Constructs a ResNet-18 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
    if pretrained:
        pretrained_model = torchvision.models.resnet18(pretrained=True)
        pretrained_dict = pretrained_model.state_dict()
        model_dict = model.state_dict()
        pretrained_dict = {k:v for k, v in pretrained_dict.items() if k in model_dict}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    return model


def resnet34(pretrained=True, **kwargs):
    """Constructs a ResNet-34 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)
    if pretrained:
        pretrained_model = torchvision.models.resnet34(pretrained=True)
        pretrained_dict = pretrained_model.state_dict()
        model_dict = model.state_dict()
        pretrained_dict = {k:v for k, v in pretrained_dict.items() if k in model_dict}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    return model


def resnet50(pretrained=True, **kwargs):
    """Constructs a ResNet-50 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
    if pretrained:
        pretrained_model = torchvision.models.resnet50(pretrained=True)
        pretrained_dict = pretrained_model.state_dict()
        model_dict = model.state_dict()
        pretrained_dict = {k:v for k, v in pretrained_dict.items() if k in model_dict}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    return model


def resnet101(pretrained=True, **kwargs):
    """Constructs a ResNet-101 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)
    if pretrained:
        pretrained_model = torchvision.models.resnet101(pretrained=True)
        pretrained_dict = pretrained_model.state_dict()
        model_dict = model.state_dict()
        pretrained_dict = {k:v for k, v in pretrained_dict.items() if k in model_dict}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    return model


def resnet152(pretrained=True, **kwargs):
    """Constructs a ResNet-152 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)
    if pretrained:
        pretrained_model = torchvision.models.resnet152(pretrained=True)
        pretrained_dict = pretrained_model.state_dict()
        model_dict = model.state_dict()
        pretrained_dict = {k:v for k, v in pretrained_dict.items() if k in model_dict}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    return model



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/README.md
================================================
# pytorch-syncbn

Tamaki Kojima(tamakoji@gmail.com)

## Overview
This is alternative implementation of "Synchronized Multi-GPU Batch Normalization" which computes global stats across gpus instead of locally computed. SyncBN are getting important for those input image is large, and must use multi-gpu to increase the minibatch-size for the training.

The code was inspired by [Pytorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding) and [Inplace-ABN](https://github.com/mapillary/inplace_abn)

## Remarks
- Unlike [Pytorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding), you don't need custom `nn.DataParallel`
- Unlike [Inplace-ABN](https://github.com/mapillary/inplace_abn), you can just replace your `nn.BatchNorm2d` to this module implementation, since it will not mark for inplace operation
- You can plug into arbitrary module written in PyTorch to enable Synchronized BatchNorm
- Backward computation is rewritten and tested against behavior of `nn.BatchNorm2d`

## Requirements
For PyTorch, please refer to https://pytorch.org/

NOTE : The code is tested only with PyTorch v0.4.0, CUDA9.1.85/CuDNN7.1.4 on ubuntu16.04

(It can also be compiled and run on the JetsonTX2, but won't work as multi-gpu synchronnized BN.)

To install all dependencies using pip, run:

```
pip install -U -r requirements.txt
```

## Build

use `make_ext.sh` to build the extension. for example:
```
PYTHON_CMD=python3 ./make_ext.sh
```

## Usage

Please refer to [`test.py`](./test.py) for testing the difference between `nn.BatchNorm2d` and `modules.nn.BatchNorm2d`

```
import torch
from modules import nn as NN
num_gpu = torch.cuda.device_count()
model = nn.Sequential(
    nn.Conv2d(3, 3, 1, 1, bias=False),
    NN.BatchNorm2d(3),
    nn.ReLU(inplace=True),
    nn.Conv2d(3, 3, 1, 1, bias=False),
    NN.BatchNorm2d(3),
).cuda()
model = nn.DataParallel(model, device_ids=range(num_gpu))
x = torch.rand(num_gpu, 3, 2, 2).cuda()
z = model(x)
```

## Math

### Forward
1. compute <img src="https://latex.codecogs.com/gif.latex?\sum{x_i},\sum{x_i^2}"/> in each gpu
2. gather all <img src="https://latex.codecogs.com/gif.latex?\sum{x_i},\sum{x_i^2}"/> from workers to master and compute <img src="https://latex.codecogs.com/gif.latex?\mu,\sigma"/> where

    <img src="https://latex.codecogs.com/gif.latex?\mu=\frac{\sum{x_i}}{N}"/>

    and

    <img src="https://latex.codecogs.com/gif.latex?\sigma^2=\frac{\sum{x_i^2}-\mu\sum{x_i}}{N}"/></a>

    and then above global stats to be shared to all gpus, update running_mean and running_var by moving average using global stats.

3. forward batchnorm using global stats by

    <img src="https://latex.codecogs.com/gif.latex?\hat{x_i}=\frac{x_i-\mu}{\sqrt{\sigma^2&plus;\epsilon}}"/>

    and then

    <img src="https://latex.codecogs.com/gif.latex?y_i=\gamma\cdot\hat{x_i}&plus;\beta"/>

    where <img src="https://latex.codecogs.com/gif.latex?\gamma"/> is weight parameter and <img src="https://latex.codecogs.com/gif.latex?\beta"/> is bias parameter.

4. save <img src="https://latex.codecogs.com/gif.latex?x,&space;\gamma\&space;\beta,&space;\mu,&space;\sigma^2"/> for backward

### Backward

1. Restore saved <img src="https://latex.codecogs.com/gif.latex?x,&space;\gamma\&space;\beta,&space;\mu,&space;\sigma^2"/>

2. Compute below sums on each gpu

    <img src="https://latex.codecogs.com/gif.latex?\sum_{i=1}^{N_j}(\frac{dJ}{dy_i})"/>

    and

    <img src="https://latex.codecogs.com/gif.latex?\sum_{i=1}^{N_j}(\frac{dJ}{dy_i}\cdot\hat{x_i})"/>

    where <img src="https://latex.codecogs.com/gif.latex?j\in[0,1,....,num\_gpu]"/>

    then gather them at master node to sum up global, and normalize with N where N is total number of elements for each channels. Global sums are then shared among all gpus.

3. compute gradients using global stats

    <img src="https://latex.codecogs.com/gif.latex?\frac{dJ}{dx_i},&space;\frac{dJ}{d\gamma},&space;\frac{dJ}{d\beta}&space;"/>

    where

    <img src="https://latex.codecogs.com/gif.latex?\frac{dJ}{d\gamma}=\sum_{i=1}^{N}(\frac{dJ}{dy_i}\cdot\hat{x_i})"/>

    and

    <img src="https://latex.codecogs.com/gif.latex?\frac{dJ}{d\beta}=\sum_{i=1}^{N}(\frac{dJ}{dy_i})"/>

    and finally,

    <img src="https://latex.codecogs.com/gif.latex?\frac{dJ}{dx_i}=\frac{dJ}{d\hat{x_i}}\frac{d\hat{x_i}}{dx_i}+\frac{dJ}{d\mu_i}\frac{d\mu_i}{dx_i}+\frac{dJ}{d\sigma^2_i}\frac{d\sigma^2_i}{dx_i}"/>  

    <img src="https://latex.codecogs.com/gif.latex?=\frac{1}{N\sqrt{(\sigma^2+\epsilon)}}(N\frac{dJ}{d\hat{x_i}}-\sum_{j=1}^{N}(\frac{dJ}{d\hat{x_j}})-\hat{x_i}\sum_{j=1}^{N}(\frac{dJ}{d\hat{x_j}}\hat{x_j}))"/>  

    <img src="https://latex.codecogs.com/gif.latex?=\frac{\gamma}{N\sqrt{(\sigma^2+\epsilon)}}(N\frac{dJ}{dy_i}-\sum_{j=1}^{N}(\frac{dJ}{dy_j})-\hat{x_i}\sum_{j=1}^{N}(\frac{dJ}{dy_j}\hat{x_j}))"/>  

   Note that in the implementation, normalization with N is performed at step (2) and above equation and implementation is not exactly the same, but mathematically is same.

   You can go deeper on above explanation at [Kevin Zakka's Blog](https://kevinzakka.github.io/2016/09/14/batch_normalization/)


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/LICENSE
================================================
MIT License

Copyright (c) 2018 Tamaki Kojima

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/make_ext.sh
================================================
#!/usr/bin/env bash

PYTHON_CMD=${PYTHON_CMD:=python}
CUDA_PATH=/usr/local/cuda-8.0
CUDA_INCLUDE_DIR=/usr/local/cuda-8.0/include
GENCODE="-gencode arch=compute_61,code=sm_61 \
         -gencode arch=compute_52,code=sm_52 \
         -gencode arch=compute_52,code=compute_52"
NVCCOPT="-std=c++11 -x cu --expt-extended-lambda -O3 -Xcompiler -fPIC"

ROOTDIR=$PWD
echo "========= Build BatchNorm2dSync ========="
if [ -z "$1" ]; then TORCH=$($PYTHON_CMD -c "import os; import torch; print(os.path.dirname(torch.__file__))"); else TORCH="$1"; fi
cd modules/functional/_syncbn/src
$CUDA_PATH/bin/nvcc -c -o syncbn.cu.o syncbn.cu $NVCCOPT $GENCODE -I $CUDA_INCLUDE_DIR
cd ../
$PYTHON_CMD build.py
cd $ROOTDIR

# END
echo "========= Build Complete ========="



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/requirements.txt
================================================
future
cffi



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/test.py
================================================
"""
/*****************************************************************************/

Test for BatchNorm2dSync with multi-gpu

/*****************************************************************************/
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import sys
import numpy as np
import torch
from torch import nn
from torch.nn import functional as F
sys.path.append("./")
from modules import nn as NN

torch.backends.cudnn.deterministic = True


def init_weight(model):
    for m in model.modules():
        if isinstance(m, nn.Conv2d):
            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            m.weight.data.normal_(0, np.sqrt(2. / n))
        elif isinstance(m, NN.BatchNorm2d) or isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

num_gpu = torch.cuda.device_count()
print("num_gpu={}".format(num_gpu))
if num_gpu < 2:
    print("No multi-gpu found. NN.BatchNorm2d will act as normal nn.BatchNorm2d")

m1 = nn.Sequential(
    nn.Conv2d(3, 3, 1, 1, bias=False),
    nn.BatchNorm2d(3),
    nn.ReLU(inplace=True),
    nn.Conv2d(3, 3, 1, 1, bias=False),
    nn.BatchNorm2d(3),
).cuda()
torch.manual_seed(123)
init_weight(m1)
m2 = nn.Sequential(
    nn.Conv2d(3, 3, 1, 1, bias=False),
    NN.BatchNorm2d(3),
    nn.ReLU(inplace=True),
    nn.Conv2d(3, 3, 1, 1, bias=False),
    NN.BatchNorm2d(3),
).cuda()
torch.manual_seed(123)
init_weight(m2)
m2 = nn.DataParallel(m2, device_ids=range(num_gpu))
o1 = torch.optim.SGD(m1.parameters(), 1e-3)
o2 = torch.optim.SGD(m2.parameters(), 1e-3)
y = torch.ones(num_gpu).float().cuda()
torch.manual_seed(123)
for _ in range(100):
    x = torch.rand(num_gpu, 3, 2, 2).cuda()
    o1.zero_grad()
    z1 = m1(x)
    l1 = F.mse_loss(z1.mean(-1).mean(-1).mean(-1), y)
    l1.backward()
    o1.step()
    o2.zero_grad()
    z2 = m2(x)
    l2 = F.mse_loss(z2.mean(-1).mean(-1).mean(-1), y)
    l2.backward()
    o2.step()
    print(m2.module[1].bias.grad - m1[1].bias.grad)
    print(m2.module[1].weight.grad - m1[1].weight.grad)
    print(m2.module[-1].bias.grad - m1[-1].bias.grad)
    print(m2.module[-1].weight.grad - m1[-1].weight.grad)
m2 = m2.module
print("===============================")
print("m1(nn.BatchNorm2d) running_mean",
      m1[1].running_mean, m1[-1].running_mean)
print("m2(NN.BatchNorm2d) running_mean",
      m2[1].running_mean, m2[-1].running_mean)
print("m1(nn.BatchNorm2d) running_var", m1[1].running_var, m1[-1].running_var)
print("m2(NN.BatchNorm2d) running_var", m2[1].running_var, m2[-1].running_var)
print("m1(nn.BatchNorm2d) weight", m1[1].weight, m1[-1].weight)
print("m2(NN.BatchNorm2d) weight", m2[1].weight, m2[-1].weight)
print("m1(nn.BatchNorm2d) bias", m1[1].bias, m1[-1].bias)
print("m2(NN.BatchNorm2d) bias", m2[1].bias, m2[-1].bias)



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/__init__.py
================================================



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/__init__.py
================================================
from .syncbn import batchnorm2d_sync



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/syncbn.py
================================================
"""
/*****************************************************************************/

BatchNorm2dSync with multi-gpu

code referenced from : https://github.com/mapillary/inplace_abn

/*****************************************************************************/
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import torch.cuda.comm as comm
from torch.autograd import Function
from torch.autograd.function import once_differentiable

from ._syncbn._ext import syncbn as _lib_bn


def _count_samples(x):
    count = 1
    for i, s in enumerate(x.size()):
        if i != 1:
            count *= s
    return count


def _check_contiguous(*args):
    if not all([mod is None or mod.is_contiguous() for mod in args]):
        raise ValueError("Non-contiguous input")


class BatchNorm2dSyncFunc(Function):

    @classmethod
    def forward(cls, ctx, x, weight, bias, running_mean, running_var,
                extra, compute_stats=True, momentum=0.1, eps=1e-05):
        # Save context
        if extra is not None:
            cls._parse_extra(ctx, extra)
        ctx.compute_stats = compute_stats
        ctx.momentum = momentum
        ctx.eps = eps
        if ctx.compute_stats:
            N = _count_samples(x) * (ctx.master_queue.maxsize + 1)
            assert N > 1
            num_features = running_mean.size(0)
            # 1. compute sum(x) and sum(x^2)
            xsum = x.new().resize_(num_features)
            xsqsum = x.new().resize_(num_features)
            _check_contiguous(x, xsum, xsqsum)
            _lib_bn.syncbn_sum_sqsum_cuda(x.detach(), xsum, xsqsum)
            if ctx.is_master:
                xsums, xsqsums = [xsum], [xsqsum]
                # master : gatther all sum(x) and sum(x^2) from slaves
                for _ in range(ctx.master_queue.maxsize):
                    xsum_w, xsqsum_w = ctx.master_queue.get()
                    ctx.master_queue.task_done()
                    xsums.append(xsum_w)
                    xsqsums.append(xsqsum_w)
                xsum = comm.reduce_add(xsums)
                xsqsum = comm.reduce_add(xsqsums)
                mean = xsum / N
                sumvar = xsqsum - xsum * mean
                var = sumvar / N
                uvar = sumvar / (N - 1)
                # master : broadcast global mean, variance to all slaves
                tensors = comm.broadcast_coalesced(
                    (mean, uvar, var), [mean.get_device()] + ctx.worker_ids)
                for ts, queue in zip(tensors[1:], ctx.worker_queues):
                    queue.put(ts)
            else:
                # slave : send sum(x) and sum(x^2) to master
                ctx.master_queue.put((xsum, xsqsum))
                # slave : get global mean and variance
                mean, uvar, var = ctx.worker_queue.get()
                ctx.worker_queue.task_done()

            # Update running stats
            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)
            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * uvar)
            ctx.N = N
            ctx.save_for_backward(x, weight, bias, mean, var)
        else:
            mean, var = running_mean, running_var

        output = x.new().resize_as_(x)
        _check_contiguous(output, x, mean, var, weight, bias)
        # do batch norm forward
        _lib_bn.syncbn_forward_cuda(
            output, x, weight if weight is not None else x.new(),
            bias if bias is not None else x.new(), mean, var, ctx.eps)
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, dz):
        x, weight, bias, mean, var = ctx.saved_tensors
        dz = dz.contiguous()
        if ctx.needs_input_grad[0]:
            dx = dz.new().resize_as_(dz)
        else:
            dx = None
        if ctx.needs_input_grad[1]:
            dweight = dz.new().resize_as_(mean).zero_()
        else:
            dweight = None
        if ctx.needs_input_grad[2]:
            dbias = dz.new().resize_as_(mean).zero_()
        else:
            dbias = None
        _check_contiguous(x, dz, weight, bias, mean, var)

        # 1. compute \sum(\frac{dJ}{dy_i}) and \sum(\frac{dJ}{dy_i}*\hat{x_i})
        num_features = mean.size(0)
        sum_dz = x.new().resize_(num_features)
        sum_dz_xhat = x.new().resize_(num_features)
        _check_contiguous(sum_dz, sum_dz_xhat)
        _lib_bn.syncbn_backward_xhat_cuda(
            dz, x, mean, var, sum_dz, sum_dz_xhat, ctx.eps)
        if ctx.is_master:
            sum_dzs, sum_dz_xhats = [sum_dz], [sum_dz_xhat]
            # master : gatther from slaves
            for _ in range(ctx.master_queue.maxsize):
                sum_dz_w, sum_dz_xhat_w = ctx.master_queue.get()
                ctx.master_queue.task_done()
                sum_dzs.append(sum_dz_w)
                sum_dz_xhats.append(sum_dz_xhat_w)
            # master : compute global stats
            sum_dz = comm.reduce_add(sum_dzs)
            sum_dz_xhat = comm.reduce_add(sum_dz_xhats)
            sum_dz /= ctx.N
            sum_dz_xhat /= ctx.N
            # master : broadcast global stats
            tensors = comm.broadcast_coalesced(
                (sum_dz, sum_dz_xhat), [mean.get_device()] + ctx.worker_ids)
            for ts, queue in zip(tensors[1:], ctx.worker_queues):
                queue.put(ts)
        else:
            # slave : send to master
            ctx.master_queue.put((sum_dz, sum_dz_xhat))
            # slave : get global stats
            sum_dz, sum_dz_xhat = ctx.worker_queue.get()
            ctx.worker_queue.task_done()

        # do batch norm backward
        _lib_bn.syncbn_backard_cuda(
            dz, x, weight if weight is not None else dz.new(),
            bias if bias is not None else dz.new(),
            mean, var, sum_dz, sum_dz_xhat,
            dx if dx is not None else dz.new(),
            dweight if dweight is not None else dz.new(),
            dbias if dbias is not None else dz.new(), ctx.eps)

        return dx, dweight, dbias, None, None, None, \
            None, None, None, None, None

    @staticmethod
    def _parse_extra(ctx, extra):
        ctx.is_master = extra["is_master"]
        if ctx.is_master:
            ctx.master_queue = extra["master_queue"]
            ctx.worker_queues = extra["worker_queues"]
            ctx.worker_ids = extra["worker_ids"]
        else:
            ctx.master_queue = extra["master_queue"]
            ctx.worker_queue = extra["worker_queue"]

batchnorm2d_sync = BatchNorm2dSyncFunc.apply

__all__ = ["batchnorm2d_sync"]



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/__init__.py
================================================



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/build.py
================================================
import os
from torch.utils.ffi import create_extension

sources = ['src/syncbn.cpp']
headers = ['src/syncbn.h']
extra_objects = ['src/syncbn.cu.o']
with_cuda = True

this_file = os.path.dirname(os.path.realpath(__file__))
extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]

ffi = create_extension(
    '_ext.syncbn',
    headers=headers,
    sources=sources,
    relative_to=__file__,
    with_cuda=with_cuda,
    extra_objects=extra_objects,
    extra_compile_args=["-std=c++11"]
)

if __name__ == '__main__':
    ffi.build()



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/_ext/__init__.py
================================================



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/_ext/syncbn/__init__.py
================================================

from torch.utils.ffi import _wrap_function
from ._syncbn import lib as _lib, ffi as _ffi

__all__ = []
def _import_symbols(locals):
    for symbol in dir(_lib):
        fn = getattr(_lib, symbol)
        if callable(fn):
            locals[symbol] = _wrap_function(fn, _ffi)
        else:
            locals[symbol] = fn
        __all__.append(symbol)

_import_symbols(locals())



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/src/common.h
================================================
#ifndef __COMMON__
#define __COMMON__
#include <cuda_runtime_api.h>

/*
 * General settings
 */
const int WARP_SIZE = 32;
const int MAX_BLOCK_SIZE = 512;

/*
 * Utility functions
 */
template <typename T>
__device__ __forceinline__ T WARP_SHFL_XOR(
  T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff) {
#if CUDART_VERSION >= 9000
  return __shfl_xor_sync(mask, value, laneMask, width);
#else
  return __shfl_xor(value, laneMask, width);
#endif
}

__device__ __forceinline__ int getMSB(int val) { return 31 - __clz(val); }

static int getNumThreads(int nElem) {
  int threadSizes[5] = {32, 64, 128, 256, MAX_BLOCK_SIZE};
  for (int i = 0; i != 5; ++i) {
    if (nElem <= threadSizes[i]) {
      return threadSizes[i];
    }
  }
  return MAX_BLOCK_SIZE;
}


#endif


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/src/syncbn.cpp
================================================
// All functions assume that input and output tensors are already initialized
// and have the correct dimensions
#include <THC/THC.h>

extern THCState *state;

void get_sizes(const THCudaTensor *t, int *N, int *C, int *S) {
    // Get sizes
    *S = 1;
    *N = THCudaTensor_size(state, t, 0);
    *C = THCudaTensor_size(state, t, 1);
    if (THCudaTensor_nDimension(state, t) > 2) {
        for (int i = 2; i < THCudaTensor_nDimension(state, t); ++i) {
            *S *= THCudaTensor_size(state, t, i);
        }
    }
}

// Forward definition of implementation functions
extern "C" {
    int _syncbn_sum_sqsum_cuda(int N, int C, int S,
                           const float *x, float *sum, float *sqsum,
                           cudaStream_t stream);
    int _syncbn_forward_cuda(
        int N, int C, int S, float *z, const float *x,
        const float *gamma, const float *beta,
        const float *mean, const float *var, float eps, cudaStream_t stream);
    int _syncbn_backward_xhat_cuda(
        int N, int C, int S, const float *dz, const float *x,
        const float *mean, const float *var, float *sum_dz, float *sum_dz_xhat,
        float eps, cudaStream_t stream);
    int _syncbn_backward_cuda(
        int N, int C, int S, const float *dz, const float *x,
        const float *gamma, const float *beta,
        const float *mean, const float *var,
        const float *sum_dz, const float *sum_dz_xhat,
        float *dx, float *dgamma, float *dbeta,
        float eps, cudaStream_t stream);
}

extern "C" int syncbn_sum_sqsum_cuda(
    const THCudaTensor *x, THCudaTensor *sum, THCudaTensor *sqsum) {
    cudaStream_t stream = THCState_getCurrentStream(state);

    int S, N, C;
    get_sizes(x, &N, &C, &S);

    // Get pointers
    const float *x_data = THCudaTensor_data(state, x);
    float *sum_data = THCudaTensor_data(state, sum);
    float *sqsum_data = THCudaTensor_data(state, sqsum);

    return _syncbn_sum_sqsum_cuda(N, C, S, x_data, sum_data, sqsum_data, stream);
}

extern "C" int syncbn_forward_cuda(
    THCudaTensor *z, const THCudaTensor *x,
    const THCudaTensor *gamma, const THCudaTensor *beta,
    const THCudaTensor *mean, const THCudaTensor *var, float eps){
    cudaStream_t stream = THCState_getCurrentStream(state);

    int S, N, C;
    get_sizes(x, &N, &C, &S);

    // Get pointers
    float *z_data = THCudaTensor_data(state, z);
    const float *x_data = THCudaTensor_data(state, x);
    const float *gamma_data = THCudaTensor_nDimension(state, gamma) != 0 ?
                               THCudaTensor_data(state, gamma) : 0;
    const float *beta_data = THCudaTensor_nDimension(state, beta) != 0 ?
                             THCudaTensor_data(state, beta) : 0;
    const float *mean_data = THCudaTensor_data(state, mean);
    const float *var_data = THCudaTensor_data(state, var);

    return _syncbn_forward_cuda(
        N, C, S, z_data, x_data, gamma_data, beta_data,
        mean_data, var_data, eps, stream);

}

extern "C" int syncbn_backward_xhat_cuda(
    const THCudaTensor *dz, const THCudaTensor *x,
    const THCudaTensor *mean, const THCudaTensor *var,
    THCudaTensor *sum_dz, THCudaTensor *sum_dz_xhat, float eps) {
    cudaStream_t stream = THCState_getCurrentStream(state);

    int S, N, C;
    get_sizes(dz, &N, &C, &S);

    // Get pointers
    const float *dz_data = THCudaTensor_data(state, dz);
    const float *x_data = THCudaTensor_data(state, x);
    const float *mean_data = THCudaTensor_data(state, mean);
    const float *var_data = THCudaTensor_data(state, var);
    float *sum_dz_data = THCudaTensor_data(state, sum_dz);
    float *sum_dz_xhat_data = THCudaTensor_data(state, sum_dz_xhat);

    return _syncbn_backward_xhat_cuda(
        N, C, S, dz_data, x_data, mean_data, var_data,
        sum_dz_data, sum_dz_xhat_data, eps, stream);

}
extern "C" int syncbn_backard_cuda(
    const THCudaTensor *dz, const THCudaTensor *x,
    const THCudaTensor *gamma, const THCudaTensor *beta,
    const THCudaTensor *mean, const THCudaTensor *var,
    const THCudaTensor *sum_dz, const THCudaTensor *sum_dz_xhat,
    THCudaTensor *dx, THCudaTensor *dgamma, THCudaTensor *dbeta, float eps) {
    cudaStream_t stream = THCState_getCurrentStream(state);

    int S, N, C;
    get_sizes(dz, &N, &C, &S);

    // Get pointers
    const float *dz_data = THCudaTensor_data(state, dz);
    const float *x_data = THCudaTensor_data(state, x);
    const float *gamma_data = THCudaTensor_nDimension(state, gamma) != 0 ?
                               THCudaTensor_data(state, gamma) : 0;
    const float *beta_data = THCudaTensor_nDimension(state, beta) != 0 ?
                             THCudaTensor_data(state, beta) : 0;
    const float *mean_data = THCudaTensor_data(state, mean);
    const float *var_data = THCudaTensor_data(state, var);
    const float *sum_dz_data = THCudaTensor_data(state, sum_dz);
    const float *sum_dz_xhat_data = THCudaTensor_data(state, sum_dz_xhat);
    float *dx_data = THCudaTensor_nDimension(state, dx) != 0 ?
                     THCudaTensor_data(state, dx) : 0;
    float *dgamma_data = THCudaTensor_nDimension(state, dgamma) != 0 ?
                          THCudaTensor_data(state, dgamma) : 0;
    float *dbeta_data = THCudaTensor_nDimension(state, dbeta) != 0 ?
                        THCudaTensor_data(state, dbeta) : 0;

    return _syncbn_backward_cuda(
        N, C, S, dz_data, x_data, gamma_data, beta_data,
        mean_data, var_data, sum_dz_data, sum_dz_xhat_data,
        dx_data, dgamma_data, dbeta_data, eps, stream);
}


================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/src/syncbn.cu
================================================
#include <thrust/device_ptr.h>
#include <thrust/transform.h>
#include <thrust/execution_policy.h>

#include "common.h"
#include "syncbn.cu.h"

/*
 * Device functions and data structures
 */
struct Float2 {
  float v1, v2;
  __device__ Float2() {}
  __device__ Float2(float _v1, float _v2) : v1(_v1), v2(_v2) {}
  __device__ Float2(float v) : v1(v), v2(v) {}
  __device__ Float2(int v) : v1(v), v2(v) {}
  __device__ Float2 &operator+=(const Float2 &a) {
    v1 += a.v1;
    v2 += a.v2;
    return *this;
  }
};

struct GradOp {
  __device__ GradOp(float _gamma, float _beta, const float *_z,
   const float *_dz, int c, int s)
      : gamma(_gamma), beta(_beta), z(_z), dz(_dz), C(c), S(s) {}
  __device__ __forceinline__ Float2 operator()(int batch, int plane, int n) {
    float _y = (z[(batch * C + plane) * S + n] - beta) / gamma;
    float _dz = dz[(batch * C + plane) * S + n];
    return Float2(_dz, _y * _dz);
  }
  const float gamma;
  const float beta;
  const float *z;
  const float *dz;
  const int C;
  const int S;
};

static __device__ __forceinline__ float warpSum(float val) {
#if __CUDA_ARCH__ >= 300
  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {
    val += WARP_SHFL_XOR(val, 1 << i, WARP_SIZE);
  }
#else
  __shared__ float values[MAX_BLOCK_SIZE];
  values[threadIdx.x] = val;
  __threadfence_block();
  const int base = (threadIdx.x / WARP_SIZE) * WARP_SIZE;
  for (int i = 1; i < WARP_SIZE; i++) {
    val += values[base + ((i + threadIdx.x) % WARP_SIZE)];
  }
#endif
  return val;
}

static __device__ __forceinline__ Float2 warpSum(Float2 value) {
  value.v1 = warpSum(value.v1);
  value.v2 = warpSum(value.v2);
  return value;
}

template <typename T, typename Op>
__device__ T reduce(Op op, int plane, int N, int C, int S) {
  T sum = (T)0;
  for (int batch = 0; batch < N; ++batch) {
    for (int x = threadIdx.x; x < S; x += blockDim.x) {
      sum += op(batch, plane, x);
    }
  }

  // sum over NumThreads within a warp
  sum = warpSum(sum);

  // 'transpose', and reduce within warp again
  __shared__ T shared[32];
  __syncthreads();
  if (threadIdx.x % WARP_SIZE == 0) {
    shared[threadIdx.x / WARP_SIZE] = sum;
  }
  if (threadIdx.x >= blockDim.x / WARP_SIZE && threadIdx.x < WARP_SIZE) {
    // zero out the other entries in shared
    shared[threadIdx.x] = (T)0;
  }
  __syncthreads();
  if (threadIdx.x / WARP_SIZE == 0) {
    sum = warpSum(shared[threadIdx.x]);
    if (threadIdx.x == 0) {
      shared[0] = sum;
    }
  }
  __syncthreads();

  // Everyone picks it up, should be broadcast into the whole gradInput
  return shared[0];
}

/*----------------------------------------------------------------------------
 *
 * BatchNorm2dSyncFunc Kernel implementations
 *
 *---------------------------------------------------------------------------*/

struct SqSumOp {
  __device__ SqSumOp(const float *t, int c, int s)
      : tensor(t), C(c), S(s) {}
  __device__ __forceinline__ Float2 operator()(int batch, int plane, int n) {
    float t = tensor[(batch * C + plane) * S + n];
    return Float2(t, t * t);
  }
  const float *tensor;
  const int C;
  const int S;
};

struct XHatOp {
  __device__ XHatOp(float _gamma, float _beta, const float *_z,
   const float *_dz, int c, int s)
      : gamma(_gamma), beta(_beta), z(_z), dz(_dz), C(c), S(s) {}
  __device__ __forceinline__ Float2 operator()(int batch, int plane, int n) {
    // xhat = (x-beta)*gamma
    float _xhat = (z[(batch * C + plane) * S + n] - beta) * gamma;
    // for dxhat*x_hat
    float _dz = dz[(batch * C + plane) * S + n];
    return Float2(_dz, _dz * _xhat);
  }
  const float gamma;
  const float beta;
  const float *z;
  const float *dz;
  const int C;
  const int S;
};

__global__ void syncbn_sum_sqsum_kernel(const float *x, float *sum, float *sqsum,
                                int N, int C, int S) {
  int plane = blockIdx.x;
  Float2 res = reduce<Float2, SqSumOp>(SqSumOp(x, C, S), plane, N, C, S);
  float _sum = res.v1;
  float _sqsum = res.v2;
  __syncthreads();
  if (threadIdx.x == 0) {
    sum[plane] = _sum;
    sqsum[plane] = _sqsum;
  }
}

__global__ void syncbn_forward_kernel(
        float *z, const float *x, const float *gamma, const float *beta,
        const float *mean, const float *var, float eps, int N, int C, int S) {

    int c = blockIdx.x;
    float _mean = mean[c];
    float _var = var[c];
    float invtsd = 0;
    if (_var != 0.f || eps != 0.f) {
      invtsd = 1 / sqrt(_var + eps);
    }
    float _gamma = gamma != 0 ? gamma[c] : 1.f;
    float _beta = beta != 0 ? beta[c] : 0.f;
    for (int batch = 0; batch < N; ++batch) {
      for (int n = threadIdx.x; n < S; n += blockDim.x) {
        float _x = x[(batch * C + c) * S + n];
        float _xhat = (_x - _mean) * invtsd;
        float _z = _xhat * _gamma + _beta;
        z[(batch * C + c) * S + n] = _z;
      }
    }
}

__global__ void syncbn_backward_xhat_kernel(
        const float *dz, const float *x, const float *mean, const float *var,
        float *sum_dz, float *sum_dz_xhat, float eps, int N, int C, int S) {

    int c = blockIdx.x;
    float _mean = mean[c];
    float _var = var[c];
    float _invstd = 0;
    if (_var != 0.f || eps != 0.f) {
          _invstd = 1 / sqrt(_var + eps);
    }
    Float2 res = reduce<Float2, XHatOp>(
        XHatOp(_invstd, _mean, x, dz, C, S), c, N, C, S);
    // \sum(\frac{dJ}{dy_i})
    float _sum_dz = res.v1;
    // \sum(\frac{dJ}{dy_i}*\hat{x_i})
    float _sum_dz_xhat = res.v2;
    __syncthreads();
    if (threadIdx.x == 0) {
        // \sum(\frac{dJ}{dy_i})
        sum_dz[c] = _sum_dz;
        // \sum(\frac{dJ}{dy_i}*\hat{x_i})
        sum_dz_xhat[c] = _sum_dz_xhat;
    }
}


__global__ void syncbn_backward_kernel(
        const float *dz, const float *x, const float *gamma, const float *beta,
        const float *mean, const float *var,
        const float *sum_dz, const float *sum_dz_xhat,
        float *dx, float *dgamma, float *dbeta,
        float eps, int N, int C, int S) {

    int c = blockIdx.x;
    float _mean = mean[c];
    float _var = var[c];
    float _gamma = gamma != 0 ? gamma[c] : 1.f;
    float _sum_dz = sum_dz[c];
    float _sum_dz_xhat = sum_dz_xhat[c];
    float _invstd = 0;
    if (_var != 0.f || eps != 0.f) {
          _invstd = 1 / sqrt(_var + eps);
    }
    /*
      \frac{dJ}{dx_i} = \frac{1}{N\sqrt{(\sigma^2+\epsilon)}} (
        N\frac{dJ}{d\hat{x_i}} -
        \sum_{j=1}^{N}(\frac{dJ}{d\hat{x_j}}) -
        \hat{x_i}\sum_{j=1}^{N}(\frac{dJ}{d\hat{x_j}}\hat{x_j})
      )
      Note : N is omitted here since it will be accumulated and
      _sum_dz and _sum_dz_xhat expected to be already normalized
      before the call.
    */
    if (dx != 0) {
        float _mul = _gamma * _invstd;
        for (int batch = 0; batch < N; ++batch) {
            for (int n = threadIdx.x; n < S; n += blockDim.x) {
                float _dz = dz[(batch * C + c) * S + n];
                float _xhat = (x[(batch * C + c) * S + n] - _mean) * _invstd;
                float _dx = (_dz - _sum_dz - _xhat * _sum_dz_xhat) * _mul;
                dx[(batch * C + c) * S + n] = _dx;
            }
        }
    }
    float _norm = N * S;
    if (dgamma != 0) {
        if (threadIdx.x == 0) {
            // \frac{dJ}{d\gamma} = \sum(\frac{dJ}{dy_i}*\hat{x_i})
            dgamma[c] += _sum_dz_xhat * _norm;
        }
    }
    if (dbeta != 0) {
        if (threadIdx.x == 0) {
            // \frac{dJ}{d\beta} = \sum(\frac{dJ}{dy_i})
            dbeta[c] += _sum_dz * _norm;
        }
    }
}

extern "C" int _syncbn_sum_sqsum_cuda(int N, int C, int S,
                                  const float *x, float *sum, float *sqsum,
                                  cudaStream_t stream) {
  // Run kernel
  dim3 blocks(C);
  dim3 threads(getNumThreads(S));
  syncbn_sum_sqsum_kernel<<<blocks, threads, 0, stream>>>(x, sum, sqsum, N, C, S);

  // Check for errors
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess)
    return 0;
  else
    return 1;
}

extern "C" int _syncbn_forward_cuda(
    int N, int C, int S, float *z, const float *x,
    const float *gamma, const float *beta, const float *mean, const float *var,
    float eps, cudaStream_t stream) {

    // Run kernel
    dim3 blocks(C);
    dim3 threads(getNumThreads(S));
    syncbn_forward_kernel<<<blocks, threads, 0, stream>>>(
        z, x, gamma, beta, mean, var, eps, N, C, S);

    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        return 0;
    else
        return 1;
}


extern "C" int _syncbn_backward_xhat_cuda(
    int N, int C, int S, const float *dz, const float *x,
    const float *mean, const float *var, float *sum_dz, float *sum_dz_xhat,
    float eps, cudaStream_t stream) {

    // Run kernel
    dim3 blocks(C);
    dim3 threads(getNumThreads(S));
    syncbn_backward_xhat_kernel<<<blocks, threads, 0, stream>>>(
        dz, x,mean, var, sum_dz, sum_dz_xhat, eps, N, C, S);

    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        return 0;
    else
        return 1;
}


extern "C" int _syncbn_backward_cuda(
    int N, int C, int S, const float *dz, const float *x,
    const float *gamma, const float *beta, const float *mean, const float *var,
    const float *sum_dz, const float *sum_dz_xhat,
    float *dx, float *dgamma, float *dbeta, float eps, cudaStream_t stream) {

    // Run kernel
    dim3 blocks(C);
    dim3 threads(getNumThreads(S));
    syncbn_backward_kernel<<<blocks, threads, 0, stream>>>(
        dz, x, gamma, beta, mean, var, sum_dz, sum_dz_xhat,
        dx, dgamma, dbeta, eps, N, C, S);

    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        return 0;
    else
        return 1;
}




================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/src/syncbn.cu.h
================================================
#ifndef __SYNCBN__
#define __SYNCBN__

/*
 * Exported functions
 */
extern "C" int _syncbn_sum_sqsum_cuda(int N, int C, int S, const float *x,
                                  float *sum, float *sqsum,
                                  cudaStream_t stream);
extern "C" int _syncbn_forward_cuda(
    int N, int C, int S, float *z, const float *x,
    const float *gamma, const float *beta, const float *mean, const float *var,
    float eps, cudaStream_t stream);
extern "C" int _syncbn_backward_xhat_cuda(
    int N, int C, int S, const float *dz, const float *x,
    const float *mean, const float *var, float *sum_dz, float *sum_dz_xhat,
    float eps, cudaStream_t stream);
extern "C" int _syncbn_backward_cuda(
    int N, int C, int S, const float *dz, const float *x,
    const float *gamma, const float *beta, const float *mean, const float *var,
    const float *sum_dz, const float *sum_dz_xhat,
    float *dx, float *dweight, float *dbias,
    float eps, cudaStream_t stream);


#endif



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/functional/_syncbn/src/syncbn.h
================================================
int syncbn_sum_sqsum_cuda(
    const THCudaTensor *x, THCudaTensor *sum, THCudaTensor *sqsum);
int syncbn_forward_cuda(
    THCudaTensor *z, const THCudaTensor *x,
    const THCudaTensor *gamma, const THCudaTensor *beta,
    const THCudaTensor *mean, const THCudaTensor *var, float eps);
int syncbn_backward_xhat_cuda(
    const THCudaTensor *dz, const THCudaTensor *x,
    const THCudaTensor *mean, const THCudaTensor *var,
    THCudaTensor *sum_dz, THCudaTensor *sum_dz_xhat,
    float eps);
int syncbn_backard_cuda(
    const THCudaTensor *dz, const THCudaTensor *x,
    const THCudaTensor *gamma, const THCudaTensor *beta,
    const THCudaTensor *mean, const THCudaTensor *var,
    const THCudaTensor *sum_dz, const THCudaTensor *sum_dz_xhat,
    THCudaTensor *dx, THCudaTensor *dgamma, THCudaTensor *dbeta, float eps);



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/nn/__init__.py
================================================
from .syncbn import *



================================================
FILE: depth-estimation/360monodepth/BoostingMonocularDepth/structuredrl/models/syncbn/modules/nn/syncbn.py
================================================
"""
/*****************************************************************************/

BatchNorm2dSync with multi-gpu

/*****************************************************************************/
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

try:
    # python 3
    from queue import Queue
except ImportError:
    # python 2
    from Queue import Queue

import torch
import torch.nn as nn
from modules.functional import batchnorm2d_sync


class BatchNorm2d(nn.BatchNorm2d):
    """
    BatchNorm2d with automatic multi-GPU Sync
    """

    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,
                 track_running_stats=True):
        super(BatchNorm2d, self).__init__(
            num_features, eps=eps, momentum=momentum, affine=affine,
            track_running_stats=track_running_stats)
        self.devices = list(range(torch.cuda.device_count()))
        if len(self.devices) > 1:
            # Initialize queues
            self.worker_ids = self.devices[1:]
            self.master_queue = Queue(len(self.worker_ids))
            self.worker_queues = [Queue(1) for _ in self.worker_ids]

    def forward(self, x):
        compute_stats = self.training or not self.track_running_stats
        if compute_stats and len(self.devices) > 1:
            if x.get_device() == self.devices[0]:
                # Master mode
                extra = {
                    "is_master": True,
                    "master_queue": self.master_queue,
                    "worker_queues": self.worker_queues,
                    "worker_ids": self.worker_ids
                }
            else:
                # Worker mode
                extra = {
                    "is_master": False,
                    "master_queue": self.master_queue,
                    "worker_queue": self.worker_queues[
                        self.worker_ids.index(x.get_device())]
                }
            return batchnorm2d_sync(x, self.weight, self.bias,
                                    self.running_mean, self.running_var,
                                    extra, compute_stats, self.momentum,
                                    self.eps)
        return super(BatchNorm2d, self).forward(x)

    def __repr__(self):
        """repr"""
        rep = '{name}({num_features}, eps={eps}, momentum={momentum},' \
              ' affine={affine}, devices={devices})'
        return rep.format(name=self.__class__.__name__, **self.__dict__)



================================================
FILE: depth-estimation/360monodepth/code/cpp/README.md
================================================

# 

The CMake 3rd party libraries configure by the `cmakeconfig.txt` file.
Before use CMake build this project, please configurate this file firstly.


================================================
FILE: depth-estimation/360monodepth/code/cpp/CMakeConfig.txt
================================================

if (WIN32)
    MESSAGE("Configure for Windows platform.")
    #############################
    # SET RUN-TIME ENV
    #############################
    unset(RUNTIMT_ENV_PATH_LIST)
    SET(RUNTIMT_ENV_PATH_LIST "PATH=%PATH%")

    #############################
    # DEPENDENCIES DIR
    #############################
    # python 
    set(Python3_ROOT_DIR "C:/Program Files/Python38")

    # Numpy
    set(Python3_NumPy_INCLUDE_DIR "D:/workenv_windows/python_3_8_instaomnidepth/Lib/site-packages/numpy/core/include")

    # gflags
    set(gflags_DIR "D:/libraries_windows/glog/glog_with_gflags_0.5.0_msvc_14_2/gflags-2.2.2-bin/lib/cmake/gflags/")
    LIST(APPEND RUNTIMT_ENV_PATH_LIST "D:/libraries_windows/glog/glog_with_gflags_0.5.0_msvc_14_2/gflags-2.2.2-bin/bin/")

    # glog
    set(glog_DIR "D:/libraries_windows/glog/glog_with_gflags_0.5.0_msvc_14_2/glog-0.5.0-bin/lib/cmake/glog/")
    LIST(APPEND RUNTIMT_ENV_PATH_LIST "D:/libraries_windows/glog/glog_with_gflags_0.5.0_msvc_14_2/glog-0.5.0-bin/bin/")
 
    # OpenCV
    set(OpenCV_DIR "D:/libraries_windows/opencv/opencv-4.5.3-vc14_vc15/build")
    LIST(APPEND RUNTIMT_ENV_PATH_LIST "D:/libraries_windows/opencv/opencv-4.5.3-vc14_vc15/build/x64/vc15/bin/")

    # Ceres
    set(Ceres_DIR "D:/libraries_windows/ceres/ceres-solver-2.0.0-bin-wo-optim/CMake")
    LIST(APPEND RUNTIMT_ENV_PATH_LIST "D:/libraries_windows/ceres/ceres-solver-2.0.0-bin-wo-optim/bin/")

    # GTest 
    set(GTEST_INCLUDE_DIR "D:/libraries_windows/gtest/googletest-release-1.11.0-bin/include")
    set(GTEST_LIBRARY "D:/libraries_windows/gtest/googletest-release-1.11.0-bin/lib/gtest.lib")
    set(GETST_LIBRARY_DEBUG "D:/libraries_windows/gtest/googletest-release-1.11.0-bin/lib/gtestd.lib")
    set(GTEST_MAIN_LIBRARY "D:/libraries_windows/gtest/googletest-release-1.11.0-bin/lib/gtest_main.lib")
    set(GTEST_MAIN_LIBRARY_DEBUG "D:/libraries_windows/gtest/googletest-release-1.11.0-bin/lib/gtest_maind.lib")

    # Boost
    set(Boost_DIR "D:/libraries_windows/boost/boost_1_76_0/")
    set(Boost_INCLUDE_DIR "D:/libraries_windows/boost/boost_1_76_0/")

    # OpenBLAS runtime
    LIST(APPEND RUNTIMT_ENV_PATH_LIST "D:/libraries_windows/openblas/OpenBLAS-0.3.9-bin/bin/")

    #############################
    # SET RUN-TIME ENV
    #############################
    unset(RUNTIMT_ENV_PATH_STR)
    string(JOIN ";" RUNTIMT_ENV_PATH_STR ${RUNTIMT_ENV_PATH_LIST})
    #string(CONCAT ";" RUNTIMT_ENV_PATH_STR ${RUNTIMT_ENV_PATH_LIST})

    unset(RUNTIMT_ENV_PATH)
    set(RUNTIMT_ENV_PATH ${RUNTIMT_ENV_PATH_STR} CACHE STRING "vs runtime env" FORCE)
    MESSAGE(STATUS "DEBUG: RUNTIMT_ENV_PATH_STR=${RUNTIMT_ENV_PATH}")
endif()

if(LINUX)
    MESSAGE("Configure for Linux platform.")
    #set(Python3_ROOT_DIR "/mnt/d/workenv_wsl/python3_8_com/bin/Python3")
    # Numpy
    set(Python3_NumPy_INCLUDE_DIR "/usr/local/lib/python3.8/dist-packages/numpy/core/include/")
endif()



================================================
FILE: depth-estimation/360monodepth/code/cpp/CMakeLists.txt
================================================
CMAKE_MINIMUM_REQUIRED(VERSION 3.16)

PROJECT(InstaOmniDepth)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

if(UNIX AND NOT APPLE)
set(LINUX TRUE)
endif()

# include 3rd libraries path
include(${CMAKE_CURRENT_SOURCE_DIR}/CMakeConfig.txt)
message("OS: ${CMAKE_SYSTEM_NAME}")
message("Generated with config types: ${CMAKE_BUILD_TYPE}")

set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${PROJECT_SOURCE_DIR}/lib)
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PROJECT_SOURCE_DIR}/lib)
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${PROJECT_SOURCE_DIR}/bin)

# OpenMP
find_package(OpenMP)
if (OPENMP_FOUND)
    set (CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}")
    set (CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}")
    set (CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}")
endif()

# GTest 
enable_testing()
find_package(GTest REQUIRED)

# EIGEN
find_package(Eigen3 REQUIRED)

# GLog
if (WIN32)
  find_package(glog REQUIRED)
endif()

# boost
set(Boost_USE_STATIC_LIBS OFF)
set(Boost_USE_MULTITHREADED ON)
set(Boost_USE_STATIC_RUNTIME OFF)
find_package(Boost REQUIRED)
message(STATUS "Boost version: ${Boost_VERSION}")

# OpenCV
find_package(OpenCV REQUIRED)

# Ceres
find_package(Ceres REQUIRED)

# Pybind11
set(pybind11_DIR ${PROJECT_SOURCE_DIR}/3rd_party/pybind11/build/mock_install/share/cmake/pybind11 )
find_package(pybind11 REQUIRED)

#=========  depth_stitch.dll =========#
set(MODULE_NAME depth_stitch)

# Source files
file(GLOB sources     ${PROJECT_SOURCE_DIR}/src/*.cpp)
list(REMOVE_ITEM sources ${PROJECT_SOURCE_DIR}/src/EigenSolvers.cpp)
file(GLOB headers_hpp ${PROJECT_SOURCE_DIR}/include/*.hpp)
file(GLOB headers_h   ${PROJECT_SOURCE_DIR}/include/*.h)

list(APPEND headers ${headers_h} ${headers_hpp} )
list(APPEND sources ${sources})

add_library(${MODULE_NAME} SHARED
    ${sources}
    ${headers}
)

# add postfix for debug lib
set_target_properties(${MODULE_NAME} PROPERTIES DEBUG_POSTFIX "_d")

if(CMAKE_CXX_COMPILER_ID MATCHES "MSVC")
    # generate *.pdb file for debug
    message("Set ${MODULE_NAME} to generate *.pdb for MSVC debugging.")
    target_compile_options(${MODULE_NAME} PRIVATE $<$<CONFIG:Release>:/Zi>)
    target_link_options(${MODULE_NAME} PRIVATE $<$<CONFIG:Release>:/DEBUG>)
    target_link_options(${MODULE_NAME} PRIVATE $<$<CONFIG:Release>:/OPT:REF>)
    target_link_options(${MODULE_NAME} PRIVATE $<$<CONFIG:Release>:/OPT:ICF>)
    # DLL export all symbols
    set_target_properties(${MODULE_NAME} PROPERTIES ENABLE_EXPORTS 1)
    set_target_properties(${MODULE_NAME} PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS 1)
endif()

target_include_directories(${MODULE_NAME} PRIVATE 
  ${PROJECT_SOURCE_DIR}/3rd_party/
  ${PROJECT_SOURCE_DIR}/include/
  ${EIGEN3_INCLUDE_DIRS}
  ${Boost_INCLUDE_DIR}
  ${OpenCV_INCLUDE_DIRS} 
)

target_link_libraries(${MODULE_NAME}
  # glog::glog
  ${GLOG_LIBRARIES}
  ${OpenCV_LIBS}
  ${Boost_LIBRARIES}
  # Ceres::ceres
  ${CERES_LIBRARIES}
)

#========= Python module (Just for Coding) =======#
find_package (Python3  REQUIRED COMPONENTS Interpreter Development NumPy)

message("Python3_FOUND:${Python3_FOUND}")
message("Python3_VERSION:${Python3_VERSION}")
message("Python3_Development_FOUND:${Python3_Development_FOUND}")
message("Python3_LIBRARIES:${Python3_LIBRARIES}")
message("Python3_NumPy_FOUND:${Python3_NumPy_FOUND}")
message("Python3_NumPy_INCLUDE_DIRS:${Python3_NumPy_INCLUDE_DIR}")
message("Python3_NumPy_VERSION:${Python3_NumPy_VERSION}")

set(MODULE_NAME depthmapAlign_pymodule)
add_library(${MODULE_NAME}  SHARED 
    ${PROJECT_SOURCE_DIR}/python/instaOmniDepth/depthmapAlignModule.cpp
)
set_target_properties(${MODULE_NAME}  PROPERTIES SUFFIX ".pyd")

if(CMAKE_CXX_COMPILER_ID MATCHES "MSVC")
    set_property(TARGET ${MODULE_NAME}  PROPERTY MSVC_RUNTIME_LIBRARY "MultiThreaded$<$<CONFIG:Debug>:>DLL")
    # target_compile_definitions(depthmapAlignModule PUBLIC Py_LIMITED_API)
endif()

target_include_directories(${MODULE_NAME}  PRIVATE 
  ${Python3_INCLUDE_DIRS}
  ${PROJECT_SOURCE_DIR}/include/
  ${Python3_NumPy_INCLUDE_DIRS}
)

target_link_libraries(${MODULE_NAME} 
  PRIVATE 
  ${Python3_LIBRARIES}
  depth_stitch
  Python3::NumPy
)

#=========  Main =========#
set(MODULE_NAME main)

add_executable(${MODULE_NAME}
  "main_depthmapstitch.cpp"
)

set_target_properties(${MODULE_NAME} PROPERTIES VS_DEBUGGER_ENVIRONMENT "${RUNTIMT_ENV_PATH}")
set_target_properties(${MODULE_NAME} PROPERTIES DEBUG_POSTFIX "_d")

target_include_directories(${MODULE_NAME} PRIVATE 
  ${headers}
  ${PROJECT_SOURCE_DIR}/include/
  ${OpenCV_INCLUDE_DIRS} 
)

target_link_libraries(${MODULE_NAME}
  depth_stitch
  # glog::glog
  ${GLOG_LIBRARIES}
  ${OpenCV_LIBS}
)

#=========  GTest =========#
set(MODULE_NAME unit_test)

file(GLOB test_sources ${PROJECT_SOURCE_DIR}/test/*.cpp)
file(GLOB test_headers ${PROJECT_SOURCE_DIR}/test/*.hpp)

add_executable(${MODULE_NAME}
  ${test_headers}
  ${test_sources}
)

target_include_directories(${MODULE_NAME} PUBLIC
  ${CMAKE_CURRENT_SOURCE_DIR}
  ${OpenCV_INCLUDE_DIRS}
  ${EIGEN3_INCLUDE_DIRS}
  ${Boost_INCLUDE_DIR}
)

target_link_libraries(${MODULE_NAME}
  depth_stitch
  GTest::GTest 
  GTest::Main
  #   glog::glog
  ${GLOG_LIBRARIES}
  ${OpenCV_LIBS}
  ${Boost_LIBRARIES}
  ${OpenCV_LIBS}
  # Ceres::ceres
  ${CERES_LIBRARIES}
)

#=========  Eigen Solvers python binding =========#
pybind11_add_module(EigenSolvers ${PROJECT_SOURCE_DIR}/src/EigenSolvers.cpp )
target_include_directories(EigenSolvers PRIVATE ${EIGEN3_INCLUDE_DIRS})
target_link_libraries(EigenSolvers PRIVATE ${GLOG_LIBRARIES})
#target_link_libraries(EigenSolvers PRIVATE glog::glog)



================================================
FILE: depth-estimation/360monodepth/code/cpp/main_depthmapstitch.cpp
================================================
#include "depthmap_stitcher.hpp"
#include "depthmap_stitcher_enum.hpp"
#include "depthmap_stitcher_group.hpp"
#include "data_imitator.hpp"

#include <glog/logging.h>

#include <gflags/gflags.h>

#include <string>
#include <iostream>


// define CLI arguments and validators
DEFINE_string(type, "stitch", "[stitch|debug]. The program model.");
static bool validate_type(const char* flagname, const std::string& type_str) {
	if (type_str.compare("stitch") != 0 && type_str.compare("debug") != 0)
		return false;
	else
		return true;
}
DEFINE_validator(type, &validate_type);

// The generated data type
DEFINE_string(data_type, "simple", "[simple|rand]. The synthetic data's coefficient is uniformed or random.");
static bool validate_data_type(const char* flagname, const std::string& data_type) {
	if (data_type.compare("simple") != 0 && data_type.compare("rand") != 0)
		return false;
	else
		return true;
}
DEFINE_validator(data_type, &validate_data_type);

DEFINE_string(root_dir, "", "The input and output folder path. ");
static bool validate_root_dir(const char* flagname, const std::string& root_dir) {
	if (root_dir.length() <= 0)
		return false;
	else
		return true;
}
DEFINE_validator(root_dir, &validate_root_dir);

DEFINE_string(method, "enum", "[group|enum]. The stitch type joint optimization or enumerate all depth map pairs.");
static bool validate_method(const char* flagname, const std::string& method_str) {
	if (method_str.compare("group") != 0 && method_str.compare("enum") != 0)
		return false;
	else
		return true;
}
DEFINE_validator(method, &validate_method);

DEFINE_string(filename_list, "", "The input images file name list. --filename_list=\"img0_000.pfm,img0_001.pfm\"");

DEFINE_int32(deform_grid_width, 5, "The depth map alignment deform grid number along the x axis, column number.");
DEFINE_int32(deform_grid_height, 5, "The depth map alignment deform grid number along the y axis, row number.");

// create fake data for debug
// The image (000) is reference image, reference image (000) = target image (001) * scale + offset
void create_debug_data(const std::string& root_dir)
{
	DataImitator di;
	di.json_filepath = root_dir + "align_coeff_gt.json";

	di.depthmap_ref_filepath = root_dir + "img0_depth_000.pfm";
	di.depthmap_tar_filepath = root_dir + "img0_depth_001.pfm";

	di.corr_ref2tar_filepath = root_dir + "img0_corr_000_001.json";
	di.corr_tar2ref_filepath = root_dir + "img0_corr_001_000.json";

	di.depthmap_overlap_ratio = 1.0;
	di.depth_scale = 0.2;
	di.depth_offset = 6.0;
	di.coeff_grid_height = FLAGS_deform_grid_height;
	di.coeff_grid_width = FLAGS_deform_grid_width;

	di.depthmap_width = 50;
	di.depthmap_hight = 80;

	LOG(INFO) << "The reference image (000) is reference image. And target image is 001.\nReference image (001) = target image (000) * scale + offset";

	if (FLAGS_data_type.compare("simple") == 0)
	{
		// 1) generate simple test data
		di.make_aligncoeffs_simple();
		di.make_depthmap_pair_simple();
	}
	else if (FLAGS_data_type.compare("rand") == 0)
	{
		// 2) generate random scale and offset coefficients
		di.make_aligncoeffs_random();
		di.make_depthmap_pair_random();
	}
	di.make_corresponding_json();
	di.output_date();

	LOG(INFO) << di;
}

void create_debug_data_multi(const std::string& root_dir)
{
	DataImitatorMultiImage di;
	di.output_root_dir = "D:/workspace_windows/InstaOmniDepth/InstaOmniDepth_github/code/cpp/bin/Release/";

	// report the data information
	di.report_data_parameters();

	// create data
	di.make_aligncoeffs_random();
	di.make_depthmap_pair();
	di.make_pixel_corresponding();
	di.output_date();
}

// stitch depth maps
void depthmap_stitch(const std::string& root_dir, const std::vector<std::string>& depthmap_filename_list, const std::string& method)
{
	std::shared_ptr<DepthmapStitcher> depthmap_stitcher;
	if (method.compare("enum") == 0)
	{
		depthmap_stitcher = std::make_shared<DepthmapStitcherEnum>(0.1); // set overlap ratio
		depthmap_stitcher->weight_reprojection = 1.0; // re-projection term lambda
		depthmap_stitcher->weight_smooth = 1e-2;     // smooth term lambda
		depthmap_stitcher->weight_scale = 1e-10;      // scale term lambda
	}
	else if (method.compare("group") == 0)
	{
		depthmap_stitcher = std::make_shared<DepthmapStitcherGroup>();
		depthmap_stitcher->weight_reprojection = 1.0; // re-projection term lambda
		depthmap_stitcher->weight_smooth = 1.0;     // smooth term lambda
		depthmap_stitcher->weight_scale = 1e-6;      // scale term lambda
		depthmap_stitcher->depthmap_ref_extidx = 0;  // the reference depth map is the first depth map;
	}
	else
		LOG(ERROR) << "The specified method name " << method << " is wrong.";

	depthmap_stitcher->ceres_max_num_iterations = 300;
	depthmap_stitcher->load_data(root_dir, depthmap_filename_list);
	depthmap_stitcher->initial(FLAGS_deform_grid_width, FLAGS_deform_grid_height);
	depthmap_stitcher->compute_align_coeff();
	depthmap_stitcher->align_depthmap_all();
	depthmap_stitcher->save_aligned_depthmap();
	depthmap_stitcher->save_align_coeff();
	depthmap_stitcher->report_error();
}


int main(int argc, char** argv)
{
	// 0) parser the CLI parameter 
	if (argc <= 2)
		LOG(ERROR) << "The input arguments is less than 3.";
	gflags::ParseCommandLineFlags(&argc, &argv, true);
	const std::string root_dir = FLAGS_root_dir;

	// 1) set-up the runtime environment
	const std::string logfile_path = root_dir + "/depth_map_stitch.log";
	google::SetLogDestination(google::GLOG_INFO, logfile_path.c_str());
	FLAGS_stderrthreshold = google::GLOG_INFO;
	google::InitGoogleLogging(argv[0]);

	LOG(INFO) << "The data root folder :" << root_dir;

	// 2) make mock data or stitch depth maps
	if (FLAGS_type.compare("stitch") == 0)
	{
		std::vector<std::string> depthmap_filename_list;
		size_t start;
		size_t end = 0;
		char delim = ',';
		while ((start = FLAGS_filename_list.find_first_not_of(delim, end)) != std::string::npos)
		{
			end = FLAGS_filename_list.find(delim, start);
			depthmap_filename_list.push_back(FLAGS_filename_list.substr(start, end - start));
		}
		if (depthmap_filename_list.size() < 2)
			LOG(ERROR) << "The input depth maps should be at least 2 depth maps.";
		depthmap_stitch(root_dir, depthmap_filename_list, FLAGS_method);
	}
	else if (FLAGS_type.compare("debug") == 0)
		//create_debug_data(root_dir);
		create_debug_data_multi(root_dir);
}


================================================
FILE: depth-estimation/360monodepth/code/cpp/3rd_party/readme.md
================================================
- pfm_io.hpp : https://stackoverflow.com/questions/29487925/opencv-how-to-read-pfm-files


================================================
FILE: depth-estimation/360monodepth/code/cpp/3rd_party/pfm_io.hpp
================================================
#ifndef _PGM_H_
#define _PGM_H_

// Reference : https://stackoverflow.com/questions/29487925/opencv-how-to-read-pfm-files

#include <fstream>
#include <iostream>
#include <algorithm>
#include <string>
#include <cstdint>
#include <cstdlib>
#include <cstring>
#include <bitset>
#include <cstdio>

#include <glog/logging.h>

enum PFM_endianness
{
	BIG,
	LITTLE,
	ERROR
};

class PFM
{
public:
	PFM() {};

	inline bool is_little_big_endianness_swap()
	{
		if (this->endianess == 0.f)
		{
			std::cerr << "this-> endianness is not assigned yet!\n";
			exit(0);
		}
		else
		{
			uint32_t endianness = 0xdeadbeef;
			//std::cout << "\n" << std::bitset<32>(endianness) << std::endl;
			unsigned char* temp = (unsigned char*)&endianness;
			//std::cout << std::bitset<8>(*temp) << std::endl;
			PFM_endianness endianType_ = ((*temp) ^ 0xef == 0 ? LITTLE : (*temp) ^ (0xde) == 0 ? BIG
				: ERROR);
			// ".pfm" format file specifies that:
			// positive scale means big endianess;
			// negative scale means little endianess.
			return ((BIG == endianType_) && (this->endianess < 0.f)) || ((LITTLE == endianType_) && (this->endianess > 0.f));
		}
	}

	template <typename T>
	T* read_pfm(const std::string& filename)
	{
		FILE* pFile;
		pFile = fopen(filename.c_str(), "rb");
		char c[128];
		if (pFile != NULL)
		{
//			int char_num =  fscanf(pFile, "%s", c);

			if (fscanf(pFile, "%s", c) < 1)
				LOG(ERROR) << filename << "read error";
			// strcmp() returns 0 if they are equal.
			if (!strcmp(c, "Pf"))
			{
				if (fscanf(pFile, "%s", c))
					LOG(ERROR) << filename << "read error";
				// atoi: ASCII to integer.
				// itoa: integer to ASCII.
				this->width = atoi(c);
				if (fscanf(pFile, "%s", c))
					LOG(ERROR) << filename << "read error";
				this->height = atoi(c);
				int length_ = this->width * this->height;
				if (fscanf(pFile, "%s", c))
					LOG(ERROR) << filename << "read error";
				this->endianess = atof(c);

				fseek(pFile, 0, SEEK_END);
				long lSize = ftell(pFile);
				long pos = lSize - this->width * this->height * sizeof(T);
				fseek(pFile, pos, SEEK_SET);

				T* img = new T[length_];
				//cout << "sizeof(T) = " << sizeof(T);
				if(fread(img, sizeof(T), length_, pFile) < 1)
					LOG(ERROR) << filename << " read error";
				fclose(pFile);

				/* The raster is a sequence of pixels, packed one after another,
				 * with no delimiters of any kind. They are grouped by row,
				 * with the pixels in each row ordered left to right and
				 * the rows ordered bottom to top.
				 */
				T* tbimg = (T*)malloc(length_ * sizeof(T)); // top-to-bottom.
				//PFM SPEC image stored bottom -> top reversing image
				for (int i = 0; i < this->height; i++)
				{
					memcpy(&tbimg[(this->height - i - 1) * (this->width)],
						&img[(i * (this->width))],
						(this->width) * sizeof(T));
				}

				if (this->is_little_big_endianness_swap())
				{
					std::cout << "little-big endianness transformation is needed.\n";
					// little-big endianness transformation is needed.
					union
					{
						T f;
						unsigned char u8[sizeof(T)];
					} source, dest;

					for (int i = 0; i < length_; ++i)
					{
						source.f = tbimg[i];
						for (unsigned int k = 0, s_T = sizeof(T); k < s_T; k++)
							dest.u8[k] = source.u8[s_T - k - 1];
						tbimg[i] = dest.f;
						//cout << dest.f << ", ";
					}
				}
				delete[] img;
				return tbimg;
			}
			else
			{
				std::cout << "Invalid magic number!"
					<< " No Pf (meaning grayscale pfm) is missing!!\n";
				fclose(pFile);
				exit(0);
			}
		}
		else
		{
			std::cout << "Cannot open file " << filename
				<< ", or it does not exist!\n";
			fclose(pFile);
			exit(0);
		}
	}

	template <typename T>
	void write_pfm(const std::string& filename, const T* imgbuffer,
		const float& endianess_)
	{
		std::ofstream ofs(filename.c_str(), std::ifstream::binary);
		// ** 1) Identifier Line: The identifier line contains the characters
		// "PF" or "Pf". PF means it's a color PFM.
		// Pf means it's a grayscale PFM.
		// ** 2) Dimensions Line:
		// The dimensions line contains two positive decimal integers,
		// separated by a blank. The first is the width of the image;
		// the second is the height. Both are in pixels.
		// ** 3) Scale Factor / Endianness:
		// The Scale Factor / Endianness line is a queer line that jams
		// endianness information into an otherwise sane description
		// of a scale. The line consists of a nonzero decimal number,
		// not necessarily an integer. If the number is negative, that
		// means the PFM raster is little endian. Otherwise, it is big
		// endian. The absolute value of the number is the scale
		// factor for the image.
		// The scale factor tells the units of the samples in the raster.
		// You use somehow it along with some separately understood unit
		// information to turn a sample value into something meaningful,
		// such as watts per square meter.

		ofs << "Pf\n"
			<< this->width << " " << this->height << "\n"
			<< endianess_ << "\n";
		/* PFM raster:
		 * The raster is a sequence of pixels, packed one after another,
		 * with no delimiters of any kind. They are grouped by row,
		 * with the pixels in each row ordered left to right and
		 * the rows ordered bottom to top.
		 * Each pixel consists of 1 or 3 samples, packed one after another,
		 * with no delimiters of any kind. 1 sample for a grayscale PFM
		 * and 3 for a color PFM (see the Identifier Line of the PFM header).
		 * Each sample consists of 4 consecutive bytes. The bytes represent
		 * a 32 bit string, in either big endian or little endian format,
		 * as determined by the Scale Factor / Endianness line of the PFM
		 * header. That string is an IEEE 32 bit floating point number code.
		 * Since that's the same format that most CPUs and compiler use,
		 * you can usually just make a program use the bytes directly
		 * as a floating point number, after taking care of the
		 * endianness variation.
		 */
		int length_ = this->width * this->height;
		this->endianess = endianess_;
		T* tbimg = (T*)malloc(length_ * sizeof(T));
		// PFM SPEC image stored bottom -> top reversing image
		for (int i = 0; i < this->height; i++)
		{
			memcpy(&tbimg[(this->height - i - 1) * this->width],
				&imgbuffer[(i * this->width)],
				this->width * sizeof(T));
		}

		if (this->is_little_big_endianness_swap())
		{
			std::cout << "little-big endianness transformation is needed.\n";
			// little-big endianness transformation is needed.
			union
			{
				T f;
				unsigned char u8[sizeof(T)];
			} source, dest;

			for (int i = 0; i < length_; ++i)
			{
				source.f = tbimg[i];
				for (size_t k = 0, s_T = sizeof(T); k < s_T; k++)
					dest.u8[k] = source.u8[s_T - k - 1];
				tbimg[i] = dest.f;
				//cout << dest.f << ", ";
			}
		}

		ofs.write((char*)tbimg, this->width * this->height * sizeof(T));
		ofs.close();
		free(tbimg);
	}

	inline float getEndianess() { return endianess; }
	inline int getHeight(void) { return height; }
	inline int getWidth(void) { return width; }
	inline void setHeight(const int& h) { height = h; }
	inline void setWidth(const int& w) { width = w; }

private:
	int height;
	int width;
	float endianess;
};

#endif /* PGM_H_ */


================================================
FILE: depth-estimation/360monodepth/code/cpp/include/data_imitator.hpp
================================================
#pragma once

#include <opencv2/opencv.hpp>

#include <string>
#include <iostream>
#include <vector>
#include <map>


/**
 * Create and store the depth map data.
 * There are composed with two depth maps, the first one is reference image, the second is the target image.
 * Reference_Image = Target_Image * scale_coeff + offset_coeff.
 */
class DataImitator
{
public:
	DataImitator();

	~DataImitator();

	/**
	 * Create a depth map pair for debug.
	 * target depth map = reference depth map * scale + offset.
	 */
	std::vector<cv::Mat>  make_depthmap_pair_simple();
	std::vector<cv::Mat>  make_depthmap_pair_random();

	/**
	 * Make mock pixel corresponding which is from target to reference, and output to JSON file .
	 * The first depth on the top and second on the bottom. So the 1st map bottom overlap the 2nd map's top.
	 */
	std::map<int, std::map<int, cv::Mat>> make_corresponding_json();
	void make_pixel_corresponding();

	/**
	 * Create the depth maps alignment coefficients.
	 */
	std::vector<cv::Mat> make_aligncoeffs_simple();
	std::vector<cv::Mat> make_aligncoeffs_random();

	/**
	 * Save depth maps and coefficients to files.
	 */
	void output_date();

	// coefficients *.json file path.
	std::string json_filepath;

	// the output file path
	std::string depthmap_ref_filepath;
	std::string depthmap_tar_filepath;

	std::string corr_ref2tar_filepath;
	std::string corr_tar2ref_filepath;

	// depth map information
	int depthmap_width = 50;
	int depthmap_hight = 80;

	// the coefficient grid size
	int coeff_grid_width = 5;
	int coeff_grid_height = 10;

	// how much area overlap
	float depthmap_overlap_ratio = 1.0;

	// depth map scale and offset
	float depth_scale = 0.2;
	float depth_offset = 6.0;

private:

	// The depth map deform coefficients. scale & offset array
	std::vector<cv::Mat> coeff_list;

	// The deformed depth map.
	std::vector<cv::Mat> depthmap_list;

	// the images corresponding relationship
	cv::Mat corresponding_mat_ref2tar;
	cv::Mat corresponding_mat_tar2ref;
};

/** Report mock data information parameters */
std::ostream& operator <<(std::ostream& os, const DataImitator& di);

// Generate the three image sequence.
// 1st image is reference image, 1st = 2nd * scale + offset, 2nd = 3rd * scale + offset
class DataImitatorMultiImage
{

public:
	// the mock depth map frame number
	// the frame index range is [0, frame_number - 1]
	const int frame_number = 3;

	// depth map information
	int depthmap_width = 60;
	int depthmap_hight = 80;

	// the coefficient grid size
	int coeff_grid_width = 6;
	int coeff_grid_height = 8;

	// how much area overlap, the depth map horizontally overlaps.
	float depthmap_overlap_ratio = 0.4;

	// depth map scale and offset
	float depth_scale = 0.2;
	float depth_offset = 6.0;

	// the root folder to output test data
	std::string output_root_dir;

	// create the template of depth map and coefficient 
	void initial();

	/**
	 * Generate the depth maps alignment coefficients.
	 */
	void make_aligncoeffs_simple(); // constant value number
	void make_aligncoeffs_random(); // random number

	// the deform grid weight is 
	void make_depthmap_pair(); // constant value number

	/**
	 * Make mock pixel corresponding which is from target to reference, and output to JSON file .
	 * The first depth on the top and second on the bottom. So the 1st map bottom overlap the 2nd map's top.
	 */
	void make_pixel_corresponding();

	/**
	 * Save depth maps and coefficients to files.
	 */
	void output_date();

	void report_data_parameters();

public:

	// 1st image index is 0, 2nd is 1, 3rd is 2.
	std::map<int, std::map<int, cv::Mat>> pixel_corresponding;

	// the deformation coefficient 3rd -> 2nd, 2nd -> 1st
	std::map<int, std::map<int, cv::Mat>> coeff_scale_list;
	std::map<int, std::map<int, cv::Mat>> coeff_offset_list;

	// the depth map list 1st, 2nd, 3rd, the last depth map is the reference depth map
	std::map<int, cv::Mat> depthmap_list; // 
	std::map<int, cv::Mat> depthmap_template_list;

	// the template of the depth map & alignment coefficient
	cv::Mat depthmap_ref_template;
	double scale_mean = 0;
	double scale_stddev = 5;
	double offset_mean = 0;
	double offset_stddev = 80.0;
	std::map<int, std::map<int, cv::Mat>> coeff_template_scale;
	std::map<int, std::map<int, cv::Mat>> coeff_template_offset;

	// output file name expression
	// pixels corresponding filename regular expression
	std::string depthmap_filename_exp = "img0_depth_$subimageindex.pfm";
	std::string pixelcorr_filename_exp = "img0_corr_$srcindex_$tarindex.json";
	std::string aligncoeff_filename_exp = "img0_coeff_align.json";

};



================================================
FILE: depth-estimation/360monodepth/code/cpp/include/data_io.hpp
================================================
#pragma once

#include <string>
#include <vector>

#include <boost/property_tree/ptree.hpp>

namespace cv
{
	class Mat;
}


class DataIO
{
public:
	/**
	 * Format the json string and output to file.
	 */
	static void write2json(const std::string& filename, const boost::property_tree::ptree& root);
};

// load and save depth map
class DepthmapIO : public DataIO
{
public:
	static void save(const std::string& filepath, const cv::Mat& depth_mat);

	static void load(const std::string& filepath, cv::Mat& depth_mat);
};

// load and save pixel corresponding ship
class PixelsCorrIO : public DataIO
{
public:
	static void save(const std::string& file_path, const std::string& src_filename, const std::string& tar_filename, const cv::Mat& pixles_corresponding);

	static void load(const std::string& file_path, std::string& src_filename, std::string& tar_filename, cv::Mat& pixles_corresponding);
};



================================================
FILE: depth-estimation/360monodepth/code/cpp/include/depthmap_stitcher.hpp
================================================
#pragma once

#include <vector>
#include <map>
#include <string>

#include <opencv2/opencv.hpp>

class AlignCoeff
{

public:
	void initial(const int grid_width, const int grid_height, const int depthmap_number);

	void set_value_const(const float scale = 0.1, const float offset = 0.0);

	void set_value_mat(const std::vector<cv::Mat>& coeff_scale, const std::vector<cv::Mat>& coeff_offset);

	/**  use random parameter as the initial parameters. */
	void set_value_rand();

	void save(const std::string& file_path, const std::vector<std::string>& filename_list);
	void load(const std::string& file_path);

	// use to parser
	int coeff_rows; // gird height, row number
	int coeff_cols; // grid width, column number
	int coeff_number; // the coefficients (scale & offset) pairs number

	// share the same memory with coeff_scale and coeff_offset
	std::vector<cv::Mat> coeff_scale_mat;
	std::vector<cv::Mat> coeff_offset_mat;

	// a continue memory store coefficient for all image, row major
	std::shared_ptr<double[]> coeff_scale;
	std::shared_ptr<double[]> coeff_offset;
};


/** Output the coefficients parameters */
std::ostream& operator <<(std::ostream& os, const AlignCoeff& di);

class DepthmapStitcher
{

public:
	DepthmapStitcher();

	~DepthmapStitcher();

	/**
	 * Load depth map and pixels corresponding.
	 * Create the depth map internal index, by get the file index from filename and re-assign index based on filename sequence.
	 * The file naming convention please refere @see doc/readme.md.
	 *
	 * @param data_root_dir the root folder of image data.
	 * @param depthmap_filename_list the sub-depth map's file name.
	 */
	void load_data(const std::string& data_root_dir, const std::vector<std::string> depthmap_filename_list);

	/**
	 * Assign depth map and pixels corresponding, and generate proxy file name.
	 * Create the depth internal index, by get the file index from filename and re-assign index based on filename sequence.
	 * The file naming convention please refere @see doc/readme.md.
	 *
	 * @param data_root_dir the root folder of image data.
	 * @param depthmap_original_data the sub-depth map's file name.
	 * @param depthmap_original_ico_index the sub-depth icosahedron face index.
	 * @param pixels_corresponding_list_data the pixel corresponding relation ship between two image.
	 */
	void initial_data(const std::string& data_root_dir,
		const std::vector<cv::Mat>& depthmap_original_data,
		const std::vector<int>& depthmap_original_ico_index,
		const std::map<int, std::map<int, cv::Mat>>& pixels_corresponding_list_data);

	/**
	 * Get the grid size .
	 * The sub-class should call this super-call function.
	 */
	virtual void initial(const int grid_width, const int grid_height);

	void set_coeff(const std::vector<cv::Mat>& coeff_scale,
		const std::vector<cv::Mat>& coeff_offset);

	/**
	 * Compute each depth maps S and A relative reference depth map.
	 * Use the first depth map in depth_map_list as the reference map.
	 * And enumerate all image pairs to compute the scale and offset.
	 */
	virtual void compute_align_coeff() = 0;

	/**
	 * Save and load coeff to and from file.
	 */
	void get_align_coeff(std::vector<cv::Mat>& coeff); 
	void save_align_coeff();

	/**
	 * Use align coefficients parameters to adjust the depth map.
	 */
	void align_depthmap_all();
	void align_depthmap(const int depthmap_intidx);

	/**
	 * Return the aligned depth maps.
	 */
	std::vector<cv::Mat> get_aligned_depthmap() { return depthmap_aligned; }

	/**
	 * save aligned depth map to data root folder.
	 */
	void save_aligned_depthmap();

	/**
	 * Evaluate the error between the original and new depth maps.
	 * Warp the adjusted depth map to others depth map with the corresponding relationship, and compute the error.
	 */
	void report_error();

	float getColorSubpix(const cv::Mat& img, const cv::Point2f pt);

	// depth map's sub map filename regular expression
	int index_digit_number = 3;
	std::string depthmap_filename_regexp = "[a-zA-Z0-9\\_]*\\_disp\\_erp\\_[0-9]{3}.pfm";
	std::string rgb_filename_regexp = "[a-zA-Z0-9\\_]*\\_rgb\\_[0-9]{3}.jpg";
	std::string corr_filename_regexp = "[a-zA-Z0-9\\_]*\\_corr\\_[0-9]{3}\\_[0-9]{3}.json";
	// pixels corresponding filename regular expression
	std::string pixelcorr_filename_exp = "$prefix_corr_$srcindex_$tarindex.json";

	// proxy filename for data fetch from memory
	std::string proxy_filename_depthmap_exp = "depthmap_depth_$index.pymodule";
	//std::string proxy_filename_pixelcorr_exp = "depthmap_corr_$index.json";s

	// output file name expression
	std::string depthmap_aligned_filename_exp = "$prefix_depth_$subindex_aligned.pfm";
	std::string align_coeff_filename_exp = "$prefix_coeff_align.json";

	// cost function terms weight
	float weight_reprojection = 1.0; // re-projection term lambda
	float weight_smooth = 10e-4;     // smooth term lambda
	float weight_scale = 10e-2;      // scale term lambda

	// the reference depth map index, start from 0. 0 is the first depth map index.
	int depthmap_ref_extidx = 0;

	// Ceres solver opinions
	int ceres_num_threads = -1;
	int ceres_max_num_iterations = -1;
	int ceres_max_linear_solver_iterations = -1;
	int ceres_min_linear_solver_iterations = -1;

	// perpixel or pergrid term weight 
	bool projection_per_pixelcost_enable = false;
	bool smooth_pergrid_enable = false;

    // the scalar and offset for each depth map, CV_64FC1 for each depth map
    // std::vector<cv::Mat> scale_list;
    // std::vector<cv::Mat> offset_list;
    AlignCoeff coeff_so;

protected:
	/**
	 * Compute the bilinear interpolation weight int the sparse weight grid.
	 * @param weight_list the offset or scale weight list. It is row-major.
	 * @param image_width the depth width.
	 */
	void get_bilinear_weight(double* weight_list,
		const int image_width, const int image_height,
		const int grid_row_number, const int grid_col_number,
		const double x, const double y);

	// the container of the depth image.
	std::vector<cv::Mat> depthmap_original;

	// the aligned depth map
	std::vector<cv::Mat> depthmap_aligned;

	// the mapping form the internal depth image index to depth file name.
	std::vector<std::string> filename_list;

	// the pixels corresponding between two image, it's row major. the sub-image index is internal index.
	//  [cur_y, cur_x, ref_y, ref_x].
	// The pixels_corresponding_list[3][2] is from 3 to 2. 2 is reference depth image.
	// Note the index are re-assigned based on filename sequence.
	std::map<int, std::map<int, cv::Mat>> pixels_corresponding_list;

	// depth map and corr file folder path
	std::string data_root_dir;
	// the prefix for depth map, coefficients and corresponding
	std::string filename_prefix;

	// the map between internal index and external index
	// external index is the icosahedron sub-image index [0-19], internal index used by Ceres solver.
	std::map<int, int> extidx2intidx;
	std::map<int, int> intidx2extidx;

	// The scale and offset grid size, @see hedman2018instant
	int grid_width = 5; // the grid column number
	int grid_height = 5; // the grid row number
};


================================================
FILE: depth-estimation/360monodepth/code/cpp/include/depthmap_stitcher_enum.hpp
================================================
#pragma once

#include "depthmap_stitcher.hpp"

namespace cv
{
	class Mat;
}

class DepthmapStitcherEnum : public DepthmapStitcher
{

public:
	DepthmapStitcherEnum(float depthmap_optim_overlap_ratio = 0.25);

	~DepthmapStitcherEnum() {};

	/**
	 * Get the grid size .
	 * The sub-class should call this super-call function.
	 */
	virtual void initial(const int grid_width, const int grid_height) override;

	/**
	 * Compute each depth maps S and A relative reference depth map.
	 * Use the first depth map in depth_map_list as the reference map.
	 * And enumerate all image pairs to compute the scale and offset.
	 */
	void compute_align_coeff() override;

	// the overlap area ratio, the selected images pairs overlap area is larger than it.
	float depthmap_optim_overlap_ratio_;

private:

	struct ReprojectionResidual;

	struct SmoothnessResidual_S;
	struct SmoothnessResidual_O;
	struct SmoothnessResidual;
	struct ScaleResidual;
};


================================================
FILE: depth-estimation/360monodepth/code/cpp/include/depthmap_stitcher_group.hpp
================================================
#pragma once

#include "depthmap_stitcher.hpp"

namespace cv
{
	class Mat;
}

class DepthmapStitcherGroup : public DepthmapStitcher
{

public:
	DepthmapStitcherGroup();

	~DepthmapStitcherGroup();

	/**
	 * Get the grid size .
	 * The sub-class should call this super-call function.
	 */
	virtual void initial(const int grid_width, const int grid_height) override;

	/**
	 * Compute each depth maps S and A relative reference depth map.
	 * Use the first depth map in depth_map_list as the reference map.
	 * And enumerate all image pairs to compute the scale and offset.
	 */
	void compute_align_coeff() override;

private:

	struct ReprojectionResidual;
	struct ReprojectionResidual_fixed;

	struct SmoothnessResidual;
	struct SmoothnessResidual_O;
	struct SmoothnessResidual_S;

	struct ScaleResidual;
};


================================================
FILE: depth-estimation/360monodepth/code/cpp/include/depthmap_utility.hpp
================================================
#pragma once

#include <opencv2/opencv.hpp>

class DepthmapUtil
{

public:
	static void normalize(const cv::Mat& depthmap, cv::Mat& depthmap_normalized);

	static void deform(const cv::Mat& depthmap, cv::Mat& depthmap_deformed, const cv::Mat& scale_mat, const cv::Mat& offset_mat);

	static void bilinear_weight(double* weight_list,
		const int image_width, const int image_height,
		const int grid_width, const int grid_height,
		const double x, const double y);

	// normalized the depth map
	static cv::Mat depthmap_orig2norm(const cv::Mat& orig_mat, double& mean, double& dev);
	static cv::Mat depthmap_norm2orig(const cv::Mat& norm_mat, const double mean, const double dev);
};



================================================
FILE: depth-estimation/360monodepth/code/cpp/include/python_binding.hpp
================================================
#pragma once

#include <opencv2/opencv.hpp>

#include <string>
#include <iostream>
#include <map>

// Note: all cv::Mat data depth is double (64F).
/**
 * Initial the depth map alignment module run time environment.
 * 
* @param method The depth map stitcher method name.
 * 
 * @return int is < 0 if there is error.
 */
int init(const std::string &method);

/**
 * Clear the depth map alignment run time environment.
 * 
 * @return int 
 */
int shutdown();

/**
 * Report the error between subimages.
 * 
 * @return int 
 */
int report_aligned_depthmap_error();

/**
 * Create mock data for debug. 
 * @param depthmap The synthetic depth map
 * @param align_coeff The synthetic depth map deformation alignment coefficients.
 * @param pixels_corresponding_list The synthetic depth map pixel corresponding relationship.
 * @param debug_data_type The debug data type, 0 is the simple debug data, 1 is the random debug data.
 * @param frame_number The frame number of the generated debug data.
 */
void create_debug_data(std::vector<cv::Mat> &depthmap,
					   std::vector<cv::Mat> &align_coeff,
					   std::map<int, std::map<int, cv::Mat>> &pixels_corresponding_list,
					   const int debug_data_type,
					   const int frame_number);

/**
* Set the Ceres solver options. Set it to a number less than or equal to 0 to use default value.
 * @param num_threads:
 * @param max_num_iterations:
 * @param max_linear_solver_iterations:
 * @param min_linear_solver_iterations:
 */
int solver_params(const int num_threads,
				  const int max_num_iterations,
				  const int max_linear_solver_iterations,
				  const int min_linear_solver_iterations);

/**
 * Align sub-images depth maps.
 * @param root_dir if is not empty, output aligned depth map and coefficients this folder.
 * @param terms_weight The term wight, projection term, smooth term and regulation term.
 * @param depthmap_original Original depth map.
 * @param depthmap_original_ico_index The original depth maps icosahedron index.
 * @param reference_depthamp_ico_index The reference depth map index of the icosahedron.
 * @param pixels_corresponding_list The pixel corresponding relationship between difference sub-images.
 * @param align_coeff_grid_height the align coefficients gird height, if use default set it to -1.
 * @param align_coeff_grid_width the align coefficients gird width, if use default set it to -1.
 * @param reproj_perpixel_enable the energy term use perpixel cost.
 * @param smooth_pergrid_enable the smooth term use pergrid cost.
 * @param align_coeff_initial_scale the initial scale coefficient
 * @param align_coeff_initial_offset the initial offset coefficient.
 * @param log_level the glog output level, 0 is Warning, 1 is Info
 * @param depthmap_aligned Aligned depth maps.
 * @param align_coeff The depth align coefficients.
 */
int depthmap_stitch(
	const std::string &root_dir,
	const std::vector<float> &terms_weight,
	const std::vector<cv::Mat> &depthmap_original,
	const std::vector<int> &depthmap_original_ico_index,
	const int reference_depthamp_ico_index,
	const std::map<int, std::map<int, cv::Mat>> &pixels_corresponding_list,
	const int align_coeff_grid_height,
	const int align_coeff_grid_width,
	const bool reproj_perpixel_enable,
	const bool smooth_pergrid_enable,
	const std::vector<cv::Mat> &align_coeff_initial_scale,
	const std::vector<cv::Mat> &align_coeff_initial_offset,
	std::vector<cv::Mat> &depthmap_aligned,
	std::vector<cv::Mat> &align_coeff);


================================================
FILE: depth-estimation/360monodepth/code/cpp/include/timer.hpp
================================================
#include <iostream>
#include <chrono>
#include <ctime>
#include <cmath>

class Timer
{
public:
    /**
     * start timer.
     * @return (void)
     */
    void start()
    {
        start_time = std::chrono::system_clock::now();
        timer_running = true;
    }

    /**
     * Stop timer.
     * @return (void)
     */
    void stop()
    {
        end_time = std::chrono::system_clock::now();
        timer_running = false;
    }

    /**
     * Return the duration since start.
     * @return the duration in millisecond
     */
    double duration_ms()
    {
        std::chrono::time_point<std::chrono::system_clock> end_time_;

        if (timer_running)
        {
            end_time_ = std::chrono::system_clock::now();
        }
        else
        {
            end_time_ = end_time;
        }

        return std::chrono::duration_cast<std::chrono::milliseconds>(end_time_ - start_time).count();
    }

    /**
     * Return the duration since start.
     * @return the duration in second
     */
    double duration_s()
    {
        return duration_ms() / 1000.0;
    }

private:
    std::chrono::time_point<std::chrono::system_clock> start_time;

    std::chrono::time_point<std::chrono::system_clock> end_time;
    
    bool timer_running = false; // true, the timer is started.
};



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/README.md
================================================



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/LICENSE.txt
================================================



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/MANIFEST.in
================================================
include pyproject.toml

# Include the README
include *.md

# Include the license file
include LICENSE.txt

# Include setup.py
include setup.py

# Include the data files
recursive-include data *



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/pyproject.toml
================================================
[build-system]
# These are the assumed default build requirements from pip:
# https://pip.pypa.io/en/stable/reference/pip/#pep-517-and-518-support
requires = ["setuptools>=40.8.0", "wheel"]
build-backend = "setuptools.build_meta"



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/reinstall.bat
================================================
rmdir /S /Q build
del dist/instaOmniDepth-0.1.0-cp38-cp38-win_amd64.whl
python ./setup.py build
python ./setup.py bdist_wheel
pip uninstall --yes instaOmniDepth
pip install dist/instaOmniDepth-0.1.0-cp38-cp38-win_amd64.whl


================================================
FILE: depth-estimation/360monodepth/code/cpp/python/reinstall.sh
================================================
# !/bin/bash
python_vers=`python -V 2>&1`
split_vers=(${python_vers//./ })
compact_vers=${split_vers[1]}${split_vers[2]}
echo $compact_vers
echo "== 0) build project =="
cd ../build/
make -j 
cd ../python/
echo "== 1) remove build cache files =="
rm -rvf build/*
rm -rvf dist/*
# rm dist/instaOmniDepth-0.1.0-cp${compact_vers}-cp${compact_vers}-linux_x86_64.whl
echo "== 2) build python binding =="
python ./setup.py build
echo "== 3) build wheell package =="
python ./setup.py bdist_wheel
echo "== 4) reinstall package =="
pip uninstall --yes instaOmniDepth
pip install dist/instaOmniDepth-0.1.0-cp${compact_vers}-cp${compact_vers}-linux_x86_64.whl



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/setup.cfg
================================================
[metadata]
# This includes the license file(s) in the wheel.
# https://wheel.readthedocs.io/en/stable/user_guide.html#including-license-files-in-the-generated-wheel-file
license_files = LICENSE.txt
# [build_ext]
# debug = 1

[options]
setup_requires =
    wheel


================================================
FILE: depth-estimation/360monodepth/code/cpp/python/setup.py
================================================
from setuptools import setup, find_packages, Extension
import pathlib

import numpy
import glob
import os
import shutil
import sys

python_version = str(sys.version_info.major) + str(sys.version_info.minor)

#----include dynamic link libraries & build options----
dll_filepath_list = []
extra_compile_args_ = []
extra_link_args_ = []
dll_src_dir = None

if os.name == 'nt':
    # 3rd-party libs
    opencv_include_dir = "D:/libraries_windows/opencv/opencv-4.5.3-vc14_vc15/build/include/"
    opencv_lib_dir = "D:/libraries_windows/opencv/opencv-4.5.3-vc14_vc15/build/x64/vc15/lib/"
    opencv_lib_files = ['opencv_world453', 'depth_stitch']
    depthmap_align_lib_dir = '../lib/Release/'
    dll_filepath_list.append('../bin/Release/depth_stitch.dll')
    dll_src_dir = "./dll/*.dll"
    dist_dll_ext = "*.dll"
    # generate *.pdb for MSVC debugging
    extra_compile_args_ = ["/Zi"]
    extra_link_args_ = ["/DEBUG", "/OPT:REF", "/OPT:ICF"]
elif os.name == 'Linux' or os.name == 'posix':
    opencv_include_dir = "/usr/include/opencv4/"
    opencv_lib_dir = "/usr/lib/x86_64-linux-gnu/"
    opencv_lib_files = ['opencv_core', 'opencv_imgproc', 'depth_stitch']
    depthmap_align_lib_dir = '../lib/'
    dll_filepath_list.append('../lib/libdepth_stitch.so')
    dll_filepath_list.append('../lib/EigenSolvers.cpython-{}-x86_64-linux-gnu.so'.format(python_version))
    dll_src_dir = "./so/*"
    dist_dll_ext = "*.so*"
    extra_compile_args_ = ["-O3", "-DNDEBUG"]
    extra_link_args_ = ["-Wl,-rpath=$ORIGIN/"]
else:
    msg = "System {} do not suport.".format(os.name)
    raise RuntimeError(msg)

if len(dll_filepath_list) == 0:
    print("Warning: Do not find dependent dynamic library.")

# clean & copy dynamic link libs
dll_tar_dir = "./instaOmniDepth/"
for file in glob.glob(dll_tar_dir + dist_dll_ext):
    os.remove(file)
for file in glob.glob(dll_src_dir):
    dll_filepath_list.append(file)
for item in dll_filepath_list:
    shutil.copy(item, dll_tar_dir)

print("Package adding the following dynamic libraries: \n {}".format(dll_filepath_list))

#----define the extension module----
depthmapAlignExt = Extension('instaOmniDepth.depthmapAlign',
                            sources=['./instaOmniDepth/depthmapAlignModule.cpp'],
                            include_dirs=[numpy.get_include(), '../include/', opencv_include_dir],
                            library_dirs=[opencv_lib_dir, depthmap_align_lib_dir],
                            extra_compile_args=extra_compile_args_,
                            extra_link_args=extra_link_args_,
                            libraries=opencv_lib_files)

#----generate the package----
# Get the long description from the README file
pwd_dir_path = pathlib.Path(__file__).parent.resolve()
long_description = (pwd_dir_path / 'README.md').read_text(encoding='utf-8')
setup(
    name='instaOmniDepth',
    version='0.1.0',
    description='Align sub-image depth maps',
    long_description=long_description,
    long_description_content_type='text/markdown',
    package_dir={'': '.'},
    packages=find_packages(where='.'),
    ext_modules=[depthmapAlignExt],
    #package_dir={'instaOmniDepth': 'depthmapAlignPackage'},
    install_requires=['numpy'],
    package_data={
        'instaOmniDepth': [dist_dll_ext],
    },
)



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/instaOmniDepth/__init__.py
================================================
import os

if os.name == 'Linux' or os.name == 'posix':
    #print("{}".format(os.path.dirname(__file__)))
    #os.environ['LD_LIBRARY_PATH'] = os.getcwd()
    os.environ['LD_LIBRARY_PATH'] = os.path.dirname(__file__)
    #print("$LD_LIBRARY_PATH: {}".format(os.environ['LD_LIBRARY_PATH']))



================================================
FILE: depth-estimation/360monodepth/code/cpp/python/instaOmniDepth/depthmapAlignModule.cpp
================================================
#define PY_SSIZE_T_CLEAN
#include <Python.h>
#include <python_binding.hpp>

#include <opencv2/opencv.hpp>

#include <vector>
#include <iostream>
#include <math.h>

//#undef _DEBUG
//#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
#include <numpy/arrayobject.h>
#include <numpy/ndarraytypes.h>

static PyObject* cv2numpy(const cv::Mat& mat_array_)
{
	cv::Mat mat_array;
	// check cv mat is double
	if (mat_array_.depth() != CV_64F)
	{
		//std::cout << "convert CV mat to float64" << std::endl;
		mat_array_.convertTo(mat_array, CV_64F);
	}
	else {
		mat_array = mat_array_;
	}

	//
	int nbRow = mat_array.rows;
	int nbCol = mat_array.cols;

	//Set the size of the numpy array
	npy_intp dims[2];
	dims[0] = nbRow;
	dims[1] = nbCol;
	double* mat = (double*)mat_array.data;
	if (mat == NULL)
	{
		PyObject* objMat = PyArray_EMPTY(2, dims, NPY_FLOAT64, 0);
		if (objMat == NULL)
		{
			PyErr_SetString(PyExc_RuntimeError, "allocMatrix : Could not allocated memory\n");
			return NULL;
		}
		return objMat;
	}

	PyArray_Descr* descr = PyArray_DescrFromType(NPY_FLOAT64);

	int dim_number = 2;

	PyArray_Dims strides = { NULL, 0 };
	strides.len = 2;
	strides.ptr = PyDimMem_NEW(2);
	strides.ptr[1] = sizeof(double); // element size
	strides.ptr[0] = (nbCol)*strides.ptr[1];

	PyObject* objMat = PyArray_NewFromDescr(&PyArray_Type, descr, dim_number, dims, strides.ptr, NULL, NPY_ARRAY_WRITEABLE, NULL);

	if (!objMat)
	{
		if (PyErr_Occurred())
		{
			PyErr_Print();
			PyErr_Clear();
			Py_XDECREF(objMat);
		}
	}
	void* numpy_data = PyArray_DATA((PyArrayObject*)objMat);
	memcpy(numpy_data, (void*)mat_array.data, sizeof(double) * nbRow * nbCol);
	//Py_INCREF(objMat);
	return objMat;
}

static int numpy2cv(PyObject* numpy_array_, cv::Mat& mat_array)
{
	if (numpy_array_ == NULL)
	{
		PyErr_SetString(PyExc_RuntimeError, "Object is NULL!\n");
		return -1;
	}
	if (PyArray_Check(numpy_array_) == false)
	{
		std::cout << "Object is not numpy object" << std::endl;
		PyErr_SetString(PyExc_RuntimeError, "Object is not numpy object!\n");
		return -1;
	}

	PyArrayObject* numpy_array = NULL;

	// check numpy data type
	int obj_type = PyArray_TYPE((PyArrayObject*)numpy_array_);
	if (NPY_FLOAT64 != obj_type)
	{
		PyErr_SetString(PyExc_RuntimeError, "The depth map data type should be float64.\n");
		return -1;
		//return cv::Mat();
		//std::cout << "Convert the data to float64 type" << std::endl;
		//numpy_array = (PyArrayObject*)PyArray_CastToType((PyArrayObject*)numpy_array_, PyArray_DescrFromType(NPY_FLOAT64), 0);
		//numpy_array = PyArray_FROM_OTF(numpy_array_, NPY_DOUBLE, NPY_ARRAY_C_CONTIGUOUS);
	}
	//else {
	//    numpy_array  = (PyArrayObject*)(numpy_array_);
	//    //numpy_array = PyArray_FROM_OTF(numpy_array_, NPY_FLOAT32, NPY_ARRAY_C_CONTIGUOUS);
	//}

	numpy_array = (PyArrayObject*)PyArray_FROM_OTF(numpy_array_, NPY_DOUBLE, NPY_ARRAY_IN_ARRAY);
	if (numpy_array == NULL)
		return -1;

	int ndim = PyArray_NDIM(numpy_array);

	if (ndim != 2)
	{
		PyErr_SetString(PyExc_RuntimeError, "The depth map size should be [height, width].\n");
		return -1;
	}
	//
	npy_intp* array_shape = PyArray_SHAPE(numpy_array);
	int row_number = (int)array_shape[0];
	int col_number = (int)array_shape[1];

	char* data = (char*)PyArray_DATA(numpy_array);
	mat_array = cv::Mat(row_number, col_number, CV_64FC1);
	memcpy(mat_array.data, data, sizeof(double) * row_number * col_number * 1);

	Py_DECREF(numpy_array);

	return 0;
}

static PyObject* create_debug_data(PyObject* self, PyObject* args)
{
	std::vector<cv::Mat> depthmap;
	std::vector<cv::Mat> align_coeff;
	std::map<int, std::map<int, cv::Mat>> pixels_corresponding_list;

	int debug_data_type;
	int frame_number;

	if (!PyArg_ParseTuple(args, "ii",
		&debug_data_type,
		&frame_number))
	{
		PyErr_SetString(PyExc_RuntimeError, "Input data parse error!\n");
		return NULL;
	}

	create_debug_data(depthmap,
		align_coeff,
		pixels_corresponding_list,
		debug_data_type,
		frame_number
	);

	// check the return value
	if (depthmap.size() == 0)
	{
		PyErr_SetString(PyExc_RuntimeError, "The generated depth map list is empty!\n");
		return NULL;
	}
	if (align_coeff.size() == 0)
	{
		PyErr_SetString(PyExc_RuntimeError, "The generated depth map coefficients list is empty!\n");
		return NULL;
	}

	// encapsulate to python objects
	PyObject* depthmap_list_py = PyList_New(depthmap.size());
	for (long unsigned int index = 0; index < depthmap.size(); index++)
	{
		if (depthmap[index].size().area() == 0)
		{
			PyErr_SetString(PyExc_RuntimeError, "The depth map generation error!\n");
			return NULL;
		}
		PyList_SetItem(depthmap_list_py, index, cv2numpy(depthmap[index]));
	}

	PyObject* align_coeff_py = PyList_New(align_coeff.size());
	for (long unsigned int index = 0; index < align_coeff.size(); index++)
	{
		if (align_coeff[index].size().area() == 0)
		{
			PyErr_SetString(PyExc_RuntimeError, "The align coefficients generation error!\n");
			return NULL;
		}
		PyList_SetItem(align_coeff_py, index, cv2numpy(align_coeff[index]));
	}

	PyObject* pixels_corresponding_list_py = PyDict_New();
	for (auto iter_src = pixels_corresponding_list.begin(); iter_src != pixels_corresponding_list.end(); ++iter_src)
	{
		int key_src = iter_src->first;
		std::map<int, cv::Mat>& value_tar = pixels_corresponding_list[key_src];

		PyObject* value_tar_py = PyDict_New();
		for (auto iter_tar = value_tar.begin(); iter_tar != value_tar.end(); ++iter_tar)
		{
			int key_tar = iter_tar->first;
			cv::Mat& pixel_corr = value_tar[key_tar];

			PyObject* key_tar_py = PyLong_FromLong(key_tar);
			PyDict_SetItem(value_tar_py, key_tar_py, cv2numpy(pixel_corr));
		}
		PyObject* key_src_py = PyLong_FromLong(key_src);
		PyDict_SetItem(pixels_corresponding_list_py, key_src_py, value_tar_py);
	}

	// return
	return PyTuple_Pack(3, depthmap_list_py, align_coeff_py, pixels_corresponding_list_py);
}

static PyObject* ceres_solver_option(PyObject* self, PyObject* args)
{
	int num_threads;
	int max_num_iterations;
	int max_linear_solver_iterations;
	int min_linear_solver_iterations;

	if (!PyArg_ParseTuple(args, "iiii",
		&num_threads,
		&max_num_iterations,
		&max_linear_solver_iterations,
		&min_linear_solver_iterations))
	{
		PyErr_SetString(PyExc_RuntimeError, "Input data parse error!\n");
		return NULL;
	}
	int result = solver_params(num_threads, max_num_iterations, max_linear_solver_iterations, min_linear_solver_iterations);
	return PyLong_FromLong(result);
}


static PyObject* depthmap_stitch(PyObject* self, PyObject* args)
{
	const char* root_dir;               // str

	PyObject* terms_weight;             // list[float]
	PyObject* depthmap_original_list;    // list[numpy]
	PyObject* depthmap_original_ico_index;// list[int]
	PyObject* pixels_corresponding_map; // dict{int:{int, numpy}}
	PyObject* align_coeff_initial_scale; // list[numpy]
	PyObject* align_coeff_initial_offset; // list[numpy]

	int reference_depthmap_index;        // the reference depth map index of ico sub depth maps.
	int align_coeff_grid_height;        // int
	int align_coeff_grid_width;
	int debug_wait_for_attach;            // 
	int reproj_perpixel_enable;
	int smooth_pergrid_enable;

	if (!PyArg_ParseTuple(args, "sO!O!O!iO!iiiiO!O!i",
		&root_dir,
		&PyList_Type, &terms_weight,
		&PyList_Type, &depthmap_original_list,
		&PyList_Type, &depthmap_original_ico_index,
		&reference_depthmap_index,
		&PyDict_Type, &pixels_corresponding_map,
		&align_coeff_grid_height,
		&align_coeff_grid_width,
		&reproj_perpixel_enable,
		&smooth_pergrid_enable,
		&PyList_Type, &align_coeff_initial_scale,
		&PyList_Type, &align_coeff_initial_offset,
		&debug_wait_for_attach))
	{
		PyErr_SetString(PyExc_RuntimeError, "Input data parse error!\n");
		return NULL;
	}

	// // 
	// std::cout << "debug_wait_for_attach:" << debug_wait_for_attach << std::endl;
	// std::cout << "reference_depthmap_index:" <<reference_depthmap_index << std::endl;
	// std::cout << "align_coeff_grid_height: " << align_coeff_grid_height << std::endl;
	// std::cout << "align_coeff_grid_width: " << align_coeff_grid_width << std::endl;
	// std::cout << "reproj_perpixel_enable: " << reproj_perpixel_enable << std::endl;
	// std::cout << "smooth_pergrid_enable: " << smooth_pergrid_enable << std::endl;

	std::cout << "Parse python data:" << std::endl;
	// 1) parse parameters
	Py_ssize_t terms_weight_size = PyList_Size(terms_weight);
	std::vector<float> terms_weight_cpp;
	//std::cout << "cost function terms weights are: " << std::endl;
	for (int index = 0; index < terms_weight_size; index++)
	{
		PyObject* item = PyList_GetItem(terms_weight, index);
		terms_weight_cpp.push_back((float)PyFloat_AsDouble(item));
		//std::cout << PyFloat_AS_DOUBLE(item) << std::endl;
	}
	if (PyErr_Occurred()) {
		PyErr_SetString(PyExc_RuntimeError, "Terms weight parse error!\n");
		return NULL;
	}

	// sub-image depth maps
	std::cout << "- Parsing sub-image depth maps" << std::endl;
	Py_ssize_t depthmap_original_list_size = PyList_Size(depthmap_original_list);
	Py_ssize_t depthmap_original_index_list_size = PyList_Size(depthmap_original_ico_index);

	if (debug_wait_for_attach > 0)
	{
		std::cout << "Attach me! Input e to exist this process and any other input will continue."; // no flush needed
		char n; std::cin >> n;
		if (n == 'e')
			exit(0);
	}

	if (depthmap_original_list_size != depthmap_original_index_list_size)
	{
		PyErr_SetString(PyExc_RuntimeError, "The depth map size is not equal index list size!\n");
		return NULL;
	}
	std::vector<cv::Mat> depthmap_original_cpp;
	std::vector<int> depthmap_original_index_cpp;
	for (int index = 0; index < depthmap_original_list_size; index++)
	{
		PyObject* mat_index = PyList_GetItem(depthmap_original_ico_index, index);
		depthmap_original_index_cpp.push_back((int)PyLong_AsLong(mat_index));

		PyObject* item = PyList_GetItem(depthmap_original_list, index);
		cv::Mat mat_data;
		//std::cout << mat_data << std::endl;
		if (numpy2cv(item, mat_data) < 0)
		{
			//PyErr_SetString(PyExc_RuntimeError, "The depth map is empty!\n");
			return NULL;
		}
		else
		{
			depthmap_original_cpp.push_back(mat_data);
		}
	}

	// pixels_corresponding_map
	std::cout << "- Parsing pixels_corresponding_map" << std::endl;
	if (PyDict_Check(pixels_corresponding_map) == false)
	{
		PyErr_SetString(PyExc_RuntimeError, "pixels_corresponding_map is not a dictory object!\n");
		return NULL;
	}
	Py_ssize_t dict_length = PyDict_Size(pixels_corresponding_map);
	if (dict_length < (depthmap_original_list_size - 1))
	{
		PyErr_SetString(PyExc_RuntimeError, "pixels_corresponding_map source map length is wrong!\n");
		return NULL;
	}
	std::map<int, std::map<int, cv::Mat>> pixels_corresponding_list;
	PyObject* pixel_corr_srckeys_list = PyDict_Keys(pixels_corresponding_map);
	int pixle_corr_srckeys_size = (int)PyList_Size(pixel_corr_srckeys_list);
	for (int src_index = 0; src_index < pixle_corr_srckeys_size; src_index++)
	{
		PyObject* srckey_py = PyList_GetItem(pixel_corr_srckeys_list, Py_ssize_t(src_index));
		long srckey_long = PyLong_AsLong(srckey_py);
		PyObject* pixel_map_tar = PyDict_GetItem(pixels_corresponding_map, srckey_py);
		if (pixel_map_tar == NULL)
		{
			char msg[128];
			sprintf(msg, "The pixel corresponding relationship source index %ld is missing!\n", srckey_long);
			PyErr_SetString(PyExc_RuntimeError, msg);
			continue;
		}

		Py_ssize_t tar_length = PyDict_Size(pixel_map_tar);
		if (tar_length < (depthmap_original_list_size - 1))
		{
			PyErr_SetString(PyExc_RuntimeError, "pixels_corresponding_map tar map length is wrong!\n");
			return NULL;
		}

		// convert target data list
		std::map<int, cv::Mat> pixels_corresponding_list_tar;
		PyObject* pixel_corr_tarkeys_list = PyDict_Keys(pixel_map_tar);
		int pixle_corr_tarkeys_size = (int)PyList_Size(pixel_corr_tarkeys_list);
		for (int tar_index = 0; tar_index < pixle_corr_tarkeys_size; tar_index++)
		{
			PyObject* tarkey_py = PyList_GetItem(pixel_corr_tarkeys_list, Py_ssize_t(tar_index));
			long tarkey_long = PyLong_AsLong(tarkey_py);
			PyObject* map_mat = PyDict_GetItem(pixel_map_tar, tarkey_py);
			if (map_mat == NULL)
			{
				char msg[128];
				sprintf(msg, "The pixel corresponding relationship target index %ld is missing!\n", tarkey_long);
				PyErr_SetString(PyExc_RuntimeError, msg);
				continue;
			}
			cv::Mat mat_data;
			// std::cout << mat_data << std::endl;
			if (numpy2cv(map_mat, mat_data) < 0)
			{
				PyErr_SetString(PyExc_RuntimeError, "The pixel corresponding mat is empty!\n");
				return NULL;
			}
			else
				pixels_corresponding_list_tar[tarkey_long] = mat_data;
		}
		pixels_corresponding_list[srckey_long] = pixels_corresponding_list_tar;
	}

	// alignment coefficients
	std::cout << "- Parsing alignment coefficients" << std::endl;
	Py_ssize_t align_coeff_initial_scales_size = PyList_Size(align_coeff_initial_scale);
	Py_ssize_t align_coeff_initial_offset_size = PyList_Size(align_coeff_initial_offset);
	if (align_coeff_initial_scales_size != align_coeff_initial_offset_size)
	{
		PyErr_SetString(PyExc_RuntimeError, "Alignment coefficient list size is not equal!\n");
		return NULL;
	}

	std::vector<cv::Mat> align_coeff_initial_scale_cpp;
	std::vector<cv::Mat> align_coeff_initial_offset_cpp;
	for (int index = 0; index < align_coeff_initial_scales_size; index++)
	{
		PyObject* scale_item = PyList_GetItem(align_coeff_initial_scale, index);
		cv::Mat scale_mat_data;
		if (numpy2cv(scale_item, scale_mat_data) < 0)
		{
			PyErr_SetString(PyExc_RuntimeError, "Alignment coefficient scale mat parse error!\n");
			return NULL;
		}
		else
			align_coeff_initial_scale_cpp.push_back(scale_mat_data);

		PyObject* offset_item = PyList_GetItem(align_coeff_initial_offset, index);
		cv::Mat offset_mat_data;
		if (numpy2cv(offset_item, offset_mat_data) < 0)
		{
			PyErr_SetString(PyExc_RuntimeError, "Alignment coefficient scale mat parse error!\n");
			return NULL;
		}
		else
			align_coeff_initial_offset_cpp.push_back(offset_mat_data);
	}

	// 2) compute the coefficients
	std::vector<cv::Mat> depthmap_aligned;
	std::vector<cv::Mat> align_coeff;

	std::cout << "Aligning depth maps..." << std::endl;
	depthmap_stitch(
		root_dir,
		terms_weight_cpp,
		depthmap_original_cpp,
		depthmap_original_index_cpp,
		reference_depthmap_index,
		pixels_corresponding_list,
		align_coeff_grid_height,
		align_coeff_grid_width,
		reproj_perpixel_enable,
		smooth_pergrid_enable,
		align_coeff_initial_scale_cpp,
		align_coeff_initial_offset_cpp,
		depthmap_aligned,
		align_coeff);

	std::cout << "return alignment coefficient and depth maps..." << std::endl;
	// 3) return he parameters
	PyObject* depthmap_aligned_py = PyList_New(depthmap_aligned.size());
	for (long unsigned int index = 0; index < depthmap_aligned.size(); index++)
	{
		PyList_SetItem(depthmap_aligned_py, index, cv2numpy(depthmap_aligned[index]));
		//PyList_SetItem(depthmap_aligned_py, index, PyLong_FromLong(index));
		//std::cout << index << depthmap_aligned[index].size() << std::endl;
	}
	//Py_INCREF(depthmap_aligned_py);

	PyObject* align_coeff_py = PyList_New(align_coeff.size());
	for (long unsigned int index = 0; index < align_coeff.size(); index++)
	{
		PyList_SetItem(align_coeff_py, index, cv2numpy(align_coeff[index]));
		//PyList_SetItem(align_coeff_py, index, PyLong_FromLong(index));
		//std::cout << index << align_coeff[index].size() << std::endl;
	}
	//Py_INCREF(depthmap_aligned_py);

	return PyTuple_Pack(2, depthmap_aligned_py, align_coeff_py);
}

static PyObject *init(PyObject *self, PyObject *args)
{
	const char* method;                 // str
	if (!PyArg_ParseTuple(args, "s", &method))
	{
		PyErr_SetString(PyExc_RuntimeError, "Input data parse error!\n");
		return NULL;
	}

	int result = init(method);
	return PyLong_FromLong(result);
}

static PyObject *shutdown(PyObject *self, PyObject *args)
{
	int result = shutdown();
	return PyLong_FromLong(result);
}

static PyObject *report_aligned_depthmap_error(PyObject *self, PyObject *args)
{
	int result = report_aligned_depthmap_error();
	return PyLong_FromLong(result);
}


static PyObject* python_numpy_mat_test(PyObject* self, PyObject* args)
{
	PyArrayObject* numpy_array;

	/*  parse single numpy array argument */
	if (!PyArg_ParseTuple(args, "O!", &PyArray_Type, &numpy_array))
	{
		PyErr_SetString(PyExc_RuntimeError, "allocMatrix : Could not allocated memory\n");
		return NULL;
	}

	cv::Mat data;
	numpy2cv((PyObject*)numpy_array, data);

	std::cout << "OpenCV Mat : M = " << std::endl
		<< " " << data << std::endl
		<< std::endl;

	PyObject* data_numpy = cv2numpy(data);
	return data_numpy;

	Py_RETURN_NONE;
}

// Module's Function Definition structure
static PyMethodDef depthmap_align_methods[] = {
	{"create_debug_data", create_debug_data, METH_VARARGS, "Generate synthetic data for testing depth map align."},
	{"depthmap_stitch", depthmap_stitch, METH_VARARGS, "align the depth map and return the align depth map and coefficient."},
	{"ceres_solver_option", ceres_solver_option, METH_VARARGS, "Set the Ceres solver option."},
	{"init", init, METH_VARARGS, "Initial depth map alignment module."},
	{"shutdown", shutdown, METH_VARARGS, "Clean the depth map alignment pyton module."},
	{"report_aligned_depthmap_error", report_aligned_depthmap_error, METH_VARARGS, "Report the error between the all subimage's aligned depth maps."},
	{"python_numpy_mat_test", python_numpy_mat_test, METH_VARARGS, "test Numpy data transformation."},
	{NULL, NULL, 0, NULL} };

// Module Definition structure
static struct PyModuleDef depthmapAlignModule = {
	PyModuleDef_HEAD_INIT,
	"depthmapAlign",
	"Align depth maps Module",
	-1,
	depthmap_align_methods };

// Initializes module using
// TODO Numpy universal functions https://numpy.org/doc/stable/reference/ufuncs.html
PyMODINIT_FUNC PyInit_depthmapAlign(void)
{
	PyObject* module;
	module = PyModule_Create(&depthmapAlignModule);
	if (module == NULL)
		return NULL;

	// numpy Initializes
	import_array();
	if (PyErr_Occurred())
		return NULL;
	return module;
}


================================================
FILE: depth-estimation/360monodepth/code/cpp/src/data_imitator.cpp
================================================
#include "data_imitator.hpp"
#include "pfm_io.hpp"
#include "data_io.hpp"
#include "depthmap_utility.hpp"
#include "depthmap_stitcher.hpp"

#include <opencv2/opencv.hpp>
#include <glog/logging.h>

#include <string>
#include <iostream>
#include <regex>
#include <filesystem>
namespace fs = std::filesystem;

DataImitator::DataImitator()
{

}

DataImitator::~DataImitator()
{

}

std::vector<cv::Mat> DataImitator::make_depthmap_pair_simple()
{
	// 0) create depth map
	cv::Mat depthmap_tar_data = cv::Mat::ones(depthmap_hight, depthmap_width, CV_64FC1);
	int counter = 0;
	for (int row_index = 0; row_index < depthmap_tar_data.rows; row_index++)
	{
		for (int col_index = 0; col_index < depthmap_tar_data.cols; col_index++)
		{
			depthmap_tar_data.at<double>(row_index, col_index) = counter;
			counter++;
		}
	}
	cv::Mat depthmap_ref_data = depthmap_tar_data * depth_scale + depth_offset;

	depthmap_list.push_back(depthmap_ref_data);
	depthmap_list.push_back(depthmap_tar_data);
	return depthmap_list;
}


void DataImitator::output_date()
{
	// 0) output coefficients to *.json file.
	AlignCoeff aligncoeff;
	aligncoeff.initial(coeff_grid_width, coeff_grid_height, 2);
	aligncoeff.coeff_scale_mat.clear();
	aligncoeff.coeff_scale_mat.push_back(coeff_list[0]);
	aligncoeff.coeff_scale_mat.push_back(coeff_list[2]);

	aligncoeff.coeff_offset_mat.clear();
	aligncoeff.coeff_offset_mat.push_back(coeff_list[1]);
	aligncoeff.coeff_offset_mat.push_back(coeff_list[3]);

	std::vector<std::string> filename_list;
	filename_list.push_back(depthmap_ref_filepath);
	filename_list.push_back(depthmap_tar_filepath);
	aligncoeff.save(json_filepath, filename_list);

	// 1) output to .pfm
	if (!depthmap_ref_filepath.empty() && !depthmap_tar_filepath.empty())
	{
		DepthmapIO::save(depthmap_ref_filepath, depthmap_list[0]);
		DepthmapIO::save(depthmap_tar_filepath, depthmap_list[1]);
	}

	// 2) output pixel corresponding to JSON file.
	if (!depthmap_ref_filepath.empty() && !depthmap_tar_filepath.empty() && !corr_ref2tar_filepath.empty() && !corr_tar2ref_filepath.empty())
	{
		std::string depthmap_ref_filename = fs::path(depthmap_ref_filepath).filename().string();
		std::string depthmap_tar_filename = fs::path(depthmap_tar_filepath).filename().string();
		PixelsCorrIO::save(corr_ref2tar_filepath, depthmap_ref_filename, depthmap_tar_filename, corresponding_mat_ref2tar);
		PixelsCorrIO::save(corr_tar2ref_filepath, depthmap_tar_filename, depthmap_ref_filename, corresponding_mat_tar2ref);
	}
}

std::vector<cv::Mat> DataImitator::make_depthmap_pair_random()
{
	// 0) get random coefficient
	std::vector<cv::Mat> depthmap_pair = make_depthmap_pair_simple();

	// 1) deform the depth map 
	DepthmapUtil::deform(depthmap_pair[0], depthmap_pair[1], coeff_list[2], coeff_list[3]);

	depthmap_list.clear();
	depthmap_list.push_back(depthmap_pair[1]); // reference depth map
	depthmap_list.push_back(depthmap_pair[0]);
	return depthmap_list;
}

std::vector<cv::Mat> DataImitator::make_aligncoeffs_simple()
{
	int ref_image_idx = 0;
	//
	//std::vector<cv::Mat> coeffs_list;
	cv::Mat ref_scale = cv::Mat::ones(coeff_grid_height, coeff_grid_width, CV_64FC1);
	coeff_list.push_back(ref_scale);
	cv::Mat ref_offset = cv::Mat::zeros(coeff_grid_height, coeff_grid_width, CV_64FC1);
	coeff_list.push_back(ref_offset);
	//
	cv::Mat dest_scale = cv::Mat::ones(coeff_grid_height, coeff_grid_width, CV_64FC1) * depth_scale;
	coeff_list.push_back(dest_scale);
	cv::Mat dest_offset = cv::Mat::ones(coeff_grid_height, coeff_grid_width, CV_64FC1) * depth_offset;
	coeff_list.push_back(dest_offset);

	return coeff_list;
}


std::vector<cv::Mat> DataImitator::make_aligncoeffs_random()
{
	coeff_list.clear();
	cv::Mat ref_scale = cv::Mat::ones(coeff_grid_height, coeff_grid_width, CV_64FC1);
	coeff_list.push_back(ref_scale);
	cv::Mat ref_offset = cv::Mat::zeros(coeff_grid_height, coeff_grid_width, CV_64FC1);
	coeff_list.push_back(ref_offset);

	// the random target depth map coefficients
	double scale_rand_max = 4;
	double scale_rand_min = -4;
	cv::Mat dest_scale(coeff_grid_height, coeff_grid_width, CV_64FC1);
	cv::randu(dest_scale, cv::Scalar(scale_rand_min), cv::Scalar(scale_rand_max));
	coeff_list.push_back(dest_scale);

	double offset_rand_max = 10;
	double offset_rand_min = -10;
	cv::Mat dest_offset(coeff_grid_height, coeff_grid_width, CV_64FC1);
	cv::randu(dest_offset, cv::Scalar(offset_rand_min), cv::Scalar(offset_rand_max));
	coeff_list.push_back(dest_offset);
	return coeff_list;
}

std::map<int, std::map<int, cv::Mat>> DataImitator::make_corresponding_json()
{
	// 0) make corresponding relationship mat
	int hight_boundary = depthmap_hight * (1.0 - depthmap_overlap_ratio);
	corresponding_mat_ref2tar = cv::Mat((depthmap_hight - hight_boundary) * depthmap_width, 4, CV_64FC1);
	corresponding_mat_tar2ref = cv::Mat((depthmap_hight - hight_boundary) * depthmap_width, 4, CV_64FC1);

	for (int index_row = 0; index_row < depthmap_hight - hight_boundary; index_row++)
	{
		for (int index_col = 0; index_col < depthmap_width; index_col++)
		{
			int index = index_row * depthmap_width + index_col;
			corresponding_mat_ref2tar.at<double>(index, 0) = hight_boundary + index_row;
			corresponding_mat_ref2tar.at<double>(index, 1) = index_col;
			corresponding_mat_ref2tar.at<double>(index, 2) = index_row;
			corresponding_mat_ref2tar.at<double>(index, 3) = index_col;

			corresponding_mat_tar2ref.at<double>(index, 0) = index_row;
			corresponding_mat_tar2ref.at<double>(index, 1) = index_col;
			corresponding_mat_tar2ref.at<double>(index, 2) = hight_boundary + index_row;
			corresponding_mat_tar2ref.at<double>(index, 3) = index_col;
		}
	}

	// return
	std::map<int, std::map<int, cv::Mat >> pixel_corr;
	std::map<int, cv::Mat> ref2tar;
	ref2tar[1] = corresponding_mat_ref2tar;
	pixel_corr[0] = ref2tar;
	std::map<int, cv::Mat> tar2ref;
	tar2ref[0] = corresponding_mat_tar2ref;
	pixel_corr[1] = tar2ref;
	return pixel_corr;
}

std::ostream& operator<<(std::ostream& os, const DataImitator& di) {
	return os
		<< "Source depth map : " << di.depthmap_ref_filepath << std::endl
		<< "Target depth map : " << di.depthmap_tar_filepath << std::endl
		<< "Source to target depth map : " << di.corr_ref2tar_filepath << std::endl
		<< "Target to source depth map : " << di.corr_tar2ref_filepath << std::endl
		<< "Source depth scale and offset : " << std::to_string(di.depth_scale) << "\t" << std::to_string(di.depth_offset) << std::endl
		<< "Depth map hight and width: " << std::to_string(di.depthmap_hight) << "\t" << std::to_string(di.depthmap_width) << std::endl;
}



void DataImitatorMultiImage::make_aligncoeffs_simple()
{
	//int ref_image_idx = 0;
	////
	////std::vector<cv::Mat> coeffs_list;
	//cv::Mat ref_scale = cv::Mat::ones(coeff_grid_height, coeff_grid_width, CV_64FC1);
	//coeff_list.push_back(ref_scale);
	//cv::Mat ref_offset = cv::Mat::zeros(coeff_grid_height, coeff_grid_width, CV_64FC1);
	//coeff_list.push_back(ref_offset);
	////
	//cv::Mat dest_scale = cv::Mat::ones(coeff_grid_height, coeff_grid_width, CV_64FC1) * depth_scale;
	//coeff_list.push_back(dest_scale);
	//cv::Mat dest_offset = cv::Mat::zeros(coeff_grid_height, coeff_grid_width, CV_64FC1) * depth_offset;
	//coeff_list.push_back(dest_offset);

	//return coeff_list;
}

void DataImitatorMultiImage::make_aligncoeffs_random()
{
	if (coeff_grid_width * depthmap_overlap_ratio != (int)coeff_grid_width * depthmap_overlap_ratio)
		LOG(ERROR) << "The depth map width should be even.";

	int template_grid_width = coeff_grid_width + coeff_grid_width * depthmap_overlap_ratio * (frame_number - 1);

	// create image [1, frame_number -1]'s image scale & offset coefficients template
	for (int i = 0; i < frame_number - 1; i++)
	{
		cv::Mat scale_template = cv::Mat::ones(coeff_grid_height, template_grid_width, CV_64FC1);
		cv::randn(scale_template, scale_mean, scale_stddev);
		std::map<int, cv::Mat> scale_temp;
		scale_temp[frame_number - 2 - i] = scale_template;
		coeff_template_scale[frame_number - 1 - i] = scale_temp;

		// create offset coefficients template
		cv::Mat offset_template = cv::Mat::ones(coeff_grid_height, template_grid_width, CV_64FC1);
		cv::randn(offset_template, offset_mean, offset_stddev);
		std::map<int, cv::Mat> offset_temp;
		offset_temp[frame_number - 2 - i] = offset_template;
		coeff_template_offset[frame_number - 1 - i] = offset_temp;
	}

	// get image [1, frame_number -1]'s coefficients
	for (int mat_idx = 0; mat_idx < 2; mat_idx++)
	{
		std::map<int, std::map<int, cv::Mat>>& coeff_list = mat_idx == 0 ? coeff_scale_list : coeff_offset_list;

		// coefficients for each image
		for (int i = 0; i < frame_number - 1; i++)
		{
			cv::Mat template_mat = coeff_template_scale.at(frame_number - 1 - i).at(frame_number - 2 - i);
			if (mat_idx == 1)
			{
				template_mat = coeff_template_offset.at(frame_number - 1 - i).at(frame_number - 2 - i);
			}

			// get the each image's coefficients
			cv::Mat coeff_scale_ = cv::Mat(coeff_grid_height, coeff_grid_width, CV_64FC1);
			int col_offset = 0 + coeff_grid_width * depthmap_overlap_ratio * (frame_number - 1 - i);
			for (int row_idx = 0; row_idx < coeff_grid_height; row_idx++)
			{
				for (int col_idx = 0; col_idx < coeff_grid_width; col_idx++)
				{
					int col_idx_template = col_offset + col_idx;
					coeff_scale_.at<double>(row_idx, col_idx) = template_mat.at<double>(row_idx, col_idx_template);
				}
			}

			std::map<int, cv::Mat> tar_mat;
			tar_mat[frame_number - 2 - i] = coeff_scale_;
			coeff_list.insert({ frame_number - 1 - i , tar_mat });
		}
	}

	// the last frame
	coeff_scale_list.at(frame_number - 1).insert({ frame_number - 1, cv::Mat::ones(coeff_grid_height, coeff_grid_width, CV_64FC1) });
	coeff_offset_list.at(frame_number - 1).insert({ frame_number - 1, cv::Mat::zeros(coeff_grid_height, coeff_grid_width, CV_64FC1) });
}


void  DataImitatorMultiImage::make_depthmap_pair()
{
	if (depthmap_width * depthmap_overlap_ratio != (int)depthmap_width * depthmap_overlap_ratio)
		LOG(ERROR) << "The depth map width should be even.";

	// 1) create reference depth map template
	int template_depthmap_width = depthmap_width + depthmap_width * depthmap_overlap_ratio * (frame_number - 1);
	// the template of reference depth map
	cv::Mat depthmap_ref_template = cv::Mat::ones(depthmap_hight, template_depthmap_width, CV_64FC1);
	int counter = 0;
	for (int row_index = 0; row_index < depthmap_ref_template.rows; row_index++)
	{
		for (int col_index = 0; col_index < depthmap_ref_template.cols; col_index++)
		{
			depthmap_ref_template.at<double>(row_index, col_index) = counter;
			counter++;
		}
	}

	// 2) deform the reference depth map to get all template depth map
	depthmap_template_list[frame_number - 1] = depthmap_ref_template;
	for (int frame_idx = 0; frame_idx < frame_number - 1; frame_idx++)
	{
		int ref_mat_idx = frame_number - 1 - frame_idx;
		int tar_mat_idx = frame_number - 2 - frame_idx;
		cv::Mat& ref_mat = depthmap_template_list[ref_mat_idx];
		cv::Mat ref_mat_deformed;
		DepthmapUtil::deform(ref_mat, ref_mat_deformed, coeff_template_scale[ref_mat_idx][tar_mat_idx], coeff_template_offset[ref_mat_idx][tar_mat_idx]);
		depthmap_template_list[tar_mat_idx] = ref_mat_deformed;
	}

	// 2) get sub-depth map
	for (int frame_idx = 0; frame_idx < frame_number; frame_idx++)
	{
		int col_offset = depthmap_width * depthmap_overlap_ratio * frame_idx;
		int col_start = 0 + col_offset;
		int col_end = depthmap_width - 1 + col_offset;

		cv::Mat subdepthmap = cv::Mat::ones(depthmap_hight, depthmap_width, CV_64FC1);
		depthmap_list[frame_idx] = subdepthmap;

		cv::Mat& depthmap_ref_template = depthmap_template_list[frame_idx];
		for (int row_index = 0; row_index < depthmap_hight; row_index++)
		{
			for (int col_index = 0; col_index < depthmap_width; col_index++)
			{
				subdepthmap.at<double>(row_index, col_index) = depthmap_ref_template.at<double>(row_index, col_index + col_offset);
			}
		}
	}
}


void DataImitatorMultiImage::make_pixel_corresponding()
{
	for (int frame_src = 0; frame_src < frame_number; frame_src++)
	{// the source frame index
		for (int frame_tar = 0; frame_tar < frame_number; frame_tar++)
		{ // the target frame index

			// 1) the overlap area column index range.
			int src_col_start = -1;
			int src_col_end = -1;
			int tar_col_start = -1;
			int tar_col_end = -1;

			int src_col_start_template = -1;
			int src_col_end_template = -1;
			int tar_col_start_template = -1;
			int tar_col_end_template = -1;

			// range in src or tar image
			if (frame_src == frame_tar)
				continue;
			else if (frame_src < frame_tar)
			{
				src_col_start = (1.0 - depthmap_overlap_ratio) * depthmap_width;
				src_col_end = depthmap_width;
				tar_col_start = 0;
				tar_col_end = depthmap_overlap_ratio * depthmap_width;
			}
			else if (frame_src > frame_tar)
			{
				src_col_start = 0;
				src_col_end = depthmap_overlap_ratio * depthmap_width;
				tar_col_start = (1.0 - depthmap_overlap_ratio) * depthmap_width;
				tar_col_end = depthmap_width;
			}

			// range in template image
			src_col_start_template = src_col_start + frame_src * (1.0 - depthmap_overlap_ratio) * depthmap_width;
			src_col_end_template = src_col_end + frame_src * (1.0 - depthmap_overlap_ratio) * depthmap_width;
			tar_col_start_template = tar_col_start + frame_tar * (1.0 - depthmap_overlap_ratio) * depthmap_width;
			tar_col_end_template = tar_col_end + frame_tar * (1.0 - depthmap_overlap_ratio) * depthmap_width;

			// check src and tar overlap range
			if (src_col_start_template >= tar_col_end_template || src_col_end_template <= tar_col_start_template)
				continue;

			// 2) create the pixel corresponding relationship
			cv::Mat corre_src2tar = cv::Mat(depthmap_hight * depthmap_width * depthmap_overlap_ratio, 4, CV_64FC1);

			// figure out the corresponding relationship pairs start number
			for (int index_row = 0; index_row < depthmap_hight; index_row++)
			{
				for (int index_col = 0; index_col < depthmap_width * depthmap_overlap_ratio; index_col++)
				{
					int index = index_row * depthmap_width * depthmap_overlap_ratio + index_col;
					corre_src2tar.at<double>(index, 0) = index_row;
					corre_src2tar.at<double>(index, 1) = src_col_start + index_col;
					corre_src2tar.at<double>(index, 2) = index_row;
					corre_src2tar.at<double>(index, 3) = tar_col_start + index_col;
				}
			}

			// 3) store in map
			pixel_corresponding[frame_src].insert({ frame_tar , corre_src2tar });
		}
	}
}

void DataImitatorMultiImage::report_data_parameters()
{
	std::stringstream ss;
	ss << "Depth map frame number: " << frame_number << std::endl;
	ss << "Depth map overlap area ratio:" << depthmap_overlap_ratio << std::endl;
	ss << "Depth map hight and width: " << std::to_string(depthmap_hight) << "\t" << std::to_string(depthmap_width) << std::endl;
	ss << "Coefficients grid size (Height * Width): " << std::to_string(coeff_grid_height) << "\t" << std::to_string(coeff_grid_width) << std::endl;
	LOG(INFO) << ss.str();
}


void DataImitatorMultiImage::output_date()
{
	const std::string filename_prefix;
	const int index_digit_number = 3;

	// 0) output depth map to *.pfm
	std::vector<std::string> pfm_filename_list;
	for (int frame_index = 0; frame_index < frame_number; frame_index++)
	{
		char frame_index_str[32];
		sprintf(frame_index_str, "%03d", frame_index);
		// depth map file name
		std::string depthmap_filepath = std::regex_replace(depthmap_filename_exp, std::regex(R"(\$subimageindex)"), frame_index_str);
		pfm_filename_list.push_back(output_root_dir + depthmap_filepath);
		DepthmapIO::save(output_root_dir + depthmap_filepath, depthmap_list[frame_index]);
	}

	// 1) output coefficients to *.json file.
	// coefficients file path
	AlignCoeff aligncoeff;
	aligncoeff.initial(coeff_grid_width, coeff_grid_height, frame_number);
	aligncoeff.coeff_scale_mat.clear();
	aligncoeff.coeff_offset_mat.clear();
	for (int frame_idx = 0; frame_idx < frame_number - 1; frame_idx++) {
		aligncoeff.coeff_scale_mat.push_back(coeff_scale_list[frame_idx + 1][frame_idx]);
		aligncoeff.coeff_offset_mat.push_back(coeff_offset_list[frame_idx + 1][frame_idx]);
	}
	aligncoeff.coeff_scale_mat.push_back(coeff_scale_list[frame_number - 1][frame_number - 1]);
	aligncoeff.coeff_offset_mat.push_back(coeff_offset_list[frame_number - 1][frame_number - 1]);

	std::string json_filepath = output_root_dir + aligncoeff_filename_exp;
	aligncoeff.save(json_filepath, pfm_filename_list);

	// 2) output pixel corresponding to JSON file.
	// generate the corresponding relation file name
	auto zeropad = [](const int number, const int digit_number)->std::string
	{
		std::ostringstream ss;
		ss << std::setw(digit_number) << std::setfill('0') << number;
		return ss.str();
	};

	for (int frame_src = 0; frame_src < frame_number; frame_src++)
	{// the source frame index
		for (int frame_tar = 0; frame_tar < frame_number; frame_tar++)
		{ // the target frame index
			if (frame_src == frame_tar)
				continue;
			std::string pixels_corr_filepath;
			pixels_corr_filepath = std::regex_replace(pixelcorr_filename_exp, std::regex(R"(\$prefix)"), filename_prefix);
			pixels_corr_filepath = std::regex_replace(pixels_corr_filepath, std::regex(R"(\$srcindex)"), zeropad(frame_src, index_digit_number));
			pixels_corr_filepath = std::regex_replace(pixels_corr_filepath, std::regex(R"(\$tarindex)"), zeropad(frame_tar, index_digit_number));

			cv::Mat corresponding_mat = pixel_corresponding[frame_src][frame_tar];
			std::string depthmap_src_filename = pfm_filename_list[frame_src];
			std::string depthmap_tar_filename = pfm_filename_list[frame_tar];

			PixelsCorrIO::save(output_root_dir + pixels_corr_filepath, depthmap_src_filename, depthmap_tar_filename, corresponding_mat);
		}
	}
}



================================================
FILE: depth-estimation/360monodepth/code/cpp/src/data_io.cpp
================================================
#include "data_io.hpp"
#include "pfm_io.hpp"

#include <boost/property_tree/json_parser.hpp>

#include <opencv2/opencv.hpp>
#include <glog/logging.h>

#include <iostream>
#include <utility>
#include <string>
#include <regex>

namespace pt = boost::property_tree;

void DataIO::write2json(const std::string& filename, const pt::ptree& root)
{
	std::stringstream ss;
	pt::write_json(ss, root);
	std::string my_string_to_send_somewhere_else = ss.str();

	// remove digital number quotes
	std::regex reg0("\\\"([+-]*[0-9]+\\.{0,1}[0-9]*)\\\"");
	std::string result = std::regex_replace(my_string_to_send_somewhere_else, reg0, "$1");
	std::regex reg1("\\\"([+-]*[0-9]+\\.{0,1}[0-9]*\\e[+-]{0,1}[0-9]*)\\\"");
	result = std::regex_replace(result, reg1, "$1");

	std::ofstream file;
	file.open(filename);
	file << result;
	file.close();
}

void DepthmapIO::load(const std::string& filepath, cv::Mat& depthmap_mat)
{
	PFM pfm_depth;
	float* depthmap_data = pfm_depth.read_pfm<float>(filepath);
	int depth_map_height = pfm_depth.getHeight();
	int depth_map_width = pfm_depth.getWidth();
	depthmap_mat = cv::Mat(depth_map_height, depth_map_width, CV_32FC1);
	memcpy(depthmap_mat.data, depthmap_data, depth_map_height * depth_map_width * sizeof(float));
	depthmap_mat.convertTo(depthmap_mat, CV_64FC1);
	delete depthmap_data;

	//cv::imshow("Depth Map window", depthmap_mat);
	//int k = cv::waitKey(0); // Wait for a keystroke in the window
}

void DepthmapIO::save(const std::string& filepath, const cv::Mat& depthmap_mat)
{
	PFM pfm_depth;
	pfm_depth.setHeight(depthmap_mat.rows);
	pfm_depth.setWidth(depthmap_mat.cols);
	cv::Mat depthmap_mat_float;
	depthmap_mat.convertTo(depthmap_mat_float, CV_32FC1);
	pfm_depth.write_pfm<float>(filepath, (float*)depthmap_mat_float.data, -1.0);
}

void PixelsCorrIO::save(const std::string& file_path, const std::string& src_filename, const std::string& tar_filename, const cv::Mat& pixles_corresponding)
{
	//pt::ptree root;
	pt::basic_ptree<std::string, std::string> root;

	// output mat
	root.put("src_image_filename", src_filename);
	root.put("src_image_sha256", "   ");

	root.put("tar_image_filename", tar_filename);
	root.put("tar_image_sha256", "   ");

	// output mat row number
	root.put<int>("pixel_corresponding_number", pixles_corresponding.rows);
	//root.push_back(std::make_pair("pixel_corresponding_number", pixel_corresponding.rows));

	if (!pixles_corresponding.empty() && pixles_corresponding.cols != 4)
		LOG(ERROR) << "The pixels corresponding column number should be 4!";

	// output data
	pt::ptree matrix_node;
	for (int i = 0; i < pixles_corresponding.rows; i++)
	{
		pt::ptree row;
		for (int j = 0; j < pixles_corresponding.cols; j++)
		{
			pt::basic_ptree<std::string, std::string> cell;
			cell.put_value(pixles_corresponding.at<double>(i, j));
			row.push_back(std::make_pair("", cell));
		}
		matrix_node.push_back(std::make_pair("", row));
	}
	root.add_child("pixel_corresponding", matrix_node);

	// output to file
	DataIO::write2json(file_path, root);
}


void PixelsCorrIO::load(const std::string& file_path, std::string& src_filename, std::string& tar_filename, cv::Mat& pixles_corresponding)
{
	pt::ptree root;
	pt::read_json(file_path, root);

	int pixel_corresponding_number = -1;

	for (pt::ptree::value_type& key_value_pair : root)
	{
		std::string key = key_value_pair.first;
		if (key.compare("src_image_filename") == 0)
		{
			src_filename = key_value_pair.second.data();
		}
		else if (key.compare("tar_image_filename") == 0)
		{
			tar_filename = key_value_pair.second.data();
		}
		else if (key.compare("pixel_corresponding") == 0)
		{
			if (pixel_corresponding_number != -1)
			{
				pixles_corresponding = cv::Mat::zeros(pixel_corresponding_number, 4, CV_64FC1);
				int x = 0;
				for (pt::ptree::value_type& row : root.get_child(key))
				{
					int y = 0;
					for (pt::ptree::value_type& cell : row.second)
					{
						pixles_corresponding.at<double>(x, y) = cell.second.get_value<float>();
						y++;
					}
					x++;
				}
			}
		}
		else if (key.compare("pixel_corresponding_number") == 0)
		{
			pixel_corresponding_number = key_value_pair.second.get_value<int>();
			//printf("%d \n", pixel_corresponding_number);
		}
		else
		{
			LOG(INFO) << "Key  \n" << key;
		}
	}
}



================================================
FILE: depth-estimation/360monodepth/code/cpp/src/depthmap_stitcher.cpp
================================================
#include "depthmap_stitcher.hpp"
#include "depthmap_utility.hpp"
#include "data_io.hpp"

#include <glog/logging.h>
#include <boost/property_tree/json_parser.hpp>

#include <iostream>
#include <string>
#include <regex>

namespace pt = boost::property_tree;
using namespace std;

DepthmapStitcher::DepthmapStitcher()
{
}

DepthmapStitcher::~DepthmapStitcher()
{
}

std::string zeropad(const int number, const int digit_number)
{
	std::ostringstream ss;
	ss << std::setw(digit_number) << std::setfill('0') << number;
	return ss.str();
}

void DepthmapStitcher::initial_data(const std::string& data_root_dir,
	const std::vector<cv::Mat>& depthmap_original_data,
	const std::vector<int>& depthmap_original_ico_index,
	const std::map<int, std::map<int, cv::Mat>>& pixels_corresponding_list_data)
{
	if (depthmap_original_data.size() < 2)
		LOG(ERROR) << "The depth map image less than 2.";

	// set the output filename's prefix
	filename_prefix = "depthmapAlignPy";

	this->data_root_dir = data_root_dir;
	// 0) generate the depth maps proxy filename for output debug and report.
	filename_list.clear();
	depthmap_original.clear();
	for (int index_inter = 0; index_inter < depthmap_original_data.size(); index_inter++)
	{
		// create the proxy filename
		int index_exter = depthmap_original_ico_index[index_inter];
		std::string filename = std::regex_replace(proxy_filename_depthmap_exp, std::regex(R"(\$index)"), std::to_string(index_exter));
		LOG(INFO) << "Generate depth map proxy filename:" << filename;
		filename_list.push_back(filename);
		if (depthmap_original_data[index_inter].depth() == CV_64F)
		{
			depthmap_original.push_back(depthmap_original_data[index_inter]);
		}
		else {
			LOG(INFO) << "The aligned depth map is not double cv::Mat, convert to float cv::Mat.";
			cv::Mat depthmap_double;
			depthmap_original_data[index_inter].convertTo(depthmap_double, CV_64F);
			depthmap_original.push_back(depthmap_double);
		}
		extidx2intidx[index_exter] = index_inter;
		intidx2extidx[index_inter] = index_exter;
	}

	// 1) the pixel corresponding relationship, and convert the sub-image index to internal index.
	pixels_corresponding_list.clear();
	for (auto src_iter = pixels_corresponding_list_data.begin(); src_iter != pixels_corresponding_list_data.end(); src_iter++)
	{
		std::map<int, int>::iterator src_intidx_it = extidx2intidx.find(src_iter->first);
		if (src_intidx_it == extidx2intidx.end())
		{
			LOG(WARNING) << "src_intidx " << src_iter->first << " do not exist!";
			continue;
		}
		int src_intidx = src_intidx_it->second;

		std::map<int, cv::Mat> tar_map = src_iter->second;
		for (auto tar_map_iter = tar_map.begin(); tar_map_iter != tar_map.end(); tar_map_iter++)
		{
			std::map<int, int>::iterator tar_intidx_it = extidx2intidx.find(tar_map_iter->first);
			if (tar_intidx_it == extidx2intidx.end())
			{
				LOG(WARNING) << "tar_intidx " << tar_map_iter->first << " do not exist!";
				continue;
			}
			int tar_intidx = tar_intidx_it->second;
			tar_map[tar_intidx] = tar_map_iter->second;
		}
		pixels_corresponding_list[src_intidx] = tar_map;
	}
	// check the corresponding relationship data
}

void DepthmapStitcher::load_data(const std::string& data_root_dir, const std::vector<std::string> depthmap_filename_list)
{
	if (depthmap_filename_list.size() < 2)
		LOG(ERROR) << "The depth map image less than 2.";

	this->data_root_dir = data_root_dir;
	// 0) test and parser the file name, get the mapping between external index and internal index
	filename_list.clear();
	std::map<int, std::string> extidx2filename;
	// for (std::string &filename : depthmap_filename_list)
	for (int index = 0; index < depthmap_filename_list.size(); index++)
	{
		std::string filename = depthmap_filename_list[index];
		filename_list.push_back(filename);

		if (!std::regex_match(filename, std::regex(depthmap_filename_regexp)))
			LOG(ERROR) << "Input file name " << filename << " do not match the file name expression" << depthmap_filename_regexp << " please check.";

		// parser the file name get sub-images external index
		std::smatch m;
		regex_search(filename, m, std::regex("_[0-9]+.pfm"));
		std::string ext_index_str = m[0].str();
		ext_index_str = ext_index_str.substr(1, ext_index_str.length() - 4);
		int depthmap_extindex = std::stoi(ext_index_str);

		extidx2filename[depthmap_extindex] = filename;
		extidx2intidx[depthmap_extindex] = index;
		intidx2extidx[index] = depthmap_extindex;
	}
	smatch m_prefixs;
	regex_search(depthmap_filename_list[0], m_prefixs, std::regex("[a-zA-Z0-9\\_]*\\_disp"));
	std::string prefix = m_prefixs[0].str();
	filename_prefix = prefix.substr(0, prefix.length()-5);
	LOG(INFO) << "File name prefix is " << filename_prefix;

	// 1) load the pixel corresponding relationship
	pixels_corresponding_list.clear();
	for (std::map<int, std::string>::iterator it_src = extidx2filename.begin(); it_src != extidx2filename.end(); ++it_src)
	{
		//disparitmap_index_list.push_back(index_src);
		int extidx_src = it_src->first;
		int intidx_src = extidx2intidx[extidx_src];
		std::map<int, cv::Mat> pixels_corresponding_sublist;

		for (std::map<int, std::string>::iterator it_tar = extidx2filename.begin(); it_tar != extidx2filename.end(); ++it_tar)
		{
			int extidx_tar = it_tar->first;
			if (extidx_src == extidx_tar)
				continue;

			int intidx_tar = extidx2intidx[extidx_tar];
			// generate the corresponding relation file name
			std::string pixels_corr_filepath;
			pixels_corr_filepath = std::regex_replace(pixelcorr_filename_exp, std::regex(R"(\$prefix)"), filename_prefix);
			pixels_corr_filepath = std::regex_replace(pixels_corr_filepath, std::regex(R"(\$srcindex)"), zeropad(extidx_src, index_digit_number));
			pixels_corr_filepath = std::regex_replace(pixels_corr_filepath, std::regex(R"(\$tarindex)"), zeropad(extidx_tar, index_digit_number));

			// load pixel corresponding relationship
			LOG(INFO) << "Loading corresponding file :" << pixels_corr_filepath;
			std::string src_filename;
			std::string tar_filename;
			cv::Mat pixles_corresponding;
			PixelsCorrIO::load(data_root_dir + pixels_corr_filepath, src_filename, tar_filename, pixles_corresponding);
			pixels_corresponding_sublist[intidx_tar] = pixles_corresponding;

			// check the filename
			if (src_filename.compare(filename_list[intidx_src]) != 0)
				LOG(ERROR) << "The pixels corresponding file " << pixels_corr_filepath << " reference image filename " << src_filename << "is not same as expect " << filename_list[intidx_src];
			if (tar_filename.compare(filename_list[intidx_tar]) != 0)
				LOG(ERROR) << "The pixels corresponding file " << pixels_corr_filepath << " target image filename " << tar_filename << "is not same as expect " << filename_list[intidx_tar];
		}
		pixels_corresponding_list[intidx_src] = pixels_corresponding_sublist;
	}

	// 2) load depth map base on the corresponding relationship
	depthmap_original.clear();
	for (const std::string& filename : depthmap_filename_list)
	{
		std::string depth_map_filepath = data_root_dir + filename;
		cv::Mat depthmap;
		DepthmapIO::load(depth_map_filepath, depthmap);
		if (depthmap.size().area() == 0)
			LOG(ERROR) << "The depth map " << depth_map_filepath << " is empty.";
		depthmap_original.push_back(depthmap);
		LOG(INFO) << "Load depth map from :" << depth_map_filepath;
	}
}

void DepthmapStitcher::initial(const int grid_width, const int grid_height)
{
	if (depthmap_original.size() != filename_list.size())
		LOG(ERROR) << "The depth loading is not completed, please load the data again!";

	// 1) initial the optimized parameters
	this->grid_width = grid_width;
	this->grid_height = grid_height;
	coeff_so.initial(grid_width, grid_height, depthmap_original.size());
}

void DepthmapStitcher::set_coeff(const std::vector<cv::Mat>& coeff_scale,
	const std::vector<cv::Mat>& coeff_offset)
{
	coeff_so.set_value_mat(coeff_scale, coeff_offset);
}

void DepthmapStitcher::align_depthmap_all()
{
	depthmap_aligned.resize(depthmap_original.size());

	// align each depth maps
	for (int index = 0; index < depthmap_original.size(); index++)
	{
		align_depthmap(index);
	}
}

void DepthmapStitcher::align_depthmap(const int depthmap_intidx)
{
	if (depthmap_aligned.size() != depthmap_original.size())
		depthmap_aligned.resize(depthmap_original.size());

	DepthmapUtil::deform(depthmap_original[depthmap_intidx], depthmap_aligned[depthmap_intidx],
		coeff_so.coeff_scale_mat[depthmap_intidx], coeff_so.coeff_offset_mat[depthmap_intidx]);
}

void DepthmapStitcher::get_align_coeff(std::vector<cv::Mat>& coeff)
{
	for (int i = 0; i < coeff_so.coeff_scale_mat.size(); i++)
	{
		
		coeff.push_back(coeff_so.coeff_scale_mat[i].clone());
		coeff.push_back(coeff_so.coeff_offset_mat[i].clone());
	}
}

void DepthmapStitcher::save_align_coeff()
{
	const std::string coeff_json_filepath = data_root_dir + filename_prefix + "_coeff.json";
	LOG(INFO) << "Output the aligned coefficients to file :" << coeff_json_filepath;
	coeff_so.save(coeff_json_filepath, filename_list);
}

void DepthmapStitcher::save_aligned_depthmap()
{
	for (int index = 0; index < depthmap_aligned.size(); index++)
	{
		int depthmap_index = intidx2extidx[index];
		std::string output_pfm_filepath = std::regex_replace(depthmap_aligned_filename_exp, std::regex(R"(\$prefix)"), filename_prefix);
		output_pfm_filepath = std::regex_replace(output_pfm_filepath, std::regex(R"(\$subindex)"), zeropad(depthmap_index, index_digit_number));
		output_pfm_filepath = data_root_dir + output_pfm_filepath;
		LOG(INFO) << "Save aligned depth map :" << output_pfm_filepath;
		DepthmapIO::save(output_pfm_filepath, depthmap_aligned[index]);
	}
}

void DepthmapStitcher::report_error()
{
	if (depthmap_aligned.size() != depthmap_original.size())
		LOG(ERROR) << "The aligned map is not completed.";

	// compute aligned depth maps RMS
	std::stringstream ss;
	// TODO multi-thread with openmp
	for (std::map<int, int>::iterator iter_src = intidx2extidx.begin(); iter_src != intidx2extidx.end(); ++iter_src)
	{
		int intidx_src = iter_src->first;
		int extidx_src = iter_src->second;
		const cv::Mat& depthmap_src = depthmap_aligned[intidx_src];
		std::map<int, cv::Mat> tar_map = pixels_corresponding_list[intidx_src];

		for (std::map<int, int>::iterator iter_tar = intidx2extidx.begin(); iter_tar != intidx2extidx.end(); ++iter_tar)
		{
			int intidx_tar = iter_tar->first;
			int extidx_tar = iter_tar->second;

			if (intidx_tar == intidx_src)
			{
				ss << extidx_src << "->" << extidx_tar << ":" << 0 << "\t";
				continue;
			}

			const cv::Mat& depthmap_tar = depthmap_aligned[intidx_tar];
			const cv::Mat& corr_src2tar = tar_map.at(intidx_tar);

			// compute RMS
			float rms = 0;
			int valuable_pixel_counter = 0;
			for (int i = 0; i < corr_src2tar.rows; i++)
			{
				valuable_pixel_counter++;

				int src_y = corr_src2tar.at<double>(i, 0);
				int src_x = corr_src2tar.at<double>(i, 1);
				int tar_y = corr_src2tar.at<double>(i, 2);
				int tar_x = corr_src2tar.at<double>(i, 3);

				float depth_value_src = getColorSubpix(depthmap_src, cv::Point2f(src_x, src_y));
				float depth_value_tar = getColorSubpix(depthmap_tar, cv::Point2f(tar_x, tar_y));

				rms += (depth_value_src - depth_value_tar) * (depth_value_src - depth_value_tar);
			}
			if (valuable_pixel_counter != 0)
				rms = std::sqrt(rms / valuable_pixel_counter);
			else
				rms = -1;
			//LOG(INFO) << "RMS between depth map \n"
			//	<< filename_list[index_src] << " and " << filename_list[index_tar]
			//	<< "\n is :" << rms;
			ss << extidx_src << "->" << extidx_tar << ":" << rms << "\t";
		}
		ss << std::endl << "-----" << std::endl;
	}
	LOG(INFO) << "RMS Error : \n " << ss.str();
}

float DepthmapStitcher::getColorSubpix(const cv::Mat &img, const cv::Point2f pt)
{
	// cv::Mat patch;
	// assert(img.depth() == CV_64FC1);
	// assert(img.channels() == 1);
	// cv::Mat imgF;
	// img.convertTo(imgF, CV_32FC1);
	// cv::getRectSubPix(imgF, cv::Size(1, 1), pt, patch);
	// return patch.at<float>(0, 0);

	int x = (int)pt.x;
	int y = (int)pt.y;
	int x0 = cv::borderInterpolate(x, img.cols, cv::BORDER_REFLECT_101);
	int x1 = cv::borderInterpolate(x + 1, img.cols, cv::BORDER_REFLECT_101);
	int y0 = cv::borderInterpolate(y, img.rows, cv::BORDER_REFLECT_101);
	int y1 = cv::borderInterpolate(y + 1, img.rows, cv::BORDER_REFLECT_101);

	float a = pt.x - (float)x;
	float c = pt.y - (float)y;

	return (double)cvRound(
		(img.at<double>(y0, x0) * (1.f - a) + img.at<double>(y0, x1) * a) * (1.f - c) +
		(img.at<double>(y1, x0) * (1.f - a) + img.at<double>(y1, x1) * a) * c);
}

void DepthmapStitcher::get_bilinear_weight(double* weight_list,
	const int image_width, const int image_height,
	const int grid_row_number, const int grid_col_number,
	const double x, const double y)
{
	DepthmapUtil::bilinear_weight(weight_list,
		image_width, image_height,
		grid_col_number, grid_row_number,
		x, y);
}

void AlignCoeff::initial(const int grid_width, const int grid_height, const int depthmap_number)
{
	coeff_number = depthmap_number;
	coeff_cols = grid_width;
	coeff_rows = grid_height;

	// allocate continued memory
	int mem_size = grid_width * grid_height * depthmap_number;
	coeff_scale = std::shared_ptr<double[]>(new double[mem_size]);
	memset(coeff_scale.get(), 0, sizeof(double) * mem_size);
	coeff_offset = std::shared_ptr<double[]>(new double[mem_size]);
	memset(coeff_offset.get(), 0, sizeof(double) * mem_size);

	for (int index = 0; index < coeff_number; index++)
	{
		cv::Mat s_param_mat = cv::Mat(coeff_rows, coeff_cols, CV_64FC1, coeff_scale.get() + coeff_cols * coeff_rows * index);
		coeff_scale_mat.push_back(s_param_mat);
		cv::Mat o_param_mat = cv::Mat(coeff_rows, coeff_cols, CV_64FC1, coeff_offset.get() + coeff_cols * coeff_rows * index);
		coeff_offset_mat.push_back(o_param_mat);
	}
}


void AlignCoeff::set_value_const(const float scale, const float offset)
{
	for (int index = 0; index < coeff_number; index++)
	{
		coeff_scale_mat[index].setTo(scale);
		coeff_offset_mat[index].setTo(offset);;
	}
}


void AlignCoeff::set_value_mat(const std::vector<cv::Mat>& coeff_scale, const std::vector<cv::Mat>& coeff_offset)
{
	// check the data type and number
	if (coeff_scale.size() != coeff_scale_mat.size() ||
		coeff_offset.size() != coeff_offset_mat.size())
	{
		LOG(ERROR) << "The initial coefficients mat size is not equal the internal size.";
	}

	if (coeff_scale[0].depth() != CV_64F || coeff_offset[0].depth() != CV_64F)
	{
		LOG(ERROR) << "The initial coefficients mat depth should be double (CV_64F).";
	}

	// set the initial value
	for (int index = 0; index < coeff_scale.size(); index++)
	{
		std::memcpy(coeff_scale_mat[index].data, coeff_scale[index].data, coeff_scale[index].total() * sizeof(double));
		std::memcpy(coeff_offset_mat[index].data, coeff_offset[index].data, coeff_offset[index].total() * sizeof(double));
	}
}


void AlignCoeff::set_value_rand()
{
	double mean = 0.0;
	double stddev = 4.0 / 3.0; // 99.7% of values will be inside [-4, +4] interval
	for (int index = 0; index < coeff_number; index++)
	{
		cv::randn(coeff_scale_mat[index], cv::Scalar(mean), cv::Scalar(stddev));
		cv::randn(coeff_offset_mat[index], cv::Scalar(mean), cv::Scalar(stddev));
	}
}


void AlignCoeff::save(const std::string& file_path, const std::vector<std::string>& filename_list)
{
	pt::ptree root;

	// output mat
	root.put("storage_order", "row_major");
	for (int index = 0; index < coeff_number * 2; index++)
	{
		const int depthmap_index = int(index / 2.0);

		pt::ptree cell;
		cv::Mat param_mat;
		if (index % 2 == 0)
		{
			cell.put("coeff_type", "scale");
			param_mat = coeff_scale_mat[depthmap_index];
		}
		else
		{
			cell.put("coeff_type", "offset");
			param_mat = coeff_offset_mat[depthmap_index];
		}

		cell.put("filename", filename_list[depthmap_index]);

		// output mat column number
		cell.put("mat_width", param_mat.cols);

		// output mat row number
		cell.put("mat_hight", param_mat.rows);

		// output data
		pt::ptree matrix_node;
		for (int i = 0; i < param_mat.rows; i++)
		{
			pt::ptree row;
			for (int j = 0; j < param_mat.cols; j++)
			{
				pt::ptree cell;
				cell.put_value(param_mat.at<double>(i, j));
				row.push_back(std::make_pair("", cell));
			}
			matrix_node.push_back(std::make_pair("", row));
		}
		cell.add_child("mat_data", matrix_node);

		// output mat description
		std::string name;
		if (index % 2 == 0)
			name = std::string("scale mat of ") + std::to_string(int(index / 2));
		else
			name = std::string("offset mat of ") + std::to_string(int(index / 2));
		cell.put("description", name);

		std::string coeff_name = "coeff_mat_" + std::to_string(index);
		root.add_child(coeff_name, cell);
	}

	DataIO::write2json(file_path, root);
}


void AlignCoeff::load(const std::string& file_path)
{
	// read from disk
	pt::ptree root;
	pt::read_json(file_path, root);

	// parser *.json 
	int scale_mat_counter = -1;
	int offset_mat_counter =-1;
	for (pt::ptree::value_type& key_value_pair : root)
	{
		int mat_width = -1;
		int mat_height = -1;
		std::string key = key_value_pair.first;

		bool scale_mat = false;
		bool offset_mat = false;

		if (key.compare("storage_order") == 0)
		{
			//
		}
		else if (key.find("coeff_mat_") != std::string::npos)
		{
			// parser the coefficient 
			for (pt::ptree::value_type& mat_term : root.get_child(key))
			{
				std::string key = mat_term.first;
				if (key.compare("coeff_type") == 0)
				{
					std::string mat_data_type = mat_term.second.data();
					if (mat_data_type.compare("scale") == 0)
					{
						scale_mat = true;
						scale_mat_counter++;
					}
					else if (mat_data_type.compare("offset") == 0)
					{
						offset_mat = true;
						offset_mat_counter++;
					}
					else
						LOG(ERROR) << "Can not parser the matrix type:" << mat_data_type;
				}
				else if (key.compare("filename") == 0)
				{
					DLOG(INFO) << "Parser the depth map " << mat_term.second.data() << " deforming coefficients.";
				}
				else if (key.compare("mat_width") == 0)
				{
					mat_width = std::stoi(mat_term.second.data());
				}
				else if (key.compare("mat_hight") == 0)
				{
					mat_height = std::stoi(mat_term.second.data());
				}
				else if (key.compare("mat_data") == 0)
				{

					if (mat_width == -1 || mat_height == -1)
						LOG(ERROR) << "JSON file parser error" + file_path;

					cv::Mat mat_data_json = cv::Mat::zeros(mat_height, mat_width, CV_64FC1);
					int x = 0;
					for (pt::ptree::value_type& mat_row : mat_term.second)
					{
						int y = 0;
						for (pt::ptree::value_type& mat_col : mat_row.second)
						{
							mat_data_json.at<double>(x, y) = mat_col.second.get_value<float>();
							y++;
						}
						x++;
					}

					// copy to internal mat data
					cv::Mat coeff_data;
					if (scale_mat)
						coeff_data = coeff_scale_mat[scale_mat_counter];
					else if (offset_mat)
						coeff_data = coeff_offset_mat[offset_mat_counter];

					int byte_size = mat_width * mat_height * sizeof(double);
					// memcpy_s(coeff_data.data, coeff_data.total() * coeff_data.elemSize() 
					// 	, mat_data_json.data, coeff_data.total() * coeff_data.elemSize());
					memcpy(coeff_data.data, mat_data_json.data, coeff_data.total() * coeff_data.elemSize());
				}
				else
				{
					std::cout << "Key  \n"
						<< key;
				}
			}
		}
	}
}


std::ostream& operator<<(std::ostream& os, const AlignCoeff& di)
{
	for (int index = 0; index < di.coeff_number * 2; index++)
	{
		const int depthmap_index = int(index / 2.0);
		os << "Coefficients matrix index : " << depthmap_index;
		cv::Mat param_mat;
		if (index % 2 == 0)
		{
			os << "\t coeff_type: scale" << std::endl;
			param_mat = di.coeff_scale_mat[depthmap_index];
		}
		else
		{
			os << "\t coeff_type: offset" << std::endl;
			param_mat = di.coeff_offset_mat[depthmap_index];
		}
		// output mat column number
		os << "mat_width" << param_mat.cols << std::endl;
		// output mat row number
		os << "mat_hight" << param_mat.rows << std::endl;
		// output data
		os << "mat_data" << param_mat << std::endl;
	}
	return os;
}




================================================
FILE: depth-estimation/360monodepth/code/cpp/src/depthmap_stitcher_enum.cpp
================================================
#include "depthmap_stitcher_enum.hpp"

#include <glog/logging.h>
#include <opencv2/opencv.hpp>
#include <ceres/ceres.h>

#include <vector>
#include <map>
#include <string>


DepthmapStitcherEnum::DepthmapStitcherEnum(float depthmap_optim_overlap_ratio) :
	depthmap_optim_overlap_ratio_(depthmap_optim_overlap_ratio)
{

}

//===== Ceres Energy Function Terms ========
// @see hedman2018instant:equ_1,equ_5
struct DepthmapStitcherEnum::ReprojectionResidual
{

	ReprojectionResidual(double* bilinear_weight_list, double depth_value_a, double depth_value_refernce, int grid_width, int grid_height) : bilinear_weight_list_(bilinear_weight_list), depth_value_a_(depth_value_a), depth_value_refernce_(depth_value_refernce), grid_width_(grid_width), grid_height_(grid_height) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		T scale(0);
		T offset(0);
		for (int index = 0; index < grid_width_ * grid_height_; index++)
		{
			scale += scale_offset_list[0][index] * bilinear_weight_list_[index];
			offset += scale_offset_list[1][index] * bilinear_weight_list_[index];
		}
		T temp = depth_value_refernce_ - depth_value_a_ * scale - offset;
		//T temp = depth_value_refernce_ - depth_value_a_ - offset;
		//T temp = depth_value_refernce_ - depth_value_a_ * scale;
		residual[0] = temp * temp;
		return true;
	}

private:
	const double* bilinear_weight_list_; // weight of S and A, length is grid_width_ * grid_height_
	const double depth_value_a_;		 // the depth value of source depth map
	const double depth_value_refernce_;	 // the depth value of corresponding pixel in reference depth map
	const int grid_width_;					 // the weight grid along the x axis
	const int grid_height_;					 // the weight grid along the y axis
};

// @see hedman2018instant:equ_6
struct DepthmapStitcherEnum::SmoothnessResidual
{
	SmoothnessResidual(int index_current, int index_neighbour) : index_current_(index_current), index_neighbour_(index_neighbour) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		const T* scale_list = scale_offset_list[0];
		const T* offset_list = scale_offset_list[1];

		T scale_diff = scale_list[index_current_] - scale_list[index_neighbour_];
		T offset_diff = offset_list[index_current_] - offset_list[index_neighbour_];
		residual[0] = scale_diff * scale_diff +offset_diff * offset_diff;
		return true;
	}

private:
	const int index_current_;	// the weight grid along the x axis
	const int index_neighbour_; // the weight grid along the y axis
};

// @see hedman2018instant:equ_6
struct DepthmapStitcherEnum::SmoothnessResidual_S
{
	SmoothnessResidual_S(int index_current, int index_neighbour) : index_current_(index_current), index_neighbour_(index_neighbour) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		const T* scale_list = scale_offset_list[0];
		const T* offset_list = scale_offset_list[1];

		T scale_diff = scale_list[index_current_] - scale_list[index_neighbour_];
		//T offset_diff = offset_list[index_current_] - offset_list[index_neighbour_];
		residual[0] = scale_diff * scale_diff;// +offset_diff * offset_diff;
		return true;
	}

private:
	const int index_current_;	// the weight grid along the x axis
	const int index_neighbour_; // the weight grid along the y axis
};

// @see hedman2018instant:equ_6
struct DepthmapStitcherEnum::SmoothnessResidual_O
{
	SmoothnessResidual_O(int index_current, int index_neighbour) : index_current_(index_current), index_neighbour_(index_neighbour) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		const T* scale_list = scale_offset_list[0];
		const T* offset_list = scale_offset_list[1];

		//T scale_diff = scale_list[index_current_] - scale_list[index_neighbour_];
		T offset_diff = offset_list[index_current_] - offset_list[index_neighbour_];
		residual[0] =  offset_diff * offset_diff;
		return true;
	}

private:
	const int index_current_;	// the weight grid along the x axis
	const int index_neighbour_; // the weight grid along the y axis
};

// @see hedman2018instant:equ_7
struct DepthmapStitcherEnum::ScaleResidual
{
	ScaleResidual(int grid_width, int grid_height) : grid_width_(grid_width), grid_height_(grid_height) {}

	template <typename T>
	bool operator()(T const* const* scale_list_list, T* residual) const
	{
		T sum(0);
		const T* scale_list = scale_list_list[0];
		for (int index = 0; index < grid_width_ * grid_height_; index++)
		{
			sum += 1.0 / scale_list[index];
		}
		residual[0] = sum;
		return true;
	}

private:
	const int grid_width_; // the weight grid along the x axis
	const int grid_height_; // the weight grid along the y axis
};

void DepthmapStitcherEnum::initial(const int grid_width , const int grid_height )
{
	if (depthmap_original.size() < 2)
		throw std::runtime_error("The depth map image less than 2.");

	DepthmapStitcher::initial(grid_width, grid_height);

	// the first depth as the reference depth map
	LOG(INFO) << "Initial the scale coefficients to 100 and offset to 50.";
	coeff_so.set_value_const(1.0, 0.0);

	// initial depth map vector
	depthmap_aligned.clear();
	depthmap_aligned.resize(depthmap_original.size(), cv::Mat());
}

void DepthmapStitcherEnum::compute_align_coeff()
{
	//std::cout << coeff_so << std::endl;

	// 0) load set the data and parameters
	int image_width = depthmap_original[0].cols;
	int image_height = depthmap_original[0].rows;

	// set the first image as the reference depth map & coefficients
	depthmap_aligned[0] = depthmap_original[0].clone();
	coeff_so.coeff_scale_mat[0].setTo(1.0);
	coeff_so.coeff_offset_mat[0].setTo(0.0);

	LOG(INFO) << "Terms weights are proj:" << this->weight_reprojection <<
		",\tsmooth:" << this->weight_smooth <<
		",\tscale:" << this->weight_scale;
	// 1) make the energy function,
	// adjusted depth map register to reference depth map to compute the scale and offset
	for (int depthmap_index_ref = 0; depthmap_index_ref < depthmap_original.size(); depthmap_index_ref++)
	{
		// find the adjusted depth as reference map
		if (depthmap_aligned[depthmap_index_ref].size().area() == 0)
			continue;

		// the aligned depth map as the reference depth map
		const cv::Mat depth_map_reference = depthmap_aligned[depthmap_index_ref];

		for (int depthmap_index_cur = 0; depthmap_index_cur < depthmap_original.size(); depthmap_index_cur++)
		{
			// skip the adjusted depth map
			if (depthmap_aligned[depthmap_index_cur].size().area() > 0)
				continue;

			LOG(INFO) << "Reference depth map " << depthmap_index_ref << " and current depth map " << depthmap_index_cur;
			const cv::Mat depth_map_current = depthmap_original[depthmap_index_cur];

			// adjusted depth map scale and offset
			double* s_ij = reinterpret_cast<double*>(coeff_so.coeff_scale_mat[depthmap_index_cur].data);
			double* o_ij = reinterpret_cast<double*>(coeff_so.coeff_offset_mat[depthmap_index_cur].data);

			// pixels corresponding relationship
			const cv::Mat pixels_corresponding = pixels_corresponding_list.at(depthmap_index_cur).at(depthmap_index_ref);
			int observation_pairs_number = pixels_corresponding.rows;

			if (observation_pairs_number < (depthmap_optim_overlap_ratio_ * depth_map_current.size().area()))
			{
				float overlap_ratio = (double)observation_pairs_number / (double)depth_map_current.size().area();
				LOG(INFO) << "The overlap between depth map " << depthmap_index_ref << " and depth map " << depthmap_index_cur << " is " << overlap_ratio * 100.0 << " which is less than " << depthmap_optim_overlap_ratio_ * 100 << "%, skip the optimization!";
				continue;
			}

			ceres::Problem problem;
			// 1-1) re-projection term
			// grid interpolation weight for each pixel, row-major
			double* bilinear_weight_list = (double*)malloc(grid_width * grid_height * observation_pairs_number * sizeof(double));
			memset(bilinear_weight_list, 0, grid_width * grid_height * observation_pairs_number * sizeof(double));

			// add the depth value and weight value
			for (int observations_index = 0; observations_index < observation_pairs_number; observations_index++)
			{
				const double y_cur = pixels_corresponding.at<double>(observations_index, 0);
				const double x_cur = pixels_corresponding.at<double>(observations_index, 1);
				const double y_ref = pixels_corresponding.at<double>(observations_index, 2);
				const double x_ref = pixels_corresponding.at<double>(observations_index, 3);

				double depth_value_current = getColorSubpix(depth_map_current, cv::Point2f(x_cur, y_cur));
				double depth_value_refernce = getColorSubpix(depth_map_reference, cv::Point2f(x_ref, y_ref));

				// compute the bilinear weights;
				double* bilinear_weight = bilinear_weight_list + grid_width * grid_height * observations_index;
				get_bilinear_weight(bilinear_weight, image_width, image_height, grid_height, grid_width, x_cur, y_cur);
				//cv::Mat bilinear_weight_list_mat(cv::Size(image_width, image_height), CV_64FC1, bilinear_weight);

				// add residual block
				//ceres::LossFunction* reprojectionLoss = new ceres::ScaledLoss(new ceres::CauchyLoss(1), weight_reprojection, ceres::TAKE_OWNERSHIP);
				ceres::LossFunction* reprojectionLoss = new ceres::ScaledLoss(nullptr, weight_reprojection, ceres::TAKE_OWNERSHIP);
				ceres::DynamicAutoDiffCostFunction<ReprojectionResidual, 4>* reprojectionCoast =
					new ceres::DynamicAutoDiffCostFunction<ReprojectionResidual, 4>(
						new ReprojectionResidual(bilinear_weight, depth_value_current, depth_value_refernce, grid_width, grid_height));
				reprojectionCoast->AddParameterBlock(grid_width * grid_height);
				reprojectionCoast->AddParameterBlock(grid_width * grid_height);
				reprojectionCoast->SetNumResiduals(1);

				problem.AddResidualBlock(reprojectionCoast,
					reprojectionLoss,
					s_ij,
					o_ij);
			}

			 //1-2) smooth term
			std::vector<int> index_current_list;
			std::vector<int> index_neighbour_list;
			for (int y_index = 0; y_index < grid_height; y_index++)
			{
				for (int x_index = 0; x_index < grid_width; x_index++)
				{
					int index_current = y_index * grid_width + x_index;
					if (x_index == (grid_width - 1) && y_index == (grid_height - 1))
					{
						continue;
					}
					else if (x_index == grid_width - 1)
					{
						index_current_list.push_back(index_current);
						index_neighbour_list.push_back(index_current + grid_width);
					}
					else if (y_index == grid_height - 1)
					{
						index_current_list.push_back(index_current);
						index_neighbour_list.push_back(index_current + 1);
					}
					else
					{
						index_current_list.push_back(index_current);
						index_neighbour_list.push_back(index_current + 1);
						index_current_list.push_back(index_current);
						index_neighbour_list.push_back(index_current + grid_width);
						index_current_list.push_back(index_current);
						index_neighbour_list.push_back(index_current + grid_width + 1);
						index_current_list.push_back(index_current + 1);
						index_neighbour_list.push_back(index_current + grid_width);
					}
				}
			}
			for (int edge_index = 0; edge_index < index_current_list.size(); edge_index++)
			{
				int index_current = index_current_list[edge_index];
				int index_neighbour = index_neighbour_list[edge_index];
				//std::cout << "current edge index:" << index_current << ", neighbour edge index:" << index_neighbour << std::endl;

				ceres::LossFunction* smoothnessloss_s = new ceres::ScaledLoss(nullptr, weight_smooth, ceres::TAKE_OWNERSHIP);
				ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_S, 4>* smoothnesscoast_s =
					new ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_S, 4>(
						new SmoothnessResidual_S(index_current, index_neighbour));
				smoothnesscoast_s->AddParameterBlock(grid_width * grid_height);
				smoothnesscoast_s->AddParameterBlock(grid_width * grid_height);
				smoothnesscoast_s->SetNumResiduals(1);

				problem.AddResidualBlock(
					smoothnesscoast_s,
					smoothnessloss_s,
					s_ij,
					o_ij);

				ceres::LossFunction* smoothnessloss_o = new ceres::ScaledLoss(nullptr, weight_smooth, ceres::TAKE_OWNERSHIP);
				ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_O, 4>* smoothnesscoast_o =
					new ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_O, 4>(
						new SmoothnessResidual_O(index_current, index_neighbour));
				smoothnesscoast_o->AddParameterBlock(grid_width * grid_height);
				smoothnesscoast_o->AddParameterBlock(grid_width * grid_height);
				smoothnesscoast_o->SetNumResiduals(1);

				problem.AddResidualBlock(
					smoothnesscoast_o,
					smoothnessloss_o,
					s_ij,
					o_ij);

				//ceres::lossfunction* smoothnessloss = new ceres::scaledloss(nullptr, weight_smooth, ceres::take_ownership);
				//ceres::dynamicautodiffcostfunction<smoothnessresidual, 4>* smoothnesscoast =
				//	new ceres::dynamicautodiffcostfunction<smoothnessresidual, 4>(
				//		new smoothnessresidual(index_current, index_neighbour));
				//smoothnesscoast->addparameterblock(grid_x * grid_y);
				//smoothnesscoast->addparameterblock(grid_x * grid_y);
				//smoothnesscoast->setnumresiduals(1);

				//problem.addresidualblock(
				//	smoothnesscoast,
				//	smoothnessloss,
				//	s_ij,
				//	o_ij);
			}

			//1-3) regularization term
			if (weight_scale != 0)
				LOG(WARNING) << "Scale Term weight is not 0.0.";
			ceres::LossFunction* scaleLoss = new ceres::ScaledLoss(nullptr, weight_scale, ceres::TAKE_OWNERSHIP);
			ceres::DynamicAutoDiffCostFunction<ScaleResidual, 4>* scaleCoast =
				new ceres::DynamicAutoDiffCostFunction<ScaleResidual, 4>(
					new ScaleResidual(grid_width, grid_height));
			scaleCoast->AddParameterBlock(grid_width* grid_height);
			scaleCoast->SetNumResiduals(1);

			problem.AddResidualBlock(
				scaleCoast,
				scaleLoss,
				s_ij);

			// Solve the problem
			ceres::Solver::Options options;
			if (ceres_num_threads > 0)
				options.num_threads = ceres_num_threads;
			if (ceres_max_num_iterations > 0)
				options.max_num_iterations = ceres_max_num_iterations;
			if (ceres_max_linear_solver_iterations > 0)
				options.max_linear_solver_iterations = ceres_max_linear_solver_iterations;
			if (ceres_min_linear_solver_iterations > 0)
				options.min_linear_solver_iterations = ceres_min_linear_solver_iterations;

			//options.linear_solver_type = ceres::CGNR;
			options.linear_solver_type = ceres::SPARSE_NORMAL_CHOLESKY;
			//options.minimizer_type = ceres::LINE_SEARCH;
			options.minimizer_progress_to_stdout = true;
			ceres::Solver::Summary summary;
			ceres::Solve(options, &problem, &summary);
			std::cout << summary.FullReport() << "\n";

			// release resource
			free(bilinear_weight_list);
			bilinear_weight_list = nullptr;

			// 2) adjust depth map and update the data
			align_depthmap(depthmap_index_cur);

		} // End of depthmap_counter_dest
	} // End of depthmap_counter_src
}



================================================
FILE: depth-estimation/360monodepth/code/cpp/src/depthmap_stitcher_group.cpp
================================================
#include "depthmap_stitcher_group.hpp"

#include <glog/logging.h>
#include <opencv2/opencv.hpp>
#include <ceres/ceres.h>

#include <vector>
#include <map>
#include <string>

DepthmapStitcherGroup::DepthmapStitcherGroup() {}

DepthmapStitcherGroup::~DepthmapStitcherGroup() {}


//struct DepthmapStitcherGroup::ReprojectionResidual_fixed {
//	ReprojectionResidual_fixed(double* bilinear_weight_list_tar, 
//		double depth_value_tar, double depth_value_ref,
//		int grid_width, int grid_height) :
//		bilinear_weight_list_tar_(bilinear_weight_list_tar),
//		depth_value_tar_(depth_value_tar), depth_value_ref_(depth_value_ref),
//		grid_width_(grid_width), grid_height_(grid_height) {}
//
//	template <typename T>
//	bool operator()(T const* const* scale_offset_list, T* residual) const
//	{
//		const T* scale_list_tar = scale_offset_list[0];
//		const T* offset_list_tar = scale_offset_list[1];
//		T scale_tar(0);
//		T offset_tar(0);
//		for (int index = 0; index < grid_width_ * grid_height_; index++)
//		{
//			scale_tar += scale_list_tar[index] * bilinear_weight_list_tar_[index];
//			offset_tar += offset_list_tar[index] * bilinear_weight_list_tar_[index];
//		}
//		T temp = (depth_value_tar_ * scale_tar + offset_tar) - depth_value_ref_ ;
//		residual[0] = temp * temp;
//		return true;
//	}
//
//private:
//	const double depth_value_ref_; // the depth value of source depth map
//	const double depth_value_tar_; // the depth value of corresponding pixel in reference depth map
//	const double* bilinear_weight_list_tar_; // weight of S and A, length is grid_x * grid_y
//	const int grid_width_; // the weight grid along the x axis
//	const int grid_height_; // the weight grid along the y axis
//};

// TODO capsule more function to refer Ceres demo. 
struct DepthmapStitcherGroup::ReprojectionResidual {
	ReprojectionResidual(double* bilinear_weight_list_tar, double* bilinear_weight_list_src,
		double depth_value_tar, double depth_value_src,
		int grid_width, int grid_height) :
		bilinear_weight_list_tar_(bilinear_weight_list_tar), bilinear_weight_list_src_(bilinear_weight_list_src),
		depth_value_tar_(depth_value_tar), depth_value_src_(depth_value_src),
		grid_width_(grid_width), grid_height_(grid_height) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		const T* scale_list_src = scale_offset_list[0];
		const T* offset_list_src = scale_offset_list[1];

		const T* scale_list_tar = scale_offset_list[2];
		const T* offset_list_tar = scale_offset_list[3];

		T scale_src(0);
		T offset_src(0);
		T scale_tar(0);
		T offset_tar(0);

		for (int index = 0; index < grid_width_ * grid_height_; index++)
		{
			scale_src += scale_list_src[index] * bilinear_weight_list_src_[index];
			offset_src += offset_list_src[index] * bilinear_weight_list_src_[index];

			scale_tar += scale_list_tar[index] * bilinear_weight_list_tar_[index];
			offset_tar += offset_list_tar[index] * bilinear_weight_list_tar_[index];
		}
		T temp = (depth_value_tar_ * scale_tar + offset_tar) - (depth_value_src_ * scale_src + offset_src);
		//T temp = (depth_value_tar_ * scale_tar + offset_tar) - depth_value_src_ ;
		residual[0] = temp * temp;
		return true;
	}

private:
	const double depth_value_src_; // the depth value of source depth map
	const double depth_value_tar_; // the depth value of corresponding pixel in reference depth map
	const double* bilinear_weight_list_tar_; // weight of S and A, length is grid_x * grid_y
	const double* bilinear_weight_list_src_; // weight of S and A, length is grid_x * grid_y
	const int grid_width_; // the weight grid along the x axis
	const int grid_height_; // the weight grid along the y axis
};

// @see hedman2018instant:equ_6
struct DepthmapStitcherGroup::SmoothnessResidual_S {
	SmoothnessResidual_S(int* index_current, int* index_neighbour, int edge_number, int grid_x, int grid_y) :
		grid_width_(grid_x), grid_height_(grid_y),
		index_current_(index_current), index_neighbour_(index_neighbour), edge_number_(edge_number) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		const T* scale_list = scale_offset_list[0];
		//const T* offset_list = scale_offset_list[1];
		T sum_value(0);
		for (int index = 0; index < edge_number_; index++)
		{
			int curr_index = index_current_[index];
			int neig_index = index_neighbour_[index];
			T scale_diff = scale_list[curr_index] - scale_list[neig_index];
			//T offset_diff = offset_list[curr_index] - offset_list[neig_index];
			sum_value += scale_diff * scale_diff;// +offset_diff * offset_diff;
		}
		residual[0] = sum_value;
		return true;
	}

private:
	const int grid_width_; // the weight grid along the x axis
	const int grid_height_; // the weight grid along the y axis
	const int* index_current_; // the weight grid along the x axis
	const int* index_neighbour_; // the weight grid along the y axis
	const int edge_number_;
};


// @see hedman2018instant:equ_6
struct DepthmapStitcherGroup::SmoothnessResidual_O {
	SmoothnessResidual_O(int* index_current, int* index_neighbour, int edge_number, int grid_x, int grid_y) :
		grid_width_(grid_x), grid_height_(grid_y),
		index_current_(index_current), index_neighbour_(index_neighbour), edge_number_(edge_number) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		const T* scale_list = scale_offset_list[0];
		const T* offset_list = scale_offset_list[1];
		T sum_value(0);
		for (int index = 0; index < edge_number_; index++)
		{
			int curr_index = index_current_[index];
			int neig_index = index_neighbour_[index];
			//T scale_diff = scale_list[curr_index] - scale_list[neig_index];
			T offset_diff = offset_list[curr_index] - offset_list[neig_index];
			sum_value +=  offset_diff * offset_diff;
		}
		residual[0] = sum_value;
		return true;
	}

private:
	const int grid_width_; // the weight grid along the x axis
	const int grid_height_; // the weight grid along the y axis
	const int* index_current_; // the weight grid along the x axis
	const int* index_neighbour_; // the weight grid along the y axis
	const int edge_number_;
};

// @see hedman2018instant:equ_6
struct DepthmapStitcherGroup::SmoothnessResidual {
	SmoothnessResidual(int* index_current, int* index_neighbour, int edge_number, int grid_x, int grid_y) :
		grid_width_(grid_x), grid_height_(grid_y),
		index_current_(index_current), index_neighbour_(index_neighbour), edge_number_(edge_number) {}

	template <typename T>
	bool operator()(T const* const* scale_offset_list, T* residual) const
	{
		const T* scale_list = scale_offset_list[0];
		const T* offset_list = scale_offset_list[1];
		T sum_value(0);
		for (int index = 0; index < edge_number_; index++)
		{
			int curr_index = index_current_[index];
			int neig_index = index_neighbour_[index];
			T scale_diff = scale_list[curr_index] - scale_list[neig_index];
			T offset_diff = offset_list[curr_index] - offset_list[neig_index];
			sum_value += scale_diff * scale_diff + offset_diff * offset_diff;
		}
		residual[0] = sum_value;
		return true;
	}

private:
	const int grid_width_; // the weight grid along the x axis
	const int grid_height_; // the weight grid along the y axis
	const int* index_current_; // the weight grid along the x axis
	const int* index_neighbour_; // the weight grid along the y axis
	const int edge_number_;
};

// @see hedman2018instant:equ_7
struct DepthmapStitcherGroup::ScaleResidual {
	ScaleResidual(int grid_width, int grid_height) :
		grid_width_(grid_width), grid_height_(grid_height) {}

	template <typename T>
	bool operator()(T const* const* scale_list_list, T* residual) const
	{
		const T* scale_list = scale_list_list[0];
		T sum_number(0);
		for (int index = 0; index < grid_width_ * grid_height_; index++)
		{
			sum_number += 1.0 / (scale_list[index] + T(1e-10));
		}
		residual[0] = sum_number;
//      if (ceres::isinf(residual[0]) || ceres::isnan(residual[0])){
//          printf("INFINITY SCALE\n");
//      }
		return true;
	}

private:
	const int grid_width_; // the weight grid along the x axis
	const int grid_height_; // the weight grid along the y axis
};

void DepthmapStitcherGroup::initial(const int grid_width, const int grid_height)
{
	if (depthmap_original.size() < 2)
		throw std::runtime_error("The depth map image less than 2.");

	DepthmapStitcher::initial(grid_width, grid_height);

	// the first depth as the reference depth map
	coeff_so.set_value_const(1.0, 0.0);
}

void DepthmapStitcherGroup::compute_align_coeff()
{
	// 0) load set the data and parameters
	double* s_ij_list = coeff_so.coeff_scale.get();
	double* o_ij_list = coeff_so.coeff_offset.get();

	int image_width = depthmap_original[0].cols;
	int image_height = depthmap_original[0].rows;

	// set the reference depth map scale and offset 
	int depthmap_ref_intidx = extidx2intidx[depthmap_ref_extidx];
	coeff_so.coeff_scale_mat[depthmap_ref_intidx].setTo(1.0);
	coeff_so.coeff_offset_mat[depthmap_ref_intidx].setTo(0.0);

	// 1) make the energy function
	// 1-1) re-projection term
	LOG(INFO) << "Adding re-projection term" << std::endl;
	ceres::Problem problem;
	std::vector<void* > bilinear_weight_list_mem;
	std::vector<std::pair<int, int>> ignore_image_pair;

	// add the parameter block & lock the parameters of reference block
	for (int i = 0; i < coeff_so.coeff_scale_mat.size(); i++)
	{
		problem.AddParameterBlock((double*)coeff_so.coeff_scale_mat[i].data, grid_height * grid_width);
		problem.AddParameterBlock((double*)coeff_so.coeff_offset_mat[i].data, grid_height * grid_width);
	}
	if (depthmap_ref_extidx > 0)
	{
		LOG(INFO) << "Fix the reference frame " << depthmap_ref_extidx << " deformation coefficients.";
		problem.SetParameterBlockConstant(s_ij_list + depthmap_ref_intidx * grid_width * grid_height);
		problem.SetParameterBlockConstant(o_ij_list + depthmap_ref_intidx * grid_width * grid_height);
	}

	//std::vector<int> constant_translation;
	//for (int idx = 0; idx < grid_width * grid_height; idx++)
	//	constant_translation.push_back(idx);
	//ceres::SubsetParameterization* subset_parameterization = new ceres::SubsetParameterization(grid_width * grid_height, constant_translation);
	//problem.SetParameterization((double*)coeff_so.coeff_scale_mat[depthmap_ref_intidx].data, subset_parameterization);
	//problem.SetParameterization((double*)coeff_so.coeff_offset_mat[depthmap_ref_intidx].data, subset_parameterization);

	if (projection_per_pixelcost_enable)
	{
		unsigned int pixel_corr_num = 0;
		for (int depthmap_index_src = 0; depthmap_index_src < depthmap_original.size(); depthmap_index_src++)
			for (int depthmap_index_tar = 0; depthmap_index_tar < depthmap_original.size(); depthmap_index_tar++)
			{
				if (depthmap_index_tar == depthmap_index_src)
					continue;
				pixel_corr_num += pixels_corresponding_list.at(depthmap_index_src).at(depthmap_index_tar).rows;
			}

		weight_reprojection = weight_reprojection * ( 1.0 / pixel_corr_num);
		LOG(INFO) << "Enable perpixel reprojection weight, pixel_corr_num is " << pixel_corr_num << ", term weight is " << weight_reprojection;
	}
	// TODO allocate memory once and assign pointer for each corresponding weights
	
	if (smooth_pergrid_enable)
	{
		unsigned int smooth_gird_numb = 0;
		smooth_gird_numb = depthmap_original.size() * grid_height * grid_width;
		weight_smooth = weight_smooth * (1.0 / smooth_gird_numb);
		LOG(INFO) << "Enable pergrid smooth weight, grid_numb is " << smooth_gird_numb << ", term weight is " << weight_smooth;
	}

	// adjusted depth map register to reference depth map to compute the scale and offset
	int omp_num_threads = omp_get_max_threads() - 2;
	LOG(INFO) << "Build ceres problem with " << omp_num_threads << " threads.";
	#pragma omp parallel for ordered schedule(dynamic) num_threads(omp_num_threads)
	for (int depthmap_index_src = 0; depthmap_index_src < depthmap_original.size(); depthmap_index_src++)
	{
		DLOG(INFO) << "Adding the " << depthmap_index_src << " depth alignment information to problem."; 
		const cv::Mat& depth_map_src = depthmap_original[depthmap_index_src];
		//LOG(INFO) << "Target depth map" << depthmap_index_tar;
		for (int depthmap_index_tar = 0; depthmap_index_tar < depthmap_original.size(); depthmap_index_tar++)
		{
			if (depthmap_index_tar == depthmap_index_src)
				continue;

			// pixels corresponding relationship
			const cv::Mat& pixels_corresponding = pixels_corresponding_list.at(depthmap_index_src).at(depthmap_index_tar);
			int observation_pairs_number = pixels_corresponding.rows;
			if (observation_pairs_number == 0)
			{
				#pragma omp critical(ignore_image_pair)
				{
					ignore_image_pair.push_back(std::pair<int, int>(depthmap_index_src, depthmap_index_tar));
				}
				continue;
			}

			// adjusted depth map
			const cv::Mat& depth_map_tar = depthmap_original[depthmap_index_tar];
			//LOG(INFO) << "Source depth map:" << depthmap_index_src;

			// grid interpolation weight for each pixel, row-major
			double* bilinear_weight_list_src = (double*)malloc(grid_width * grid_height * observation_pairs_number * sizeof(double));
			// memset(bilinear_weight_list_src, 0, grid_width * grid_height * observation_pairs_number * sizeof(double));
			double* bilinear_weight_list_tar = (double*)malloc(grid_width * grid_height * observation_pairs_number * sizeof(double));
			// memset(bilinear_weight_list_tar, 0, grid_width * grid_height * observation_pairs_number * sizeof(double));

			#pragma omp critical(bilinear_weight_list_mem)
			{
				bilinear_weight_list_mem.push_back((void*)bilinear_weight_list_src);
				bilinear_weight_list_mem.push_back((void*)bilinear_weight_list_tar);
			}

			// add the depth value and weight value
			for (int observations_index = 0; observations_index < observation_pairs_number; observations_index++)
			{
				const double y_src = pixels_corresponding.at<double>(observations_index, 0);
				const double x_src = pixels_corresponding.at<double>(observations_index, 1);
				const double y_tar = pixels_corresponding.at<double>(observations_index, 2);
				const double x_tar = pixels_corresponding.at<double>(observations_index, 3);

				double depth_value_src = getColorSubpix(depth_map_src, cv::Point2f(x_src, y_src));
				double depth_value_tar = getColorSubpix(depth_map_tar, cv::Point2f(x_tar, y_tar));

				// compute the bilinear weights;
				double* bilinear_weight_src = bilinear_weight_list_src + grid_width * grid_height * observations_index;
				get_bilinear_weight(bilinear_weight_src, image_width, image_height, grid_height, grid_width, x_src, y_src);
				double* bilinear_weight_tar = bilinear_weight_list_tar + grid_width * grid_height * observations_index;
				get_bilinear_weight(bilinear_weight_tar, image_width, image_height, grid_height, grid_width, x_tar, y_tar);

				//// for debug visuzlize the cv::Mat
				//cv::Mat bilinear_weight_list_mat_src(cv::Size( coeff_so.coeff_cols, coeff_so.coeff_rows), CV_64FC1, bilinear_weight_src);
				//cv::Mat bilinear_weight_list_mat_tar(cv::Size(coeff_so.coeff_cols, coeff_so.coeff_rows), CV_64FC1, bilinear_weight_tar);

				// add residual block 
				//if (depthmap_index_tar != depthmap_ref_intidx && depthmap_index_src != depthmap_ref_intidx)
				{
					// TODO The cauchyless is made bad result. figure our reason.
					//ceres::LossFunction* reprojectionLoss = new ceres::ScaledLoss(new ceres::CauchyLoss(1), weight_re-projection, ceres::TAKE_OWNERSHIP);
					ceres::LossFunction* reprojectionLoss = new ceres::ScaledLoss(nullptr, weight_reprojection, ceres::TAKE_OWNERSHIP);
					ceres::DynamicAutoDiffCostFunction<ReprojectionResidual, 4>* reprojectionCoast =
						new ceres::DynamicAutoDiffCostFunction<ReprojectionResidual, 4>(
							new ReprojectionResidual(bilinear_weight_tar, bilinear_weight_src,
								depth_value_tar, depth_value_src,
								grid_width, grid_height));

					// separate each depth map coefficients to reduce the Jacobian matrix scale
					reprojectionCoast->AddParameterBlock(grid_width * grid_height);
					reprojectionCoast->AddParameterBlock(grid_width * grid_height);
					reprojectionCoast->AddParameterBlock(grid_width * grid_height);
					reprojectionCoast->AddParameterBlock(grid_width * grid_height);
					reprojectionCoast->SetNumResiduals(1);

					#pragma omp critical(problem)
					{
						problem.AddResidualBlock(
							reprojectionCoast,
							reprojectionLoss,
							(s_ij_list + depthmap_index_src * grid_width * grid_height),
							(o_ij_list + depthmap_index_src * grid_width * grid_height),
							(s_ij_list + depthmap_index_tar * grid_width * grid_height),
							(o_ij_list + depthmap_index_tar * grid_width * grid_height));
					}
				}
				//else if(depthmap_index_src == depthmap_ref_intidx){
				//	ceres::LossFunction* reprojectionLoss = new ceres::ScaledLoss(nullptr, weight_reprojection, ceres::TAKE_OWNERSHIP);
				//	//ceres::LossFunction* reprojectionLoss = new ceres::ScaledLoss(new ceres::CauchyLoss(1), weight_reprojection, ceres::TAKE_OWNERSHIP);
				//	ceres::DynamicAutoDiffCostFunction<ReprojectionResidual_fixed, 4>* reprojectionCoast =
				//		new ceres::DynamicAutoDiffCostFunction<ReprojectionResidual_fixed, 4>(
				//			new ReprojectionResidual_fixed(bilinear_weight_tar,
				//				depth_value_tar, depth_value_src, grid_width, grid_height));

				//	// separate each depth map coefficients to reduce the Jacobian matrix scale
				//	reprojectionCoast->AddParameterBlock(grid_width * grid_height);
				//	reprojectionCoast->AddParameterBlock(grid_width * grid_height);
				//	reprojectionCoast->SetNumResiduals(1);

				//	problem.AddResidualBlock(
				//		reprojectionCoast,
				//		reprojectionLoss,
				//		(s_ij_list + depthmap_index_tar * grid_width * grid_height),
				//		(o_ij_list + depthmap_index_tar * grid_width * grid_height));
				//	//problem.AddResidualBlock(
				//	//	reprojectionCoast,
				//	//	nullptr,
				//	//	(s_ij_list + depthmap_index_tar * grid_width * grid_height),
				//	//	(o_ij_list + depthmap_index_tar * grid_width * grid_height));
				//}
			}

		}// End of depthmap_counter_adjust
	}// End of depthmap_counter_ref

	// report the overlap 0 image pairs.
	if (ignore_image_pair.size() != 0)
	{
		std::stringstream ss;
		ss << "The overlap between depth map ";
		for (auto item : ignore_image_pair)
		{
			ss << item.first << "=>" << item.second << "\t";
		}
		ss << " is 0%, skip!";
		LOG(INFO) << ss.str();
	}

	// 1-2) smooth term
	LOG(INFO) << "Adding smooth term..";
	// pre-compute the edge size for smooth term
	int grid_edge_number = 4 * (grid_width - 1) * (grid_height - 1) + (grid_width - 1) + (grid_height - 1);
	int* index_current_list = (int*)malloc(grid_edge_number * sizeof(int));
	memset(index_current_list, 0, grid_edge_number * sizeof(int));
	int* index_neighbour_list = (int*)malloc(grid_edge_number * sizeof(int));
	memset(index_neighbour_list, 0, grid_edge_number * sizeof(int));

	// to be free
	bilinear_weight_list_mem.push_back((void*)index_current_list);
	bilinear_weight_list_mem.push_back((void*)index_neighbour_list);

	int edge_counter = 0;
	for (int y_index = 0; y_index < grid_height; y_index++)
	{
		for (int x_index = 0; x_index < grid_width; x_index++)
		{
			int index_current = y_index * grid_width + x_index;

			if (x_index == (grid_width - 1) && y_index == (grid_height - 1))
			{
				continue;
			}
			else if (x_index == grid_width - 1)
			{
				index_current_list[edge_counter] = (index_current);
				index_neighbour_list[edge_counter] = (index_current + grid_width);
				edge_counter++;
			}
			else if (y_index == grid_height - 1)
			{
				index_current_list[edge_counter] = (index_current);
				index_neighbour_list[edge_counter] = (index_current + 1);
				edge_counter++;
			}
			else
			{
				index_current_list[edge_counter] = (index_current);
				index_neighbour_list[edge_counter] = (index_current + 1);
				edge_counter++;
				index_current_list[edge_counter] = (index_current);
				index_neighbour_list[edge_counter] = (index_current + grid_width);
				edge_counter++;
				index_current_list[edge_counter] = (index_current);
				index_neighbour_list[edge_counter] = (index_current + grid_width + 1);
				edge_counter++;
				index_current_list[edge_counter] = (index_current + 1);
				index_neighbour_list[edge_counter] = (index_current + grid_width);
				edge_counter++;
			}
		}
	}
	if (edge_counter != grid_edge_number)
		LOG(ERROR) << "Smooth term edge number error! " << edge_counter << " , it should be " << grid_edge_number;

	for (int depthmap_index = 0; depthmap_index < depthmap_original.size(); depthmap_index++)
	{
		ceres::LossFunction* smoothnessLoss = new ceres::ScaledLoss(nullptr, weight_smooth, ceres::TAKE_OWNERSHIP);
		ceres::DynamicAutoDiffCostFunction<SmoothnessResidual, 4>* smoothnessCoast =
			new ceres::DynamicAutoDiffCostFunction<SmoothnessResidual, 4>(
				new SmoothnessResidual(index_current_list, index_neighbour_list, grid_edge_number, grid_width, grid_height));
		smoothnessCoast->AddParameterBlock(grid_width* grid_height);
		smoothnessCoast->AddParameterBlock(grid_width* grid_height);
		smoothnessCoast->SetNumResiduals(1);

		problem.AddResidualBlock(
			smoothnessCoast,
			smoothnessLoss,
			s_ij_list + depthmap_index * grid_width * grid_height,
			o_ij_list + depthmap_index * grid_width * grid_height);

		//ceres::LossFunction* smoothnessLoss_s = new ceres::ScaledLoss(nullptr, weight_smooth, ceres::TAKE_OWNERSHIP);
		//ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_S, 4>* smoothnessCoast_s =
		//	new ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_S, 4>(
		//		new SmoothnessResidual_S(index_current_list, index_neighbour_list, grid_edge_number, grid_width, grid_height));
		//smoothnessCoast_s->AddParameterBlock(grid_width* grid_height);
		//smoothnessCoast_s->AddParameterBlock(grid_width* grid_height);
		//smoothnessCoast_s->SetNumResiduals(1);

		//problem.AddResidualBlock(
		//	smoothnessCoast_s,
		//	smoothnessLoss_s,
		//	s_ij_list + depthmap_index * grid_width * grid_height,
		//	o_ij_list + depthmap_index * grid_width * grid_height);

		//ceres::LossFunction* smoothnessLoss_o = new ceres::ScaledLoss(nullptr, weight_smooth , ceres::TAKE_OWNERSHIP);
		//ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_O, 4>* smoothnessCoast_o =
		//	new ceres::DynamicAutoDiffCostFunction<SmoothnessResidual_O, 4>(
		//		new SmoothnessResidual_O(index_current_list, index_neighbour_list, grid_edge_number, grid_width, grid_height));
		//smoothnessCoast_o->AddParameterBlock(grid_width* grid_height);
		//smoothnessCoast_o->AddParameterBlock(grid_width* grid_height);
		//smoothnessCoast_o->SetNumResiduals(1);

		//problem.AddResidualBlock(
		//	smoothnessCoast_o,
		//	smoothnessLoss_o,
		//	s_ij_list + depthmap_index * grid_width * grid_height,
		//	o_ij_list + depthmap_index * grid_width * grid_height);
	}

	//1-3) regularization term
	LOG(INFO) << "Adding regular term..";
	if (weight_scale != 0 && depthmap_ref_extidx > 0)
		LOG(WARNING) << "Both scale weight and reference frame index are set!";

	for (int depthmap_index = 0; depthmap_index < depthmap_original.size(); depthmap_index++)
	{
		ceres::LossFunction* scaleLoss = new ceres::ScaledLoss(nullptr, weight_scale, ceres::TAKE_OWNERSHIP);
		ceres::DynamicAutoDiffCostFunction<ScaleResidual, 4>* scaleCoast =
			new ceres::DynamicAutoDiffCostFunction<ScaleResidual, 4>(
				new ScaleResidual(grid_width, grid_height));
		scaleCoast->AddParameterBlock(grid_width * grid_height);
		scaleCoast->SetNumResiduals(1);
		problem.AddResidualBlock(
			scaleCoast,
			scaleLoss,
			s_ij_list + depthmap_index * grid_width * grid_height);
	}

	// fix the reference depthmap's scale and offset coefficients
	//problem.SetParameterBlockConstant(s_ij_list + depthmap_ref_intidx * grid_width * grid_height);
	//problem.SetParameterBlockConstant(o_ij_list + depthmap_ref_intidx * grid_width * grid_height);

	// 2) Solve the problem
	ceres::Solver::Options options;
	if (ceres_num_threads > 0)
	{
		LOG(INFO) << "Ceres solver num_threads is: " << ceres_num_threads;
		options.num_threads = ceres_num_threads;
	}
	if (ceres_max_num_iterations > 0)
	{
		LOG(INFO) << "Ceres solver ceres_max_num_iterations is: " << ceres_max_num_iterations;
		options.max_num_iterations = ceres_max_num_iterations;
	}
	if (ceres_max_linear_solver_iterations > 0)
	{
		LOG(INFO) << "Ceres solver ceres_max_linear_solver_iterations is: " << ceres_max_linear_solver_iterations;
		options.max_linear_solver_iterations = ceres_max_linear_solver_iterations;
	}
	if (ceres_min_linear_solver_iterations > 0)
	{
		LOG(INFO) << "Ceres solver min_linear_solver_iterations is: " << ceres_min_linear_solver_iterations;
		options.min_linear_solver_iterations = ceres_min_linear_solver_iterations;
	}

	//options.minimizer_type = ceres::TRUST_REGION;
	options.minimizer_type = ceres::LINE_SEARCH;

	options.linear_solver_type = ceres::SPARSE_NORMAL_CHOLESKY;
	// options.linear_solver_type = ceres::SPARSE_SCHUR; // support multi-thread

	//options.line_search_direction_type = ceres::STEEPEST_DESCENT;
	//options.line_search_direction_type = ceres::LBFGS;

	//options.line_search_type = ceres::WOLFE;

	options.minimizer_progress_to_stdout = true;
	ceres::Solver::Summary summary;
	ceres::Solve(options, &problem, &summary);
	std::cout << summary.FullReport() << "\n";

	std::cout << "The iteration number is: " << summary.iterations.size() << std::endl;
	std::cout << "Number inner iteration number:" << summary.num_inner_iteration_steps << std::endl;
	std::cout << "The total time consume (second): " << summary.total_time_in_seconds << std::endl;
	double time_consume_each_iter = summary.total_time_in_seconds / summary.iterations.size();
	std::cout << "Each iteration average time (second) " << time_consume_each_iter << std::endl;

	// assign & release resource
	for (void* pointer : bilinear_weight_list_mem)
		free(pointer);
}



================================================
FILE: depth-estimation/360monodepth/code/cpp/src/depthmap_utility.cpp
================================================
#include "depthmap_utility.hpp"


void DepthmapUtil::normalize(const cv::Mat& depthmap, cv::Mat& depthmap_normalized)
{
	// 1) compute the media number
	int histSize = 256;
	float range[] = { 0, 256 };
	const float* histRange = { range };
	bool uniform = true;
	bool accumulate = false;
	cv::Mat hist;
	cv::calcHist(&depthmap_normalized, 1, 0, cv::Mat(), hist, 1, &histSize, &histRange, uniform, accumulate);

	int bin = 0;
	double median = -1.0;
	double middle_idx = (depthmap_normalized.rows * depthmap_normalized.cols) / 2;
	for (int i = 0; i < histSize && median < 0.0; ++i)
	{
		bin += cvRound(hist.at< float >(i));
		if (bin > middle_idx && median < 0.0)
			median = i;
	}

	//// normalize the image
	//dev_dispmap = np.sum(np.abs(dispmap[mask] - median_dispmap)) / np.sum(mask)
	//dispmap_norm = np.full(dispmap.shape, np.nan, dtype = np.float64)
	//dispmap_norm[mask] = (dispmap[mask] - median_dispmap) / dev_dispmap

}


void DepthmapUtil::deform(const cv::Mat& depth_map_original, cv::Mat& depthmap_deformed, const cv::Mat& scale_mat, const cv::Mat& offset_mat)
{
	int image_width = depth_map_original.cols;
	int image_height = depth_map_original.rows;

	int grid_width = scale_mat.cols;
	int grid_height = scale_mat.rows;

	// 1) compute the bilinear weight for the scale and offset
	// 1-0) get the mesh grid
	// the sub-pixels location used to interpolation the scale and offset grid
	std::vector<float> grid_list_x, grid_list_y;
	float grid_col_interval = (image_width - 1.0) / (grid_width - 1.0);
	for (int i = 0; i < image_width; i++)
		grid_list_x.push_back((float)i / grid_col_interval);
	cv::Mat grid_list_x_mat = cv::Mat(grid_list_x);

	float grid_row_interval = (image_height - 1.0) / (grid_height - 1.0);
	for (int i = 0; i < image_height; i++)
		grid_list_y.push_back((float)i / grid_row_interval);
	cv::Mat  grid_list_y_mat = cv::Mat(grid_list_y);

	cv::Mat meshgrid_x, meshgrid_y;
	cv::repeat(grid_list_x_mat.reshape(1, 1), image_height, 1, meshgrid_x);
	cv::repeat(grid_list_y_mat.reshape(1, image_height), 1, image_width, meshgrid_y);

	// 1-1) compute the pixels scale and offset
	// the origin is Top-Left
	cv::Mat scale_mat_pixels, offset_mat_pixels;
	cv::remap(scale_mat, scale_mat_pixels, meshgrid_x, meshgrid_y, cv::INTER_LINEAR, cv::BORDER_CONSTANT, cv::Scalar(0));
	cv::remap(offset_mat, offset_mat_pixels, meshgrid_x, meshgrid_y, cv::INTER_LINEAR, cv::BORDER_CONSTANT, cv::Scalar(0));

	// 2) compute the depth map
	depthmap_deformed = scale_mat_pixels.mul(depth_map_original) + offset_mat_pixels;
}


void DepthmapUtil::bilinear_weight(double* weight_list,
	const int image_width, const int image_height,
	const int grid_width, const int grid_height,
	const double x, const double y)
{
	std::memset(weight_list, 0, grid_height * grid_width * sizeof(double));
	// for (int idx = 0; idx < grid_height * grid_width; idx++)
	// {
	// 	weight_list[idx] = 0;
	// }

	// get the range of the pixel's block, and the origin is Top-Left
	// the col up and low
	double grid_col_interval = (image_width - 1.0) / (grid_width - 1.0);
	int grid_col_low = int(x / grid_col_interval);
	double grid_col_low_pixel = grid_col_low * grid_col_interval;
	int grid_col_up;
	if (grid_col_low_pixel == x)
		grid_col_up = grid_col_low;
	else
		grid_col_up = grid_col_low + 1;
	double grid_col_up_pixel = grid_col_up * grid_col_interval;

	// the row up and low
	double grid_row_interval = (image_height - 1.0) / (grid_height - 1.0);
	int grid_row_low = int(y / grid_row_interval);
	double grid_row_low_pixel = grid_row_low * grid_row_interval;
	int grid_row_up;
	if (grid_row_low_pixel == y)
		grid_row_up = grid_row_low;
	else
		grid_row_up = grid_row_low + 1;
	double grid_row_up_pixel = grid_row_up * grid_row_interval;

	// compute the bilinear weights
	if (grid_row_low == grid_row_up && grid_col_low != grid_col_up)
	{ 
		// in the same row
		double weight_denominator = grid_col_up_pixel - grid_col_low_pixel;
		weight_list[grid_row_low * grid_width + grid_col_low] = (grid_col_up_pixel - x) / weight_denominator; //the Top-Left && Bottom-Left
		weight_list[grid_row_low * grid_width + grid_col_up] = (x - grid_col_low_pixel) / weight_denominator; // The Top-Right && Bottom-Right
	}
	else if (grid_row_low != grid_row_up && grid_col_low == grid_col_up)
	{
		// in the same column
		double weight_denominator = grid_row_up_pixel - grid_row_low_pixel;
		weight_list[grid_row_low * grid_width + grid_col_low] = (grid_row_up_pixel - y) / weight_denominator; //the Top-Left && Top-Right
		weight_list[grid_row_up * grid_width + grid_col_low] = (y - grid_row_low_pixel) / weight_denominator; // The Bottom-Left && Bottom-Right
	}
	else if (grid_row_low == grid_row_up && grid_col_low == grid_col_up)
	{
		// in the same point
		weight_list[grid_row_low * grid_width + grid_col_low] = 1.0;
	}
	else
	{
		double weight_denominator = (grid_row_up_pixel - grid_row_low_pixel) * (grid_col_up_pixel - grid_col_low_pixel);
		weight_list[grid_row_low * grid_width + grid_col_low] = (grid_col_up_pixel - x) * (grid_row_up_pixel - y) / weight_denominator; //the Top-Left
		weight_list[grid_row_low * grid_width + grid_col_up] = (x - grid_col_low_pixel) * (grid_row_up_pixel - y) / weight_denominator; // The Top-Right
		weight_list[grid_row_up * grid_width + grid_col_low] = (grid_col_up_pixel - x) * (y - grid_row_low_pixel) / weight_denominator; // The Bottom-Left
		weight_list[grid_row_up * grid_width + grid_col_up] = (x - grid_col_low_pixel) * (y - grid_row_low_pixel) / weight_denominator; // The Bottom-Right
	}
}


cv::Mat DepthmapUtil::depthmap_orig2norm(const cv::Mat& orig_mat, double& mean, double& stddev)
{
	cv::Scalar mean_, stddev_;
	cv::meanStdDev(orig_mat, mean_, stddev_);
	mean = mean_[0];
	stddev = stddev_[0];

	cv::Mat norm_mat = (orig_mat - mean_) / stddev_;
	return norm_mat;
}


cv::Mat DepthmapUtil::depthmap_norm2orig(const cv::Mat& norm_mat, const double mean, const double dev)
{
	return norm_mat * dev + mean;
}



================================================
FILE: depth-estimation/360monodepth/code/cpp/src/EigenSolvers.cpp
================================================
//
// Created by manuel on 11/10/2021.
//
#include<Eigen/SparseCore>
#include<Eigen/SparseCholesky>
#include<Eigen/SparseQR>
#include<Eigen/SparseLU>
#include<Eigen/IterativeLinearSolvers>
#include <iostream>
#include <memory>
#include <glog/logging.h>
#include "chrono"
#include <iomanip>
#include "pybind11/pybind11.h"
#include "pybind11/eigen.h"

class LinearSolver
{

public:
    // Supported Solvers. More can be added
    enum solverType
    {
        SimplicialLLT,
        SimplicialLDLT,
        SparseLU,
        ConjugateGradient,
        LeastSquaresConjugateGradient,
        BiCGSTAB
    };

    // Constructors
    LinearSolver(const Eigen::SparseMatrix<double>& A, solverType type) : A(A), type(type) {
        factorize();
    };

    LinearSolver(const Eigen::SparseMatrix<double>& A, solverType type, int maxiters, double tol) :
            A(A), type(type), maxiters(maxiters), tol(tol) {
        factorize();
    };

    LinearSolver(){
        type = solverType::BiCGSTAB;
    }

    explicit LinearSolver(solverType type) : type(type){ }

    ~LinearSolver() = default;

    // Compute Factorization of A matrix as soon as the object is created.
    void factorize(){
        switch (type) {
            case SimplicialLLT:
                solverLLT = std::make_unique<Eigen::SimplicialLLT<Eigen::SparseMatrix<double>>>();
                solverLLT->compute(A);
                break;
            case SimplicialLDLT:
                solverLDLT = std::make_unique<Eigen::SimplicialLDLT<Eigen::SparseMatrix<double>>>();
                solverLDLT->compute(A);
                break;
            case SparseLU:
                solverLU = std::make_unique<Eigen::SparseLU<Eigen::SparseMatrix<double>>>();
                solverLU->compute(A);
                break;
            case ConjugateGradient:
                solverCG = std::make_unique<Eigen::ConjugateGradient<Eigen::SparseMatrix<double>>>();
                solverCG->setMaxIterations(maxiters);
                solverCG->setTolerance(tol);
                solverCG->compute(A);
                break;
            case LeastSquaresConjugateGradient:
                solverLSCG = std::make_unique<Eigen::LeastSquaresConjugateGradient<Eigen::SparseMatrix<double>>>();
                solverLSCG->setMaxIterations(maxiters);
                solverLSCG->setTolerance(tol);
                solverLSCG->compute(A);
                break;
            case BiCGSTAB:
                solverBiCGSTAB = std::make_unique<Eigen::BiCGSTAB<Eigen::SparseMatrix<double>>>();
                solverBiCGSTAB->setMaxIterations(maxiters);
                solverBiCGSTAB->setTolerance(tol);
                solverBiCGSTAB->compute(A);
                break;
        }
    }

    // Solve the system for an arbitrary b vector
    Eigen::VectorXd solve(const Eigen::VectorXd& b) {
        Eigen::VectorXd x;
        auto start_time = std::chrono::system_clock::now();
        switch (LinearSolver::type) {
            case SimplicialLLT:
                LOG(INFO) << "Solving with SimplicialLLT ... \n";
                x = solverLLT->solve(b);
                break;
            case SimplicialLDLT:
                LOG(INFO) << "Solving with SimplicialLDLT ... \n";
                x = solverLDLT->solve(b);
                break;
            case SparseLU:
                LOG(INFO) << "Solving with Sparse LU ... \n";
                x = solverLU->solve(b);
                break;
            case ConjugateGradient:
                LOG(INFO) << "Solving with Conjugate Gradient ... \n";
                x = solverCG->solve(b);
                break;
            case LeastSquaresConjugateGradient:
                LOG(INFO) << "Solving with LeastSquaresConjugateGradient ... \n";
                x = solverLSCG->solve(b);
                break;
            case BiCGSTAB:
                LOG(INFO) << "Solving with BiCGSTAB ... \n";
                x = solverBiCGSTAB->solve(b);
                break;
        }

        std::chrono::duration<double> elapsed_time = std::chrono::system_clock::now() - start_time;
        double solving_time = elapsed_time.count();
        std::stringstream stream;
        stream << std::fixed << std::setprecision(5) << solving_time;
        std::string solving_time_str = stream.str();
        Eigen::Matrix<double, Eigen::Dynamic, 1> temp = A * x;
        rel_error = (b-temp).norm()/b.norm();
        LOG(INFO) << "Solving time   = " << solving_time_str << std::endl;
        LOG(INFO) << "Relative error = " << rel_error << std::endl;
        return x;
    }

    void setIters(int iters){
        maxiters = iters;
        switch (type) {
            case SimplicialLLT:
                LOG(WARNING) << "SimplicialLLT does not use maxiters";
                return;
            case SimplicialLDLT:
                LOG(WARNING) << "SimplicialLDLT does not use maxiters";
                return;
            case SparseLU:
                LOG(WARNING) << "SparseLU does not use maxiters";
                return;
            case ConjugateGradient:
                solverCG->setMaxIterations(maxiters);
                return;
            case LeastSquaresConjugateGradient:
                solverLSCG->setMaxIterations(maxiters);
                return;
            case BiCGSTAB:
                solverBiCGSTAB->setMaxIterations(maxiters);
                return;
        }
    }

    void setTol(const double& new_tol) {
        tol = new_tol;
        switch (type) {
            case SimplicialLLT:
                LOG(WARNING) << "SimplicialLLT does not use tol";
                return;
            case SimplicialLDLT:
                LOG(WARNING) << "SimplicialLDLT does not use tol";
                return;
            case SparseLU:
                LOG(WARNING) << "SparseLU does not use tol";
                return;
            case ConjugateGradient:
                solverCG->setTolerance(tol);
                return;
            case LeastSquaresConjugateGradient:
                solverLSCG->setTolerance(tol);
                return;
            case BiCGSTAB:
                solverBiCGSTAB->setTolerance(tol);
                return;
        }
    }

    std::string getType() {
        switch (type)
        {
            case SimplicialLLT:                 return "SimplicialLLT";
            case SimplicialLDLT:                return "SimplicialLDLT";
            case SparseLU:                      return "SparseLU";
            case ConjugateGradient:             return "ConjugateGradient";
            case LeastSquaresConjugateGradient: return "LeastSquaresConjugateGradient";
            case BiCGSTAB:                      return "BiCGSTAB";
            default:                            return "NO SOLVER SELECTED";
        }
    }

    double getTol() const { return tol; }
    int getMaxIters() const { return maxiters; }
    Eigen::SparseMatrix<double> getA() const { return A; }
    void setA (const Eigen::SparseMatrix<double>& new_A){
        A = new_A;
        factorize();
    }

private:
    solverType type;
    double rel_error = 0.0;

    //Only used for iterative solvers
    int maxiters = 100;
    double tol = 1e-08;

    Eigen::SparseMatrix<double> A;
//    std::shared_ptr<Eigen::internal::noncopyable> *solver;
    std::unique_ptr<Eigen::SimplicialLLT<Eigen::SparseMatrix<double>>> solverLLT;
    std::unique_ptr<Eigen::SimplicialLDLT<Eigen::SparseMatrix<double>>> solverLDLT;
    std::unique_ptr<Eigen::SparseLU<Eigen::SparseMatrix<double>>> solverLU;
    std::unique_ptr<Eigen::ConjugateGradient<Eigen::SparseMatrix<double>>> solverCG;
    std::unique_ptr<Eigen::LeastSquaresConjugateGradient<Eigen::SparseMatrix<double>>> solverLSCG;
    std::unique_ptr<Eigen::BiCGSTAB<Eigen::SparseMatrix<double>>> solverBiCGSTAB;
};


namespace py = pybind11;

PYBIND11_MODULE(EigenSolvers, m) {
    m.doc() = "Python binding for Eigen Linear System Solvers"; // optional module docstring

    py::class_<LinearSolver> linearSolver(m, "LinearSolver"); // Define class
    linearSolver.def(py::init<const Eigen::SparseMatrix<double>&, LinearSolver::solverType, int, double>()); //ctor
    linearSolver.def(py::init<const Eigen::SparseMatrix<double>&, LinearSolver::solverType>()); //ctor
    linearSolver.def(py::init<>()); //ctor
    linearSolver.def(py::init<LinearSolver::solverType>()); //ctor
    linearSolver.def("solve", &LinearSolver::solve);
    linearSolver.def_property("maxiters", &LinearSolver::getMaxIters, &LinearSolver::setIters);
    linearSolver.def_property("tol", &LinearSolver::getTol, &LinearSolver::setTol);
    linearSolver.def_property("A", &LinearSolver::getA, &LinearSolver::setA);
    linearSolver.def_property_readonly("solver_type", &LinearSolver::getType);

    py::enum_<LinearSolver::solverType>(linearSolver, "solverType") //enum type
            .value("SimplicialLLT", LinearSolver::solverType::SimplicialLLT)
            .value("SimplicialLDLT", LinearSolver::solverType::SimplicialLDLT)
            .value("SparseLU", LinearSolver::solverType::SparseLU)
            .value("ConjugateGradient", LinearSolver::solverType::ConjugateGradient)
            .value("LeastSquaresConjugateGradient", LinearSolver::solverType::LeastSquaresConjugateGradient)
            .value("BiCGSTAB", LinearSolver::solverType::BiCGSTAB)
            .export_values();
}


================================================
FILE: depth-estimation/360monodepth/code/cpp/src/python_binding.cpp
================================================
#include "python_binding.hpp"

#include "depthmap_stitcher.hpp"
#include "depthmap_stitcher_enum.hpp"
#include "depthmap_stitcher_group.hpp"
#include "data_imitator.hpp"

#include <glog/logging.h>

#include <string>
#include <iostream>
#include <algorithm>

std::shared_ptr<DepthmapStitcher> depthmap_stitcher;

// create fake data for debug
void create_debug_data(std::vector<cv::Mat> &depthmap,
					   std::vector<cv::Mat> &align_coeff,
					   std::map<int, std::map<int, cv::Mat>> &pixels_corresponding_list,
					   const int debug_data_type,
					   const int frame_number)
{
	std::stringstream ss;
	ss << "Create synthetic debug data :";
	if (frame_number == 2)
	{
		ss << " Frame Number is: " << frame_number;
		DataImitator di;
		di.depthmap_ref_filepath.clear();
		di.depthmap_tar_filepath.clear();
		di.corr_ref2tar_filepath.clear();
		di.corr_tar2ref_filepath.clear();
		//di.depthmap_overlap_ratio = 0.5;
		di.depthmap_overlap_ratio = 1.0;
		di.depth_scale = 0.2;
		di.depth_offset = 6.0;

		if (debug_data_type == 0)
		{
			ss << "Deform coefficient is Simple.";
			LOG(INFO) << ss.str();
			align_coeff = di.make_aligncoeffs_simple();
			depthmap = di.make_depthmap_pair_simple();
		}
		else if (debug_data_type == 1)
		{
			ss << "Deform coefficient is Random.";
			LOG(INFO) << ss.str();
			align_coeff = di.make_aligncoeffs_random();
			depthmap = di.make_depthmap_pair_random();
		}
		else
		{
			LOG(ERROR) << "Only support debug data type is simple and random.";
		}
		pixels_corresponding_list = di.make_corresponding_json();
	}
	else if (frame_number > 2)
	{
		// TODO create multi frame for depth map alignment
		if (debug_data_type == 0)
		{
			ss << "Deform coefficient is Simple.";
			LOG(INFO) << ss.str();
		}
		else if (debug_data_type == 1)
		{
			ss << "Deform coefficient is Random.";
			LOG(INFO) << ss.str();
		}
		else
		{
			LOG(ERROR) << "Only support debug data type is simple and random.";
		}
	}
	else
	{
		LOG(ERROR) << "Debug data frame number is " << frame_number;
	}
}

int solver_params(const int num_threads,
				  const int max_num_iterations,
				  const int max_linear_solver_iterations,
				  const int min_linear_solver_iterations)
{
	int py_ceres_num_threads = -1;
	int py_max_num_iterations = -1;
	int py_max_linear_solver_iterations = -1;
	int py_min_linear_solver_iterations = -1;

	py_ceres_num_threads = num_threads;
	py_max_num_iterations = max_num_iterations;
	py_max_linear_solver_iterations = max_linear_solver_iterations;
	py_min_linear_solver_iterations = min_linear_solver_iterations;
	//

	// set Ceres soler options
	if (py_ceres_num_threads > 0)
	{
		LOG(INFO) << "The Ceres thread number is :" << py_ceres_num_threads;
		depthmap_stitcher->ceres_num_threads = py_ceres_num_threads;
		py_ceres_num_threads = -1;
	}
	if (py_max_num_iterations > 0)
	{
		LOG(INFO) << "The Ceres max iteration number is :" << py_max_num_iterations;
		depthmap_stitcher->ceres_max_num_iterations = py_max_num_iterations;
		py_max_num_iterations = -1;
	}
	if (py_max_linear_solver_iterations > 0)
	{
		LOG(INFO) << "The Ceres max linear solver iteration number is :" << py_max_linear_solver_iterations;
		depthmap_stitcher->ceres_max_linear_solver_iterations = py_max_linear_solver_iterations;
		py_max_linear_solver_iterations = -1;
	}
	if (py_min_linear_solver_iterations > 0)
	{
		LOG(INFO) << "The Ceres min linear solver iteration number is :" << py_min_linear_solver_iterations;
		depthmap_stitcher->ceres_min_linear_solver_iterations = py_min_linear_solver_iterations;
		py_min_linear_solver_iterations = -1;
	}

	return 1;
}

int init(const std::string &method)
{
	// initial log
	static bool glog_initialized = false;
	const int log_level = 1;
	if (log_level == 0)
		FLAGS_stderrthreshold = google::GLOG_WARNING;
	else if (log_level == 1)
		FLAGS_stderrthreshold = google::GLOG_INFO;

    if (!glog_initialized)
    {
    	google::InitGoogleLogging("depthmap_stitch");
    	glog_initialized = true;
    }

	// initial the depth map alignment stitcher
	if (method.compare("enum") == 0)
	{
		depthmap_stitcher = std::make_shared<DepthmapStitcherEnum>();
		LOG(INFO) << "Depth map align with enum method.";
	}
	else if (method.compare("group") == 0)
	{
		depthmap_stitcher = std::make_shared<DepthmapStitcherGroup>();
		LOG(INFO) << "Depth map align with group method.";
	}
	else
	{
		LOG(ERROR) << "The specified method name " << method << " is wrong.";
		return -1;
	}
	return 0;
}

int shutdown()
{
	// shut down glog
	google::ShutdownGoogleLogging();
	return 0;
}

// align sub-image depth-map
int report_aligned_depthmap_error()
{
	// aligned depth map error report
	depthmap_stitcher->report_error();
	return 0;
}

// stitch depth maps
int depthmap_stitch(
	const std::string &root_dir,
	const std::vector<float> &terms_weight,
	const std::vector<cv::Mat> &depthmap_original,
	const std::vector<int> &depthmap_original_ico_index,
	const int reference_depthamp_ico_index,
	const std::map<int, std::map<int, cv::Mat>> &pixels_corresponding_list,
	const int align_coeff_grid_height,
	const int align_coeff_grid_width,
	const bool reproj_perpixel_enable,
	const bool smooth_pergrid_enable,
	const std::vector<cv::Mat> &align_coeff_initial_scale,
	const std::vector<cv::Mat> &align_coeff_initial_offset,
	std::vector<cv::Mat> &depthmap_aligned,
	std::vector<cv::Mat> &align_coeff)
{
	// check the term's weight
	if (terms_weight.size() != 3)
		return -1;

	// check the reference depth map
	if (reference_depthamp_ico_index >= 0)
	{
		std::vector<int>::const_iterator it = std::find(depthmap_original_ico_index.begin(), depthmap_original_ico_index.end(), reference_depthamp_ico_index);
		if (it != depthmap_original_ico_index.end())
			LOG(ERROR) << "The reference index " << reference_depthamp_ico_index << " is not in the ico depth map list.";
	}

	depthmap_stitcher->depthmap_ref_extidx = reference_depthamp_ico_index;

	// set term's weight
	depthmap_stitcher->weight_reprojection = terms_weight[0]; // re-projection term lambda
	depthmap_stitcher->weight_smooth = terms_weight[1];		  // smooth term lambda
	depthmap_stitcher->weight_scale = terms_weight[2];		  // scale term lambda
	LOG(INFO) << "Terms weights of re-projection:" << depthmap_stitcher->weight_reprojection << ",\tsmooth:" << depthmap_stitcher->weight_smooth << ",\tscale:" << depthmap_stitcher->weight_scale;

	// set if use the per-pixel/grid weight 
	depthmap_stitcher->projection_per_pixelcost_enable = reproj_perpixel_enable;
	depthmap_stitcher->smooth_pergrid_enable = smooth_pergrid_enable;
	LOG(INFO) << "projection_per_pixelcost_enable : " << reproj_perpixel_enable;
	LOG(INFO) << "smooth_pergrid_enable : " << smooth_pergrid_enable;	

	// convert external data to internal data format
	depthmap_stitcher->initial_data(root_dir, depthmap_original, depthmap_original_ico_index, pixels_corresponding_list);

	// set the coefficients grid size
	if (align_coeff_grid_height != -1 && align_coeff_grid_width != -1)
	{
		depthmap_stitcher->initial(align_coeff_grid_width, align_coeff_grid_height);
		LOG(INFO) << "Set the coefficient grid size to " << align_coeff_grid_height << " x " << align_coeff_grid_width;
	}
	else
		LOG(ERROR) << "Set the coefficient grid size do not set!";

	// set the alignment coefficient
	if (!align_coeff_initial_scale.empty() && !align_coeff_initial_offset.empty())
	{
		depthmap_stitcher->set_coeff(align_coeff_initial_scale, align_coeff_initial_offset);
	}

	LOG(INFO) << "Begin compute alignment coefficient.";
	depthmap_stitcher->compute_align_coeff();
	depthmap_stitcher->align_depthmap_all();
	if (!root_dir.empty())
	{
		depthmap_stitcher->save_aligned_depthmap();
		depthmap_stitcher->save_align_coeff();
	}

	// return alignment data and coefficients.
	depthmap_aligned = depthmap_stitcher->get_aligned_depthmap();
	depthmap_stitcher->get_align_coeff(align_coeff);

	depthmap_stitcher->coeff_so.coeff_scale_mat.clear();
	depthmap_stitcher->coeff_so.coeff_offset_mat.clear();

	return 0;
}


================================================
FILE: depth-estimation/360monodepth/code/cpp/test/depth_stitch_test.cpp
================================================
#include "include/data_io.hpp"
#include "include/depthmap_utility.hpp"
#include "include/depthmap_stitcher.hpp"
#include "include/timer.hpp"

#include <opencv2/opencv.hpp>

#include <ceres/ceres.h>
#include <glog/logging.h>
#include <gtest/gtest.h>

#include <iostream>
#include <string>
#include <random>
#include <regex>
#include <iterator>

//const std::string test_data_dir("D:/workspace_windows/InstaOmniDepth/data/");
const std::string test_data_dir("D:/workspace_windows/InstaOmniDepth/InstaOmniDepth_github/code/cpp/bin/Release/");
const std::string data_root_dir = test_data_dir + "fisheye_00/";

int main(int argc, char **argv)
{
	::testing::InitGoogleTest(&argc, argv);
	return RUN_ALL_TESTS();
}

TEST(DepthmaputilTest, defbilinear_weightorm)
{
	const int grid_width = 5;
	const int grid_height = 8;
	const int image_width = 50;
	const int image_height = 80;
	double *weight_list = (double *)malloc(grid_width * grid_height * sizeof(double));

	for (int y = 0; y < image_height; y = y + 10)
	{
		for (int x = 0; x < image_width; x = x + 10)
		{
			DepthmapUtil::bilinear_weight(weight_list,
										  image_width, image_height,
										  grid_width, grid_height,
										  x, y);
			cv::Mat bilinear_weight_list_mat(cv::Size(grid_width, grid_height), CV_64FC1, weight_list);
		}
	}
}

//void test_depthmap_deform()
TEST(DepthmaputilTest, deform)
{
	// 0) create depth map
	int image_width = 50;
	int image_height = 80;

	cv::Mat depthmap = cv::Mat::ones(image_height, image_width, CV_64FC1);
	int counter = 0;
	for (int row_index = 0; row_index < depthmap.rows; row_index++)
	{
		for (int col_index = 0; col_index < depthmap.cols; col_index++)
		{
			depthmap.at<double>(row_index, col_index) = counter;
			counter++;
		}
	}

	// 1) create offset and scale grid
	int grid_width = 5;
	int grid_height = 8;
	cv::Mat scale_mat = cv::Mat::ones(grid_height, grid_width, CV_64FC1);
	counter = 0;
	for (int row_index = 0; row_index < scale_mat.rows; row_index++)
	{
		for (int col_index = 0; col_index < scale_mat.cols; col_index++)
		{
			scale_mat.at<double>(row_index, col_index) = counter;
			counter++;
		}
	}
	cv::Mat offset_mat = cv::Mat::ones(grid_height, grid_width, CV_64FC1);
	counter = 0;
	for (int row_index = 0; row_index < offset_mat.rows; row_index++)
	{
		for (int col_index = 0; col_index < offset_mat.cols; col_index++)
		{
			offset_mat.at<double>(row_index, col_index) = counter;
			counter++;
		}
	}

	//cv::randu(scale_mat, cv::Scalar(-10), cv::Scalar(10));
	//cv::randu(offset_mat, cv::Scalar(-10), cv::Scalar(10));

	cv::Mat depthmap_deformed;
	DepthmapUtil::deform(depthmap, depthmap_deformed, scale_mat, offset_mat);
}

TEST(DepthmaputilTest, deform_load_data)
{
	// 0) create depth map
	int image_width = 50;
	int image_height = 80;
	cv::Mat depthmap = cv::Mat::ones(image_height, image_width, CV_64FC1);

	//cv::Mat depthmap;
	//DepthmapIO::load(test_data_dir + "img0_depth_001.pfm", depthmap);

	// 1) load offset and scale grid from file
	std::string coeff_file_path = test_data_dir + "img0_coeff.json";
	AlignCoeff align_coeff;
	align_coeff.initial(5, 8, 2);
	align_coeff.load(coeff_file_path);
	//cv::Mat scale_mat = align_coeff.coeff_scale_mat[1];
	cv::Mat scale_mat = cv::Mat::ones(align_coeff.coeff_scale_mat[1].size(), CV_64FC1);
	cv::Mat offset_mat = align_coeff.coeff_offset_mat[1];
	//cv::Mat offset_mat = cv::Mat::ones(align_coeff.coeff_offset_mat[1].size(), CV_64FC1);

	cv::Mat depthmap_deformed;
	DepthmapUtil::deform(depthmap, depthmap_deformed, scale_mat, offset_mat);
}

TEST(DepthmaputilTest, normalization)
{
	cv::Mat depthmap = cv::Mat::ones(50, 80, CV_64FC1);
	//cv::randu(depthmap, cv::Scalar(-10), cv::Scalar(10));
	int counter = 0;
	for (int row_index = 0; row_index < depthmap.rows; row_index++)
	{
		for (int col_index = 0; col_index < depthmap.cols; col_index++)
		{
			depthmap.at<double>(row_index, col_index) = counter;
			counter++;
		}
	}

	double mean, stddev;
	cv::Mat depthmap_norm = DepthmapUtil::depthmap_orig2norm(depthmap, mean, stddev);
	cv::Mat depthmap_restore = DepthmapUtil::depthmap_norm2orig(depthmap_norm, mean, stddev);

	cv::Mat diff = depthmap - depthmap_restore;
}

TEST(DepthmapStitcherTest, subpixel_performance)
{
	// create test data
	unsigned int image_width = 200;
	unsigned int image_height = 150;
	cv::Mat img = cv::Mat::zeros(image_height, image_width, CV_64FC1);

	int counter = 0;
	for (int row_idx = 0; row_idx < image_height; row_idx++)
		for (int col_idx = 0; col_idx < image_width; col_idx++)
		{
			img.at<double>(row_idx, col_idx) = counter;
			counter++;
		}

	std::random_device dev;
	std::mt19937 rng(dev());
	std::uniform_int_distribution<std::mt19937::result_type> width_dist(0, image_width - 1);
	std::uniform_int_distribution<std::mt19937::result_type> height_dist(0, image_height - 1);
	cv::Point2f pt(width_dist(rng), height_dist(rng));

	Timer timer;
	int test_time = 100000;

	// method 1:
	double data_method_1;
	timer.start();
	for (int i = 0; i < test_time; i++)
	{
		cv::Mat patch;
		cv::Mat imgF;
		img.convertTo(imgF, CV_32FC1);
		cv::getRectSubPix(imgF, cv::Size(1, 1), pt, patch);
		data_method_1 = patch.at<float>(0, 0);
	}
	printf("Method 1: %f, %f ms\n", data_method_1, timer.duration_ms());
	timer.stop();

	// method 2:
	timer.start();
	double data_method_2;
	for (int i = 0; i < test_time; i++)
	{
		cv::Mat patch;
		cv::remap(img, patch, cv::Mat(1, 1, CV_32FC2, &pt), cv::noArray(), cv::INTER_LINEAR, cv::BORDER_REFLECT_101);
		data_method_2 = patch.at<double>(0, 0);
	}
	printf("Method 2: %f, %f ms\n", data_method_2, timer.duration_ms());
	timer.stop();

	// method 3:
	assert(!img.empty());
	assert(img.channels() == 3);
	timer.start();
	double data_method_3;
	for (int i = 0; i < test_time; i++)
	{
		int x = (int)pt.x;
		int y = (int)pt.y;
		int x0 = cv::borderInterpolate(x, img.cols, cv::BORDER_REFLECT_101);
		int x1 = cv::borderInterpolate(x + 1, img.cols, cv::BORDER_REFLECT_101);
		int y0 = cv::borderInterpolate(y, img.rows, cv::BORDER_REFLECT_101);
		int y1 = cv::borderInterpolate(y + 1, img.rows, cv::BORDER_REFLECT_101);

		float a = pt.x - (float)x;
		float c = pt.y - (float)y;

		data_method_3 = (double)cvRound(
			(img.at<double>(y0, x0) * (1.f - a) + img.at<double>(y0, x1) * a) * (1.f - c) +
			(img.at<double>(y1, x0) * (1.f - a) + img.at<double>(y1, x1) * a) * c);
	}
	printf("Method 3: %f , %f ms\n", data_method_3, timer.duration_ms());
	timer.stop();
}

void test_data_loading()
{
	// 0) load pixels corresponding from JSON
	std::string pixles_corresponding_json = data_root_dir + "pixels_corresponding_0_7.json";
	cv::Mat pixles_corresponding;
	std::string src_filename;
	std::string tar_filename;
	PixelsCorrIO::load(pixles_corresponding_json, src_filename, tar_filename, pixles_corresponding);

	// 1) load depth from .pfm file.
	std::string pfm_depthmap_filepath = data_root_dir + "img0_000.pfm";
	cv::Mat depth_mat;
	DepthmapIO::load(pfm_depthmap_filepath, depth_mat);
}



================================================
FILE: depth-estimation/360monodepth/code/python/requirements.txt
================================================
colorama>=0.4.4
matplotlib>=3.4.1
scikit-image>=0.18.1
scipy>=1.6.2
opencv-python>=4.5.1.48
torch>=1.8.1
torchvision>=0.9.1
wheel>=0.36.2
timm>=0.4.12
transformers



================================================
FILE: depth-estimation/360monodepth/code/python/src/main.py
================================================
from utility import fs_utility
from utility.fs_utility import FileNameConvention
# import matplotlib
# matplotlib.use("TkAgg")

import os
import time
import re
from pathlib import Path
import numpy as np
from PIL import Image
import shutil
import argparse

from utility import depthmap_align, image_io
from utility import depthmap_utils, metrics
from utility import blending
from utility import serialization
from utility import projection_icosahedron as proj_ico
from utility import MAIN_DATA_DIR

from utility.logger import Logger

log = Logger(__name__)
log.logger.propagate = False


def grid_size_type(arg_value, pat=re.compile(r"^[2-9]+[xX][2-9]+$")):
    if not pat.match(arg_value):
        raise argparse.ArgumentTypeError
    return arg_value


class Options():

    def __init__(self) -> None:
        self.parser = None
        self.expname = None
        self.data_fns = None
        self.sample_size = None

        # 0) input data option
        self.available_steps = [3, 4, 5]
        self.test = False
        self.grid_search = False

        # 1) subimage generation option
        self.subimage_available_list = list(range(0, 20))
        self.subimage_padding_size = 0.3
        self.subimage_tangent_image_width = 400
        self.persp_monodepth = "midas2"

        # 3) subimage depthmap alignment parameters
        self.dispalign_corr_thread_number = 10
        self.dispalign_pyramid_layer_number = 1
        self.multi_res_grid = False
        self.dispalign_pixelcorr_downsample_ratio = 0.001
        self.dispalign_iter_num = 100
        self.dispalign_ceres_max_linear_solver_iterations = 10
        self.dispalign_method = "group"
        self.dispalign_weight_project = 1.0
        self.dispalign_weight_smooth = 40.
        self.dispalign_weight_scale = 0.007
        self.coeff_fixed_face_index = -1
        self.dispalign_align_coeff_grid_width = 8
        self.dispalign_align_coeff_grid_height = 7
        self.dispalign_output_dir = None
        self.dispalign_debug_enable = False

        # 2) subimage blending option
        self.blending_method = None

        # 3) debug option
        self.debug_enable = False
        self.debug_output_dir = None

        # 4) post-process
        self.rm_debug_folder = False

        # 5) dataset configuration
        self.dataset_matterport_hexagon_mask_enable = False
        # the circumradius in gnomonic coordinate system, 
        self.dataset_matterport_blur_area_height = 0
        self.dataset_matterport_blurarea_shape = "circle"  # "hexagon",  "circle"

    def parser_arguments(self, parser):
        self.parser = parser

        # 1) parser CLI arguments
        parser.add_argument("--expname", type=str, default="monodepth", help="experiment name")
        parser.add_argument("--blending_method", type=str, default="poisson",
                            choices=['poisson', 'frustum', 'radial', 'nn', 'mean', 'all'])
        parser.add_argument("--data", type=str, default="../../../data/data.txt",
                            help="The format of this file needs to be one line per sample as following: "
                                 "/path/to/rgb.[png,jpg] /path/to/depth_gt.dpt")
        parser.add_argument("--grid_size", type=grid_size_type, default="8x7", help="width x height")
        parser.add_argument("--padding", type=float, default="0.3")
        parser.add_argument("--multires_levels", type=int, default=1, help="Levels of multi-resolution pyramid. If > 1"
                                                                           "then --grid_size is the lowest resolution")
        parser.add_argument("--persp_monodepth", type=str, default="depthanything", choices=["midas2", "midas3", "boost", "depthanything", "depthanythingv2"])
        parser.add_argument('--depthalignstep', type=int, nargs='+', default=[1, 2, 3, 4])
        parser.add_argument("--rm_debug_folder", default=True, action='store_false')
        parser.add_argument("--intermediate_data", default=False, action='store_true', help="save intermediate data"
                                                                                            "generated during the "
                                                                                            "pipeline")
        parser.add_argument("--grid_search", default=False, action='store_true')
        parser.add_argument("--sample_size", type=int, default=0, help="Sample a subset from --data")
        opt_arguments = parser.parse_args()

        # 2) update options
        self.expname = opt_arguments.expname
        self.blending_method = opt_arguments.blending_method
        self.data_fns = opt_arguments.data
        self.rm_debug_folder = opt_arguments.rm_debug_folder
        self.grid_search = opt_arguments.grid_search
        self.debug_enable = opt_arguments.intermediate_data
        self.dispalign_debug_enable = opt_arguments.intermediate_data
        self.subimage_padding_size = opt_arguments.padding
        self.dispalign_align_coeff_grid_width = int(opt_arguments.grid_size.lower().split("x")[0])
        self.dispalign_align_coeff_grid_height = int(opt_arguments.grid_size.lower().split("x")[1])
        self.dispalign_pyramid_layer_number = opt_arguments.multires_levels
        if self.dispalign_pyramid_layer_number > 1:
            self.dispalign_iter_num = 50
            self.multi_res_grid = True
        self.persp_monodepth = opt_arguments.persp_monodepth
        self.available_steps = opt_arguments.depthalignstep
        self.sample_size = opt_arguments.sample_size

        self.print()

    def print(self):
        message = ''
        message += '----------------- Options ---------------\n'
        for k, v in sorted(vars(opt).items()):
            comment = ''
            default = self.parser.get_default(k)
            if v != default:
                comment = '\t[default: %s]' % str(default)
            message += '{:>25}: {:<30}{}\n'.format(str(k), str(v), comment)
        message += '----------------- Options End -------------------'
        print(message)


def depthmap_estimation(erp_rgb_image_data, fnc, opt, blendIt, idx=1):
    """ Estimate the ERP image depth map from ERP rgb image.

    :param erp_rgb_image_data: RGB image data, [height, width, 3]
    :type erp_rgb_image_data: numpy
    :param filenameconv: the file name convention object.
    :type filenameconv: dict
    :return: the estimated depth map, [height, width]
    :rtype: numpy
    """
    times = []  # Per stage
    total_time = 0.0

    erp_image_height = erp_rgb_image_data.shape[0]
    subimage_dispmap_erp_list = []
    subimage_depthmap_erp_list = []  # the depth map in ERP image space
    subimage_rgb_list = []
    dispmap_aligned_list = []
    subimage_cam_param_list = []
    tangent_image_gnomo_xy = []  # to convert the perspective image to ERP image

    # 1) load ERP image & project to 20 face images
    if 1 in opt.available_steps:
        log.info("1) load ERP image & project to 20 face images")
        tic = time.perf_counter()
        # project to 20 images
        subimage_rgb_list, _, points_gnomocoord = proj_ico.erp2ico_image(erp_rgb_image_data,
                                                                         opt.subimage_tangent_image_width,
                                                                         opt.subimage_padding_size,
                                                                         full_face_image=True)
        tangent_image_gnomo_xy = points_gnomocoord[1]

        if opt.debug_enable:
            log.debug("Debug enable, Step 1 output subimage rgb to {}".format(fnc.root_dir))
            for index in range(0, len(subimage_rgb_list)):
                src_image_output_path = fnc.subimage_rgb_filename_expression.format(index)
                Image.fromarray(subimage_rgb_list[index].astype(np.uint8)).save(src_image_output_path)
                if index % 4 == 0:
                    print("Output subimage to {}.".format(src_image_output_path))

            # output subimage's rgb to image array
            subimage_rgb_array_filepath = fnc.subimage_rgb_filename_expression.format(999)
            image_io.subimage_save_ico(subimage_rgb_list, subimage_rgb_array_filepath)

        toc = time.perf_counter()
        log.info(f"Load and split image in {toc - tic:0.4f} seconds.")
        total_time += toc - tic
        times.append(toc-tic)

    # 2) MiDaS estimate disparity maps
    if 2 in opt.available_steps:
        log.info("2) MiDaS estimate disparity maps")
        # load subimage rgb data from disk
        if not subimage_rgb_list or not tangent_image_gnomo_xy:
            log.info("load subimage's rgb data from disk.")
            for index in list(range(0, 20)):
                src_image_output_path = fnc.subimage_rgb_filename_expression.format(index)
                subimage_rgb_list.append(np.asarray(Image.open(src_image_output_path)))
            log.info("generate face gnomonic coordinate")
            _, _, points_gnomocoord = proj_ico.erp2ico_image(erp_rgb_image_data, opt.subimage_tangent_image_width,
                                                             opt.subimage_padding_size, full_face_image=True)
            tangent_image_gnomo_xy = points_gnomocoord[1]

        tic = time.perf_counter()
        subimage_depthmap_persp_list = []
        # estimate disparity map
        subimage_dispmap_persp_list = depthmap_utils.run_persp_monodepth(subimage_rgb_list, opt.persp_monodepth)
        # convert disparity map to depth map
        for dispmap_persp in subimage_dispmap_persp_list:
            subimage_depthmap_persp_list.append(depthmap_utils.disparity2depth(dispmap_persp))
        # convert each subimage's perspective depth map to ERP depth map.
        for depthmap_persp in subimage_depthmap_persp_list:
            subimage_depthmap_erp_list.append(
                depthmap_utils.subdepthmap_tang2erp(depthmap_persp, tangent_image_gnomo_xy))
        # convert each subimage's from ERP depth map to perspective map.
        for depthmap_erp in subimage_depthmap_erp_list:
            subimage_dispmap_erp_list.append(depthmap_utils.depth2disparity(depthmap_erp).astype(np.float32))

        # output disparity map and visualized result.
        if opt.debug_enable:
            # output disparity map array
            dispmap_array_filename = fnc.subimage_dispmap_persp_filename_expression.format(999)
            depthmap_utils.depth_ico_visual_save(subimage_dispmap_persp_list, dispmap_array_filename + ".jpg")
            depthmap_persp_array_filename = fnc.subimage_depthmap_persp_filename_expression.format(999)
            depthmap_utils.depth_ico_visual_save(subimage_depthmap_persp_list, depthmap_persp_array_filename + ".jpg")
            depthmap_erp_array_filename = fnc.subimage_depthmap_erp_filename_expression.format(999)
            depthmap_utils.depth_ico_visual_save(subimage_depthmap_erp_list, depthmap_erp_array_filename + ".jpg")
            subimage_dispmap_erp_filepath = fnc.subimage_dispmap_erp_filename_expression.format(999)
            depthmap_utils.depth_ico_visual_save(subimage_dispmap_erp_list, subimage_dispmap_erp_filepath + ".jpg")

            # output disparity map
            for index in range(0, len(subimage_rgb_list)):
                depth_filename = fnc.subimage_dispmap_erp_filename_expression.format(index)
                depthmap_utils.write_pfm(depth_filename, subimage_dispmap_erp_list[index], scale=1)
                depthmap_utils.depth_visual_save(subimage_dispmap_erp_list[index], depth_filename + ".jpg")
                if index % 4 == 0:
                    print("Output subimages disparity map to {}".format(depth_filename))
        toc = time.perf_counter()
        log.info(f"MiDaS estimate disparity maps in {toc - tic:0.4f} seconds.")
        total_time += toc - tic
        times.append(toc-tic)

    # 3) align disparity maps
    if 3 in opt.available_steps:
        log.info("3) align disparity maps")
        # load subimage disparity map from disk
        if not subimage_dispmap_erp_list or not subimage_rgb_list:
            log.info("load subimage's MiDaS disparity maps from disk.")
            for index in list(range(0, 20)):
                src_image_output_path = fnc.subimage_rgb_filename_expression.format(index)
                subimage_rgb_list.append(np.asarray(Image.open(src_image_output_path)))
                depth_filename = fnc.subimage_dispmap_erp_filename_expression.format(index)
                subimage_dispmap_erp_list.append(depthmap_utils.read_pfm(depth_filename)[0])

        tic = time.perf_counter()
        # subimages dispmap align parameters
        # set alignment parameter
        depthmap_aligner = depthmap_align.DepthmapAlign()
        depthmap_aligner.opt = opt  # the global configuration
        depthmap_aligner.pyramid_layer_number = opt.dispalign_pyramid_layer_number
        depthmap_aligner.multi_res_grid = opt.multi_res_grid
        depthmap_aligner.downsample_pixelcorr_ratio = opt.dispalign_pixelcorr_downsample_ratio
        depthmap_aligner.align_method = opt.dispalign_method
        depthmap_aligner.ceres_max_num_iterations = opt.dispalign_iter_num
        depthmap_aligner.weight_project = opt.dispalign_weight_project
        depthmap_aligner.weight_smooth = opt.dispalign_weight_smooth
        depthmap_aligner.weight_scale = opt.dispalign_weight_scale
        depthmap_aligner.coeff_fixed_face_index = opt.coeff_fixed_face_index
        depthmap_aligner.align_coeff_grid_width = opt.dispalign_align_coeff_grid_width
        depthmap_aligner.align_coeff_grid_height = opt.dispalign_align_coeff_grid_height
        depthmap_aligner.ceres_max_linear_solver_iterations = opt.dispalign_ceres_max_linear_solver_iterations
        if opt.dispalign_output_dir is not None:
            depthmap_aligner.output_dir = opt.dispalign_output_dir  # output cpp module alignment coefficient
        else:
            depthmap_aligner.output_dir = fnc.root_dir
        depthmap_aligner.debug = opt.dispalign_debug_enable
        depthmap_aligner.subimages_rgb = subimage_rgb_list  # original 20 subimages

        # set the output file name for debug, if not set do not output debug files.
        # depthmap_aligner.subimage_alignment_intermedia_filepath_expression = fnc.subimage_alignment_intermedia_filename_expression
        depthmap_aligner.subimage_pixelcorr_filepath_expression = fnc.subimage_pixelcorr_filename_expression
        # depthmap_aligner.subimage_warpedimage_filepath_expression = fnc.subimage_warpedimage_filename_expression
        # depthmap_aligner.subimage_warpeddepth_filename_expression = fnc.subimage_warpeddepth_filename_expression
        # depthmap_aligner.subimage_depthmap_aligning_filepath_expression = fnc.subimage_alignment_depthmap_input_filename_expression

        subimage_dispmap_list_sub = [subimage_dispmap_erp_list[i] for i in opt.subimage_available_list]
        dispmap_aligned_list, coeffs_scale, coeffs_offset, subimage_cam_param_list = \
            depthmap_aligner.align_multi_res(erp_rgb_image_data, subimage_dispmap_list_sub, opt.subimage_padding_size,
                                             opt.subimage_available_list)

        if opt.debug_enable:
            # visualized cpp module output aligned subimage's disparity map
            if opt.dispalign_debug_enable:
                for subimage_index in opt.subimage_available_list:
                    subimage_filepath = fnc.subimage_dispmap_cpp_aligned_filename_expression.format(subimage_index)
                    dispmap, _ = depthmap_utils.read_pfm(str(subimage_filepath))
                    dispmap_vis_filepath = subimage_filepath + ".jpg"
                    depthmap_utils.depth_visual_save(dispmap, dispmap_vis_filepath)

            # output the scale ans offset to json
            serialization.subimage_alignment_params(fnc.subimage_dispmap_aligned_coeffs_filename_expression,
                                                    coeffs_scale, coeffs_offset, opt.subimage_available_list)

            # visualize align coefficients to image files
            depthmap_utils.depth_ico_visual_save(coeffs_scale,
                                                 fnc.subimage_dispmap_aligned_coeffs_filename_expression + "_scale.jpg",
                                                 opt.subimage_available_list)
            depthmap_utils.depth_ico_visual_save(coeffs_offset,
                                                 fnc.subimage_dispmap_aligned_coeffs_filename_expression + "_offset.jpg",
                                                 opt.subimage_available_list)

            # output the camera parameters to json files
            serialization.save_cam_params(fnc.subimage_camsparams_list_filename_expression, opt.subimage_available_list,
                                          subimage_cam_param_list)

            # output alignment disparity map array image
            depth_array_filename = fnc.subimage_dispmap_aligned_filename_expression.format(999)
            depthmap_utils.depth_ico_visual_save(dispmap_aligned_list, depth_array_filename + ".jpg",
                                                 opt.subimage_available_list)

            # output visualized alignment disparity map
            for index in range(0, len(dispmap_aligned_list)):
                depth_filename = fnc.subimage_dispmap_aligned_filename_expression.format(
                    opt.subimage_available_list[index])
                depthmap_utils.write_pfm(depth_filename, dispmap_aligned_list[index].astype(np.float32), scale=1)
                depthmap_utils.depth_visual_save(dispmap_aligned_list[index], depth_filename + ".jpg")
                if index % 4 == 0:
                    print("Output aligned subimages disparity map to {}".format(depth_filename))

        toc = time.perf_counter()
        log.info(f"Align disparity maps in {toc - tic:0.4f} seconds.")
        total_time += toc - tic
        times.append(toc-tic)

    # 4) blender to ERP image & output ERP disparity map and visualized image
    if 4 in opt.available_steps:
        if not dispmap_aligned_list or not subimage_cam_param_list:
            log.info("load subimage's aligned disparity maps from disk.")
            for index in opt.subimage_available_list:
                depth_filename = fnc.subimage_dispmap_aligned_filename_expression.format(index)
                if os.path.isfile(depth_filename):
                    dispmap_aligned_list.append(depthmap_utils.read_pfm(depth_filename)[0])
                else:
                    dispmap_aligned_list.append(subimage_dispmap_erp_list[index])
            log.info("load subimage's camera parameters from disk.")

        log.info("4) blender to ERP image")
        tic = time.perf_counter()

        erp_dispmap_blend = None
        if len(opt.subimage_available_list) == 20:
            if idx == 0:
                blendIt.tangent_images_coordinates(erp_image_height, dispmap_aligned_list[0].shape)
                blendIt.erp_blendweights(subimage_cam_param_list, erp_image_height, dispmap_aligned_list[0].shape)
                blendIt.compute_linear_system_matrices(erp_image_height, erp_image_height * 2, blendIt.frustum_blendweights)

            erp_dispmap_blend = blendIt.blend(dispmap_aligned_list, erp_image_height)
        else:
            # available faces number is not 20, output subimages linear blend result
            log.warn("Linear blend {} subimages.".format(len(opt.subimage_available_list)))
            # file with blank depth
            dispmap_aligned_list_filled = depthmap_utils.fill_ico_subimage(dispmap_aligned_list,
                                                                           opt.subimage_available_list)
            erp_dispmap_blend = proj_ico.ico2erp_image(dispmap_aligned_list_filled, erp_image_height,
                                                       opt.subimage_padding_size, blender_method="mean")

        if opt.debug_enable:
            if opt.blending_method == 'all':
                erp_dispmap_blend_save = erp_dispmap_blend['poisson']
            else:
                erp_dispmap_blend_save = erp_dispmap_blend[opt.blending_method]

            # output blending result disparity map
            erp_aligned_dispmap_filepath = fnc.erp_depthmap_blending_result_filename_expression
            depthmap_utils.write_pfm(erp_aligned_dispmap_filepath, erp_dispmap_blend_save.astype(np.float32), scale=1)
            erp_aligned_dispmap_vis_filepath = fnc.erp_depthmap_vis_blending_result_filename_expression
            depthmap_utils.depth_visual_save(erp_dispmap_blend_save, erp_aligned_dispmap_vis_filepath + ".jpg")

        toc = time.perf_counter()
        log.info(f"Blender to ERP image in {toc - tic:0.4f} seconds.")
        total_time += toc - tic
        times.append(toc-tic)

        times.append(total_time)
        return erp_dispmap_blend, times


def error_metric(depthmap_estimated, erp_gt_depthmap):
    """ Metric the error of the depthmap.

    :param depthmap_estimated: The estimated depth map.
    :type depthmap_estimated: numpy
    :param erp_gt_depthmap: The depth map ground truth.
    :type erp_gt_depthmap: numpy
    """
    # report the error
    log.info("output ERP disparity map and visualized image")

    pred_metrics = []
    for key in depthmap_estimated.keys():
        pred_metrics.append(metrics.report_error(erp_gt_depthmap, depthmap_estimated[key]))

    return pred_metrics


def monodepth_360(opt):
    """Pipeline."""
    # 0) settting parameters
    # 0-0) data file name and folder
    output_folder = os.path.join(Path(MAIN_DATA_DIR).parent.absolute(), "results")
    print("************OUTPUT FOLDER: ",output_folder,"********************")
    output_results_file = os.path.join(output_folder, "{}.txt".format(opt.expname))
    Path(output_folder).mkdir(exist_ok=True, parents=True)

    with open(opt.data_fns, 'r') as f:
        data_fns = f.readlines()

    if opt.sample_size > 0:
        np.random.seed(1337)
        data_fns = np.random.choice(data_fns, size=opt.sample_size, replace=False)

    # Grid Search
    if opt.grid_search:
        energy_weights = grid_search(fidelity_term=True)
    else:
        energy_weights = np.array([opt.dispalign_weight_smooth, opt.dispalign_weight_scale])[None, ...]

    # BlendIt object. Equation 7 of the paper
    blend_it = blending.BlendIt(opt.subimage_padding_size, len(opt.subimage_available_list), opt.blending_method)
    blend_it.fidelity_weight = 0.1

    for weights in energy_weights:
        if isinstance(weights, np.ndarray):
            opt.dispalign_weight_smooth = weights[0]
            opt.dispalign_weight_scale = weights[1]
            if opt.dispalign_weight_scale > 0:
                opt.coeff_fixed_face_index = -1
            else:
                opt.coeff_fixed_face_index = 7
        else:
            weights = np.array([weights])
            blend_it.fidelity_weight = weights[0]

        if opt.grid_search:
            if check_weights_processed(output_results_file, weights):
                continue

        # enable all steps [extraction, midas, alignment, blending]
        times_header = ["t_extraction(s)", "t_midas(s)", "t_alignment(s)", "t_blending(s)"]
        times_header = [times_header[i-1] for i in opt.available_steps]
        times_header.append("t_total(s)")

        metrics_list = []
        iter = 0
        for idx, line in enumerate(data_fns):

            line = line.splitlines()[0].split(" ")
            erp_image_filename = line[0]
            erp_gtdepth_filename = line[1] if line[1] != 'None' else ""

            if "matterport" in erp_image_filename:
                opt.dataset_matterport_hexagon_mask_enable = True
                opt.dataset_matterport_blur_area_height = 140  # * 0.75
            else:
                opt.dataset_matterport_hexagon_mask_enable = False

            erp_pred_filename = erp_gtdepth_filename.replace("depth.dpt", "dispmap_aligned.pfm")
            data_root = os.path.dirname(erp_image_filename)
            debug_output_dir = os.path.join(data_root, "debug/")
            Path(debug_output_dir).mkdir(parents=True, exist_ok=True)

            erp_aligned_dispmap_filepath = erp_pred_filename
            erp_image_filepath = erp_image_filename
            erp_gt_filepath = erp_gtdepth_filename
            filename_base, _ = os.path.splitext(os.path.basename(erp_image_filename))

            fnc = FileNameConvention()
            fnc.set_filename_basename(filename_base)
            fnc.set_filepath_folder(debug_output_dir)

            # load ERP rgb image and estimate the ERP depth map
            erp_rgb_image_data = image_io.image_read(erp_image_filepath)
            # Load matrices for blending linear system
            estimated_depthmap, times = depthmap_estimation(erp_rgb_image_data, fnc, opt, blend_it, iter)

            # get error fo ERP depth map
            erp_gt_depthmap = depthmap_utils.read_dpt(erp_gt_filepath) if erp_gt_filepath != "" else None
            pred_metrics = error_metric(estimated_depthmap, erp_gt_depthmap) if erp_gt_filepath != "" else None

            serialization.save_predictions(output_folder, erp_image_filename, erp_gt_depthmap, erp_rgb_image_data, estimated_depthmap,
                                           opt.persp_monodepth, idx=idx)

            #if opt.grid_search:
                #metrics_list.append(list(weights) + [item for dic in pred_metrics for item in dic.values()])
            #else:
                #serialization.save_metrics(output_results_file, pred_metrics, times, times_header,
                                           #idx, list(estimated_depthmap.keys()))

            # Remove temporal storage folder
            if opt.rm_debug_folder and os.path.isdir(debug_output_dir):
                shutil.rmtree(debug_output_dir)
            iter += 1

        if opt.grid_search:
            metrics_list = np.array(metrics_list)
            metrics_list = np.mean(metrics_list, axis=0)
            with open(output_results_file, 'a') as f:
                np.savetxt(f, metrics_list.reshape(1, -1), delimiter=',', fmt='%1.11f')


def grid_search(fidelity_term=False):
    if not fidelity_term:
        weight_smooth = np.array([10 ** i for i in range(-4, 10)] + [0])
        weight_scale = np.array([10 ** i for i in range(-10, 4)] + [0])
        exhaustive_pairs = np.array(np.meshgrid(weight_smooth, weight_scale)).T.reshape(-1, 2)
        return exhaustive_pairs
    else:
        fidelity_term_values = np.array([0] + [10 ** i for i in range(-4, 5)])
        return fidelity_term_values


def check_weights_processed(file, weights):
    exist = False
    if not os.path.isfile(file):
        return exist

    with open(file, 'r') as f:
        for line in f.readlines():
            line = line.splitlines()[0].split(',')
            if len(line) > 1:
                if len(weights) > 1:
                    if weights[0] == float(line[0]) and weights[1] == float(line[1]):
                        exist = True
                        break
                else:
                    if weights[0] == float(line[0]):
                        exist = True
                        break

    if exist:
        log.info("PASS on weights: " + ",".join(str(weight) for weight in weights))
    else:
        log.info("USING weights: " + ",".join(str(weight) for weight in weights))

    return exist


if __name__ == "__main__":
    # parser arguments
    opt = Options()
    parser = argparse.ArgumentParser()
    opt.parser_arguments(parser)

    # estimate depth map from ERP rgb image
    monodepth_360(opt)



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/__init__.py
================================================
import os
import sys

# Add project library to Python path
python_src_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
print(f"Adding '{python_src_dir}' to sys.path")
sys.path.append(python_src_dir)  # /code/python/src/
sys.path.append(python_src_dir + "/utility/")  # /code/python/src/
sys.path.append(os.path.dirname(python_src_dir))  # /code/python/
sys.path.append(os.path.abspath(os.path.join(python_src_dir, os.pardir, os.pardir, "cpp/lib")))
# sys.path.append(os.path.join(python_src_dir, "../.."))  # CR: unused?

# Data directory /data/
# TODO: remove the trailing slash once all usages of TEST_DATA_DIR are updated
TEST_DATA_DIR = os.path.abspath("../../../data/") + "/"
MAIN_DATA_DIR = os.path.abspath("../../../data/") + "/"

# Set the PyTorch hub folder as an environment variable
# TODO: use os.path.join instead of string
os.environ['TORCH_HOME'] = TEST_DATA_DIR + 'models/'


================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/blending.py
================================================
import depthmap_utils
import projection_icosahedron as proj_ico
import gnomonic_projection as gp
import spherical_coordinates as sc

from EigenSolvers import LinearSolver
from utility import metrics
from utility import depthmap_utils
import matplotlib
# matplotlib.use('TkAgg')


from scipy import ndimage
import scipy.sparse
from scipy.sparse.linalg import spsolve
import scipy.sparse.linalg
import scipy.sparse
import matplotlib.pyplot as plt

import json

import numpy as np

from logger import Logger

log = Logger(__name__)
log.logger.propagate = False
import time


class BlendIt:
    def __init__(self, padding, n_subimages, blending_method):
        # sub-image number
        self.fidelity_weight = 1.0
        self.inflection_point = 10  # point where slope starts to affect the radial blendweights
        self.diagonal_percentage = 48.5  # Percentage of diagonal until weights start decaying in frustum weights
        self.n_subimages = n_subimages
        self.blending_method = blending_method
        self.padding = padding
        self.triangle_coordinates_erp = []  # Pixel coordinates of the triangular tangent face in equirect image
        self.triangle_coordinates_tangent = []
        self.squared_coordinates_erp = []  # Pixel coordinates of the squared (plane) tangent face in equirect image
        self.squared_coordinates_tangent = []
        self.radial_blendweights = None
        self.frustum_blendweights = None
        self.A = None
        self.x_grad_mat = None
        self.y_grad_mat = None
        if blending_method == "all" or blending_method == "poisson":
            # Supported solvers: [SimplicialLLT, SimplicialLDLT, SparseLU, ConjugateGradient,
            #                     LeastSquaresConjugateGradient, BiCGSTAB]
            self.eigen_solver = LinearSolver(LinearSolver.solverType.BiCGSTAB)
        else:
            self.eigen_solver = None

    def blend(self, subimage_dispmap, erp_image_height):
        """Blending the 20 face disparity map to ERP disparity map.

        This function use data in CPU memory, which have been pre-loaded or generated.
        To reduce the time of load data from disk.

        :param subimage_dispmap: A list store the subimage dispartiy data .
        :type subimage_dispmap: list
        :param sub_image_param: The subimage camera parameter.
        :type sub_image_param: dict
        :param erp_image_height: The height of output image.
        :type erp_image_height: float
        """
        tangent_disp_imgs = []
        if isinstance(subimage_dispmap, str):
            for index in range(0, 20):
                tangent_disp_imgs.append(depthmap_utils.read_pfm(subimage_dispmap.format(index))[0])
        elif isinstance(subimage_dispmap, list):
            tangent_disp_imgs = subimage_dispmap
        else:
            log.error("Disparity map type error. {}".format(type(subimage_dispmap)))

        if len(subimage_dispmap) != 20:
            log.error("The blending input subimage size is not 20.")

        # 0) get tangent image information
        erp_image_width = erp_image_height * 2
        erp_size = (erp_image_height, erp_image_width)
        tangent_image_size = subimage_dispmap[0].shape

        # Get erp depth image with as many channels as tangent images along with nearest neighbour (nn) blended image.
        # Also, get erp weight images with the same channels.
        equirect_depth_tensor, nn_blending = self.misc_data(tangent_disp_imgs, erp_size)

        # Normalize
        norm_radial_blendweights = np.divide(self.radial_blendweights,
                                             np.sum(self.radial_blendweights, axis=2)[..., None],
                                             where=(np.sum(self.radial_blendweights, axis=2) > 0)[..., None])

        norm_frustum_blendweights = np.divide(self.frustum_blendweights,
                                              np.sum(self.frustum_blendweights, axis=2)[..., None],
                                              where=(np.sum(self.frustum_blendweights, axis=2) > 0)[..., None])

        radial_blended = np.nansum(norm_radial_blendweights * equirect_depth_tensor, axis=2)
        frustum_blended = np.nansum(norm_frustum_blendweights * equirect_depth_tensor, axis=2)
        mean_blended = np.nanmean(equirect_depth_tensor, axis=2)

        blended_img = dict()
        if self.blending_method == 'poisson':
            blended_img[self.blending_method] = self.gradient_blending(equirect_depth_tensor, self.frustum_blendweights,
                                                                       nn_blending)
        if self.blending_method == 'frustum':
            blended_img[self.blending_method] = frustum_blended

        if self.blending_method == 'radial':
            blended_img[self.blending_method] = radial_blended

        if self.blending_method == 'nn':
            blended_img[self.blending_method] = nn_blending

        if self.blending_method == 'mean':
            blended_img[self.blending_method] = mean_blended

        if self.blending_method == 'all':
            blended_img['poisson'] = self.gradient_blending(equirect_depth_tensor, self.frustum_blendweights,
                                                            nn_blending)
            blended_img['frustum'] = frustum_blended
            blended_img['radial'] = radial_blended
            blended_img['nn'] = nn_blending
            blended_img['mean'] = mean_blended

        return blended_img

    def tangent_images_coordinates(self, erp_image_height, tangent_img_size):
        """
        Based on Mingze's erp2ico_image method in projection_icosahedron.py
        :param tangent_images:
        :param sub_image_param_expression:
        :param erp_size:
        :param tangent_img_size:
        """
        erp_image_width = 2 * erp_image_height
        tangent_image_height, tangent_image_width = tangent_img_size

        if erp_image_width != erp_image_height * 2:
            raise Exception("the ERP image dimession is {}x{}".format(erp_image_height, erp_image_width))

        # stitch all tangnet images to ERP image
        for triangle_index in range(0, 20):
            log.debug("stitch the tangent image {}".format(triangle_index))
            triangle_param = proj_ico.get_icosahedron_parameters(triangle_index, self.padding)

            # 1) get all tangent triangle's available pixels coordinate
            availied_ERP_area = triangle_param["availied_ERP_area"]
            erp_image_col_start, erp_image_row_start = sc.sph2erp(availied_ERP_area[0], availied_ERP_area[2],
                                                                  erp_image_height, sph_modulo=False)
            erp_image_col_stop, erp_image_row_stop = sc.sph2erp(availied_ERP_area[1], availied_ERP_area[3],
                                                                erp_image_height, sph_modulo=False)

            # process the image boundary
            erp_image_col_start = int(erp_image_col_start + 0.5)
            erp_image_col_stop = int(erp_image_col_stop + 0.5)
            erp_image_row_start = int(erp_image_row_start + 0.5)
            erp_image_row_stop = int(erp_image_row_stop + 0.5)

            triangle_x_range = np.linspace(erp_image_col_start, erp_image_col_stop,
                                           erp_image_col_stop - erp_image_col_start, endpoint=False)
            triangle_y_range = np.linspace(erp_image_row_start, erp_image_row_stop,
                                           erp_image_row_stop - erp_image_row_start, endpoint=False)
            triangle_xv, triangle_yv = np.meshgrid(triangle_x_range, triangle_y_range)
            # process the wrap around
            triangle_xv = np.remainder(triangle_xv, erp_image_width)
            triangle_yv = np.clip(triangle_yv, 0, erp_image_height - 1)

            # 2) sample the pixel value from tanget image
            # project spherical coordinate to tangent plane
            spherical_uv = sc.erp2sph([triangle_xv, triangle_yv], erp_image_height=erp_image_height, sph_modulo=False)
            theta_0 = triangle_param["tangent_point"][0]
            phi_0 = triangle_param["tangent_point"][1]
            # Tangent img coordinates normalize to the [0,1]?[-1,1]? tangent plane
            tangent_xv, tangent_yv = gp.gnomonic_projection(spherical_uv[0, :, :], spherical_uv[1, :, :], theta_0,phi_0)

            # the pixels in the tangent triangle
            triangle_points_tangent_nopad = np.array(triangle_param["triangle_points_tangent_nopad"])
            triangle_points_tangent = np.array(triangle_param["triangle_points_tangent"])
            gnomonic_x_min = np.amin(triangle_points_tangent[:, 0], axis=0)
            gnomonic_x_max = np.amax(triangle_points_tangent[:, 0], axis=0)
            gnomonic_y_min = np.amin(triangle_points_tangent[:, 1], axis=0)
            gnomonic_y_max = np.amax(triangle_points_tangent[:, 1], axis=0)

            tangent_gnomonic_range = [gnomonic_x_min, gnomonic_x_max, gnomonic_y_min, gnomonic_y_max]
            pixel_eps = abs(tangent_xv[0, 0] - tangent_xv[0, 1]) / (2 * tangent_image_width)

            square_points_tangent = [[gnomonic_x_min, gnomonic_y_max],
                                     [gnomonic_x_max, gnomonic_y_max],
                                     [gnomonic_x_max, gnomonic_y_min],
                                     [gnomonic_x_min, gnomonic_y_min]]
            inside_tri_pixels_list = gp.inside_polygon_2d(
                np.stack((tangent_xv.flatten(), tangent_yv.flatten()), axis=1),
                triangle_points_tangent_nopad, on_line=True, eps=pixel_eps). \
                reshape(tangent_xv.shape)

            inside_square_pixels_list = gp.inside_polygon_2d(
                np.stack((tangent_xv.flatten(), tangent_yv.flatten()), axis=1),
                square_points_tangent, on_line=True, eps=pixel_eps). \
                reshape(tangent_xv.shape)

            # Tangent coordinates in pixels [subimage_height, subimage_width]
            tangent_sq_xv, tangent_sq_yv = gp.gnomonic2pixel(tangent_xv[inside_square_pixels_list],
                                                             tangent_yv[inside_square_pixels_list],
                                                             0.0, tangent_image_width, tangent_image_height,
                                                             tangent_gnomonic_range)

            tangent_tri_xv, tangent_tri_yv = gp.gnomonic2pixel(tangent_xv[inside_tri_pixels_list],
                                                               tangent_yv[inside_tri_pixels_list],
                                                               0.0, tangent_image_width, tangent_image_height,
                                                               tangent_gnomonic_range)

            self.triangle_coordinates_tangent.append([tangent_tri_xv, tangent_tri_yv])
            self.squared_coordinates_tangent.append([tangent_sq_xv, tangent_sq_yv])
            self.triangle_coordinates_erp.append([triangle_xv[inside_tri_pixels_list], triangle_yv[inside_tri_pixels_list]])
            self.squared_coordinates_erp.append([triangle_xv[inside_square_pixels_list], triangle_yv[inside_square_pixels_list]])

    def erp_blendweights(self, sub_image_param_expression, erp_image_height, tangent_img_size, n_images=20):
        erp_image_width = 2 * erp_image_height
        if erp_image_width != erp_image_height * 2:
            raise Exception("the ERP image dimession is {}x{}".format(erp_image_height, erp_image_width))

        tangent_cam_params = None
        if isinstance(sub_image_param_expression, str):
            tangent_cam_params = json.load(open(sub_image_param_expression.format(0)))
        elif isinstance(sub_image_param_expression, list):
            tangent_cam_params = sub_image_param_expression[0]
        else:
            log.error("Camera parameter type error. {}".format(type(sub_image_param_expression)))

        erp_radial_weights = np.full([erp_image_height, erp_image_width, n_images], 0, np.float64)
        erp_frustum_weights = np.full([erp_image_height, erp_image_width, n_images], 0, np.float64)

        tangent_img_blend_radial_weights = self.get_radial_blendweights(tangent_cam_params, tangent_img_size)

        # mx + b normalization. The slope is -1/(fov-inflection_point)
        # The bias is calculated knowing that in the inflection point we want a weight of 1.
        focal_lengths = np.array([tangent_cam_params['intrinsics']['focal_length_y'],
                                  tangent_cam_params['intrinsics']['focal_length_x']])
        min_f_length = np.argmax(focal_lengths)
        fov = np.degrees(np.arctan(tangent_img_size[min_f_length] * 0.5 / focal_lengths[min_f_length]))
        slope = -1 / (fov - self.inflection_point)
        bias = -self.inflection_point * slope + 1

        # Clip values out of bounds. Weights belong to [0,1]
        tangent_img_blend_radial_weights = np.clip(tangent_img_blend_radial_weights * slope + bias, 0, 1)

        tangent_img_blend_frustum_weights = self.get_frustum_blendweights(tangent_img_size)

        for triangle_index in range(0, n_images):
            tangent_sq_xv, tangent_sq_yv = self.squared_coordinates_tangent[triangle_index]
            erp_sq_xv, erp_sq_yv = self.squared_coordinates_erp[triangle_index]

            erp_face_radial_weights = ndimage.map_coordinates(tangent_img_blend_radial_weights,
                                                              [tangent_sq_yv, tangent_sq_xv],
                                                              order=1, mode='constant', cval=0.)

            erp_face_frustum_weights = ndimage.map_coordinates(tangent_img_blend_frustum_weights,
                                                               [tangent_sq_yv, tangent_sq_xv],
                                                               order=1, mode='constant', cval=0.)

            erp_radial_weights[erp_sq_yv.astype(np.int32), erp_sq_xv.astype(np.int32), triangle_index] = erp_face_radial_weights

            erp_frustum_weights[erp_sq_yv.astype(np.int32), erp_sq_xv.astype(np.int32),
                                triangle_index] = erp_face_frustum_weights

        self.frustum_blendweights = erp_frustum_weights
        self.radial_blendweights = erp_radial_weights

    def misc_data(self, tangent_images, erp_size):
        """
        Based on Mingze's erp2ico_image method in projection_icosahedron.py
        :param tangent_images:
        :param sub_image_param_expression:
        :param erp_size:
        :param tangent_img_size:
        """
        erp_image_height, erp_image_width = erp_size

        erp_depth_tensor = np.full([erp_image_height, erp_image_width, len(tangent_images)], np.nan, np.float64)
        nn_blending = np.zeros(erp_size)

        if erp_image_width != erp_image_height * 2:
            raise Exception("the ERP image dimession is {}".format(np.shape(erp_depth_tensor)))

        # stitch all tangent images to ERP image
        for triangle_index in range(0, 20):
            tangent_tri_xv, tangent_tri_yv = self.triangle_coordinates_tangent[triangle_index]
            tangent_sq_xv, tangent_sq_yv = self.squared_coordinates_tangent[triangle_index]
            erp_tri_xv, erp_tri_yv = self.triangle_coordinates_erp[triangle_index]
            erp_sq_xv, erp_sq_yv = self.squared_coordinates_erp[triangle_index]

            erp_face_image = ndimage.map_coordinates(tangent_images[triangle_index], [tangent_sq_yv, tangent_sq_xv],
                                                     order=1, mode='constant', cval=0.)

            nn_blending[erp_tri_yv.astype(np.int32), erp_tri_xv.astype(np.int32)] = \
                ndimage.map_coordinates(tangent_images[triangle_index], [tangent_tri_yv, tangent_tri_xv],
                                        order=1, mode='constant', cval=0.)

            erp_depth_tensor[erp_sq_yv.astype(np.int32), erp_sq_xv.astype(np.int32),
                             triangle_index] = erp_face_image.astype(np.float64)

        return erp_depth_tensor, nn_blending

    def get_radial_blendweights(self, img_params, size):
        # Weights for each tangent image. Angular distance wrt to the principal point
        height, width = size
        x_list = np.linspace(0, width, width, endpoint=False)
        y_list = np.linspace(0, height, height, endpoint=False)
        grid_x, grid_y = np.meshgrid(x_list, y_list)
        points2d = np.stack((grid_x.ravel(), grid_y.ravel(), np.ones_like(grid_x.ravel())), axis=1).T
        points3d = np.linalg.inv(img_params["intrinsics"]["matrix"]) @ points2d
        points3d = np.divide(points3d, np.linalg.norm(points3d, axis=0))
        points3d = np.moveaxis(points3d.reshape(-1, height, width), 0, -1)
        principal_point = np.array([img_params['intrinsics']['principal_point'][0],
                                    img_params['intrinsics']['principal_point'][1], 1.])
        principal_point_vec = np.linalg.inv(img_params["intrinsics"]["matrix"]) @ principal_point[..., None]
        principal_point_vec = principal_point_vec / np.linalg.norm(principal_point_vec)
        angles = np.degrees(np.arccos(np.clip(np.dot(points3d, principal_point_vec.squeeze()), -1.0, 1.0)))
        return angles

    def get_frustum_blendweights(self, size):
        height, width = size
        weight_matrix = np.zeros((height, width), dtype=np.float64)

        x_list = np.linspace(0, width, width, endpoint=False)
        y_list = np.linspace(0, height, height, endpoint=False)
        grid_x, grid_y = np.meshgrid(x_list, y_list)

        # Distances to y=0, y=height, x=0, x=width lines. They are the 4 lateral planes of the view frustum.
        dist_to_right  = np.abs(grid_x - width)
        dist_to_left   = grid_x
        dist_to_top    = grid_y
        dist_to_bottom = np.abs(grid_y - height)

        # Build pyramid of distances
        total_dist = np.dstack((dist_to_right, dist_to_left, dist_to_top, dist_to_bottom))
        total_dist = np.min(total_dist, axis=2)
        total_dist = (total_dist - np.min(total_dist)) / np.ptp(total_dist)
        peak_coors = np.where(total_dist == 1)
        peak_top_left = np.array([np.min(peak_coors[0]), np.min(peak_coors[1])])
        peak_bottom_right = np.array([np.max(peak_coors[0]), np.max(peak_coors[1])])

        unit_dir = np.array([1/np.sqrt(2), 1/np.sqrt(2)])
        top_left = (peak_top_left - 2*self.diagonal_percentage*unit_dir).astype(np.int32)
        bottom_right = (peak_bottom_right + 2*self.diagonal_percentage*unit_dir).astype(np.int32)
        total_dist[top_left[0]:bottom_right[0]+1, top_left[1]:bottom_right[1]+1] = 0
        total_dist = (total_dist - np.min(total_dist)) / np.ptp(total_dist)
        total_dist[top_left[0]:bottom_right[0] + 1, top_left[1]:bottom_right[1] + 1] = 1
        return total_dist

    def laplacian_matrix(self, n, m):
        """Generate the Poisson matrix.

        Refer to:
        https://en.wikipedia.org/wiki/Discrete_Poisson_equation

        Note: it's the transpose of the wiki's matrix
        """
        mat_D = scipy.sparse.lil_matrix((m, m))
        mat_D.setdiag(-1, -1)
        mat_D.setdiag(4)
        mat_D.setdiag(-1, 1)

        mat_A = scipy.sparse.block_diag([mat_D] * n).tolil()

        mat_A.setdiag(-1, 1 * m)
        mat_A.setdiag(-1, -1 * m)

        return mat_A

    def concatenate_csc_matrices_by_col(self, matrix1, matrix2):
        new_data = np.concatenate((matrix1.data, matrix2.data))
        new_indices = np.concatenate((matrix1.indices, matrix2.indices))
        new_ind_ptr = matrix2.indptr + len(matrix1.data)
        new_ind_ptr = new_ind_ptr[1:]
        new_ind_ptr = np.concatenate((matrix1.indptr, new_ind_ptr))

        return scipy.linalg.csc_matrix((new_data, new_indices, new_ind_ptr))

    def concatenate_csr_matrices_by_row(self, blocks):
        data = []
        indices = []
        ind_ptr = []
        for idx, block in enumerate(blocks):
            if not isinstance(block, scipy.sparse.csr_matrix):
                block = block.tocsr()
            data.append(block.data)
            indices.append(block.indices)
            if idx > 0:
                ind_ptr.append((block.indptr + ind_ptr[idx-1][-1])[1:])
            else:
                ind_ptr.append(block.indptr)

        data = np.concatenate(data)
        indices = np.concatenate(indices)
        ind_ptr = np.concatenate(ind_ptr)

        return scipy.sparse.csr_matrix((data, indices, ind_ptr))

    def concatenate_coo_matrices_by_row(self, blocks):
        row_indices = []
        col_indices = []
        data = []
        for idx, block in enumerate(blocks):
            if not isinstance(block, scipy.sparse.coo_matrix):
                block = block.tocoo()
            row_indices.append(block.row + idx*block.shape[0])
            col_indices.append(block.col)
            data.append(block.data)

        row_indices = np.concatenate(row_indices)
        col_indices = np.concatenate(col_indices)
        data = np.concatenate(data)
        return scipy.sparse.coo_matrix((data, (row_indices, col_indices)))

    def gradient_blending(self, equirect_tangent_imgs, equirect_weights, color_blended, eigen_solver=None):
        n_images = equirect_tangent_imgs.shape[2]
        equirect_tangent_imgs[np.isnan(equirect_tangent_imgs)] = 0
        t0 = time.time()

        rows, cols = equirect_tangent_imgs[..., 0].shape

        # matrices = self.get_linear_system_matrices(rows, cols, equirect_weights)

        b = []
        for i in range(0, n_images):
            img = equirect_tangent_imgs[..., i]
            weights = equirect_weights[..., i]

            # grad_x = self.x_grad_mat.dot(img.ravel()).reshape((rows, cols)) * weights
            # grad_y = self.y_grad_mat.dot(img.ravel()).reshape((rows, cols)) * weights
            grad_x = np.diff(img, axis=1, append=img[:, 0, None]) * weights
            grad_y = np.diff(img, axis=0, append=np.zeros_like(img[None, 0])) * weights
            b.append(np.concatenate((grad_x.flatten(), grad_y.flatten())))

        b.append(self.fidelity_weight * color_blended.ravel())
        b = np.concatenate(b)

        if self.eigen_solver is not None:
            x = self.eigen_solver.solve(self.A.transpose().dot(b))
        else:
            x, _ = scipy.sparse.linalg.cg(self.A.transpose().dot(self.A), self.A.transpose().dot(b))
            # x = scipy.sparse.linalg.spsolve(self.A.transpose().dot(self.A), self.A.transpose().dot(b))

        t1 = time.time()
        total = t1 - t0
        print("Blending time = {:3f} (s)".format(total))
        return x.reshape((rows, cols))

    def compute_linear_system_matrices(self, rows, cols, equirect_weights):

        if self.blending_method != "poisson" and self.blending_method != "all":
            return
        # Horizontal forward finite differences
        x_grad_mat = scipy.sparse.coo_matrix((cols, cols))
        x_grad_mat.setdiag(-1)
        x_grad_mat.setdiag(1, 1)
        x_grad_mat = scipy.sparse.lil_matrix(x_grad_mat)
        x_grad_mat[-1, 0] = 1  # Wrap around for edges
        x_grad_mat = scipy.sparse.block_diag([x_grad_mat] * rows).tocsr()

        # Vertical forward finite differences
        y_grad_mat = scipy.sparse.coo_matrix((rows * cols, rows * cols))
        y_grad_mat.setdiag(-1)
        y_grad_mat.setdiag(1, cols)
        y_grad_mat = y_grad_mat.tocsr()

        # Mat A. [x_ffd_1, y_ffd_1, x_ffd_2, y_ffd_2, ... , x_ffd_n, y_ffd_n, fidelity]
        blocks = []
        for i in range(0, self.n_subimages):
            weights = equirect_weights[..., i]

            #   Weight the [x_grad, y_grad] blocks of the A matrix
            blocks.append(x_grad_mat.multiply(weights.ravel()[:, None]))
            blocks.append(y_grad_mat.multiply(weights.ravel()[:, None]))

        blocks.append(self.fidelity_weight * scipy.sparse.eye(blocks[0].shape[1]))
        mat_A = self.concatenate_csr_matrices_by_row(blocks)
        self.A = mat_A
        if self.eigen_solver is not None:
            self.eigen_solver.A = self.A.transpose().dot(self.A)
        # self.x_grad_mat = x_grad_mat
        # self.y_grad_mat = y_grad_mat




================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/cam_models.py
================================================
from MiDaS.monodepth_net import MonoDepthNet
from MiDaS.run import *
import MiDaS.MiDaS_utils as MiDaS_utils

import numpy as np
from scipy import ndimage
from scipy.spatial.transform import Rotation as R

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches
matplotlib.use('TkAgg')

import sys
import os
import json


from logger import Logger

log = Logger(__name__)
log.logger.propagate = False

def world2cam_slow(point3D, cam_model):
    """
    code is from: https://github.com/matsuren/ocamcalib_undistort

    Our coordinate system is 
    The original code's 3D coordinate system, x:right direction, y:down direction, z:front direction
    # is [y,x,z]

    return is [x, y]
    """
    """ world2cam(point3D) projects a 3D point on to the image.
    If points are projected on the outside of the fov, return (-1,-1).
    Also, return (-1, -1), if point (x, y, z) = (0, 0, 0).
    The coordinate is different than that of the original OcamCalib.
    point3D coord: x:right direction, y:down direction, z:front direction
    point2D coord: x:row direction, y:col direction (OpenCV image coordinate).

    Parameters
    ----------
    point3D : numpy array or list([x, y, z]), in OpenCV coordinate system.
        array of points in camera coordinate (3xN)

    Returns
    -------
    point2D : numpy array,  in OpenCV coordinate system.
        array of points in image (2xN)

    Examples
    --------
    >>> ocam = OcamCamera('./calib_results_0.txt')
    >>> ocam.world2cam([1,1,2.0]).tolist() # project a point on image
    [[1004.8294677734375], [1001.1594848632812]]
    >>> tmp = ocam.world2cam(np.random.rand(3, 10)) # project multiple points without error
    >>> ocam.world2cam([0,0,2.0]).tolist() # return optical center
    [[798.1757202148438], [794.3086547851562]]
    >>> ocam.world2cam([0,0,0]).tolist()
    [[-1.0], [-1.0]]
    """
    # in case of point3D = list([x,y,z])
    if isinstance(point3D, list):
        point3D = np.array(point3D)
    if point3D.ndim == 1:
        point3D = point3D[:, np.newaxis]
    assert point3D.shape[0] == 3

    # transform the OpenCV coordinate system to OCamCalib's 
    x_ocamcalib = point3D[1]
    y_ocamcalib = point3D[0]
    z_ocamcalib = -point3D[2]

    # 0) transfrom the camera parameter
    _yc = cam_model['intrinsics']['distortion_center'][1]
    _xc = cam_model['intrinsics']['distortion_center'][0]
    _invpol = np.array(cam_model['intrinsics']['poly'])[::-1]
    _affine = np.array(cam_model['intrinsics']['stretch_matrix'])
    _fov = 185

    # return value
    point2D_ocamcalib = np.zeros((2, x_ocamcalib.shape[0]), dtype=np.float32)

    norm = np.sqrt(x_ocamcalib * x_ocamcalib + y_ocamcalib * y_ocamcalib)
    valid_flag = (norm != 0)

    # optical center
    point2D_ocamcalib[0][~valid_flag] = _xc
    point2D_ocamcalib[1][~valid_flag] = _yc
    # point = (0, 0, 0)
    # zero_flag = (point3D == 0).all(axis=0)
    zero_flag = np.logical_and.reduce((x_ocamcalib == 0, y_ocamcalib == 0 , z_ocamcalib == 0))

    point2D_ocamcalib[0][zero_flag] = -1
    point2D_ocamcalib[1][zero_flag] = -1

    # else
    theta = np.arctan(z_ocamcalib[valid_flag] / norm[valid_flag])
    invnorm = 1 / norm[valid_flag]
    rho = 0
    tmp_theta = None
    #     rho = np.array([element * theta ** i for (i, element) in enumerate(self._invpol)]).sum(axis=0) is slow
    for (i, element) in enumerate(_invpol):
        if i == 0:
            rho = np.full_like(theta, element)
            tmp_theta = theta.copy()
        else:
            rho += element * tmp_theta
            tmp_theta *= theta

    u = x_ocamcalib[valid_flag] * invnorm * rho
    v = y_ocamcalib[valid_flag] * invnorm * rho
    point2D_valid_0 = v * _affine[2] + u + _yc
    point2D_valid_1 = v * _affine[0] + u * _affine[1] + _xc

    if _fov < 360:
        # finally deal with points are outside of fov
        thresh_theta = np.deg2rad(_fov / 2) - np.pi / 2
        # set flag when  or point3D == (0, 0, 0)
        outside_flag = theta > thresh_theta
        point2D_valid_0[outside_flag] = -1
        point2D_valid_1[outside_flag] = -1

    point2D_ocamcalib[0][valid_flag] = point2D_valid_0
    point2D_ocamcalib[1][valid_flag] = point2D_valid_1

    # change the 2D points from OCamCalib to OpenCV coordinate system
    point2D_opencv = point2D_ocamcalib[[1,0],:]
    return point2D_opencv


def world2cam(matrix_3d, cam_model):
    """
    TODO check the coordinate system. Make it same as world2cam_slow

    World to pixel coordinates function from Scaramuzza
    :param matrix_3d: points in world coords
    :param cam_model: camera parameters
    :return: projected points in pixel coordinates
    """
    # R = np.array(cam_model['rotation']).reshape(3, 3)
    # t = np.array(cam_model['translation'])
    invpol = np.array(cam_model['intrinsics']['fast_poly'])[::-1]
    #   distorsion_c s inverted because the image is now rotated (3840-height, 2880-width)
    distorsion_c = np.array(cam_model['intrinsics']['distortion_center'])[::-1]
    A = np.array(cam_model['intrinsics']['stretch_matrix']).reshape(2, 2)
    length_invpol = len(invpol)

    norm = np.linalg.norm(matrix_3d[:, :-1], axis=1)
    norm[np.where(norm == 0)] = sys.float_info.epsilon
    theta = np.arctan(np.divide(matrix_3d[:, -1], norm))

    rho = np.ones(norm.shape)*invpol[0]
    t_i = np.ones(norm.shape)

    for i in range(1, length_invpol):
        t_i *= theta
        rho += np.multiply(t_i, invpol[i])

    u = matrix_3d[:, 1]/norm*rho
    v = matrix_3d[:, 0]/norm*rho

    point2D = np.dstack((v, u)).reshape(-1, 2)
    point2D = point2D @ A.T + distorsion_c

    return point2D

    # invpol = np.array([730.949123, 315.876984, -177.960849, -352.468231, -678.144608, -615.917273,
    # -262.086205, -42.961956 ])
    # invpol = np.array([8.8382e3, -6.6872e4, 2.2249e5, -4.2715e5, 5.2136e5, -4.1954e5, 2.2254e5, -7.5488e4, 1.4624e4])[::-1]


def cam2world(matrix_2d, cam_model):
    """
    Project the fisheye image to spherical coordinate 3D point.
    Input and output data coordinate is OpenCV coordinate system.
    Fisheye camera parameters are OcamCalib coordinate system.

    cite from OCamCalib official webpage:
    `
        Back-projects a pixel point m onto the unit sphere M.
        M=[X;Y;Z] is a 3xN matrix with which contains the coordinates of the vectors emanating from the single-effective-viewpoint to the unit sphere, therefore, X^2 + Y^2 + Z^2 = 1.
    `

    More please refer https://sites.google.com/site/scarabotix/ocamcalib-toolbox

    :param matrix_2d: pixel coordinates in OpenCV coordinate system [pointnumber ,2 ] , [:,0] is x , [:,1] is y
    :type matrix_2d: numpy
    :param cam_model: camera parameters, in OCamCalib coordinate system (convention).
    :type cam_model dict
    :return: 3D points in OpenCV coordinate system [x,y,z]
    :rtype: numpy
    """
    # OpenCV to OCamClib coordinate system
    matrix_2d_ocamclib = matrix_2d[:,[1,0]]

    # The following is OCamClib coordinate system
    # all parameter store in OCamClib coordinate system, because it computed by OCamCablib tool-box
    pol = np.array(cam_model['intrinsics']['mapping_coefficients'])
    distorsion_c = np.array(cam_model['intrinsics']['distortion_center'])
    # A = np.array(cam_model['intrinsics']['stretch_matrix']).reshape(2, 2)
    c = cam_model['intrinsics']['stretch_matrix'][0]
    d = cam_model['intrinsics']['stretch_matrix'][1]
    e = cam_model['intrinsics']['stretch_matrix'][2]

    invdet = 1 / (c - d * e)    # 1/det(A), where A = [c,d;e,1] as in the Matlab file
    # point2D = matrix_2d @ A.I - distorsion_c
    u_ocam = invdet * ((matrix_2d_ocamclib[:, 0] - distorsion_c[0]) - d * (matrix_2d_ocamclib[:, 1] - distorsion_c[1]))
    v_ocam = invdet * (-e * (matrix_2d_ocamclib[:, 0] - distorsion_c[0]) + c * (matrix_2d_ocamclib[:, 1] - distorsion_c[1]))
    # point2D = matrix_2d @ A.I - distorsion_c
    # point2D = matrix_2d @ np.linalg.inv(A) - distorsion_c
    # point2D = matrix_2d @ np.linalg.inv(A) - distorsion_c
    # point2D = np.empty_like(matrix_2d)
    # point2D[:, 0] = xp
    # point2D[:, 1] = yp

    # norm = np.linalg.norm(matrix_2d, axis=1)
    norm = np.sqrt(u_ocam * u_ocam + v_ocam * v_ocam)
    z = np.ones(norm.shape)*pol[0]
    r_i = np.ones(norm.shape)

    for i in range(1, len(pol)):
        r_i *= norm
        z += r_i * pol[i]

    # change the spherical 3D points from OCamCalib to OpenCV coordinate system
    u_opencv = v_ocam
    v_opencv = u_ocam
    z_opencv = -z
    points3D = np.stack((u_opencv ,v_opencv , z_opencv)).T

    # normalize to unit norm
    points3D /= np.linalg.norm(points3D, axis=1)[:, np.newaxis]
    return points3D


def create_perspective_undistortion_LUT(img_shape, cam_model, sf=4, patch=None):
    """
    This function is used in case we want to undistort the whole fisheye image or just a patch without taking
    perspective subimages

    :param img_shape: in matrix form (rows, cols)
    :param cam_model: camera parameters
    :param sf: scale factor. From Scaramuzza: it works as a zoom factor?
    :param patch: ROI to undistory. If None it undistorts the whole image
    :return: undistorted image
    """
    width = img_shape[1]  # New width
    height = img_shape[0]  # New height
    Nyc = height/2.0
    Nxc = width/2.0
    Nz  = width/sf

    if patch is not None:
        aux_x = patch[..., 0]
        aux_y = patch[..., 1]
    else:
        aux_y, aux_x = np.mgrid[0:height, 0:width]

    aux_y = aux_y - Nyc
    aux_x = aux_x - Nxc
    aux_z = -np.ones(aux_y.shape) * Nz

    matrix_3d = np.dstack((aux_x.flatten(), aux_y.flatten(), aux_z.flatten()))
    matrix_3d = matrix_3d.reshape(-1, 3)
    matrix_2d = world2cam(matrix_3d, cam_model)

    if patch is not None:
        matrix_2d = matrix_2d.reshape((patch.shape[1], patch.shape[0], 2))
    else:
        matrix_2d = matrix_2d.reshape((height, width, 2))
    return matrix_2d[..., 0], matrix_2d[..., 1]


def point3d2obj(points_3d, obj_path):
    """
    show ocamcalib's 3d points.
    """
    # fig = plt.figure()
    # ax = fig.add_subplot(111, projection='3d')
    # x =fisheye_3d_points[:,0]
    # y =fisheye_3d_points[:,1]
    # z =fisheye_3d_points[:,2]
    # ax.scatter(x, y, z, c='r', marker='o')
    # ax.set_xlabel('X Label')
    # ax.set_ylabel('Y Label')
    # ax.set_zlabel('Z Label')
    # plt.show()
    with open(obj_path, "w") as f:
        for index in range(0,points_3d.shape[0]):
            f.write("v {} {} {}\n".format(points_3d[index, 0],points_3d[index, 1],points_3d[index,2]))

    
def stitch_rgb_image(image_data_list, image_param_list, fisheye_model, subimage_fov=60, fisheye_fov=180):
    """Stitch perspective images to fisheye image.

    :param image_data_list: The perspective images data.
    :type image_data_list: list
    :param image_param_list: The perspective images parameters.
    :type image_param_list: list
    :param fisheye_model: the fisheye image parameters. It's Brown data fromat.
    :type fisheye_model: dict
    :return: the stitched fisheye image.
    :rtype: numpy
    """
    # get the fisheye image size
    fisheye_image_height = fisheye_model["intrinsics"]["image_size"][0]
    fisheye_image_width = fisheye_model["intrinsics"]["image_size"][1]
    channel_number = image_data_list[0].shape[2]

    # project the pinhole image to world coords (spherical 3D points)
    x_list = np.linspace(0, fisheye_image_width, fisheye_image_width, endpoint=False)  
    y_list = np.linspace(0, fisheye_image_height, fisheye_image_height, endpoint=False)  
    grid_x, grid_y = np.meshgrid(x_list, y_list)
    fisheye_2d_points = np.stack((grid_x.ravel(), grid_y.ravel()), axis=1)
    fisheye_3d_points = cam2world(fisheye_2d_points, fisheye_model) 
    # point3d2obj(fisheye_3d_points, "D:/1.obj")

    fisheye_image = np.zeros((fisheye_image_height, fisheye_image_width, channel_number), np.float64)
    fisheye_image_weight = np.zeros((fisheye_image_height, fisheye_image_width), np.float64)
    for index in range(0, len(image_data_list)):
        fisheye_image_weight_subimg = np.zeros((fisheye_image_height, fisheye_image_width), np.float64)
        fisheye_image_subimage = np.zeros((fisheye_image_height, fisheye_image_width, channel_number), np.float64)

        image_param = image_param_list[index]
        image_data = image_data_list[index]

        pinhole_image_height = image_data.shape[0]
        pinhole_image_width = image_data.shape[1]

        pinhole_3d_points = image_param['rotation'] @ fisheye_3d_points.T + image_param['translation'][:, np.newaxis]

        # check the pixel in the available hfov and vfov range of fisheye image
        hfov = subimage_fov
        vfov = subimage_fov
        radius = np.linalg.norm(pinhole_3d_points.T, axis=1)
        points_azimuth = np.degrees(np.arctan2(pinhole_3d_points[0, :], pinhole_3d_points[2, :]))
        points_altitude = np.degrees(np.arcsin(np.divide(-pinhole_3d_points[1, :], radius)))

        points_azimuth_inhfov = np.logical_and(points_azimuth > - 0.5 * hfov, points_azimuth < 0.5 * hfov)
        points_altitude_invfov = np.logical_and(points_altitude > - 0.5 * vfov, points_altitude < 0.5 * vfov)
        available_pixels_list_fov = np.logical_and(points_azimuth_inhfov, points_altitude_invfov)
        available_pixels_list_fov_mat = available_pixels_list_fov.reshape(fisheye_image_height, fisheye_image_width)

        fisheye_2d_points_subimage = fisheye_2d_points[available_pixels_list_fov].astype(np.int32)

        # projection to pin-hole image
        pinhole_3d_points = (np.divide(pinhole_3d_points, pinhole_3d_points[2, :]))
        pinhole_2d_points = (image_param["intrinsics"]["matrix"] @ pinhole_3d_points)[:2, :]
        pinhole_2d_points = pinhole_2d_points[:, available_pixels_list_fov]

        # just use the 3D point in the available pinhole image range
        available_pixels_list = np.logical_and.reduce((
            pinhole_3d_points[2, :][available_pixels_list_fov] > 0,
            pinhole_2d_points[0, :] >= 0, pinhole_2d_points[0, :] <= pinhole_image_width - 1,
            pinhole_2d_points[1, :] >= 0, pinhole_2d_points[1, :] <= pinhole_image_height - 1))

        for channel in range(0, channel_number):
            fisheye_image_subimage[fisheye_2d_points_subimage[:, 1][available_pixels_list], fisheye_2d_points_subimage[:, 0][available_pixels_list], channel] = \
                ndimage.map_coordinates(image_data[:, :, channel],
                                        [pinhole_2d_points[1, :][available_pixels_list], pinhole_2d_points[0, :][available_pixels_list]],
                                        order=1, mode='constant', cval=255.0)

        # compute blend weight
        available_pixels_weight = np.ones(available_pixels_list.shape,  np.float64)
        available_pixels_weight[~available_pixels_list] = 0
        fisheye_image_weight_subimg[available_pixels_list_fov_mat] = available_pixels_weight

        fisheye_image_weight += fisheye_image_weight_subimg
        fisheye_image += fisheye_image_subimage

    # get pixels mean
    available_weight_list = fisheye_image_weight != 0
    for channel in range(0, channel_number):
        fisheye_image[:, :, channel][available_weight_list] = np.divide(fisheye_image[:, :, channel][available_weight_list], fisheye_image_weight[available_weight_list])

    return fisheye_image.astype(np.uint8)


def sample_rgb_image(img, model, fov=[60, 60], canvas_size=[400, 400], sample_grid_size=[3, 3]):
    """ Sample perspective images from fish-eye image.

    :param img: fisheye rgb image data
    :type img: numpy
    :param model: Scaramuzza camera model
    :type model: dict
    :param fov: fov for the perspective views, [h_fov, v_fov]
    :type fov: list
    :param canvas_size: the perspective images size, [image_height, image_width]
    :type canvas_size: list
    :param sample_grid_size: the sample grid size [horizontal grid, vertical grid].
    :type sample_grid_size: list
    :return: perspective images and camera parameters.
    """
    hfov_fisheye = 180  # TODO load from camera_model dict
    vfov_fisheye = 180
    hfov_pinhole = fov[0]
    vfov_pinhole = fov[1]
    horizontal_size = sample_grid_size[0]
    vertical_size = sample_grid_size[1]
    image_height_pinhole = canvas_size[0]
    image_width_pinhole = canvas_size[1]

    # 0) generate the perspective camera parameters
    # 0-0) get the camera orientation
    xyz_rotation_array = generate_camera_orientation(hfov_fisheye, vfov_fisheye, hfov_pinhole, vfov_pinhole, horizontal_size, vertical_size, 20)

    # 0-1) get the camera intrinsic and extrinsic parameters
    sub_image_param_list = get_perspective_camera_parameters(
        hfov_pinhole, vfov_pinhole, image_width_pinhole, image_height_pinhole,  xyz_rotation_array.T)

    # 1) generate the perspective images
    # TODO perspective image size is base on fov ane original fish-eye image size
    canvas_yy, canvas_xx = np.mgrid[0:image_height_pinhole, 0:image_width_pinhole]
    canvas_2d = np.squeeze(np.dstack((canvas_xx.flatten(), canvas_yy.flatten())))
    # canvas_2d each column is 3D point [x,y,1]
    canvas_2d = np.hstack((canvas_2d, np.ones((image_height_pinhole * image_width_pinhole, 1))))
    sub_images = []
    channel_number = img.shape[2]

    for index in range(0, len(sub_image_param_list)):
        sub_image_param = sub_image_param_list[index]

        # Transform pixel coords to world coords
        pinhole_cs = np.linalg.inv(sub_image_param['intrinsics']['matrix']) @ canvas_2d.T
        world_cs = np.linalg.inv(sub_image_param['rotation']) @ pinhole_cs

        # Fetch RGB from fisheye image
        # NOTE world2cam use fast_poly, world2cam_slow use poly
        # fetch_from = world2cam(world_cs.T, model)
        fetch_from = world2cam_slow(world_cs, model).T
        tangential_img = np.zeros(tuple(canvas_size) + (channel_number,), dtype=np.float64)

        for channel in range(0, channel_number):
            tangential_img[:, :, channel] = ndimage.map_coordinates(img[:, :, channel], [fetch_from[:, 1].reshape(canvas_size), fetch_from[:, 0].reshape(canvas_size)], order=1, mode='constant')

        sub_images.append(tangential_img.astype(np.uint8))

    return sub_images, sub_image_param_list


def sample_img(img, cam_model, fov=53, run_midas=False):
    """
    :param img: fisheye input img
    :param cam_model: Scaramuzza model
    :param fov: fov for the perspective views
    :param run_midas: Set to False in case you already run it once and have the depth maps saved in memory
    :return: perspective subviews. I also call them tangential views
    """

    """
    Thinkin loud...
    Lets wrap up here what I need to do:
    - The goal is to stitch the tangent images (its depth estimation) in the equirectangular domain. How?
        1- Sample the fisheye with each virtual camera getting the tangential images
            1.1) We do this by backprojecting the 2D points to 3D and project these 3D coords back to the fisheye.
            Then, we take the portion of the fisheye correspondent to what the virtual camera sees, i.e. the
            tangential image
        2- We create the empty equirectangular image. We backproject the points to 3D and we find for each 3D point
        its projection into the N virtual cameras. If the 2D projection lies within the camera FOV of ONE of the
        virtuals, then we fetch the tangent image pixel value to its equirectangular equivalent.
        3- What if the equirectangular 3D point lies in more than one virtual camera?? WE NEED BLENDING. Try Poisson.
            3.1- Maybe build an array: (x, y, z, n_proj, [virtual idxs]) where (x,y,z) is the 3D point, n_proj is
            the amount of virtual cameras that point projects to (i.e within its FOV) and [virtual idxs] shows the
            virtual camera indices which contains that point.
            3.2- We should filter that array to only contain the points with n_proj > 1 since those are the only ones 
            for which blending is necessary.
    """
    fisheye_v_fov, fisheye_h_fov = get_fisheyeFOV(img, cam_model)   # Get FOV

    #   Save pinhole camera parameters
    # params_file = open("pinhole_params.json", "w")

    #   TODO: Need to figure out a better way to sample the fisheye domain.
    #   At the moment I just choose a 5x5 virtual array of perspective cameras
    v_angles = np.linspace(-fisheye_v_fov/2, fisheye_v_fov/2, 5)
    h_angles = np.linspace(-fisheye_h_fov/2, fisheye_h_fov/2, 5)[::-1]
    yy, xx = np.meshgrid(v_angles, h_angles, indexing='ij')
    angle_mesh = np.dstack((xx, yy))
    plt.figure()
    plt.imshow(img)
    height, width = img.shape[:-1]

    canvas_size = np.array([384, 384])  # Size for perspective images
    canvas_yy, canvas_xx = np.mgrid[0:canvas_size[0], 0:canvas_size[1]]
    canvas_2d = np.squeeze(np.dstack((canvas_xx.flatten(), canvas_yy.flatten())))
    canvas_2d = np.hstack((canvas_2d, np.ones((canvas_size[0] * canvas_size[1], 1))))

    """
    equirect_3D_points has size (7, 1000x2000)
    Rows [0,1,2] store the 3D points in equirectangular cam coordinates of the equirect image plane. No needed???
    Rows [3,4,5] store the RGB values for the equirect image fetched from the perspective views 
    Rows [6] store the depth values for the equirect image fetched from the estimated depth map of the perspective views
    
    When we finish fetching RGB and depth, the RGB equirect is obtaining by resizing 
    equirect_3D_points[3:6,:] to (3,1000,2000)
    
    and for the depth panorama we need to resize  
    equirect_3D_points[6,:] to (1,1000,2000)
    """

    equirect_size = (3, 1000, 2000)     # Size for equirectangular image
    equirect_3D_points, _ = equirect_cam2world(equirect_size[1:])
    equirect_3D_points_rgb = np.zeros((7, equirect_3D_points.shape[-1]), dtype=np.float64)
    equirect_3D_points_rgb[0, :] = equirect_3D_points[0, :]
    equirect_3D_points_rgb[1, :] = equirect_3D_points[1, :]
    equirect_3D_points_rgb[2, :] = equirect_3D_points[2, :]

    fisheye2equirec = np.zeros((3, equirect_3D_points.shape[-1]), dtype=np.float64)
    #   Lines 200-205 is for converting the whole fisheye to equirectangular
    #   Points at the back of the cylinder are mapped to nan
    nan_boolean = np.bitwise_not(np.isnan(np.sum(equirect_3D_points.T, axis=1)))
    fetch_from = world2cam(equirect_3D_points.T, cam_model).astype(int).T
    fetch_from[0, :] = np.clip(fetch_from[0, :], 0, width - 1)
    fetch_from[1, :] = np.clip(fetch_from[1, :], 0, height - 1)
    fisheye2equirec[:, nan_boolean] = img[fetch_from[1, nan_boolean], fetch_from[0, nan_boolean]].T
    fisheye2equirec = np.moveaxis(fisheye2equirec.ravel().reshape(equirect_size), 0, -1)
    RGB_sub_images = []
    depth_sub_images = []

    if run_midas:
        os.makedirs('../../../../depth_subviews/', exist_ok=True)

    virtual_pinhole_params = []
    #   Iterate through the NxN virtual camera array
    for i in range(0, angle_mesh.shape[0]):
        for j in range(0, angle_mesh.shape[1]):
            idx = i*angle_mesh.shape[1] + j
            angle_pair = angle_mesh[i, j]

            #   Get perspective camera model with angle_pair as parameter for extrinsic params
            pinhole_camera, params_2json = \
                getVirtualCameraMatrix(fov, canvas_size, cam_model, x_angle=angle_pair[1], y_angle=angle_pair[0])
            virtual_pinhole_params.append(params_2json)

            #   Transform pixel coords to world coords
            pinhole_cs = np.linalg.inv(pinhole_camera['intrinsics']['matrix']) @ canvas_2d.T
            world_cs = np.linalg.inv(pinhole_camera['rotation']) @ pinhole_cs

            #   Fetch RGB from fisheye image to assemble perspective subview
            fetch_from = world2cam(world_cs.T, cam_model).astype(np.int32)
            fetch_from[:, 0] = np.clip(fetch_from[:, 0], 0, width-1)
            fetch_from[:, 1] = np.clip(fetch_from[:, 1], 0, height-1)
            virtual2fisheye_idxs = np.dstack((fetch_from[:, 0].reshape(canvas_size), fetch_from[:, 1].reshape(canvas_size)))

            tangential_img = img[virtual2fisheye_idxs[..., 1], virtual2fisheye_idxs[..., 0]]

            if not os.path.isfile('../../../../color_subviews/{:06}_{}_{}.png'.format(idx, int(angle_pair[1]), int(angle_pair[0]))):
                os.makedirs('../../../../color_subviews/', exist_ok=True)
                cv2.imwrite('../../../../color_subviews/frame_{:06}.png'.format(idx), tangential_img)

            if run_midas:
                tangential_depth = run_depth(tangential_img[None], 'MiDaS/model.pt', MonoDepthNet, MiDaS_utils)[0]
                os.makedirs('../../../../depth_subviews/', exist_ok=True)
                np.save('../../../../depth_subviews/frame_{:06}.npy'.format(idx), tangential_depth)
            else:
                tangential_depth = np.load('../../../../depth_subviews/frame_{:06}.npy'.format(idx))

            # tangential_depth = (tangential_depth - np.min(tangential_depth)) / np.ptp(tangential_depth)
            print('frame {}: {}---{}'.format(idx, np.max(tangential_depth), np.min(tangential_depth)))
            RGB_sub_images.append(tangential_img)
            depth_sub_images.append(tangential_depth)

            #   Which equirect 3D points fall in the current perspective subview camera plane?
            equi2tangential_3d = pinhole_camera['intrinsics']['matrix'] @ pinhole_camera['rotation'] @ equirect_3D_points
            equi2tangential = np.divide(equi2tangential_3d, np.tile(equi2tangential_3d[2, :], (3, 1)))[:-1].astype(int)
            inside_fov = (equi2tangential[0, :] >= 0) & (equi2tangential[0, :] < canvas_size[1]) & \
                         (equi2tangential[1, :] >= 0) & (equi2tangential[1, :] < canvas_size[0]) & \
                         (equi2tangential_3d[2, :] >= 0)
            fetch_pixel = equi2tangential[:, inside_fov]

            # Fetch RGB and depth values for the equirect 3D points which fall in the currently evaluated perspective
            # RGB and depth subview respectively
            equirect_3D_points_rgb[3:6, inside_fov] = tangential_img[fetch_pixel[1], fetch_pixel[0]].T
            equirect_3D_points_rgb[6, inside_fov] = tangential_depth[fetch_pixel[1], fetch_pixel[0]].T

    # json.dump(virtual_pinhole_params, params_file)
    # params_file.close()

    #   Final RGB and depth equirectangular image
    equirect_rgb = np.moveaxis(equirect_3D_points_rgb[3:6].ravel().reshape(equirect_size), 0, -1)
    equirect_depth = equirect_3D_points_rgb[6].reshape(equirect_size[1:])

    return np.array(RGB_sub_images), np.array(depth_sub_images), xx.shape


def get_fisheyeFOV(img, model):
    #   Get (top center vs bottom center) and (left center vs right center) pixels to calculate FOV
    top_center_pt = np.array([0, 0])  
    bot_center_pt = np.array([0, img.shape[0]])
    left_center_pt = np.array([0, 0])
    right_center_pt = np.array([img.shape[1], 0])

    top_center_world = cam2world(np.array([top_center_pt]), model)[0]
    bot_center_world = cam2world(np.array([bot_center_pt]), model)[0]
    left_center_world = cam2world(np.array([left_center_pt]), model)[0]
    right_center_world = cam2world(np.array([right_center_pt]), model)[0]

    v_fov = np.degrees(np.arccos(np.clip(np.dot(bot_center_world, top_center_world), -1.0, 1.0)))
    h_fov = np.degrees(np.arccos(np.clip(np.dot(right_center_world, left_center_world), -1.0, 1.0)))

    return v_fov*2, h_fov*2


def plot_img_array(camera_array_size, sub_images):
    fig, axes = plt.subplots(nrows=camera_array_size[0], ncols=camera_array_size[1])
    for i in range(0, camera_array_size[0]):
        for j in range(0, camera_array_size[1]):
            idx = i*camera_array_size[1]+j
            axes[i, j].axis("off")
            axes[i, j].imshow(sub_images[idx])
    plt.subplots_adjust(wspace=.05, hspace=.05)
    plt.show()


def cam2world_single(point2D, model):
    """Converts a single camera pixel (2D) to world-space ray (3D).
    NB: Straightforward translation of Scaramuzza's C++ code."""

    c = model['intrinsics']['stretch_matrix'][0]
    d = model['intrinsics']['stretch_matrix'][1]
    e = model['intrinsics']['stretch_matrix'][2]
    invdet = 1. / (c - d * e)  # 1/det(A), where A = [c,d;e,1] as in the Matlab file

    xc = model['intrinsics']['distortion_center'][1]
    yc = model['intrinsics']['distortion_center'][0]
    xp = invdet * ((point2D[0] - xc) - d * (point2D[1] - yc) )
    yp = invdet * (-e * (point2D[0] - xc) + c * (point2D[1] - yc) )

    pol = model['intrinsics']['mapping_coefficients']
    r   = np.sqrt(xp * xp + yp * yp)  # distance [pixels] of  the point from the image center
    zp  = pol[0]
    r_i = 1

    for i in range(1, len(pol)):
        r_i *= r
        zp  += r_i * pol[i]

    ## Normalise to unit norm.
    invnorm = 1. / np.sqrt(xp * xp + yp * yp + zp * zp)
    point3D = invnorm * np.array([xp, yp, zp])

    return point3D


def world2cam_single(point3D, model):
    """Converts a single world-space ray (3D) to a camera pixel (2D).
    NB: Straightforward translation of Scaramuzza's C++ code."""

    xc = model['intrinsics']['distortion_center'][0]
    yc = model['intrinsics']['distortion_center'][1]

    norm = np.sqrt(point3D[0] * point3D[0] + point3D[1] * point3D[1])

    if norm != 0:
        theta = np.arctan(point3D[2] / norm)

        invpol = np.array(model['intrinsics']['fast_poly'])[::-1]
        invnorm = 1.0 / norm
        t  = theta
        rho = invpol[0]
        t_i = 1

        for i in range(1, len(invpol)):
            t_i *= t
            rho += t_i * invpol[i]

        x = point3D[0] * invnorm * rho
        y = point3D[1] * invnorm * rho

        c = model['intrinsics']['stretch_matrix'][0]
        d = model['intrinsics']['stretch_matrix'][1]
        e = model['intrinsics']['stretch_matrix'][2]
        point2D = np.zeros(2)
        point2D[0] = x * c + y * d + xc
        point2D[1] = x * e + y + yc

    else:
        point2D = np.array([xc, yc])

    return point2D


def equirect_world2cam(matrix3d, im_size=(1000, 1000)):
    # FIXME This is not tested and wrong. But at the moment I dont need it
    # implemented from https://ttic.uchicago.edu/~rurtasun/courses/CV/lecture08.pdf slide 32. What is s????????
    # s might be size of the image. [height, width] maybe square?
    # sphere_xy_center = (matrix3d[0, :] == 0) & (matrix3d[1, :] == 0)
    # fix_x = np.where(matrix3d[0, :] == 0)
    #
    # xp = im_size[1] * np.clip(np.arctan(matrix3d[1, :] / matrix3d[0, :])/(np.pi*0.5), -1.0, 1.0) + im_size[1]
    # xp[fix_x] = im_size[1]
    # yp = np.clip(im_size[0] * (matrix3d[2, :] / np.linalg.norm(matrix3d[[0, 1], :], axis=0)), -im_size[0], im_size[0])
    # yp[sphere_xy_center] = im_size[0]
    #
    #
    # points_2D = np.zeros((2, xp.shape[0]))
    # points_2D[0, :] = xp
    # points_2D[1, :] = yp

    #   Other implementation
    matrix3d = np.divide(matrix3d, np.tile(np.linalg.norm(matrix3d, axis=0), (3, 1)))
    lon = np.arctan2(matrix3d[1, :], matrix3d[0, :])
    lat = np.arctan2(matrix3d[2, :], np.linalg.norm(matrix3d[[0, 1], :], axis=0))

    xp = (im_size[1]*0.5) * (lon / np.pi) + (im_size[1]*0.5)
    yp = (im_size[0]*0.5) * (2 * lat / np.pi) + (im_size[0]*0.5)

    points_2D = np.zeros((2, xp.shape[0]))
    points_2D[0, :] = xp
    points_2D[1, :] = yp

    return points_2D

    # Remap with new 2D equirectangular coords


def equirect_cam2world(im_size=(1000, 1000)):

    canvas_yy, canvas_xx = np.mgrid[0:im_size[0], 0:im_size[1]]
    canvas_2d = np.squeeze(np.dstack((canvas_xx.flatten(), canvas_yy.flatten()))).reshape(-1, 2).T

    theta = (2*np.pi*canvas_2d[0, :]/(im_size[1]))
    phi = (np.pi * canvas_2d[1, :] / (im_size[0]))

    Y = np.cos(phi)
    Z = np.sin(phi) * -np.cos(theta)
    X = np.sin(phi) * -np.sin(theta)

    Z[Z < 0] = np.nan   # Dont consider anything on the back of the cylinder

    # Scaramuzza original 3D coordinate system. Z points inwards
    #
    #   Z                                                           ^ Y
    #   *------>Y                                                   |
    #   |                                                           |
    #   |           Remember that we rotated the image 90º          |
    #   |           counter-clockwise therefore now               Z *-------->X
    #   v X         it changes to. Check graph in the right
    #

    points_3D = np.zeros((3, im_size[0]*im_size[1]))
    points_3D[0, :] = X
    points_3D[1, :] = Y[::-1]
    points_3D[2, :] = Z

    unit_sphere = np.divide(points_3D, np.tile(np.linalg.norm(points_3D, axis=0), (3, 1)))

    return points_3D, unit_sphere


def generate_camera_orientation(hfov_fisheye, vfov_fisheye, hfov_pinhole, vfov_pinhole, horizontal_size, vertical_size, padding=0):
    """ Generate the camera orientation from the FOV and grid size

    :param vertical_size: camera orientation list, [3, horizontal_size * vertical_size], each row are rotation along the x, y and z axises.
    :type vertical_size: numpy
    :param padding: the padding area angle (degree) of the fisheye image along the edge of image.
    :type padding: float
    """
    hfov_interval = (hfov_fisheye - hfov_pinhole - padding * 2) / (horizontal_size - 1)
    vfov_interval = (vfov_fisheye - vfov_pinhole - padding * 2) / (vertical_size - 1)

    h_index = (np.linspace(0, horizontal_size, horizontal_size, endpoint=False) - (horizontal_size - 1) / 2.0) * hfov_interval
    v_index = (np.linspace(0, vertical_size, vertical_size, endpoint=False) - (vertical_size - 1) / 2.0) * vfov_interval
    x_rotation, y_rotation = np.meshgrid(h_index, v_index)  # the camere orientation

    # compute the overlap of perspective images.
    overlap_area_h = h_index[0] + hfov_pinhole / 2.0 - (h_index[1] - hfov_pinhole / 2.0)
    log.debug("the horizontal overlap angle is {}".format(overlap_area_h))
    overlap_area_v = v_index[0] + vfov_pinhole / 2.0 - (v_index[1] - vfov_pinhole / 2.0)
    log.debug("the vertical overlap angle is {}".format(overlap_area_v))

    z_rotation = np.zeros(x_rotation.shape, np.float64)
    xyz_rotation_array = np.stack((x_rotation, y_rotation, z_rotation), axis=0)
    xyz_rotation_array = xyz_rotation_array.reshape([3, horizontal_size * vertical_size])
    return xyz_rotation_array


def get_perspective_camera_parameters(hfov, vfov, image_width, image_height, camera_direction):
    """create the pinhole camera parameters.

    :param hfov: the horizental field of view, radian.
    :type hfov: float
    :param vfov: the verical field of view, radian.
    :type vfov: float
    :param image_width: pin-hole image width.
    :type image_width: float
    :param image_height: pin-hole image height.
    :type image_height: float
    :param camera_direction: the camera direction array, the 3 columns are angle rotate along the x,y,z axises, size is [n,3].
    :type camera_direction: numpy
    :return: camera intrinsic and extrinsic parameters, the array is row mojor.
    :rtype: dict
    """  
    fx = 0.5*image_width / np.tan(np.radians(0.5*hfov))
    fy = 0.5*image_height / np.tan(np.radians(0.5*vfov))

    rotation = R.from_euler("zyx", camera_direction[:,[2,1,0]], degrees=True)
    rotation_mat_list = rotation.as_matrix()

    cx = image_width / 2.0
    cy = image_height / 2.0
    intrinsic_matrix = np.array([[fx, 0, cx],
                                [0, fy, cy],
                                [0, 0, 1]])

    params_list= []
    for rotation_mat in rotation_mat_list:
        params = {'rotation': rotation_mat,
                'translation': np.array([0, 0, 0]),
                'intrinsics': {
                    'image_width': image_width,
                    'image_height':image_height,
                    'focal_length_x': fx,
                    'focal_length_y': fy,
                    'principal_point': [cx, cy],
                    'matrix': intrinsic_matrix}
                }

        params_list.append(params)

    return params_list


def getVirtualCameraMatrix(viewfield, size, fisheye_cam, x_angle=0, y_angle=0, z_angle=0):
    """Create virtual camera intrinsic parameters. 

    Use the camera fov and image size to figure out the camera intrinsic parameters.

    TODO suport rectangle pinthole image

    :param viewfield: the camera's horizontal fov (field of view)
    :type viewfield: list or float
    :param size: the camera's horizontal fov (field of view)
    :type size: list
    :param x_angle: x_anlge, defaults to 0
    :type x_angle: int, optional
    :param y_angle: x_angle, defaults to 0
    :type y_angle: int, optional
    :param z_angle: z_angle, defaults to 0
    :type z_angle: int, optional
    :return: camera intrinsic
    :rtype: dict
    """  
    f = 0.5*size[1] / np.tan(np.radians(0.5*viewfield))

    intrinsic_matrix = np.array([[f, 0, size[1]/2],
                                 [0, f, size[0]/2],
                                 [0, 0, 1]])

    rmat_x = np.array([[1,             0,                      0],
                       [0, np.cos(np.radians(x_angle)), -np.sin(np.radians(x_angle))],
                       [0, np.sin(np.radians(x_angle)), np.cos(np.radians(x_angle))]])

    rmat_y = np.array([[np.cos(np.radians(y_angle)), 0, np.sin(np.radians(y_angle))],
                       [0,                           1,                 0],
                       [-np.sin(np.radians(y_angle)), 0, np.cos(np.radians(y_angle))]])

    rmat_z = np.array([[np.cos(np.radians(z_angle)), -np.sin(np.radians(z_angle)), 0],
                       [np.sin(np.radians(z_angle)), np.cos(np.radians(z_angle)),  0],
                       [0,             0,                      1]])

    params = {'rotation': rmat_z @ rmat_y @ rmat_x,
              'translation': np.array([0, 0, 0]),
              'intrinsics': {
                  'image_size': size,
                  'focal_length': f,
                  'principal_point': [size[0]/2, size[1]/2],
                  'matrix': intrinsic_matrix}}

    params_json = {'rotation': (rmat_z @ rmat_y @ rmat_x).tolist(),
                   'translation': [0, 0, 0],
                   'intrinsics': {
                        'image_size': size.tolist(),
                        'focal_length': f.tolist(),
                        'principal_point': [size[0]/2, size[1]/2],
                        'matrix': intrinsic_matrix.tolist()}}

    return params, params_json



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/data_mocker.py
================================================
from utility import depth_stitch, serialization
from utility import depthmap_utils
from utility import serialization
from utility import image_io
from utility import fs_utility
from utility import subimage

import numpy as np
import projection_icosahedron as proj_ico

import pathlib
import os

from logger import Logger
log = Logger(__name__)
log.logger.propagate = False


def create_alignment_data(erp_depthmap, subimage_size=500,
                          padding_size=0.3,
                          coeff_scale_list=None, coeff_offset_list=None, grid_size=[5, 5]):
    """Create the depth alignment test data.

    Project the ERP depth map to 20 sub-images, with the scale and offset.
  
    """
    # 0) check the scale and offset coefficients
    if coeff_scale_list is None:
        rng = np.random.default_rng(seed=0)
        coeff_scale_list = rng.random((grid_size, 20))

    if coeff_offset_list is None:
        rng = np.random.default_rng(seed=1)
        coeff_offset_list = rng.random((grid_size, 20))

    # 1) erp to 20 images
    subimage_list, _, _ = proj_ico.erp2ico_image(erp_depthmap, subimage_size, padding_size, full_face_image=True)

    # 2) scale and offset the subimage disparity map
    subimage_list_new = []
    for idx in range(len(subimage_list)):
        temp = depth_stitch.depthmap_deform(subimage_list[idx], coeff_scale_list[idx], coeff_offset_list[idx])
        subimage_list_new.append(temp)

    return subimage_list_new, coeff_scale_list, coeff_offset_list


def data_visualizing(mock_data_root_dir, frame_number):
    """
    """
    # 0) visualize the depth map (*.pfm)
    filenameConv = fs_utility.FileNameConvention()
    filenameConv.set_filename_basename("img0")
    filenameConv.set_filepath_folder(mock_data_root_dir)

    depthmap_list = {}
    for idx in range(frame_number):
        depthmap_filename = pathlib.Path(filenameConv.subimage_depthmap_filename_expression.format(idx))
        dispmap, _ = depthmap_utils.read_pfm(str(depthmap_filename))
        dispmap_vis_filepath = str(depthmap_filename) + ".jpg"
        depthmap_utils.depth_visual_save(dispmap, str(dispmap_vis_filepath))
        log.debug("visualize disparity map {}".format(str(depthmap_filename)))

        depthmap_list[idx] = dispmap

    image_height = depthmap_list[0].shape[0]
    image_width = depthmap_list[0].shape[1]

    # 1) visualize the pixel corresponding relationship (*.json)
    for src_idx in range(frame_number):
        for tar_idx in range(frame_number):
            if src_idx == tar_idx:
                continue
            filename_src2tar = filenameConv.subimage_pixelcorr_filename_expression.format(src_idx, tar_idx)
            pixels_corresponding = serialization.pixel_corresponding_load(filename_src2tar)
            # print("load the pixels corresponding file: {}".format(filename_src2tar))

            src_image_filepath = pixels_corresponding["src_image_filename"]
            tar_image_filepath = pixels_corresponding["tar_image_filename"]

            # load subimage
            if os.path.isabs(src_image_filepath) and os.path.isabs(tar_image_filepath):
                # src_image_data = image_io.image_read(src_image_filepath + ".jpg")
                # tar_image_data = image_io.image_read(tar_image_filepath+ ".jpg")
                image1_output_path = src_image_filepath + "_{}_{}.jpg".format(src_idx, tar_idx)
                image2_output_path = tar_image_filepath + "_{}_{}.jpg".format(tar_idx, src_idx)
            else:
                # src_image_data = image_io.image_read(mock_data_root_dir + src_image_filepath+ ".jpg")
                # tar_image_data = image_io.image_read(mock_data_root_dir + tar_image_filepath+ ".jpg")
                image1_output_path = mock_data_root_dir + src_image_filepath + "_{}_{}.jpg".format(src_idx, tar_idx)
                image2_output_path = mock_data_root_dir + tar_image_filepath + "_{}_{}.jpg".format(tar_idx, src_idx)

            src_image_data = np.full((image_height, image_width, 3), 255, dtype=np.uint8)
            tar_image_data = np.full((image_height, image_width, 3), 255, dtype=np.uint8)
            # plot the pixels points
            pixel_corresponding_array = pixels_corresponding["pixel_corresponding"]
            pixel_corresponding_number = pixels_corresponding["pixel_corresponding_number"]

            if pixel_corresponding_array is None or pixel_corresponding_number <= 0:
                continue

            src_image_data_image_np, tar_image_data_image_np, _ = subimage.draw_corresponding(src_image_data, tar_image_data, pixel_corresponding_array)

            image_io.image_save(src_image_data_image_np, image1_output_path)
            image_io.image_save(tar_image_data_image_np, image2_output_path)
            # , image1_output_path, image2_output_path

    # 2) visualize the scale and offset coefficients (*.json)



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/depth_stitch.py
================================================
import cam_models

from scipy import ndimage
import numpy as np


from logger import Logger
log = Logger(__name__)
log.logger.propagate = False


def depthmap_deform(depthmap, scale_array, offset_array):
    """    Deform the depth map with scale and offset.


    :param depthmap: The original depth map [height, width]
    :type depthmap: numpy 
    :param scale_array: The offset array[height, width]
    :type scale_array: numpy
    :param offset_array: The offset array, [height, width]
    :type offset_array: numpy
    :return: deformed depth map
    :rtype: numpy
    """    
    if scale_array.shape != offset_array.shape:
        log.error("The offset size is not equal scale's shape.")

    image_height = depthmap.shape[0]
    image_width = depthmap.shape[1]
    
    # 0) get scale and offset for each pixel
    corr_yv, corr_xv = np.mgrid[0:image_height, 0:image_width]
    corr_yv_grid = corr_yv / (image_height - 1) * (scale_array.shape[0] - 1)
    corr_xv_grid = corr_xv / (image_width - 1) * (scale_array.shape[1] - 1)

    scale_pixelwise = ndimage.map_coordinates(scale_array, [corr_yv_grid, corr_xv_grid], order=1, mode='constant', cval=0.0)
    offset_pixelwise = ndimage.map_coordinates(offset_array, [corr_yv_grid, corr_xv_grid], order=1, mode='constant', cval=0.0)

    # 1) get new depth map
    dephtmap_deformed = depthmap * scale_pixelwise + offset_pixelwise
    return dephtmap_deformed


def stitch_depth_subimage(depth_data_list, image_param_list, fisheye_model):
    """Stitch perspective images to fisheye image.

    :param depth_data_list: The perspective images data.
    :type depth_data_list: list
    :param image_param_list: The perspective images parameters.
    :type image_param_list: list
    :param fisheye_model: the fisheye image parameters. It's Brown data fromat.
    :type fisheye_model: dict
    :return: the stitched fisheye image.
    :rtype: numpy
    """
    # get the fisheye image size
    fisheye_image_height = fisheye_model["intrinsics"]["image_size"][0]
    fisheye_image_width = fisheye_model["intrinsics"]["image_size"][1]

    fisheye_depth = np.zeros((fisheye_image_height, fisheye_image_width), np.float64)
    fisheye_image_weight = np.zeros((fisheye_image_height, fisheye_image_width), np.float64)

    # project the pinhole image to 3D
    x_list = np.linspace(0, fisheye_image_width, fisheye_image_width, endpoint=False)
    y_list = np.linspace(0, fisheye_image_height, fisheye_image_height, endpoint=False)
    grid_x, grid_y = np.meshgrid(x_list, y_list)
    fisheye_2d_points = np.stack((grid_x.ravel(), grid_y.ravel()), axis=1)
    fisheye_3d_points = cam_models.cam2world(fisheye_2d_points, fisheye_model)

    for index in range(0, len(depth_data_list)):
        image_param = image_param_list[index]
        depth_data = depth_data_list[index]

        pinhole_image_height = depth_data.shape[0]
        pinhole_image_width = depth_data.shape[1]

        # # Transform pixel coords to world coords (spherical 3D points)
        pinhole_3d_points_original = image_param['rotation'] @ fisheye_3d_points.T  # + image_param['translation'][:, np.newaxis]
        pinhole_3d_points = (np.divide(pinhole_3d_points_original, pinhole_3d_points_original[2, :]))
        pinhole_2d_points = (image_param["intrinsics"]["matrix"] @ pinhole_3d_points)[:2, :]

        # just use  the available 3D point, in +z and have corresponding 2D image points
        available_pixels_list = np.logical_and.reduce((
            pinhole_3d_points_original[2, :] > 0,
            pinhole_2d_points[0, :] >= 0, pinhole_2d_points[0, :] < pinhole_image_width,
            pinhole_2d_points[1, :] >= 0, pinhole_2d_points[1, :] < pinhole_image_height))

        available_pixels_list_mat = available_pixels_list.reshape((fisheye_image_height, fisheye_image_width))
        fisheye_depth[:, :][available_pixels_list_mat] += ndimage.map_coordinates(
            depth_data[:, :],
            [pinhole_2d_points[1, :][available_pixels_list], pinhole_2d_points[0, :][available_pixels_list]],
            order=1, mode='constant', cval=0.0)

        fisheye_image_weight[available_pixels_list_mat] += 1

    # get pixels mean
    available_weight_list = fisheye_image_weight != 0
    fisheye_depth[:, :][available_weight_list] = np.divide(fisheye_depth[:, :][available_weight_list], fisheye_image_weight[available_weight_list])

    return fisheye_depth


def find_corresponding(src_image, src_param, tar_image, tar_param, fisheye_model):
    """Get two images' pixels corresponding relationship.

    Input image size is [height, width, 2].

    :param src_image: source image data array
    :type src_image: numpy
    :param src_param: source image's intrinsic and extrinsic parameters
    :type src_param: dict
    :param tar_image:  target image data array
    :type tar_image: numpy
    :param tar_param: target image's intrinsic and extrinsic parameters
    :type tar_param: dict
    :param fisheye_model: fisheye model.
    :type fisheye_model: dict
    :return: dict record the pixels relationship,[points_number, 4], [src_y, src_x, tar_y, tar_x]
    :rtype: dict
    """
    #
    pixel_matching = []  # recording the pixel corresponding [4, pixels_number]
    src_image_height = src_image.shape[0]
    src_image_width = src_image.shape[1]
    tar_image_height = tar_image.shape[0]
    tar_image_width = tar_image.shape[1]
    if src_image_height != tar_image_height or tar_image_width != src_image_width:
        log.error("the source image size is not same as the target image size.")

    # 1) project source image pinhole image to fisheye image
    src_image_u_list = np.linspace(0, src_image_width, src_image_width, endpoint=False)  # x
    src_image_v_list = np.linspace(0, src_image_height, src_image_height, endpoint=False)  # y
    src_image_grid_u, src_image_grid_v = np.meshgrid(src_image_u_list, src_image_v_list)
    src_image_grid_z = np.ones(src_image_grid_u.shape, np.float64)
    src_image_2d_points = np.stack((src_image_grid_u.ravel(), src_image_grid_v.ravel(), src_image_grid_z.ravel()), axis=1)

    # project the pinhole image to world coords (spherical 3D points)
    src_image_3d_points = np.linalg.inv(src_param['intrinsics']['matrix']) @ src_image_2d_points.T
    src_image_3d_points_world = np.linalg.inv(src_param['rotation']) @ (src_image_3d_points - src_param['translation'][:, np.newaxis])
    fisheye_2d_points = cam_models.world2cam_slow(src_image_3d_points_world, fisheye_model).T

    # 2) from fisheye image to target image
    fisheye_3d_points = cam_models.cam2world(fisheye_2d_points, fisheye_model)
    # pinhole_3d_points = (np.divide(fisheye_3d_points, fisheye_3d_points[2, :]))
    tar_image_3d_points = tar_param['rotation'] @ fisheye_3d_points.T + tar_param['translation'][:, np.newaxis]

    # projection to pin-hole image
    tar_image_3d_points = (np.divide(tar_image_3d_points, tar_image_3d_points[2, :]))
    tar_image_2d_points = (tar_param["intrinsics"]["matrix"] @ tar_image_3d_points)[:2, :]

    # get the available pixels index
    available_pixels_list = np.logical_and.reduce((
        tar_image_2d_points[0, :] >= 0, tar_image_2d_points[0, :] <= tar_image_width - 1,
        tar_image_2d_points[1, :] >= 0, tar_image_2d_points[1, :] <= tar_image_height - 1))

    # 3) check the similarity of the corresponding pixels
    tar_image_2d_points = tar_image_2d_points.T
    src_image_2d_points_available = src_image_2d_points[available_pixels_list][:, :2]
    tar_image_2d_points_available = tar_image_2d_points[available_pixels_list]

    # x-y to y-x
    src_image_2d_points_available = src_image_2d_points_available[:, [1, 0]]
    tar_image_2d_points_available = tar_image_2d_points_available[:, [1, 0]]

    if src_image_2d_points_available.shape[0] == 0:
        log.debug("the do not have overlap between two images.")
    else:
        src_image_avail_pixel_data = src_image[src_image_2d_points_available.astype(np.int32)]
        tar_image_avail_pixel_data = tar_image[tar_image_2d_points_available.astype(np.int32)]
        rms = np.sqrt(np.mean((src_image_avail_pixel_data - tar_image_avail_pixel_data) ** 2))
        log.debug("The corresponding pixel rms is {}".format(rms))

    # 4) save to numpy array
    # src_image_2d_points_available_np = np.array(src_image_2d_points_available)
    # tar_image_2d_points_available_np = np.array(tar_image_2d_points_available)
    pixel_matching = np.hstack((src_image_2d_points_available, tar_image_2d_points_available))
    return pixel_matching



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/depthmap_align.py
================================================
import cv2
from utility import subimage
from utility import depthmap_utils
from utility import serialization
from utility import image_io

from skimage.transform import pyramid_gaussian
import numpy as np

import os
import pickle

from logger import Logger
log = Logger(__name__)
log.logger.propagate = False

try:
    from instaOmniDepth import depthmapAlign
    log.info("depthmapAlign python module installed!")
except ModuleNotFoundError:
    log.error("depthmapAlign python module do not install, please build and install it reference the readme.")


class DepthmapAlign:
    """The class wrap cpp module.

    The cpp python module function declaration:
    1. `depthmap_stitch` align subimage's depth maps together, parameters list:
        str: root_dir,
        str: method,
        list: terms_weight,
        list: depthmap_original_list,
        list: depthmap_original_ico_index,
        int: reference_depthmap_index, 
        list: pixels_corresponding_map,
        int: align_coeff_grid_height,
        int: align_coeff_grid_width,
        list: align_coeff_initial_scale,
        list: align_coeff_initial_offset.

    """

    def __init__(self):
        # sub-image number
        self.depthmap_number = -1
        self.output_dir = ""  # output alignment coefficient
        self.depthmap_aligned = None

        # output debug information filepath exp
        self.debug = False
        # if output path is None do not output the data to files
        self.subimage_pixelcorr_filepath_expression = None
        self.subimage_depthmap_aligning_filepath_expression = None  # save the normalized depth map for each resolution
        self.subimage_warpedimage_filepath_expression = None
        self.subimage_warpeddepth_filename_expression = None
        self.subimage_alignment_intermedia_filepath_expression = None   # pickle file
        self.subimages_rgb = None  # the original 20 rgb subimages, if is not warp depth map

        # align grid
        self.align_coeff_grid_width = 5           # the width of the initial grid
        self.align_coeff_grid_height = 8          # the height of the initial grid
        self.align_coeff_grid_width_finest = 10   # the grid width of the finest grid
        self.align_coeff_grid_height_finest = 16  # the grid height of the finest grid.
        self.align_coeff_initial_scale_list = []
        self.align_coeff_initial_offset_list = []
        self.depthmap_original_ico_index = []      # the subimage's depth map ico face index

        # ceres options
        self.ceres_thread_number = 12
        self.ceres_max_num_iterations = 25
        self.ceres_max_linear_solver_iterations = 10
        self.ceres_min_linear_solver_iterations = -1

        # align parameter
        self.align_method = "group"
        self.weight_project = 1.0
        self.weight_smooth = 0.1
        self.weight_scale = 0.0
        self.depthmap_norm_mothod = "midas"
        self.coeff_fixed_face_index = 7

        # multi-resolution parameters
        self.multi_res_grid = False
        self.pyramid_layer_number = 1
        self.pyramid_downscale = 2

        # pixel correpsonding down-sample parameter
        self.downsample_pixelcorr_ratio = 0.4

        # initial the align process (depthmapAlign) run time
        depthmapAlign.init(self.align_method)
        # clear the alignment run time, call depthmapAlign.shutdown when the interpreter exits
        import atexit
        atexit.register(depthmapAlign.shutdown)

        # the global configuration
        self.opt = None

    def align_coeff_init(self):
        """
        Create & initial subimages alignment coefficient.
        """
        for ico_face_index in self.depthmap_original_ico_index:
            align_coeff_initial_scale = np.full((self.align_coeff_grid_height, self.align_coeff_grid_width), 1.0, np.float64)
            align_coeff_initial_offset = np.full((self.align_coeff_grid_height, self.align_coeff_grid_width), 0.0, np.float64)
            self.align_coeff_initial_scale_list.append(align_coeff_initial_scale)
            self.align_coeff_initial_offset_list.append(align_coeff_initial_offset)

    def report_cost(self, depthmap_list, pixel_corr_list):
        """ Report the cost of depth map alignment.

        :param depthmap_list: the depth map lists
        :type depthmap_list: list
        :param pixel_corr_list: the pixel corresponding relationship between two subimage.
        :type pixel_corr_list: dict
        """
        diff_sum = 0
        pixel_numb = 0
        # the cost of projection term
        for src_idx in range(0,20):
            for tar_idx in range(0,20):
                if src_idx == tar_idx:
                    continue

                pixel_corr = pixel_corr_list[src_idx][tar_idx]
                if pixel_corr.size == 0:
                    continue

                src_depthmap = depthmap_list[src_idx]
                tar_depthmap = depthmap_list[tar_idx]

                src_y = pixel_corr[:,0]
                src_x = pixel_corr[:,1]
                tar_y = pixel_corr[:,2]
                tar_x = pixel_corr[:,3]

                from scipy import ndimage
                src_depthmap_points = ndimage.map_coordinates(src_depthmap, [src_y, src_x], order=1, mode='constant', cval=0)
                tar_depthmap_points = ndimage.map_coordinates(tar_depthmap, [tar_y, tar_x], order=1, mode='constant', cval=0)

                diff = src_depthmap_points - tar_depthmap_points
                diff_sum += np.sum(diff * diff)
                pixel_numb += pixel_corr.shape[0]

        print("Re-projection cost is {}, per pixel is {}".format(diff_sum, diff_sum / pixel_numb))

        # the cost of smooth term
        # the cost of scale term

    def align_single_res(self, depthmap_original_list, pixels_corresponding_list):
        """
        Align the sub-images depth map in single layer.

        :param depthmap_original_list: the not alignment depth maps.
        :type depthmap_original_list: list
        :param pixels_corresponding_list: the pixels corresponding relationship.
        :type pixels_corresponding_list: 
        """
        if self.align_method not in ["group", "enum"]:
            log.error("The depth map alignment method {} specify error! ".format(self.align_method))

        # report the alignment information:
        if False:
            # 0) get how many pixel corresponding relationship
            pixels_corr_number = 0
            info_str = ""
            for src_key, _ in enumerate(pixels_corresponding_list):
                # print("source image {}:".format(src_key))
                info_str = info_str + "\nsource image {}:\n".format(src_key)
                for tar_key, _ in enumerate(pixels_corresponding_list[src_key]):
                    # print("\t Target image {} , pixels corr number is {}".format(tar_key, pixels_corresponding_list[src_key][tar_key].size))
                    info_str = info_str + "{}:{}  ".format(tar_key, pixels_corresponding_list[src_key][tar_key].size)
                    pixels_corr_number = pixels_corr_number + pixels_corresponding_list[src_key][tar_key].size
            print(info_str)
            print("The total pixel corresponding is {}".format(pixels_corr_number))

            # 1) get the cost
            self.report_cost(depthmap_original_list, pixels_corresponding_list)

        try:
            # set Ceres solver options
            ceres_setting_result = depthmapAlign.ceres_solver_option(self.ceres_thread_number,  self.ceres_max_num_iterations,
                                                                     self.ceres_max_linear_solver_iterations, self.ceres_min_linear_solver_iterations)

            if ceres_setting_result < 0:
                log.error("Ceres solver option setting error.")

            # align depth maps
            cpp_module_debug_flag = 1 if self.debug else 0

            # align the subimage's depth maps
            self.depthmap_aligned, align_coeff = depthmapAlign.depthmap_stitch(
                self.output_dir,
                [self.weight_project, self.weight_smooth, self.weight_scale],
                depthmap_original_list,
                self.depthmap_original_ico_index,
                self.coeff_fixed_face_index,
                pixels_corresponding_list,
                self.align_coeff_grid_height,
                self.align_coeff_grid_width,
                True,
                True,
                self.align_coeff_initial_scale_list,
                self.align_coeff_initial_offset_list,
                False)

            ## report the error between the aligned depth maps
            # depthmapAlign.report_aligned_depthmap_error()

        except RuntimeError as error:
            log.error('Error: ' + repr(error))

        # update the coeff
        for index in range(0, self.depthmap_number):
            assert self.align_coeff_initial_scale_list[index].shape == align_coeff[index * 2].shape
            assert self.align_coeff_initial_offset_list[index].shape == align_coeff[index * 2 + 1].shape
            self.align_coeff_initial_scale_list[index] = align_coeff[index * 2]
            self.align_coeff_initial_offset_list[index] = align_coeff[index * 2 + 1]

    def align_multi_res(self, erp_rgb_image_data, subimage_depthmap, padding_size, depthmap_original_ico_index=None):
        """
        Align the sub-images depth map in multi-resolution.

        :param erp_rgb_image_data: the erp image used to compute the pixel corresponding relationship.
        :type erp_rgb_image_data: numpy
        :param subimage_depthmap: The sub-images depth map, generated by MiDaS.
        :type subimage_depthmap: list[numpy]
        :param padding_size: the padding size
        :type padding_size: float
        :param subsample_corr_factor: the pixel corresponding subimage factor
        :type subsample_corr_factor: float
        :param depthmap_original_ico_index: the subimage depth map's index.
        :type depthmap_original_ico_index: list
        :return: aligned depth map and coefficient.
        :rtype: tuple
        """
        self.depthmap_number = len(subimage_depthmap)

        if depthmap_original_ico_index is None and len(subimage_depthmap) == 20:
            self.depthmap_original_ico_index = list(range(0, 20))
        elif depthmap_original_ico_index is not None and len(depthmap_original_ico_index) == len(subimage_depthmap):
            self.depthmap_original_ico_index = depthmap_original_ico_index
        else:
            log.error("Do not set the ico face index.")

        # normalize the data
        log.debug("Normalization the depth map with {} norm method".format(self.depthmap_norm_mothod))
        subimage_depthmap_norm_list = []
        for depthmap in subimage_depthmap:
            subimage_depthmap_norm = depthmap_utils.dispmap_normalize(depthmap, self.depthmap_norm_mothod)
            subimage_depthmap_norm_list.append(subimage_depthmap_norm)

        # 0) generate the gaussion pyramid of each sub-image depth map
        # the 1st list is lowest resolution
        if self.multi_res_grid:
            depthmap_pryamid = [subimage_depthmap_norm_list] * self.pyramid_layer_number
            # pyramid_grid = [[4, 3], [8, 7], [16, 14]]   # Values reported in paper
            pyramid_grid = [[self.align_coeff_grid_width*(2**level),
                             self.align_coeff_grid_height*(2**level)] for
                            level in range(0, self.pyramid_layer_number)]
        else:
            depthmap_pryamid = depthmap_utils.depthmap_pyramid(subimage_depthmap_norm_list, self.pyramid_layer_number, self.pyramid_downscale)

        # 1) multi-resolution to compute the alignment coefficient
        subimage_cam_param_list = None
        for pyramid_layer_index in range(0, self.pyramid_layer_number):
            if pyramid_layer_index == 0:
                if self.multi_res_grid:
                    self.align_coeff_grid_width = pyramid_grid[pyramid_layer_index][0]
                    self.align_coeff_grid_height = pyramid_grid[pyramid_layer_index][1]
                self.align_coeff_init()

            log.info("Aligen the depth map in resolution {}".format(depthmap_pryamid[pyramid_layer_index][0].shape))
            tangent_image_width = depthmap_pryamid[pyramid_layer_index][0].shape[1]

            pixel_corr_list = None
            subimage_cam_param_list = None
            # load the corresponding relationship from file
            if self.debug:
                tangent_image_height = depthmap_pryamid[pyramid_layer_index][0].shape[0]
                image_size_str = "{}x{}".format(tangent_image_height, tangent_image_width)

                # load depthmap and relationship from pickle for fast debug
                if self.subimage_alignment_intermedia_filepath_expression is not None:
                    pickle_file_path = self.subimage_alignment_intermedia_filepath_expression.format(image_size_str)
                    if os.path.exists(pickle_file_path) and os.path.getsize(pickle_file_path) > 0:
                        log.warn("Load depthmap alignment data from {}".format(pickle_file_path))
                        alignment_data = None
                        with open(pickle_file_path, 'rb') as file:
                            alignment_data = pickle.load(file)

                        subimage_depthmap = alignment_data["subimage_depthmap"]
                        depthmap_original_ico_index = alignment_data["depthmap_original_ico_index"]
                        pixel_corr_list = alignment_data["pixel_corr_list"]
                        subimage_cam_param_list = alignment_data["subimage_cam_param_list"]

            # 1-0) get subimage the pixel corresponding relationship
            if pixel_corr_list is None or subimage_cam_param_list is None:
                _, subimage_cam_param_list, pixel_corr_list = \
                    subimage.erp_ico_proj(erp_rgb_image_data, padding_size, tangent_image_width, self.downsample_pixelcorr_ratio, self.opt)

            # save intermedia data for debug output pixel corresponding relationship and warped source image
            if self.debug:
                tangent_image_height = depthmap_pryamid[pyramid_layer_index][0].shape[0]
                image_size_str = "{}x{}".format(tangent_image_height, tangent_image_width)

                # save depth map and relationship and etc. to pickle for debug
                if self.subimage_alignment_intermedia_filepath_expression is not None:
                    pickle_file_path = self.subimage_alignment_intermedia_filepath_expression.format(image_size_str)
                    with open(pickle_file_path, 'wb') as file:
                        pickle.dump({"subimage_depthmap": subimage_depthmap,
                                     "depthmap_original_ico_index": depthmap_original_ico_index,
                                     "pixel_corr_list": pixel_corr_list,
                                     "subimage_cam_param_list": subimage_cam_param_list}, file)
                        log.warn("Save depth map alignment data to {}".format(pickle_file_path))

                # output the all subimages depth map corresponding relationship to json
                if self.subimage_pixelcorr_filepath_expression is not None:
                    log.debug("output the all subimages corresponding relationship to {}".format(self.subimage_pixelcorr_filepath_expression))
                    for subimage_index_src in range(0, 20):
                        for subimage_index_tar in range(0, 20):
                            if subimage_index_src == subimage_index_tar:
                                continue

                            pixel_corresponding = pixel_corr_list[subimage_index_src][subimage_index_tar]
                            json_file_path = self.subimage_pixelcorr_filepath_expression \
                                .format(subimage_index_src, subimage_index_tar, image_size_str)
                            serialization.pixel_corresponding_save(
                                json_file_path, str(subimage_index_src), None,
                                str(subimage_index_tar), None, pixel_corresponding)

                # draw the corresponding relationship in available subimage rgb images
                if self.subimage_warpedimage_filepath_expression is not None and self.subimages_rgb is not None:
                    log.debug("draw the corresponding relationship in subimage rgb and output to {}".format(self.subimage_warpedimage_filepath_expression))
                    for index_src in range(len(depthmap_original_ico_index)):
                        for index_tar in range(len(depthmap_original_ico_index)):
                            # draw relationship in rgb images
                            face_index_src = depthmap_original_ico_index[index_src]
                            face_index_tar = depthmap_original_ico_index[index_tar]
                            pixel_corresponding = pixel_corr_list[face_index_src][face_index_tar]
                            src_image_rgb = self.subimages_rgb[face_index_src]
                            tar_image_rgb = self.subimages_rgb[face_index_tar]
                            _, _, src_warp = subimage.draw_corresponding(src_image_rgb, tar_image_rgb, pixel_corresponding)
                            warp_image_filepath = self.subimage_warpedimage_filepath_expression \
                                .format(face_index_src, face_index_tar, image_size_str)
                            image_io.image_save(src_warp, warp_image_filepath)

                # draw the corresponding relationship in available subimage depth maps
                if self.subimage_warpeddepth_filename_expression is not None:
                    log.debug("draw the corresponding relationship in subimage depth map and output to {}".format(self.subimage_warpeddepth_filename_expression))
                    for index_src in range(len(depthmap_original_ico_index)):
                        for index_tar in range(len(depthmap_original_ico_index)):
                            src_image_data = depthmap_pryamid[pyramid_layer_index][index_src]
                            tar_image_data = depthmap_pryamid[pyramid_layer_index][index_tar]
                            # visualize depth map
                            src_image_rgb = depthmap_utils.depth_visual(src_image_data)
                            tar_image_rgb = depthmap_utils.depth_visual(tar_image_data)
                            # draw relationship
                            face_index_src = depthmap_original_ico_index[index_src]
                            face_index_tar = depthmap_original_ico_index[index_tar]
                            pixel_corresponding = pixel_corr_list[face_index_src][face_index_tar]
                            _, _, src_warp = subimage.draw_corresponding(src_image_rgb, tar_image_rgb, pixel_corresponding)
                            warp_image_filepath = self.subimage_warpeddepth_filename_expression \
                                .format(face_index_src, face_index_tar, image_size_str)
                            image_io.image_save(src_warp, warp_image_filepath)

                # output input depth map of each subimage
                if self.subimage_depthmap_aligning_filepath_expression is not None:
                    log.debug("output subimage's depth map of multi-layers: layer {}".format(pyramid_layer_index))
                    for index in range(len(depthmap_original_ico_index)):
                        image_data = depthmap_pryamid[pyramid_layer_index][index]
                        face_index = depthmap_original_ico_index[index]
                        subimage_depthmap_filepath = self.subimage_depthmap_aligning_filepath_expression.format(face_index, image_size_str)
                        depthmap_utils.depth_visual_save(image_data, subimage_depthmap_filepath)

            # 1-1) align depth maps, to update align coeffs and subimages depth maps.
            if self.multi_res_grid:
                if self.depthmap_aligned is not None:
                    self.align_single_res(self.depthmap_aligned, pixel_corr_list)
                else:
                    self.align_single_res(depthmap_pryamid[pyramid_layer_index], pixel_corr_list)
            else:
                self.align_single_res(depthmap_pryamid[pyramid_layer_index], pixel_corr_list)

            if self.multi_res_grid:
                if pyramid_layer_index < self.pyramid_layer_number - 1:
                    self.align_coeff_grid_width = pyramid_grid[pyramid_layer_index + 1][0]
                    self.align_coeff_grid_height = pyramid_grid[pyramid_layer_index + 1][1]
                    for i in range(0, len(self.align_coeff_initial_scale_list)):
                        self.align_coeff_initial_scale_list[i] = cv2.resize(self.align_coeff_initial_scale_list[i],
                                                                            dsize=pyramid_grid[pyramid_layer_index + 1],
                                                                            interpolation=cv2.INTER_LINEAR)

                        self.align_coeff_initial_offset_list[i] = cv2.resize(self.align_coeff_initial_offset_list[i],
                                                                             dsize=pyramid_grid[pyramid_layer_index + 1],
                                                                             interpolation=cv2.INTER_LINEAR)

        # 2) return alignment coefficients and aligned depth maps
        return self.depthmap_aligned, \
               self.align_coeff_initial_scale_list, self.align_coeff_initial_offset_list, \
               subimage_cam_param_list



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/depthmap_utils.py
================================================
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from skimage.transform import pyramid_gaussian

from PIL import Image
import numpy as np

from struct import unpack
import os
import sys
import re
import gc

import fs_utility
from logger import Logger

log = Logger(__name__)
log.logger.propagate = False


def fill_ico_subimage(depth_data_list_, subimage_idx_list):
    """ replace missed subimage with zero matrix.
    """
    depth_data_list = [np.zeros_like(depth_data_list_[0])] * 20
    for subimage_index in range(len(subimage_idx_list)):
        subimage_face_idx = subimage_idx_list[subimage_index]
        depth_data_list[subimage_face_idx] = depth_data_list_[subimage_index]
    return depth_data_list


def depth_ico_visual_save(depth_data_list_, output_path, subimage_idx_list=None):
    """save the visualized depth map array to image file with value-bar.

    :param dapthe_data: The depth data.
    :type dapthe_data: numpy
    :param output_path: the absolute path of output image.
    :type output_path: str
    :param subimage_idx_list: available subimages index list.
    :type subimage_idx_list: list
    """
    # get vmin and vmax
    # for dispmap in depth_data_list:
    #     if vmin_ > np.amin(dispmap):
    #         vmin_ = np.amin(dispmap)
    #     if vmax_ < np.amax(dispmap):
    #         vmax_ = np.amax(dispmap)
    vmin_ = 0
    vmax_ = 0
    dispmap_array = np.concatenate(depth_data_list_).flatten()
    vmin_idx = int(dispmap_array.size * 0.05)
    vmax_idx = int(dispmap_array.size * 0.95)
    vmin_ = np.partition(dispmap_array, vmin_idx)[vmin_idx]
    vmax_ = np.partition(dispmap_array, vmax_idx)[vmax_idx]

    # add blank image to miss subimage to fill the sub-image array
    depth_data_list = None
    if len(depth_data_list_) != 20 \
            and subimage_idx_list is not None \
            and len(depth_data_list_) == len(subimage_idx_list):
        log.debug("The ico's sub-image size is {}, fill blank sub-images.".format(len(depth_data_list_)))
        # depth_data_list = [np.zeros_like(depth_data_list_[0])] * 20
        # for subimage_index in range(len(subimage_idx_list)):
        #     subimage_face_idx = subimage_idx_list[subimage_index]
        #     depth_data_list[subimage_face_idx] = depth_data_list_[subimage_index]
        depth_data_list = fill_ico_subimage(depth_data_list_, subimage_idx_list)
    elif len(depth_data_list_) == 20:
        depth_data_list = depth_data_list_
    else:
        raise log.error("The sub-image is not completed.")

    # draw image
    figure, axes = plt.subplots(4, 5)
    counter = 0
    for row_index in range(0, 4):
        for col_index in range(0, 5):
            axes[row_index, col_index].get_xaxis().set_visible(False)
            axes[row_index, col_index].get_yaxis().set_visible(False)
            # add sub caption
            axes[row_index, col_index].set_title(str(counter))
            counter = counter + 1
            #
            dispmap_index = row_index * 5 + col_index
            im = axes[row_index, col_index].imshow(depth_data_list[dispmap_index],
                                                   cmap=cm.jet, vmin=vmin_, vmax=vmax_)

    figure.tight_layout()
    plt.colorbar(im, ax=axes.ravel().tolist())
    plt.savefig(output_path, dpi=150)
    # plt.show()
    plt.close(figure)

def depth_visual_save(depth_data, output_path, overwrite=True):
    """save the visualized depth map to image file with value-bar.

    :param dapthe_data: The depth data.
    :type dapthe_data: numpy
    :param output_path: the absolute path of output image.
    :type output_path: str
    """
    depth_data_temp = depth_data.astype(np.float64)

    if fs_utility.exist(output_path, 1) and overwrite:
        log.warn("{} exist.".format(output_path))
        fs_utility.file_rm(output_path)

    # draw image
    fig = plt.figure()
    plt.subplots_adjust(left=0, bottom=0, right=0.1, top=0.1, wspace=None, hspace=None)
    ax = fig.add_subplot(111)
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    fig.tight_layout()
    im = ax.imshow(depth_data_temp, cmap="turbo")
    cbar = ax.figure.colorbar(im, ax=ax)
    plt.savefig(output_path, dpi=150)
    # plt.imsave(output_path, dapthe_data_temp, cmap="turbo")
    plt.close(fig)


def depth_visual(depth_data):
    """
    visualize the depth map
    """
    min = np.min(depth_data)
    max = np.max(depth_data)
    norm = mpl.colors.Normalize(vmin=min, vmax=max)
    cmap = plt.get_cmap('jet')

    m = cm.ScalarMappable(norm=norm, cmap=cmap)
    return (m.to_rgba(depth_data)[:, :, :3] * 255).astype(np.uint8)


def rgb2dispmap(image_filepath, pytorch_hub=True):
    """
    Estimate dispmap from rgb image.

    :param image_filepath: the rgb image filepath
    :type image_filepath: str
    :param pytorch_hub: which module should use, defaults to True
    :type pytorch_hub: bool, optional
    :return: MiDaS estimated dispmap
    :rtype: numpy
    """
    depthmap_data = None
    if pytorch_hub:
        log.debug("use PyTorch Hub MiDaS.")
        depthmap_data = MiDaS_torch_hub_file(image_filepath)
    else:
        log.debug("use local MiDaS.")
        # add local MiDas to python path
        dir_scripts = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        sys.path.append(os.path.join(dir_scripts, "../../MiDaS/"))
        from MiDaS import MiDaS_utils
        from MiDaS.monodepth_net import MonoDepthNet
        from MiDaS.run import run_depth

        image_data = np.asarray(Image.open(image_filepath)[..., :3])
        image_data = image_data[np.newaxis, :, :, [2, 0, 1]]
        MiDaS_module_filepath = dir_scripts + '../../MiDas/model.pt'
        if os.path.exists(MiDaS_module_filepath):
            log.error("MiDaS local module {} does not exist.".format(MiDaS_module_filepath))

        depthmap_data = run_depth(image_data, MiDaS_module_filepath, MonoDepthNet, MiDaS_utils)[0]

    return depthmap_data


def run_persp_monodepth(rgb_image_data_list, persp_monodepth, use_large_model=True):
    if (persp_monodepth == "midas2") or (persp_monodepth == "midas3"):
        return MiDaS_torch_hub_data(rgb_image_data_list, persp_monodepth, use_large_model=use_large_model)
    if persp_monodepth == "boost":
        return boosting_monodepth(rgb_image_data_list)
    if persp_monodepth == "depthanything":
        return DepthAnything(rgb_image_data_list)
    if persp_monodepth == "depthanythingv2":
        return DepthAnythingV2(rgb_image_data_list)


def DepthAnythingV2(rgb_image_data_list):
    import os
    import cv2
    from transformers import AutoImageProcessor, AutoModelForDepthEstimation
    import torch
    import numpy as np

    print("using depth anything v2")

    disparity_map_list = []

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    # use hugging face library transformers to import large ver
    image_processor = AutoImageProcessor.from_pretrained("pcuenq/Depth-Anything-V2-Large-hf")
    model = AutoModelForDepthEstimation.from_pretrained("pcuenq/Depth-Anything-V2-Large-hf")
    model = model.to(device)
    model.eval()

    for index, image in enumerate(rgb_image_data_list):
        # prepare image for the model
        inputs = image_processor(images=image, return_tensors="pt")
        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            predicted_depth = outputs.predicted_depth

        # interpolate to original size
        prediction = torch.nn.functional.interpolate(
            predicted_depth.unsqueeze(1),
            size=image.shape[:2], # is the index correct?
            mode="bicubic",
            align_corners=False,
        ).squeeze().cpu().numpy()

        disparity_map_list.append(prediction)

        del prediction
        torch.cuda.empty_cache()

        if index % 10 == 0:
            print(f"DepthAnythingV2 estimated {index} rgb image's disparity map.")

    return disparity_map_list


def DepthAnything(rgb_image_data_list):
    import os
    # print(os.getcwd())
    # os.environ['TRANSFORMERS_CACHE'] = '/data/bshowell/.cache/huggingface/hub'
    from transformers import AutoImageProcessor, AutoModelForDepthEstimation
    import torch
    import numpy as np

    disparity_map_list = []

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    image_processor = AutoImageProcessor.from_pretrained("LiheYoung/depth-anything-large-hf")
    model = AutoModelForDepthEstimation.from_pretrained("LiheYoung/depth-anything-large-hf")
    model = model.to(device)
    model.eval()

    for index, image in enumerate(rgb_image_data_list):

        # prepare image for the model
        inputs = image_processor(images=image, return_tensors="pt")
        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            predicted_depth = outputs.predicted_depth

        # interpolate to original size
        prediction = torch.nn.functional.interpolate(
            predicted_depth.unsqueeze(1),
            size=image.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()

        # visualize the prediction
        output = prediction.cpu().numpy()
        disparity_map_list.append(output)

        # # Debugging: Outputs each individual subimage and its calculated depthmap
        # formatted = (output * 255 / np.max(output)).astype("uint8")
        # depth = Image.fromarray(output)
        # depth.save(f"/monodepth/results/temp/{index}_depth.png")

        # formatted = (image / np.max(output)).astype("uint8")
        # img = Image.fromarray(image)
        # img.save(f"/monodepth/results/temp/{index}_image.png")


        del output
        del prediction
        torch.cuda.empty_cache()

        if index % 10 == 0:
            print(f"DepthAnything estimated {index} rgb image's disparity map.")

    return disparity_map_list


def MiDaS_torch_hub_data(rgb_image_data_list, persp_monodepth, use_large_model=True):
    """Estimation the single RGB image's depth with MiDaS downloading from Torch Hub.
    reference: https://pytorch.org/hub/intelisl_midas_v2/

    :param rgb_image_path: the RGB image file path.
    :type rgb_image_path: str
    :param use_large_model: the MiDaS model type.
    :type use_large_model: bool, optional
    """
    import torch

    # 1)initial PyTorch run-time environment
    if use_large_model:
        if persp_monodepth == "midas2":
            midas = torch.hub.load("intel-isl/MiDaS", "MiDaS")
        if persp_monodepth == "midas3":
            midas = torch.hub.load("intel-isl/MiDaS", "DPT_Large")
    else:
        if persp_monodepth == "midas2":
            midas = torch.hub.load("intel-isl/MiDaS", "MiDaS_small")
        if persp_monodepth == "midas3":
            midas = torch.hub.load("intel-isl/MiDaS", "DPT_Hybrid")

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    midas.to(device)
    midas.eval()

    midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
    if use_large_model:
        transform = midas_transforms.default_transform
    else:
        transform = midas_transforms.small_transform

    disparity_map_list = []

    for index in range(0, len(rgb_image_data_list)):
        img = rgb_image_data_list[index]
        input_batch = transform(img).to(device)
        with torch.no_grad():
            prediction = midas(input_batch)

            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),
                size=img.shape[:2],
                mode="bicubic",
                align_corners=False,
            ).squeeze()

        output = prediction.cpu().numpy()
        disparity_map_list.append(output)
        del output
        del input_batch
        del prediction
        torch.cuda.empty_cache()

        if index % 10 ==0:
            log.debug("MiDaS estimate {} rgb image's disparity map.".format(index))

    del midas
    gc.collect()
    torch.cuda.empty_cache()
    return disparity_map_list



def MiDaS_torch_hub_file(rgb_image_path, use_large_model=True):
    """Estimation the single RGB image's depth with MiDaS downloading from Torch Hub.
    reference: https://pytorch.org/hub/intelisl_midas_v2/

    :param rgb_image_path: the RGB image file path.
    :type rgb_image_path: str
    :param use_large_model: the MiDaS model type.
    :type use_large_model: bool, optional
    """
    import cv2
    import torch
    # import urllib.request

    # import matplotlib.pyplot as plt

    # url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
    # urllib.request.urlretrieve(url, filename)
    # use_large_model = True

    if use_large_model:
        midas = torch.hub.load("intel-isl/MiDaS", "MiDaS")
    else:
        midas = torch.hub.load("intel-isl/MiDaS", "MiDaS_small")

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    midas.to(device)
    midas.eval()

    midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")

    if use_large_model:
        transform = midas_transforms.default_transform
    else:
        transform = midas_transforms.small_transform

    img = cv2.imread(rgb_image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    input_batch = transform(img).to(device)

    with torch.no_grad():
        prediction = midas(input_batch)

        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=img.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()

    output = prediction.cpu().numpy()
    # plt.imshow(output)
    # plt.show()
    return output


def boosting_monodepth(rgb_image_data_list):
    # Load merge network
    import cv2
    import argparse
    import torch
    import warnings
    warnings.simplefilter('ignore', np.RankWarning)

    class Object(object):
        pass

    # Settings from the official repo
    option = Object()
    option.R0 = False
    option.R20 = False
    option.Final = True
    option.output_resolution = 1
    option.pix2pixsize = 1024
    option.depthNet = 0
    option.max_res = 2000

    currfile_dir = os.path.dirname(__file__)
    boost_path = f"{os.path.join(currfile_dir, os.pardir, os.pardir, os.pardir, os.pardir, 'BoostingMonocularDepth')}"
    sys.path.append(os.path.abspath(boost_path))
    sys.path.append(os.path.abspath(os.path.dirname(boost_path)))

    # This import fixes relative imports in subfiles within BoostingMonocularDepth project
    sys.path.append(os.path.abspath(os.path.join(boost_path, "structuredrl", "models", "syncbn")))

    import BoostingMonocularDepth.run

    # OUR
    from BoostingMonocularDepth.utils import ImageandPatchs, generatemask, calculateprocessingres

    # MIDAS
    import BoostingMonocularDepth.midas.utils
    from BoostingMonocularDepth.midas.models.midas_net import MidasNet

    # PIX2PIX : MERGE NET
    from BoostingMonocularDepth.pix2pix.options.test_options import TestOptions
    from BoostingMonocularDepth.pix2pix.models.pix2pix4depth_model import Pix2Pix4DepthModel

    # select device
    device = torch.device("cuda")
    print("device: %s" % device)

    whole_size_threshold = 3000  # R_max from the paper
    GPU_threshold = 1600 - 32  # Limit for the GPU (NVIDIA RTX 2080), can be adjusted

    # Handle pix2pix parser
    opt = TestOptions()
    parser_pix2pix = argparse.ArgumentParser()
    parser_pix2pix = opt.initialize(parser_pix2pix)
    # Remove arguments causing conflict with main arguments
    parser_pix2pix.__dict__['_option_string_actions'].pop('--dataroot')
    parser_pix2pix.__dict__['_option_string_actions'].pop('--dataset_mode')
    parser_pix2pix.__dict__['_option_string_actions'].pop('--data_dir')
    opt = parser_pix2pix.parse_known_args()[0]
    opt.isTrain = False
    opt.gpu_ids = [0]

    BoostingMonocularDepth.run.pix2pixmodel = Pix2Pix4DepthModel(opt)
    BoostingMonocularDepth.run.pix2pixmodel.save_dir = os.path.join(boost_path, "pix2pix", "checkpoints", "mergemodel")
    BoostingMonocularDepth.run.pix2pixmodel.load_networks('latest')
    BoostingMonocularDepth.run.pix2pixmodel.eval()

    # Decide which depth estimation network to load
    if option.depthNet == 0:
        option.net_receptive_field_size = 384
        option.patch_netsize = 2 * option.net_receptive_field_size
        midas_model_path = os.path.join(boost_path, "midas", "model.pt")
        BoostingMonocularDepth.run.midasmodel = MidasNet(midas_model_path, non_negative=True)
        BoostingMonocularDepth.run.midasmodel.to(device)
        BoostingMonocularDepth.run.midasmodel.eval()

    # Generate mask used to smoothly blend the local pathc estimations to the base estimate.
    # It is arbitrarily large to avoid artifacts during rescaling for each crop.
    mask_org = generatemask((3000, 3000))
    mask = mask_org.copy()

    # Value x of R_x defined in the section 5 of the main paper.
    r_threshold_value = 0.2
    if option.R0:
        r_threshold_value = 0
    elif option.R20:
        r_threshold_value = 0.2

    # Go through all images
    depthmaps = []
    print("start processing")
    for image_ind, image in enumerate(rgb_image_data_list):
        print('processing image', image_ind)

        # Load image from dataset
        img = image
        input_resolution = img.shape

        scale_threshold = 3  # Allows up-scaling with a scale up to 3

        # Find the best input resolution R-x. The resolution search described in section 5-double estimation of the main paper and section B of the
        # supplementary material.
        whole_image_optimal_size, patch_scale = calculateprocessingres(img, option.net_receptive_field_size,
                                                                       r_threshold_value, scale_threshold,
                                                                       whole_size_threshold)

        print('\t wholeImage being processed in :', whole_image_optimal_size)

        # Generate the base estimate using the double estimation.
        whole_estimate = BoostingMonocularDepth.run.doubleestimate(img, option.net_receptive_field_size,
                                                                   whole_image_optimal_size, option.pix2pixsize,
                                                                   option.depthNet)

        # Compute the multiplier described in section 6 of the main paper to make sure our initial patch can select
        # small high-density regions of the image.
        BoostingMonocularDepth.run.factor = max(min(1, 4 * patch_scale * whole_image_optimal_size / whole_size_threshold), 0.2)
        factor = BoostingMonocularDepth.run.factor
        print('Adjust factor is:', 1 / factor)

        # Check if Local boosting is beneficial.
        if option.max_res < whole_image_optimal_size:
            print("No Local boosting. Specified Max Res is smaller than R20")
            continue

        # Compute the default target resolution.
        if img.shape[0] > img.shape[1]:
            a = 2 * whole_image_optimal_size
            b = round(2 * whole_image_optimal_size * img.shape[1] / img.shape[0])
        else:
            a = round(2 * whole_image_optimal_size * img.shape[0] / img.shape[1])
            b = 2 * whole_image_optimal_size
        b = int(round(b / factor))
        a = int(round(a / factor))

        # recompute a, b and saturate to max res.
        if max(a, b) > option.max_res:
            print('Default Res is higher than max-res: Reducing final resolution')
            if img.shape[0] > img.shape[1]:
                a = option.max_res
                b = round(option.max_res * img.shape[1] / img.shape[0])
            else:
                a = round(option.max_res * img.shape[0] / img.shape[1])
                b = option.max_res
            b = int(b)
            a = int(a)

        img = cv2.resize(img, (b, a), interpolation=cv2.INTER_CUBIC)

        # Extract selected patches for local refinement
        base_size = option.net_receptive_field_size * 2
        patchset = BoostingMonocularDepth.run.generatepatchs(img, base_size)

        print('Target resolution: ', img.shape)

        # Computing a scale in case user prompted to generate the results as the same resolution of the input.
        # Notice that our method output resolution is independent of the input resolution and this parameter will only
        # enable a scaling operation during the local patch merge implementation to generate results with the same resolution
        # as the input.
        if option.output_resolution == 1:
            mergein_scale = input_resolution[0] / img.shape[0]
            print('Dynamicly change merged-in resolution; scale:', mergein_scale)
        else:
            mergein_scale = 1

        imageandpatchs = ImageandPatchs(None, None, patchset, img, mergein_scale)
        whole_estimate_resized = cv2.resize(whole_estimate, (round(img.shape[1] * mergein_scale),
                                                             round(img.shape[0] * mergein_scale)),
                                            interpolation=cv2.INTER_CUBIC)
        imageandpatchs.set_base_estimate(whole_estimate_resized.copy())
        imageandpatchs.set_updated_estimate(whole_estimate_resized.copy())

        print('\t Resulted depthmap res will be :', whole_estimate_resized.shape[:2])
        print('patchs to process: ' + str(len(imageandpatchs)))

        # Enumerate through all patches, generate their estimations and refining the base estimate.
        for patch_ind in range(len(imageandpatchs)):

            # Get patch information
            patch = imageandpatchs[patch_ind]  # patch object
            patch_rgb = patch['patch_rgb']  # rgb patch
            patch_whole_estimate_base = patch['patch_whole_estimate_base']  # corresponding patch from base
            rect = patch['rect']  # patch size and location
            patch_id = patch['id']  # patch ID
            org_size = patch_whole_estimate_base.shape  # the original size from the unscaled input
            print('\t processing patch', patch_ind, '|', rect)

            # We apply double estimation for patches. The high resolution value is fixed to twice the receptive
            # field size of the network for patches to accelerate the process.
            patch_estimation = BoostingMonocularDepth.run.doubleestimate(patch_rgb, option.net_receptive_field_size,
                                                                         option.patch_netsize, option.pix2pixsize,
                                                                         option.depthNet)

            # Output patch estimation if required
            patch_estimation = cv2.resize(patch_estimation, (option.pix2pixsize, option.pix2pixsize),
                                          interpolation=cv2.INTER_CUBIC)

            patch_whole_estimate_base = cv2.resize(patch_whole_estimate_base, (option.pix2pixsize, option.pix2pixsize),
                                                   interpolation=cv2.INTER_CUBIC)

            # Merging the patch estimation into the base estimate using our merge network:
            # We feed the patch estimation and the same region from the updated base estimate to the merge network
            # to generate the target estimate for the corresponding region.
            BoostingMonocularDepth.run.pix2pixmodel.set_input(patch_whole_estimate_base, patch_estimation)

            # Run merging network
            BoostingMonocularDepth.run.pix2pixmodel.test()
            visuals = BoostingMonocularDepth.run.pix2pixmodel.get_current_visuals()

            prediction_mapped = visuals['fake_B']
            prediction_mapped = (prediction_mapped + 1) / 2
            prediction_mapped = prediction_mapped.squeeze().cpu().numpy()

            mapped = prediction_mapped

            # We use a simple linear polynomial to make sure the result of the merge network would match the values of
            # base estimate
            p_coef = np.polyfit(mapped.reshape(-1), patch_whole_estimate_base.reshape(-1), deg=1)
            merged = np.polyval(p_coef, mapped.reshape(-1)).reshape(mapped.shape)

            merged = cv2.resize(merged, (org_size[1], org_size[0]), interpolation=cv2.INTER_CUBIC)

            # Get patch size and location
            w1 = rect[0]
            h1 = rect[1]
            w2 = w1 + rect[2]
            h2 = h1 + rect[3]

            # To speed up the implementation, we only generate the Gaussian mask once with a sufficiently large size
            # and resize it to our needed size while merging the patches.
            if mask.shape != org_size:
                mask = cv2.resize(mask_org, (org_size[1], org_size[0]), interpolation=cv2.INTER_LINEAR)

            tobemergedto = imageandpatchs.estimation_updated_image

            # Update the whole estimation:
            # We use a simple Gaussian mask to blend the merged patch region with the base estimate to ensure seamless
            # blending at the boundaries of the patch region.
            tobemergedto[h1:h2, w1:w2] = np.multiply(tobemergedto[h1:h2, w1:w2], 1 - mask) + np.multiply(merged, mask)
            imageandpatchs.set_updated_estimate(tobemergedto)

        # Output the result
        output = cv2.resize(imageandpatchs.estimation_updated_image,
                            (input_resolution[1], input_resolution[0]),
                            interpolation=cv2.INTER_CUBIC)
        depthmaps.append(output)

    return depthmaps


def read_dpt(dpt_file_path):
    """read depth map from *.dpt file.

    :param dpt_file_path: the dpt file path
    :type dpt_file_path: str
    :return: depth map data
    :rtype: numpy
    """
    TAG_FLOAT = 202021.25  # check for this when READING the file

    ext = os.path.splitext(dpt_file_path)[1]

    assert len(ext) > 0, ('readFlowFile: extension required in fname %s' % dpt_file_path)
    assert ext == '.dpt', exit('readFlowFile: fname %s should have extension ''.flo''' % dpt_file_path)

    fid = None
    try:
        fid = open(dpt_file_path, 'rb')
    except IOError:
        print('readFlowFile: could not open %s', dpt_file_path)

    tag = unpack('f', fid.read(4))[0]
    width = unpack('i', fid.read(4))[0]
    height = unpack('i', fid.read(4))[0]

    assert tag == TAG_FLOAT, ('readFlowFile(%s): wrong tag (possibly due to big-endian machine?)' % dpt_file_path)
    assert 0 < width and width < 100000, ('readFlowFile(%s): illegal width %d' % (dpt_file_path, width))
    assert 0 < height and height < 100000, ('readFlowFile(%s): illegal height %d' % (dpt_file_path, height))

    # arrange into matrix form
    depth_data = np.fromfile(fid, np.float32)
    depth_data = depth_data.reshape(height, width)

    fid.close()

    return depth_data


def read_exr(exp_file_path):
    """Read depth map from EXR file

    :param exp_file_path: file path
    :type exp_file_path: str
    :return: depth map data
    :rtype: numpy
    """
    import array
    import OpenEXR
    import Imath

    # Open the input file
    file = OpenEXR.InputFile(exp_file_path)

    # Compute the size
    dw = file.header()['dataWindow']
    sz = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1)

    # Read the three color channels as 32-bit floats
    FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)
    (R, G, B) = [array.array('f', file.channel(Chan, FLOAT)).tolist() for Chan in ("R", "G", "B")]
    # R,G,B channel is same
    R_np = np.array(R).reshape((sz[1], sz[0]))

    return R_np



def read_pfm(path):
    """Read pfm file.

    :param path: the PFM file's path.
    :type path: str
    :return: the depth map array and scaler of depth
    :rtype: tuple: (data, scale)
    """
    with open(path, "rb") as file:

        color = None
        width = None
        height = None
        scale = None
        endian = None

        header = file.readline().rstrip()
        if header.decode("ascii") == "PF":
            color = True
        elif header.decode("ascii") == "Pf":
            color = False
        else:
            log.error("Not a PFM file: " + path)

        dim_match = re.match(r"^(\d+)\s(\d+)\s$", file.readline().decode("ascii"))
        if dim_match:
            width, height = list(map(int, dim_match.groups()))
        else:
            log.error("Malformed PFM header.")

        scale = float(file.readline().decode("ascii").rstrip())
        if scale < 0:
            # little-endian
            endian = "<"
            scale = -scale
        else:
            # big-endian
            endian = ">"

        data = np.fromfile(file, endian + "f")
        shape = (height, width, 3) if color else (height, width)

        data = np.reshape(data, shape)
        data = np.flipud(data)

        return data, scale


def write_pfm(path, image, scale=1):
    """Write depth data to pfm file.

    :param path: pfm file path
    :type path: str
    :param image: depth data
    :type image: numpy
    :param scale: Scale, defaults to 1
    :type scale: int, optional
    """
    if image.dtype.name != "float32":
        #raise Exception("Image dtype must be float32.")
        log.warn("The depth map data is {}, convert to float32 and save to pfm format.".format(image.dtype.name))
        image_ = image.astype(np.float32)
    else :
        image_ = image

    image_ = np.flipud(image_)

    color = None
    if len(image_.shape) == 3 and image_.shape[2] == 3:  # color image
        color = True
    elif len(image_.shape) == 2 or len(image_.shape) == 3 and image_.shape[2] == 1:  # greyscale
        color = False
    else:
        log.error("Image must have H x W x 3, H x W x 1 or H x W dimensions.")
        # raise Exception("Image must have H x W x 3, H x W x 1 or H x W dimensions.")

    with open(path, "wb") as file:
        file.write("PF\n" if color else "Pf\n".encode())
        file.write("%d %d\n".encode() % (image_.shape[1], image_.shape[0]))
        endian = image_.dtype.byteorder
        if endian == "<" or endian == "=" and sys.byteorder == "little":
            scale = -scale
        file.write("%f\n".encode() % scale)
        image_.tofile(file)


def depthmap_histogram(data):
    """
    Visualize the pixel value distribution.
    """
    data_max = int(np.max(data))
    data_min = int(np.min(data) + 0.5)
    big_number = 40 #int((data_max - data_min + 1) / 10000)
    print("depth map max is {}, min is {}, bin nubmer is {}".format(np.max(data), np.min(data), big_number))
    bin_range = np.linspace(data_min, data_max+1, big_number)
    histogram, bin_edges = np.histogram(data, bin_range, range=(np.min(data), np.max(data)))
    # configure and draw the histogram figure
    plt.figure()
    plt.title("Depth map data distribution.")
    plt.xlabel("depth value")
    plt.ylabel("pixels")
    plt.plot(bin_edges[0:-1], histogram)  # <- or here
    plt.show()


def depth2disparity(depth_map, baseline=1.0, focal=1.0):
    """
    Convert the depth map to disparity map.

    :param depth_map: depth map data
    :type depth_map: numpy
    :param baseline: [description], defaults to 1
    :type baseline: float, optional
    :param focal: [description], defaults to 1
    :type focal: float, optional
    :return: disparity map data,
    :rtype: numpy
    """
    no_zeros_index = np.where(depth_map != 0)
    disparity_map = np.full(depth_map.shape, np.Inf, np.float64)
    disparity_map[no_zeros_index] = (baseline * focal) / depth_map[no_zeros_index]
    return disparity_map


def disparity2depth(disparity_map,  baseline=1.0, focal=1.0):
    """Convert disparity value to depth value.
    """
    no_zeros_index = np.where(disparity_map != 0)
    depth_map = np.full(disparity_map.shape, np.Inf, np.float64)
    depth_map[no_zeros_index] = (baseline * focal) / disparity_map[no_zeros_index]
    return depth_map


def dispmap_normalize(dispmap, method = "", mask = None):
    """Normalize a disparity map.

    TODO support mask

    :param dispmap: the original disparity map.
    :type dispmap: numpy
    :param method: the normalization method's name.
    :type method: str
    :param mask: The mask map, available pixel is 1, invalid is 0.
    :type mask: numpy
    :return: normalized disparity map.
    :rtype: numpy
    """
    if mask is None:
        mask = np.ones_like(dispmap, dtype= bool)

    dispmap_norm = None
    if method == "naive":
        dispmap_mean = np.mean(dispmap)
        dispmap_norm = dispmap / dispmap_mean
    elif method == "midas":
        median_dispmap = np.median(dispmap[mask])
        dev_dispmap = np.sum(np.abs(dispmap[mask] - median_dispmap)) / np.sum(mask)
        dispmap_norm = np.full(dispmap.shape, np.nan, dtype= np.float64)
        dispmap_norm[mask] = (dispmap[mask] - median_dispmap) / dev_dispmap
    elif method == "range01":
        max_index = np.argsort(dispmap, axis=None)[int(dispmap.size * 0.96)]
        min_index = np.argsort(dispmap, axis=None)[int(dispmap.size * 0.04)]
        max = dispmap.flatten()[max_index]
        min = dispmap.flatten()[min_index]
        dispmap_norm = (dispmap - min) / (max-min)
    else:
        log.error("Normalize methoder {} do not supprot".format(method))
    return dispmap_norm


def subdepthmap_erp2tang(subimage_depthmap_erp, gnomonic_coord_xy):
    """ Covert the subimage's depth map from erp to tangent space.

    :param subimage_depthmap: the subimage's depth map in perspective projection, [height, width].
    :param gnomonic_coord: The tangent image each pixels location in gnomonic space, [height, width] * 2.
    """
    gnomonic_coord_x = gnomonic_coord_xy[0]
    gnomonic_coord_y = gnomonic_coord_xy[1]

    # convert the ERP depth map value to tangent image coordinate depth value
    center2pixel_length = np.sqrt(np.square(gnomonic_coord_x) + np.square(gnomonic_coord_y) + np.ones_like(gnomonic_coord_y))
    subimage_depthmap_persp = np.divide(subimage_depthmap_erp, center2pixel_length)
    return subimage_depthmap_persp


def subdepthmap_tang2erp(subimage_depthmap_persp, gnomonic_coord_xy):
    """ Convert the depth map from perspective to ERP space.

    :param subimage_erp_depthmap: subimage's depth map of ERP space.
    :type subimage_erp_depthmap: numpy
    :param gnomonic_coord_xy: The tangent image's pixels gnomonic coordinate, x and y.
    :type gnomonic_coord_xy: list
    """
    gnomonic_coord_x = gnomonic_coord_xy[0]
    gnomonic_coord_y = gnomonic_coord_xy[1]
    center2pixel_length = np.sqrt(np.square(gnomonic_coord_x) + np.square(gnomonic_coord_y) + np.ones_like(gnomonic_coord_y))
    subimage_depthmap_erp = subimage_depthmap_persp * center2pixel_length
    return subimage_depthmap_erp


def depthmap_pyramid(depthmap_list, pyramid_layer_number, pyramid_downscale):
    """ Create the all depth maps pyramid.

    :param depthmap_list: The list of depth map
    :type depthmap_list: list
    :param pyramid_layer_number: the pyramid level number
    :type pyramid_layer_number: int
    :param pyramid_downscale: pyramid downsample ration, coarse_level_size = fine_level_size * pyramid_downscale
    :type pyramid_downscale: float
    :return: the pyramid for each depth map. the 1st index is pyramid level, 2nd is image index, [pyramid_idx][image_idx], 1st (index 0) level is coarsest image.
    :rtype: list
    """
    depthmap_number = len(depthmap_list)
    depthmap_pryamid = [[0] * depthmap_number for i in range(pyramid_layer_number)]
    for index in range(0, depthmap_number):
        if pyramid_layer_number == 1:
            depthmap_pryamid[0][index] = depthmap_list[index].astype(np.float64)
        else:
            depthmap = depthmap_list[index]
            pyramid = tuple(pyramid_gaussian(depthmap, max_layer=pyramid_layer_number - 1, downscale=pyramid_downscale, multichannel=False))
            for layer_index in range(0, pyramid_layer_number):
                depthmap_pryamid[pyramid_layer_number - layer_index - 1][index] = pyramid[layer_index].astype(np.float64)

    return depthmap_pryamid



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/fs_utility.py
================================================

from logging import root
import pathlib

import os

from logger import Logger

log = Logger(__name__)
log.logger.propagate = False


# The file name convention.
class FileNameConvention:
    """
    Visualized depth/disparity map is the end with *.pdf.jpg, having same file name with the file.
    The first bracket is the file name prefix.
    """

    def __init__(self):
        self.prefix_name = None
        self.root_dir = None

        # ERP image filename expression
        self.erp_dispmap_filename_expression                       = "{prefix_name}_disp.pfm"
        self.erp_dispmap_vis_filename_expression                   = "{prefix_name}_disp_vis.jpg"
        self.erp_depthmap_filename_expression                      = "{prefix_name}_depth.pfm"
        self.erp_depthmap_vis_filename_expression                  = "{prefix_name}_depth_vis.jpg"
        self.erp_depthmap_blending_result_filename_expression      = "{prefix_name}_depth_blending.pfm"
        self.erp_depthmap_vis_blending_result_filename_expression  = "{prefix_name}_depth_blending_vis.jpg" 

        # RGB image file name expression
        self.erp_rgb_filename_expression                           = "{}"
        self.subimage_rgb_filename_expression                      = "{prefix_name}_rgb_{:03d}.jpg"

        # disparity map file name expression
        self.subimage_dispmap_filename_expression                  = "{prefix_name}_disp_{:03d}.pfm"
        self.subimage_dispmap_aligned_filename_expression          = "{prefix_name}_disp_{:03d}_aligned.pfm"

        self.subimage_dispmap_persp_filename_expression            = "{prefix_name}_disp_persp_{:03d}.pfm"
        self.subimage_dispmap_persp_aligned_filename_expression    = "{prefix_name}_disp_persp_{:03d}_aligned.pfm"
        self.subimage_dispmap_erp_filename_expression              = "{prefix_name}_disp_erp_{:03d}.pfm"
        self.subimage_dispmap_erp_aligned_filename_expression      = "{prefix_name}_disp_erp_{:03d}_aligned.pfm"
 
        self.subimage_dispmap_aligning_filename_expression         = "{prefix_name}_disp_{:03d}_{}_aligning.pfm"
        self.subimage_dispmap_cpp_aligned_filename_expression      = "depthmapAlignPy_depth_{:03d}_aligned.pfm"

        # depth map file name expression
        self.subimage_depthmap_filename_expression                 = "{prefix_name}_depth_{:03d}.pfm"
        self.subimage_depthmap_aligned_filename_expression         = "{prefix_name}_depth_{:03d}_aligned.pfm"

        self.subimage_depthmap_persp_filename_expression           = "{prefix_name}_depth_persp_{:03d}.pfm"
        self.subimage_depthmap_persp_aligned_filename_expression   = "{prefix_name}_depth_persp_{:03d}_aligned.pfm"
        self.subimage_depthmap_erp_filename_expression             = "{prefix_name}_depth_erp_{:03d}.pfm"                  # the radius self.depth map
        self.subimage_depthmap_erp_aligned_filename_expression     = "{prefix_name}_depth_erp_{:03d}_aligned.pfm"

        # The intermedia file to debug the depthmap alignment process
        self.subimage_dispmap_aligned_coeffs_filename_expression   = "{prefix_name}_disp_coeff.json"                       # the depth map alignment coefficients
        self.subimage_pixelcorr_filename_expression                = "{prefix_name}_corr_{:03d}_{:03d}.json"               # the depth map alignment corresponding relationship file
        self.subimage_warpedimage_filename_expression              = "{prefix_name}_srcwarp_rgb_{:03d}_{:03d}_{}.jpg"
        self.subimage_warpeddepth_filename_expression              = "{prefix_name}_srcwarp_disp_{:03d}_{:03d}_{}.jpg"
        self.subimage_alignment_intermedia_filename_expression     = "{prefix_name}_alignment_{}.pickle"
        self.subimage_alignment_depthmap_input_filename_expression = "{prefix_name}_alignment_input_{}_{}.jpg"
        self.subimage_camparam_filename_expression                 = "{prefix_name}_cam_{:03d}.json" 
        self.subimage_camparam_list_filename_expression            = "{prefix_name}_cam_{:03d}.json"
        self.subimage_camsparams_list_filename_expression          = "{prefix_name}_cam_all.json"

    def set_filename_basename(self, prefix_name):
        """
        Set all filename expression's filename basename.
        """
        print("Set filename prefix: {}".format(prefix_name))
        self.prefix_name = prefix_name
        for attr in dir(self):
            if not callable(getattr(self, attr)) and not attr.startswith("__"):
                attr_value = getattr(self, attr)
                if attr_value is None:
                    continue
                newfilename = attr_value.replace("{prefix_name}", prefix_name)
                setattr(self, attr, newfilename)


    def set_filepath_folder(self, root_dir):
        """
        Set the file root folder.
        """
        print("Set file root dir: {}".format(root_dir))
        self.root_dir = root_dir
        for attr in dir(self):
            if not callable(getattr(self, attr)) and not attr.startswith("__"):
                attr_value = getattr(self, attr)
                if attr_value is None:
                    continue
                setattr(self, attr, os.path.join(root_dir, attr_value))


def dir_make(directory):
    """
    check the existence of directory, if not mkdir
    :param directory: the directory path
    :type directory: str
    """
    # check
    if isinstance(directory, str):
        directory_path = pathlib.Path(directory)
    elif isinstance(directory, pathlib.Path):
        directory_path = directory
    else:
        log.warn("Directory is neither str nor pathlib.Path {}".format(directory))
        return
    # create folder
    if not directory_path.exists():
        directory_path.mkdir()
    else:
        log.info("Directory {} exist".format(directory))


def dir_ls(dir_path, postfix = None):
    """Find all files in a directory with extension.

    :param dir_path: folder path.
    :type dir_path: str
    :param postfix: extension, e.g. ".txt", if it's none list all folders name.
    :type postfix: str
    """
    file_list = []
    for file_name in os.listdir(dir_path):
        if os.path.isdir(dir_path + "/" + file_name) and postfix is None:
            file_list.append(file_name)
        elif postfix is not None:
            if file_name.endswith(postfix):
                file_list.append(file_name)
    file_list.sort()
    return file_list


def dir_rm(dir_path):
    """Deleting folders recursively.

    :param dir_path: The folder path.
    :type dir_path: str
    """
    directory = pathlib.Path(dir_path)
    if not directory.exists():
        log.warn("Directory {} do not exist".format(dir_path))
        return
    for item in directory.iterdir():
        if item.is_dir():
            dir_rm(item)
        else:
            item.unlink()
    directory.rmdir()


def exist(path, dest_type=0):
    """File exist.

    :param path: [description]
    :type path: [type]
    :param dest_type: 1 is file, 2 is directory, 0 is both.
    :type dest_type: int
    """
    if dest_type == 1:
        return os.path.isfile(path)
    elif dest_type == 2:
        return os.path.isdir(path)
    elif dest_type == 0:
        return os.path.exists(path)


def file_rm(path):
    """Remove a file.

    :param path: [description]
    :type path: str
    """
    if not exist(path, 1):
        log.debug("{} do not exist.".format(path))
        return
    elif exist(path, 1):
        os.remove(path)
    elif exist(path, 2):
        log.warn("{} is a folder".format(path))




================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/gnomonic_projection.py
================================================

import numpy as np

from logger import Logger

log = Logger(__name__)
log.logger.propagate = False

"""
Implement the Gnomonic projection (forward and reverse projection).
Reference:
[1]: https://mathworld.wolfram.com/GnomonicProjection.html
"""


def inside_polygon_2d(points_list, polygon_points, on_line=False, eps=1e-4):
    """ Test the points inside the polygon. 
    Implement 2D PIP (Point Inside a Polygon).
    
    :param points_list: The points locations numpy array whose size is [point_numb, 2]. The point storage list is as [[x_1, y_1], [x_2, y_2],...[x_n, y_n]].
    :type points_list: numpy
    :param polygon_points: The clock-wise points sequence. The storage is the same as points_list.
    :type polygon_points: numpy
    :param on_line: The inside point including the boundary points, if True. defaults to False
    :type on_line: bool, optional
    :param eps: Use the set the polygon's line width. The distance between two pixel. defaults to 1e-4
    :type eps: float, optional
    :return: A numpy Boolean array, True is inside the polygon, False is outside.
    :rtype: numpy
    """
    point_inside = np.full(np.shape(points_list)[0], False, dtype=bool)  # the point in the polygon
    online_index = np.full(np.shape(points_list)[0], False, dtype=bool)  # the point on the polygon lines

    points_x = points_list[:, 0]
    points_y = points_list[:, 1]

    def GREATER(a, b): return a >= b
    def LESS(a, b): return a <= b

    # try each line segment
    for index in range(np.shape(polygon_points)[0]):
        polygon_1_x = polygon_points[index][0]
        polygon_1_y = polygon_points[index][1]

        polygon_2_x = polygon_points[(index + 1) % len(polygon_points)][0]
        polygon_2_y = polygon_points[(index + 1) % len(polygon_points)][1]

        # exist points on the available XY range
        test_result = np.logical_and(GREATER(points_y, min(polygon_1_y, polygon_2_y)), LESS(points_y, max(polygon_1_y, polygon_2_y)))
        test_result = np.logical_and(test_result, LESS(points_x, max(polygon_1_x, polygon_2_x)))
        if not test_result.any():
            continue

        # get the intersection points
        if LESS(abs(polygon_1_y - polygon_2_y), eps):
            test_result = np.logical_and(test_result, GREATER(points_x, min(polygon_1_x, polygon_2_x)))
            intersect_points_x = points_x[test_result]
        else:
            intersect_points_x = (points_y[test_result] - polygon_1_y) * \
                (polygon_2_x - polygon_1_x) / (polygon_2_y - polygon_1_y) + polygon_1_x

        # the points on the line
        on_line_list = LESS(abs(points_x[test_result] - intersect_points_x), eps)
        if on_line_list.any():
            online_index[test_result] = np.logical_or(online_index[test_result], on_line_list)

        # the point on the left of the line
        if LESS(points_x[test_result], intersect_points_x).any():
            test_result[test_result] = np.logical_and(test_result[test_result], LESS(points_x[test_result], intersect_points_x))
            point_inside[test_result] = np.logical_not(point_inside[test_result])

    if on_line:
        return np.logical_or(point_inside, online_index).reshape(np.shape(points_list[:, 0]))
    else:
        return np.logical_and(point_inside, np.logical_not(online_index)).reshape(np.shape(points_list[:, 0]))


def gnomonic_projection(theta, phi, theta_0, phi_0):
    """ Gnomonic projection.
    Convet point form the spherical coordinate to tangent image's coordinate.
        https://mathworld.wolfram.com/GnomonicProjection.html

    :param theta: spherical coordinate's longitude.
    :type theta: numpy
    :param phi: spherical coordinate's latitude.
    :type phi: numpy
    :param theta_0: the tangent point's longitude of gnomonic projection.
    :type theta_0: float
    :param phi_0: the tangent point's latitude of gnomonic projection.
    :type phi_0: float
    :return: The gnomonic coordinate normalized coordinate.
    :rtype: numpy
    """
    cos_c = np.sin(phi_0) * np.sin(phi) + np.cos(phi_0) * np.cos(phi) * np.cos(theta - theta_0)

    # get cos_c's zero element index
    zeros_index = cos_c == 0
    if np.any(zeros_index):
        cos_c[zeros_index] = np.finfo(np.float64).eps

    x = np.cos(phi) * np.sin(theta - theta_0) / cos_c
    y = (np.cos(phi_0) * np.sin(phi) - np.sin(phi_0) * np.cos(phi) * np.cos(theta - theta_0)) / cos_c

    if np.any(zeros_index):
        x[zeros_index] = 0
        y[zeros_index] = 0

    return x, y


def reverse_gnomonic_projection(x, y, lambda_0, phi_1):
    """ Reverse gnomonic projection.
    Convert the gnomonic nomalized coordinate to spherical coordinate.

    :param x: the gnomonic plane coordinate x.
    :type x: numpy 
    :param y: the gnomonic plane coordinate y.
    :type y: numpy
    :param theta_0: the gnomonic projection tangent point's longitude.
    :type theta_0: float
    :param phi_0: the gnomonic projection tangent point's latitude f .
    :type phi_0: float
    :return: the point array's spherical coordinate location. the longitude range is continuous and exceed the range [-pi, +pi]
    :rtype: numpy
    """
    rho = np.sqrt(x**2 + y**2)

    # get rho's zero element index
    zeros_index = rho == 0
    if np.any(zeros_index):
        rho[zeros_index] = np.finfo(np.float64).eps

    c = np.arctan2(rho, 1)
    phi_ = np.arcsin(np.cos(c) * np.sin(phi_1) + (y * np.sin(c) * np.cos(phi_1)) / rho)
    lambda_ = lambda_0 + np.arctan2(x * np.sin(c), rho * np.cos(phi_1) * np.cos(c) - y * np.sin(phi_1) * np.sin(c))

    if np.any(zeros_index):
        phi_[zeros_index] = phi_1
        lambda_[zeros_index] = lambda_0

    return lambda_, phi_


def gnomonic2pixel(coord_gnom_x, coord_gnom_y,
                   padding_size,
                   tangent_image_width, tangent_image_height=None,
                   coord_gnom_xy_range=None):
    """Transform the tangent image's gnomonic coordinate to tangent image pixel coordinate.

    The tangent image gnomonic x is right, y is up.
    The tangent image pixel coordinate is x is right, y is down.

    :param coord_gnom_x: tangent image's normalized x coordinate
    :type coord_gnom_x: numpy
    :param coord_gnom_y: tangent image's normalized y coordinate
    :type coord_gnom_y: numpy
    :param padding_size: in gnomonic coordinate system, padding outside to boundary, in most case it's 0.0
    :type padding_size: float
    :param tangent_image_width: the image width with padding
    :type tangent_image_width: float
    :param tangent_image_height: the image height with padding
    :type tangent_image_height: float
    :param coord_gnom_xy_range: the range of gnomonic coordinate, [x_min, x_max, y_min, y_max]. It's often [-1.0 - padding_size, +1.0 + padding_size, ]
    :type coord_gnom_xy_range: list
    :retrun: the pixel's location
    :rtype: numpy (int)
    """
    if tangent_image_height is None:
        tangent_image_height = tangent_image_width

    # the gnomonic coordinate range of tangent image
    if coord_gnom_xy_range is None:
        x_min = -1.0
        x_max = 1.0 
        y_min = -1.0 
        y_max = 1.0 
    else:
        x_min = coord_gnom_xy_range[0]
        x_max = coord_gnom_xy_range[1]
        y_min = coord_gnom_xy_range[2]
        y_max = coord_gnom_xy_range[3]

    if padding_size != 0.0 and coord_gnom_xy_range is not None:
        log.warning("set the padding size and gnomonic range at same time! Please double check!")

    # normailzed tangent image space --> tangent image space
    # TODO check add the padding whether necessary
    gnomonic2image_width_ratio = (tangent_image_width - 1.0) / (x_max - x_min + padding_size * 2.0)
    coord_pixel_x = (coord_gnom_x - x_min + padding_size) * gnomonic2image_width_ratio
    coord_pixel_x = (coord_pixel_x + 0.5).astype(np.int32)

    gnomonic2image_height_ratio = (tangent_image_height - 1.0) / (y_max - y_min + padding_size * 2.0)
    coord_pixel_y = -(coord_gnom_y - y_max - padding_size) * gnomonic2image_height_ratio
    coord_pixel_y = (coord_pixel_y + 0.5).astype(np.int32)

    return coord_pixel_x, coord_pixel_y


def pixel2gnomonic(coord_pixel_x, coord_pixel_y,  padding_size,
                   tangent_image_width, tangent_image_height=None,
                   coord_gnom_xy_range=None):
    """Transform the tangent image's from tangent image pixel coordinate to gnomonic coordinate.

    :param coord_pixel_x: tangent image's pixels x coordinate
    :type coord_pixel_x: numpy
    :param coord_pixel_y: tangent image's pixels y coordinate
    :type coord_pixel_y: numpy
    :param padding_size: in gnomonic coordinate system, padding outside to boundary
    :type padding_size: float
    :param tangent_image_width: the image size with padding
    :type tangent_image_width: numpy
    :param tangent_image_height: the image size with padding
    :type tangent_image_height: numpy
    :param coord_gnom_xy_range: the range of gnomonic coordinate, [x_min, x_max, y_min, y_max]. It desn't includes padding outside to boundary.
    :type coord_gnom_xy_range: list
    :retrun: the pixel's location 
    :rtype:
    """
    if tangent_image_height is None:
        tangent_image_height = tangent_image_width

    # the gnomonic coordinate range of tangent image
    if coord_gnom_xy_range is None:
        x_min = -1.0
        x_max = 1.0
        y_min = -1.0
        y_max = 1.0
    else:
        x_min = coord_gnom_xy_range[0]
        x_max = coord_gnom_xy_range[1]
        y_min = coord_gnom_xy_range[2]
        y_max = coord_gnom_xy_range[3]

    # tangent image space --> tangent normalized space
    gnomonic_size_x = abs(x_max - x_min)
    gnomonic2image_ratio_width = (tangent_image_width - 1.0) / (gnomonic_size_x + padding_size * 2.0)
    coord_gnom_x = coord_pixel_x / gnomonic2image_ratio_width + x_min - padding_size

    gnomonic_size_y = abs(y_max - y_min)
    gnomonic2image_ratio_height = (tangent_image_height - 1.0) / (gnomonic_size_y + padding_size * 2.0)
    coord_gnom_y = - coord_pixel_y / gnomonic2image_ratio_height + y_max + padding_size

    return coord_gnom_x, coord_gnom_y



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/image_io.py
================================================
import os
import numpy as np

from PIL import Image
from matplotlib import pyplot as plt

from logger import Logger
from utility import depthmap_utils

log = Logger(__name__)
log.logger.propagate = False


def subimage_save_ico(subimage_list_, output_path, subimage_idx_list=None):
    """save the visualized depth map array to image file with value-bar.

    :param dapthe_data: The depth data.
    :type dapthe_data: numpy 
    :param output_path: the absolute path of output image.
    :type output_path: str
    :param subimage_idx_list: available subimages index list.
    :type subimage_idx_list: list
    """
    # add blank image to miss subimage to fill the sub-image array
    subimage_list = None
    if len(subimage_list_) != 20 \
            and subimage_idx_list is not None \
            and len(subimage_list_) == len(subimage_idx_list):
        log.debug("The ico's sub-image size is {}, fill blank sub-images.".format(len(subimage_list_)))
        subimage_list = [np.zeros_like(subimage_list_[0])] * 20
        for subimage_index in range(len(subimage_idx_list)):
            subimage_face_idx = subimage_idx_list[subimage_index]
            subimage_list[subimage_face_idx] = subimage_list_[subimage_index]
    elif len(subimage_list_) == 20:
        subimage_list = subimage_list_
    else:
        raise log.error("The sub-image is not completed.")

    # draw image
    figure, axes = plt.subplots(4, 5)
    counter = 0
    for row_index in range(0, 4):
        for col_index in range(0, 5):
            axes[row_index, col_index].get_xaxis().set_visible(False)
            axes[row_index, col_index].get_yaxis().set_visible(False)
            # add sub caption
            axes[row_index, col_index].set_title(str(counter))
            counter = counter + 1
            #
            dispmap_index = row_index * 5 + col_index
            im = axes[row_index, col_index].imshow(subimage_list[dispmap_index].astype(np.uint8))

    figure.tight_layout()
    # plt.colorbar(im, ax=axes.ravel().tolist())
    plt.savefig(output_path, dpi=150)
    # plt.show()
    plt.close(figure)


def image_read(image_file_path):
    """[summary]

    :param image_file_path: the absolute path of image
    :type image_file_path: str
    :return: the numpy array of image
    :rtype: numpy
    """    
    if not os.path.exists(image_file_path):
        log.error("{} do not exist.".format(image_file_path))

    return np.asarray(Image.open(image_file_path))


def image_show(image, title=" ",  verbose=True):
    """
    visualize the numpy array
    """
    if len(np.shape(image)) == 3:
        print("show 3 channels rgb image")
        image_rgb = image.astype(int)
        plt.title(title)
        plt.axis("off")
        plt.imshow(image_rgb)
        plt.show()
    elif len(np.shape(image)) == 2:
        print("visualize 2 channel raw data")
        images = []
        cmap = plt.get_cmap('rainbow')
        fig, axs = plt.subplots(nrows=1, sharex=True, figsize=(3, 5))
        axs.set_title(title)
        images.append(axs.imshow(image, cmap=cmap))
        fig.colorbar(images[0], ax=axs, orientation='horizontal', fraction=.1, shrink=0.4)
        plt.show()
    elif len(np.shape(image)) == 1:
        print("show 1 channels data array")
        image_rgb = visual_data(image, verbose=False)
        plt.title(title)
        plt.axis("off")
        plt.imshow(image_rgb)
        plt.show()

    else:
        print("the data channel is {}, should be visualized in advance.".format(len(np.shape(image))))

def image_save(image_data, image_file_path):
    """Save numpy array as image.

    :param image_data: Numpy array store image data. numpy 
    :type image_data: numpy
    :param image_file_path: The image's path
    :type image_file_path: str
    """
    # 0) convert the datatype
    image = None
    if image_data.dtype in [np.float64, np.int64, np.int32]:
        print("saved image array type is {}, converting to uint8".format(image_data.dtype))
        image = image_data.astype(np.uint8)
    else:
        image = image_data

    # 1) save to image file
    image_channels_number = image.shape[2]
    if image_channels_number == 3:
        im = Image.fromarray(image)
        im.save(image_file_path)
    else:
        log.error("The image channel number is {}".format(image_channels_number))


def image_show_pyramid(subimage_list):
    """save the visualized depth map array to image file with value-bar.

    :param dapthe_data: The depth data.
    :type dapthe_data: numpy 
    :param output_path: the absolute path of output image.
    :type output_path: str
    :param subimage_idx_list: available subimages index list.
    :type subimage_idx_list: list
    """
    # add blank image to miss subimage to fill the sub-image array
    image_number = len(subimage_list)
    pyramid_depth = len(subimage_list[0])

    # draw image
    figure, axes = plt.subplots(image_number, pyramid_depth)
    for image_idx in range(0,image_number):
        for pyramid_idx in range(0, pyramid_depth):
            axes[image_idx, pyramid_idx].get_xaxis().set_visible(False)
            axes[image_idx, pyramid_idx].get_yaxis().set_visible(False)
            #
            dispmap_vis = depthmap_utils.depth_visual(subimage_list[pyramid_idx][image_idx])
            # add sub caption
            image_size_str = "Idx:{}, Level:{}, {}x{}".format(image_idx, pyramid_idx, dispmap_vis.shape[0], dispmap_vis.shape[1])
            axes[image_idx, pyramid_idx].set_title(image_size_str)
            im = axes[image_idx, pyramid_idx].imshow(dispmap_vis)#.astype(np.uint8)

    figure.tight_layout()
    plt.show()



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/logger.py
================================================
import logging
import traceback

import colorama
from colorama import Fore, Back, Style
colorama.init()


class CustomFormatter(logging.Formatter):
    """Logging Formatter to add colors and count warning / errors
    reference: https://stackoverflow.com/questions/384076/how-can-i-color-python-logging-output/
    """

    import platform
    if platform.system() == 'Windows':
        grey = "\x1b[38;21m"
        yellow = "\x1b[33;21m"
        magenta = "\x1b[35;21m"
        red = "\x1b[31;21m"
        reset = "\x1b[0m"
    else:
        grey = Style.DIM
        yellow = Fore.YELLOW
        magenta = Fore.MAGENTA
        red = Fore.RED
        reset = Style.RESET_ALL
    # format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)"
    format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    FORMATS = {
        logging.DEBUG: grey + format + reset,
        logging.INFO: grey + format + reset,
        logging.WARNING: yellow + format + reset,
        logging.ERROR: red + format + reset,
        logging.CRITICAL: red + format + reset
    }

    def format(self, record):
        log_fmt = self.FORMATS.get(record.levelno)
        formatter = logging.Formatter(log_fmt)
        return formatter.format(record)


class Logger:

    def __init__(self, name=None):
        # create logger
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.DEBUG)

        # create console handler and set level to debug
        handler = logging.StreamHandler()
        handler.setLevel(logging.DEBUG)
        # add formatter
        handler.setFormatter(CustomFormatter())

        # add handler to logger
        self.logger.addHandler(handler)

    def debug(self, message):
        self.logger.debug(message)

    def info(self, message):
        self.logger.info(message)

    def warn(self, message):
        self.logger.warning(message)

    def print_stack(self):
        print("---traceback---")
        for line in traceback.format_stack():
            print(line.strip())

    def error(self, message):
        self.logger.error(message)
        self.print_stack()
        exit()

    def critical(self, message):
        self.logger.critical(message)
        self.print_stack()



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/metrics.py
================================================
import os.path

import numpy as np
from PIL import Image

from logger import Logger
import depthmap_utils

log = Logger(__name__)
log.logger.propagate = False


# ==========================
# Depth Prediction Metrics
# Refernece
# - zioulis2018omnidepth
# ==========================
eps = 1e-7


def abs_rel_error(pred, gt, mask):
    """Compute absolute relative difference error"""
    return np.mean(np.abs(pred[mask > 0] - gt[mask > 0]) / np.maximum(np.abs(gt[mask > 0]),
                                                                      np.full_like(gt[mask > 0], eps)))


def abs_rel_error_map(pred, gt, mask):
    """ per pixels' absolute relative difference.

    Parameters @see delta_inlier_ratio_map
    :return: invalid pixel is NaN
    """
    are_map = np.zeros_like(pred)
    are_map[mask > 0] = np.abs(pred[mask > 0] - gt[mask > 0]) / gt[mask > 0]
    are_map[mask <= 0] = np.nan
    return are_map


def sq_rel_error(pred, gt, mask):
    """Compute squared relative difference error"""
    return np.mean((pred[mask > 0] - gt[mask > 0]) ** 2 / np.maximum(np.abs(gt[mask > 0]),
                                                                     np.full_like(gt[mask > 0], eps)))


def sq_rel_error_map(pred, gt, mask):
    """ squared relative difference error map.
    Parameters @see delta_inlier_ratio_map
    """
    are_map = np.zeros_like(pred)
    are_map[mask > 0] = (pred[mask > 0] - gt[mask > 0]) ** 2 / gt[mask > 0]
    are_map[mask <= 0] = np.nan
    return are_map


def mean_absolute_error(pred, gt, mask):
    """Mean absolute error"""
    return np.mean(np.abs(pred[mask > 0] - gt[mask > 0]))


def lin_rms_sq_error(pred, gt, mask):
    """Compute the linear RMS error except the final square-root step"""
    return np.mean((pred[mask > 0] - gt[mask > 0]) ** 2)


def lin_rms_sq_error_map(pred, gt, mask):
    """ Each pixel RMS.
    """
    lin_rms_map = np.zeros_like(pred)
    lin_rms_map[mask > 0] = (pred[mask > 0] - gt[mask > 0]) ** 2
    lin_rms_map[mask <= 0] = np.nan
    return lin_rms_map


def log_rms_sq_error(pred, gt, mask):
    """Compute the log RMS error except the final square-root step"""
    # if np.any(pred[mask] < 0) or np.any(gt[mask] < 0):
    #     log.error("The disparity map has negative value! The metric log will generate NaN")

    mask = (mask > 0) & (pred > eps) & (gt > eps)  # Compute a mask of valid values
    return np.mean((np.log10(pred[mask]) - np.log10(gt[mask])) ** 2)


def log_rms_sq_error_map(pred, gt, mask):
    """ Each pixel log RMS.
    Parameters @see delta_inlier_ratio_map
    """
    # if np.any(pred[mask] < 0) or np.any(gt[mask] < 0):
    #     log.error("The disparity map has negative value! The metric log will generate NaN")
    mask = (mask > 0) & (pred > eps) & (gt > eps)  # Compute a mask of valid values

    log_rms_map = np.zeros_like(pred)
    log_rms_map[mask > 0] = (np.log10(pred[mask > 0]) - np.log10(gt[mask > 0])) ** 2 / gt[mask > 0]
    log_rms_map[mask <= 0] = np.nan
    return log_rms_map


def log_rms_scale_invariant(pred, gt, mask):
    """ scale-invariant log RMSE.
    """
    if np.any(pred[mask] < 0) or np.any(gt[mask] < 0):
        log.error("The disparity map has negative value! The metric log will generate NaN")

    alpha_depth = np.mean(np.log(pred[mask > 0]) - np.log(gt[mask > 0]))
    log_rms_scale_inv = np.mean(np.log(pred[mask > 0]) - np.log(gt[mask > 0]) + alpha_depth)
    return log_rms_scale_inv


def log_rms_scale_invariant_map(pred, gt, mask):
    """ Each pixel scale invariant log RMS.
    Parameters @see delta_inlier_ratio_map
    """
    if np.any(pred[mask] < 0) or np.any(gt[mask] < 0):
        log.error("The disparity map has negative value! The metric log will generate NaN")

    log_rms_map = np.zeros_like(pred)
    alpha_depth = np.mean(np.log(pred[mask > 0]) - np.log(gt[mask > 0]))
    log_rms_map[mask > 0] = np.log(pred[mask > 0]) - np.log(gt[mask > 0]) + alpha_depth
    log_rms_map[mask <= 0] = np.nan
    return log_rms_map


def delta_inlier_ratio(pred, gt, mask, degree=1):
    """Compute the delta inlier rate to a specified degree (def: 1)"""
    return np.mean(np.maximum(pred[mask > 0] / gt[mask > 0], gt[mask > 0] / pred[mask > 0]) < (1.25 ** degree))


def delta_inlier_ratio_map(pred, gt, mask, degree=1):
    """ Get the δ < 1.25^degree map.

    Get the δ map, if pixels less than thr is 1, larger is 0, invalid is -1.

    :param pred: predict disparity map, [height, width]
    :type pred: numpy
    :param gt: ground truth disparity map, [height, width]
    :type gt: numpy
    :param mask: If the mask is greater than 0 the pixel is available, otherwise it's invalided.
    :type mask: numpy
    :param degree: The exponent of 1.24, defaults to 1
    :type degree: int, optional
    :return: The δ map, [height, width]
    :rtype: numpy
    """
    delta_max = np.maximum(pred[mask > 0] / gt[mask > 0], gt[mask > 0] / pred[mask > 0])

    delta_map = np.zeros_like(delta_max)
    delta_less = delta_max < (1.25 ** degree)
    delta_map[delta_less] = 1

    delta_larger = delta_max >= (1.25 ** degree)
    delta_map[delta_larger] = 0

    delta_map_all = np.zeros_like(pred)
    delta_map_all[mask > 0] = delta_map
    delta_map_all[mask <= 0] = -1
    return delta_map_all


def normalize_depth_maps(pred, gt, mask):

    # Compute median and substract
    median_gt = np.median(gt[mask])
    median_pred = np.median(pred[mask])
    sub_med_pred = pred - median_pred

    #   Get the deviation of the valid pixels
    dev_gt = np.sum(np.abs(gt[mask] - median_gt)) / np.sum(mask)
    dev_pred = np.sum(np.abs(pred[mask] - median_pred)) / np.sum(mask)

    pred = sub_med_pred / dev_pred * dev_gt + median_gt

    return gt, pred


def pred2gt_least_squares(pred, gt, mask, max_depth=10):
    gt = depthmap_utils.depth2disparity(gt)
    a_00 = np.sum(pred[mask] * pred[mask])
    a_01 = np.sum(pred[mask])
    a_11 = np.sum(mask)

    b_0 = np.sum(pred[mask] * gt[mask])
    b_1 = np.sum(gt[mask])

    det = a_00 * a_11 - a_01 * a_01

    s = (a_11 * b_0 - a_01 * b_1) / det
    o = (-a_01 * b_0 + a_00 * b_1) / det

    pred = s * pred + o
    pred = depthmap_utils.disparity2depth(pred)
    # return pred
    return np.clip(pred, 0, max_depth)


def report_error(gt, pred, max_depth=10.0):
    mask = (gt > 0) & (~np.isinf(gt)) & (~np.isnan(gt)) & (gt <= max_depth)

    pred = pred2gt_least_squares(pred, gt, mask)

    metrics_res = {"AbsRel": abs_rel_error(pred, gt, mask),
                   "SqRel": sq_rel_error(pred, gt, mask),
                   "MAE": mean_absolute_error(pred, gt, mask),
                   "RMSE": np.sqrt(lin_rms_sq_error(pred, gt, mask)),
                   "RMSELog": np.sqrt(log_rms_sq_error(pred, gt, mask)),
                   "Delta1": delta_inlier_ratio(pred, gt, mask, degree=1),
                   "Delta2": delta_inlier_ratio(pred, gt, mask, degree=2),
                   "Delta3": delta_inlier_ratio(pred, gt, mask, degree=3)}

    return metrics_res


def normalize_depth_maps2(pred, gt, mask):
    median_gt = np.median(gt[mask])
    median_pred = np.median(pred[mask])

    pred *= median_gt/median_pred

    return gt, pred


def visualize_error_maps(pred, gt, mask, idx=0, save=False, input=None, filename=""):
    import matplotlib.pyplot as plt
    from mpl_toolkits.axes_grid1 import make_axes_locatable
    import gc
    fig = plt.figure(figsize=(9.24, 9.82))

    output_path = "../../../results/"
    if os.path.dirname(filename) != '':
        dir_name = os.path.dirname(filename)
        filename = os.path.basename(filename)
        output_path = os.path.join(output_path, dir_name)
        os.makedirs(output_path, exist_ok=True)

    input_filename = os.path.join(output_path, '{:04}.png'.format(idx))
    error_map_filename = os.path.join(output_path, '{:04}_{}_error.png'.format(idx, filename)) if filename != "" else \
        os.path.join(output_path, '{:04}_error.png'.format(idx))
    i = 0
    if input is not None and not os.path.isfile(input_filename):
        Image.fromarray(np.uint8(input)).save(input_filename)

    if os.path.isfile(error_map_filename):
        return

    gs = fig.add_gridspec(5, 2)
    ax = fig.add_subplot(gs[i, 0])
    ax.axis("off")
    ax.title.set_text("GT")
    im0 = ax.imshow(gt, cmap="turbo", vmin=0, vmax=10)
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')
    ax = fig.add_subplot(gs[i, 1])
    ax.axis("off")
    ax.title.set_text("Pred")
    im0 = ax.imshow(pred, cmap="turbo", vmin=0, vmax=10)
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')

    i += 1
    ax = fig.add_subplot(gs[i, 0])
    ax.axis("off")
    ax.title.set_text("Abs_rel")
    im0 = ax.imshow(abs_rel_error_map(pred, gt, mask), cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')
    ax = fig.add_subplot(gs[i, 1])
    ax.axis("off")
    ax.title.set_text("Sq_rel")
    im0 = ax.imshow(sq_rel_error_map(pred, gt, mask), cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')

    i += 1
    ax = fig.add_subplot(gs[i, 0])
    ax.axis("off")
    ax.title.set_text("RMS")
    im0 = ax.imshow(lin_rms_sq_error_map(pred, gt, mask), cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')
    ax = fig.add_subplot(gs[i, 1])
    ax.axis("off")
    ax.title.set_text("RMS(log)")
    im0 = ax.imshow(log_rms_sq_error_map(pred, gt, mask), cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')

    i+=1
    ax = fig.add_subplot(gs[i, 0])
    ax.axis("off")
    ax.title.set_text("Delta 1")
    im0 = ax.imshow(delta_inlier_ratio_map(pred, gt, mask, 1), cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')
    ax = fig.add_subplot(gs[i, 1])
    ax.axis("off")
    ax.title.set_text("Delta 2")
    im0 = ax.imshow(delta_inlier_ratio_map(pred, gt, mask, 2), cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')

    i += 1
    ax = fig.add_subplot(gs[i, 0])
    ax.axis("off")
    ax.title.set_text("Delta 3")
    im0 = ax.imshow(delta_inlier_ratio_map(pred, gt, mask, 3), cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')
    ax = fig.add_subplot(gs[i, 1])
    ax.axis("off")
    abs_dif = np.zeros_like(pred)
    abs_dif[mask > 0] = np.abs(pred[mask > 0] - gt[mask > 0])
    abs_dif[mask <= 0] = np.nan
    ax.title.set_text("|GT-Pred|")
    im0 = ax.imshow(abs_dif, cmap="RdPu")
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im0, cax=cax, orientation='vertical')
    plt.tight_layout(pad=0.1, h_pad=-0.5, w_pad=3)
    if save:
        plt.savefig(error_map_filename, dpi=150)
    plt.cla()
    plt.clf()
    plt.close('all')
    plt.close(fig)
    gc.collect()



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/plot_figure.py
================================================
from pathlib import Path

import numpy as np

import projection_icosahedron as pro_ico
import spherical_coordinates as sc
from utility import polygon
from utility import image_io

from logger import Logger

log = Logger(__name__)
log.logger.propagate = False


class PlotFigure():
    """
    This class is used to draw figure for paper.
    """

    def __init__(self):
        self.icosahedron_radius = 1.0
        self.output_dir = None

    def draw_ico_tangent_planes_texture_stitch(self, root_dir, subimage_filename_list, texture_filename):
        """ Stitch all 20 subimage rgb image (W * H) to a single long image ( 8W * H).
        
        :return:
        :rtype: numpy
        """
        subimage_data_list = []
        subimage_width = -1
        subimage_height = -1
        subimage_channel = None
        for subimage_filename in subimage_filename_list:
            subimage_data = image_io.image_read(root_dir + subimage_filename)
            subimage_data_list.append(subimage_data)

            subimage_width = subimage_data.shape[1]
            subimage_height = subimage_data.shape[0]
            subimage_channel = subimage_data.shape[2]

            # if texture_height < 0:
            #     texture_height = subimage_height
            # elif texture_height > 0 and texture_height != subimage_height:
            #     log.error("the subimage size is different!")

        if subimage_channel != 3:
            log.warn("Remove the subimage alpha channel.")

        texture_height = subimage_height * 4
        texture_width = subimage_width * 5

        texture_data = np.zeros((texture_height, texture_width, 3), dtype=np.uint8)
        texture_subimage_row_start = 0

        # for idx in range(len(subimage_filename_list)):
        #     texture_subimage_end = texture_subimage_start + subimage_data_list[idx].shape[1]

        #     texture_subimage_start = texture_subimage_end
        for row_idx in range(4):
            texture_subimage_row_end = texture_subimage_row_start + subimage_height
            texture_subimage_col_start = 0
            for col_idx in range(5):
                texture_subimage_col_end = texture_subimage_col_start + subimage_width
                texture_data[texture_subimage_row_start:texture_subimage_row_end, texture_subimage_col_start: texture_subimage_col_end, :] \
                    = subimage_data_list[row_idx * 5 + col_idx][:, :, 0:3]

                texture_subimage_col_start = texture_subimage_col_end
            texture_subimage_row_start = texture_subimage_row_end

        # return texture_data
        image_io.image_save(texture_data, root_dir + texture_filename)
        # im.save('uncompressed.tga', compression=None)

    def draw_ico_tangent_planes(self, radius, padding_size, obj_file_path, subimage_shift_ratio=0.0,         obj_file_texture_enable=False, texture_image_filename="ico_rgb_image.png"):
        """ Draw the ico's 3D tangent plan.

        :param radius: the radius of icosahedron.
        :type radius: float
        :param radius: the tangent image padding size
        :type radius: float
        :param obj_file_path: the obj file output path.
        :type obj_file_path: str
        """
        # 1) the panes center 3D center point
        if obj_file_texture_enable:
            mtl_file_path_path = Path(obj_file_path)
            mtl_file_path_path = mtl_file_path_path.with_suffix('.mtl')
            obj_file_head = "mtllib {}\n".format(str(mtl_file_path_path))
            obj_file_head += "usemtl material_0\n"
            obj_file_texture_coor_str = ""
        else:
            obj_file_head = ""

        obj_file_vertex_str = ""
        # the texture coordinate
        obj_file_face_str = ""
        obj_file_line_str = ""

        for tangent_image_idx in range(0, 20, 1):
            tangent_image_param = \
                pro_ico.get_icosahedron_parameters(tangent_image_idx, padding_size)

            triangle_points_sph = tangent_image_param["triangle_points_sph"]
            triangle_points_sph = np.array(triangle_points_sph, dtype=np.float32)

            triangle_points_sph_sort_idx = np.argsort(triangle_points_sph[:, 0])
            if (triangle_points_sph_sort_idx != np.array((0, 1, 2))).any():
                triangle_points_sph_sort_idx = np.tile(triangle_points_sph_sort_idx, 2).reshape((2, 3))
                triangle_points_sph = np.take_along_axis(triangle_points_sph, triangle_points_sph_sort_idx.T, axis=0)

            triangle_points_3d = sc.sph2car(triangle_points_sph[:, 0], triangle_points_sph[:, 1], radius=radius)
            # find the head vertex
            triangle_points_3d_head = None
            triangle_points_3d_edge = np.zeros((2, 3), dtype=np.float32)
            if triangle_points_sph[0, 1] == triangle_points_sph[1, 1]:
                triangle_points_3d_head = triangle_points_3d[:, 2]
                triangle_points_3d_edge[0, :] = triangle_points_3d[:, 0]
                triangle_points_3d_edge[1, :] = triangle_points_3d[:, 1]
            elif triangle_points_sph[0, 1] == triangle_points_sph[2, 1]:
                triangle_points_3d_head = triangle_points_3d[:, 1]
                triangle_points_3d_edge[0, :] = triangle_points_3d[:, 0]
                triangle_points_3d_edge[1, :] = triangle_points_3d[:, 2]
            else:
                triangle_points_3d_head = triangle_points_3d[:, 0]
                triangle_points_3d_edge[0, :] = triangle_points_3d[:, 1]
                triangle_points_3d_edge[1, :] = triangle_points_3d[:, 2]

            # 2) Output the vertices, texture information
            # the tangent plane rectangle tangent images
            if subimage_shift_ratio != 0.0:
                # offset the tangent images
                triangle_normal_norm_sph = tangent_image_param["tangent_point"]
                triangle_normal_norm_3d = sc.sph2car(triangle_normal_norm_sph[0], triangle_normal_norm_sph[1], radius=radius)
                subimage_shift = triangle_normal_norm_3d * subimage_shift_ratio
                triangle_points_3d_edge += subimage_shift
                triangle_points_3d_head += subimage_shift

            # 2-0) 3D points
            rectangle_points_3d = polygon.triangle_bounding_rectangle_3D(triangle_points_3d_head, triangle_points_3d_edge)
            # write vertex
            for vec_idx in range(4):
                vec_val = rectangle_points_3d[vec_idx]
                obj_file_vertex_str += "v {} {} {}\n".format(vec_val[0], vec_val[1], vec_val[2])

            # 2-1) add texture
            # write texture name and image file
            if obj_file_texture_enable:
                # texture file information
                # obj_file_face_str += "usemtl material_0\n"

                # texture offset
                texture_x_offset_start = 1.0 / 5.0 * (tangent_image_idx % 5)
                texture_x_offset_end = 1.0 / 5.0 + texture_x_offset_start
                texture_y_offset_start = 1.0 - 1.0 / 4.0 * int(tangent_image_idx / 5)
                texture_y_offset_end = texture_y_offset_start - 1.0 / 4.0
                obj_file_texture_coor_str += "vt {} {}\n".format(texture_x_offset_start, texture_y_offset_start)
                obj_file_texture_coor_str += "vt {} {}\n".format(texture_x_offset_start, texture_y_offset_end)
                obj_file_texture_coor_str += "vt {} {}\n".format(texture_x_offset_end, texture_y_offset_end)
                obj_file_texture_coor_str += "vt {} {}\n".format(texture_x_offset_end, texture_y_offset_start)

            # 2-2) write face
            # the head vertex alway at the top, make a face outside
            head_vertex_up = triangle_points_3d_head[1] < triangle_points_3d_edge[0, 1]
            # if head_vertex_up:
            #     triangle_points_3d_edge[[0, 1], :] = triangle_points_3d_edge[[1, 0], :]
            idx_offset = tangent_image_idx * 4
            if obj_file_texture_enable:
                if head_vertex_up:
                    print("{}:up".format(tangent_image_idx))
                    obj_file_face_str += "f {}/{} {}/{} {}/{}\n".format(idx_offset + 1, 1 + idx_offset,
                                                                        idx_offset + 2, 2 + idx_offset,
                                                                        idx_offset + 3, 3 + idx_offset)
                    obj_file_face_str += "f {}/{} {}/{} {}/{}\n".format(idx_offset + 1, 1 + idx_offset,
                                                                        idx_offset + 3, 3 + idx_offset,
                                                                        idx_offset + 4, 4 + idx_offset)
                else:
                    print("{}:down".format(tangent_image_idx))
                    obj_file_face_str += "f {}/{} {}/{} {}/{}\n".format(idx_offset + 3, 1 + idx_offset,
                                                                        idx_offset + 4, 2 + idx_offset,
                                                                        idx_offset + 1, 3 + idx_offset)
                    obj_file_face_str += "f {}/{} {}/{} {}/{}\n".format(idx_offset + 3, 1 + idx_offset,
                                                                        idx_offset + 1, 3 + idx_offset,
                                                                        idx_offset + 2, 4 + idx_offset)
            else:
                obj_file_face_str += "f {} {} {}\n".format(idx_offset + 1, idx_offset + 2, idx_offset + 3)
                obj_file_face_str += "f {} {} {}\n".format(idx_offset + 1, idx_offset + 3, idx_offset + 4)

            obj_file_line_str += "f {} {} {} {}\n".format(idx_offset + 1, idx_offset + 2, idx_offset + 3, idx_offset + 4)

        # 3) write obj file
        # 3-0) tangent images obj file
        # write tangent image obj file
        log.info("output the mesh to {}".format(obj_file_path))
        with open(obj_file_path, "w") as obj_file:
            obj_file.write(obj_file_head)
            obj_file.write(obj_file_vertex_str)
            if obj_file_texture_enable:
                obj_file.write(obj_file_texture_coor_str)

            obj_file.write(obj_file_face_str)

        # write tangent image mtl files
        if obj_file_texture_enable:
            # output mtl
            mtl_file_str = ""
            mtl_file_str += "newmtl material_0\n"
            mtl_file_str += "Ka 0.200000 0.200000 0.200000\n"
            mtl_file_str += "Kd 1.000000 1.000000 1.000000\n"
            mtl_file_str += "Ks 1.000000 1.000000 1.000000\n"
            mtl_file_str += "map_Kd {}\n\n".format(texture_image_filename)
            with open(str(mtl_file_path_path), "w") as mtl_file:
                mtl_file.write(mtl_file_str)

        # 3-1) write framewire obj file
        # write line obj
        obj_framewire_obj_file = obj_file_path + "_frameware.obj"
        log.info("output the framewire to {}".format(obj_file_path))
        with open(obj_framewire_obj_file, "w") as obj_file:
            obj_file.write(obj_file_vertex_str)
            obj_file.write(obj_file_line_str)



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/pointcloud_utils.py
================================================
import spherical_coordinates as sc

from scipy.spatial.transform import Rotation as R
import numpy as np
import os
import struct

from logger import Logger
log = Logger(__name__)
log.logger.propagate = False

"""
Point cloud utility.
"""

def depthmap2pointcloud_erp(depth_map, rgb_image, output_ply_file_path):
    """ Convert the ERP depth map and rgb_image to 3D colored point cloud.

    :param depth_map: The ERP depth map, shape is [height, width]
    :type depth_map: numpy
    :param rgb_image: The ERP rgb image, shape is [height, widht, 3]
    :type rgb_image: numpy
    :param output_ply_file_path: 3D point cloud output path.
    :type output_ply_file_path: str
    """
    # spherical coordinate
    pixel_x, pixel_y = np.meshgrid(range(depth_map.shape[1]), range(rgb_image.shape[0]))
    theta, phi = sc.erp2sph([pixel_x, pixel_y])

    # spherical coordinate to point cloud
    x = (depth_map * np.cos(phi) * np.sin(theta)).flatten()
    y = -(depth_map * np.sin(phi)).flatten()
    z = (depth_map * np.cos(phi) * np.cos(theta)).flatten()

    r = rgb_image[pixel_y, pixel_x, 0].flatten()
    g = rgb_image[pixel_y, pixel_x, 1].flatten()
    b = rgb_image[pixel_y, pixel_x, 2].flatten()

    # return np.stack([x, y, z], axis=1)
    point_cloud_data = np.stack([x, y, z, r, g, b], axis=1)

    # 2)  Output point cloud to obj file.
    # # convert to vertices list
    # obj_text = []
    # for line in point_cloud_data.T:
    #     v_list = line.tolist()
    #     obj_text.append("v {} {} {}".format(v_list[0], v_list[1], v_list[2]))

    # # output to target obj file
    # # print("generate obj file {}".format(obj_file_path))
    # output_obj_handle = open(output_obj_file_path, 'w')
    # obj_points_str = '\n'.join(obj_text)
    # output_obj_handle.write(obj_points_str)
    # output_obj_handle.close()

    # output color point cloud to PLY

    # Write header of .ply file
    fid = open(output_ply_file_path, 'wb')
    fid.write(bytes('ply\n', 'utf-8'))
    fid.write(bytes('format binary_little_endian 1.0\n', 'utf-8'))
    fid.write(bytes('element vertex %d\n' % point_cloud_data.shape[0], 'utf-8'))
    fid.write(bytes('property float x\n', 'utf-8'))
    fid.write(bytes('property float y\n', 'utf-8'))
    fid.write(bytes('property float z\n', 'utf-8'))
    fid.write(bytes('property uchar red\n', 'utf-8'))
    fid.write(bytes('property uchar green\n', 'utf-8'))
    fid.write(bytes('property uchar blue\n', 'utf-8'))
    fid.write(bytes('end_header\n', 'utf-8'))

    # Write 3D points to .ply file
    for i in range(point_cloud_data.shape[0]):
        fid.write(bytearray(struct.pack("fffccc", point_cloud_data[i, 0], point_cloud_data[i, 1], point_cloud_data[i, 2],
                                        bytes(point_cloud_data[i, 3].astype(np.uint8).data),
                                        bytes(point_cloud_data[i, 4].astype(np.uint8).data),
                                        bytes(point_cloud_data[i, 5].astype(np.uint8).data))))

    fid.close()


def depthmap2pointclouds_perspective(depth_map, rgb_image, cam_int_param, output_path, rgb_image_path=None):
    """Convert the depth map to 3D mesh and export to file.

    The input numpy array is [height, width, x].

    :param depth_map: depth map
    :type depth_map: numpy
    :param rgb_image: rgb image data
    :type rgb_image: numpy
    :param cam_int_param: camera 3x3 calibration matrix, [[fx, 0, cx], [0,fy,cy], [0,0,1]]
    :type cam_int_param: numpy
    :param output_path: the absolute path of output mesh, support ply and obj.
    :type output_path: str
    :param rgb_image_path: the rgb image relative path, used by obj's mtl file.
    :type rgb_image_path: str
    """
    # 1) check the function arguments
    _, output_path_ext = os.path.splitext(output_path)

    if not output_path_ext == ".obj" and not output_path_ext == ".ply":
        log.error("Current do not support {}  format".format(output_path_ext[1:]))

    # 2) convert the depth map to 3d points
    image_height = depth_map.shape[0]
    image_width = depth_map.shape[1]

    x_list = np.linspace(0, image_width, image_width, endpoint=False)
    y_list = np.linspace(0, image_height, image_height, endpoint=False)
    grid_x, grid_y = np.meshgrid(x_list, y_list)
    gird_z = np.ones(grid_x.shape, np.float64)
    points_2d_pixel = np.stack((grid_x.ravel(), grid_y.ravel(), gird_z.ravel()), axis=1)
    points_2d_pixel = np.multiply(points_2d_pixel.T, depth_map.ravel())
    points_3d_pixel = np.linalg.inv(cam_int_param) @ points_2d_pixel
    points_3d_pixel = points_3d_pixel.T.reshape((depth_map.shape[:2] + (3,)))

    # 3) output to file
    log.debug("save the mesh to {}".format(output_path_ext))

    if output_path_ext == ".obj":
        if os.path.exists(output_path):
            log.warn("{} exist, overwrite it.".format(output_path))

        output_mtl_path = None
        if rgb_image is None or rgb_image_path is None:
            rgb_image_path = None
            output_mtl_path = None
            log.debug("Do not specify texture for the obj file.")
        else:
            output_mtl_path = os.path.splitext(output_path)[0] + ".mtl"
            if os.path.exists(output_mtl_path):
                log.warn("{} exist, overwrite it.".format(output_mtl_path))

        create_obj(depth_map, points_3d_pixel, output_path, output_mtl_path, texture_filepath=rgb_image_path)
    elif output_path_ext == ".ply":
        log.critical("do not implement!")
        # create_ply()


def pointcloud_tang2world(point_cloud_data, tangent_point):
    """ Rotation tangent point cloud to world coordinate system.
    Tranfrom the point cloud from tangent space to world space.

    :param point_cloud_data: The point cloud array [3, points_number]
    :type point_cloud_data: numpy 
    :param tangent_point:  the tangent point rotation [theta,phi] in radiant.
    :type tangent_point: list
    """
    assert len(point_cloud_data.shape) == 2
    assert point_cloud_data.shape[0] == 3

    rotation_matrix = R.from_euler("xyz", [tangent_point[1], tangent_point[0], 0], degrees=False).as_dcm()
    xyz_rotated = np.dot(rotation_matrix, point_cloud_data)
    return xyz_rotated


def create_obj(depthmap, point3d, obj_filepath, mtl_filepath=None, mat_name="material0", texture_filepath=None):
    """This method does the same as :func:`depthmap2mesh`
    """
    use_material = False
    if mtl_filepath is not None:
        use_material = True

    # create mtl file
    if use_material:
        with open(mtl_filepath, "w") as f:
            f.write("newmtl " + mat_name + "\n")
            f.write("Ns 10.0000\n")
            f.write("d 1.0000\n")
            f.write("Tr 0.0000\n")
            f.write("illum 2\n")
            f.write("Ka 1.000 1.000 1.000\n")
            f.write("Kd 1.000 1.000 1.000\n")
            f.write("Ks 0.000 0.000 0.000\n")
            f.write("map_Ka " + texture_filepath + "\n")
            f.write("map_Kd " + texture_filepath + "\n")

    # create obj file
    width = depthmap.shape[1]
    hight = depthmap.shape[0]

    with open(obj_filepath, "w") as file:
        # output
        if use_material:
            file.write("mtllib " + mtl_filepath + "\n")
            file.write("usemtl " + mat_name + "\n")

        # the triangle's vertex index
        pixel_vertex_index = np.zeros((width, hight), int)  # pixels' vertex index number
        vid = 1  # vertex index

        # output vertex
        for u in range(0, width):
            for v in range(hight-1, -1, -1):
                # vertex index
                pixel_vertex_index[v, u] = vid
                if depthmap[v, u] == 0.0:
                    pixel_vertex_index[u, v] = 0
                vid += 1

                # output 3d vertex
                x = point3d[v, u, 0]
                y = point3d[v, u, 1]
                z = point3d[v, u, 2]
                file.write("v " + str(x) + " " + str(y) + " " + str(z) + "\n")

        # output texture location
        for u in range(0, width):
            for v in range(0, hight):
                file.write("vt " + str(u/width) + " " + str(v/hight) + "\n")

        # output face index
        for u in range(0, width-1):
            for v in range(0, hight-1):
                v1 = pixel_vertex_index[u, v]
                v2 = pixel_vertex_index[u+1, v]
                v3 = pixel_vertex_index[u, v+1]
                v4 = pixel_vertex_index[u+1, v+1]

                if v1 == 0 or v2 == 0 or v3 == 0 or v4 == 0:
                    continue

                file.write("f " + str(v1)+"/"+str(v1) + " " + str(v2)+"/"+str(v2) + " " + str(v3)+"/"+str(v3) + "\n")
                file.write("f " + str(v3)+"/"+str(v3) + " " + str(v2)+"/"+str(v2) + " " + str(v4)+"/"+str(v4) + "\n")



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/polygon.py
================================================
import numpy as np
from logger import Logger

log = Logger(__name__)
log.logger.propagate = False


def find_intersection(p1,  p2,  p3,  p4):
    """Find the point of intersection between two line.
    Work on 3D plane.
    
    The two lines are p1 --> p2 and p3 --> p4.
    Reference:http://csharphelper.com/blog/2020/12/enlarge-a-polygon-that-has-colinear-vertices-in-c/

    :param p1: line 1's start point
    :type p1: list
    :param p2: line 1's end point
    :type p2: list
    :param p3: line 2's start point
    :type p3: list
    :param p4: line 2's end point
    :type p4: list
    :return: The intersection point of two line
    :rtype: list
    """
    # the segments
    dx12 = p2[0] - p1[0]
    dy12 = p2[1] - p1[1]
    dx34 = p4[0] - p3[0]
    dy34 = p4[1] - p3[1]

    denominator = (dy12 * dx34 - dx12 * dy34)
    if denominator == 0:
        # The two lines are parallel
        return None

    t1 = ((p1[0] - p3[0]) * dy34 + (p3[1] - p1[1]) * dx34) / denominator
    t2 = ((p3[0] - p1[0]) * dy12 + (p1[1] - p3[1]) * dx12) / -denominator

    # point of intersection.
    intersection = [p1[0] + dx12 * t1, p1[1] + dy12 * t1]
    return intersection


def is_clockwise(point_list):
    """Check whether the list is clockwise. 

    :param point_list: the point lists
    :type point_list: numpy 
    :return: yes if the points are clockwise, otherwise is no
    :rtype: bool
    """
    sum = 0
    for i in range(len(point_list)):
        cur = point_list[i]
        next = point_list[(i+1) % len(point_list)]
        sum += (next[0] - cur[0]) * (next[1] + cur[1])
    return sum > 0


def enlarge_polygon(old_points, offset):
    """Return points representing an enlarged polygon.

    Reference: http://csharphelper.com/blog/2016/01/enlarge-a-polygon-in-c/
    
    :param old_points: the polygon vertexes, and the points should be in clock wise
    :type: list [[x_1,y_1], [x_2, y_2]......]
    :param offset: the ratio of the polygon enlarged
    :type: float
    :return: the offset points
    :rtype: list
    """
    enlarged_points = []
    num_points = len(old_points)
    for j in range(num_points):
        # 0) find "out" side
        if not is_clockwise(old_points):
            log.error("the points list is not clockwise.")

        # the points before and after j.
        i = (j - 1)
        if i < 0:
            i += num_points
        k = (j + 1) % num_points

        # 1) Move the points by the offset.
        # the points of line parallel to ij
        v1 = np.array([old_points[j][0] - old_points[i][0], old_points[j][1] - old_points[i][1]], np.float64)
        norm = np.linalg.norm(v1)
        v1 = v1 / norm * offset
        n1 = [-v1[1], v1[0]]
        pij1 = [old_points[i][0] + n1[0], old_points[i][1] + n1[1]]
        pij2 = [old_points[j][0] + n1[0], old_points[j][1] + n1[1]]

        # the points of line parallel to jk
        v2 = np.array([old_points[k][0] - old_points[j][0], old_points[k][1] - old_points[j][1]], np.float64)
        norm = np.linalg.norm(v2)
        v2 = v2 / norm * offset
        n2 = [-v2[1], v2[0]]
        pjk1 = [old_points[j][0] + n2[0], old_points[j][1] + n2[1]]
        pjk2 = [old_points[k][0] + n2[0], old_points[k][1] + n2[1]]

        # 2) get the shifted lines ij and jk intersect
        lines_intersect = find_intersection(pij1, pij2, pjk1, pjk2)
        enlarged_points.append(lines_intersect)

    return enlarged_points


def generate_hexagon(circumradius=1.0, hexagon_type=0, draw_enable=False):
    """ Figure out the 6 vertexes of regular hexagon.
    The coordinate center is hexagon center.
    The original of coordinate at the center of hexagon.

    :param circumradius: The circumradius of the hexagon, defaults to 1.0
    :type circumradius: float, optional
    :param hexagon_type: the hexagon type, 0 the point is on the y-axis, 1 the vertexes on the x-axis, defaults to 0
    :type hexagon_type: int, optional
    :return: the six vertexes of hexagon.[6,2],  the 1st and 2nd is x,y respectively.
    :rtype: numpy 
    """
    vertex_list = np.zeros((6, 2), dtype=np.double)

    angle_interval = -60.0
    for idx in range(0, 6):
        if hexagon_type == 0:
            angle = angle_interval * idx
        elif hexagon_type == 1:
            angle = angle_interval * idx + 30.0
        else:
            log.error("Do not support hexagon type {}".format(hexagon_type))
        vertex_list[idx, 0] = np.cos(np.radians(angle)) * circumradius
        vertex_list[idx, 1] = np.sin(np.radians(angle)) * circumradius

    if draw_enable:
        from PIL import Image, ImageDraw
        image_width = circumradius * 3
        image_height = circumradius * 3
        offset_width = image_width * 0.5
        offset_height = image_height * 0.5
        image = Image.new('RGB', (image_height, image_width), 'white')
        draw = ImageDraw.Draw(image)
        vertex_list_ = np.zeros_like(vertex_list)
        vertex_list_[:, 0] = vertex_list[:, 0] + offset_width  # the origin at upper-left of image.
        vertex_list_[:, 1] = vertex_list[:, 1] + offset_height
        draw.polygon(tuple(map(tuple, vertex_list_)), outline='black', fill='red')
        image.show()

    return vertex_list


def isect_line_plane_3D(line_p0, line_p1, plane_point, plane_norm):
    """ Get the intersection point between plane and line.

    :param line_p0: a point on the line, shape is [3]
    :type line_p0: numpy
    :param line_p1: a point on the line, shape is [3]
    :type line_p1: numpy
    :param plane_point: a point on the 3D plane, shape is [3]
    :type plane_point: numpy
    :param plane_norm: a normal vector of the plane. shape is [3]
    :type plane_norm: numpy
    :return: the intersection point, shape is [3]
    :rtype: numpy
    """
    u = line_p0 - line_p1
    dot = np.dot(plane_norm, u)

    if abs(dot) > np.finfo(np.float32).eps:
        w = line_p0 - plane_point
        fac = - np.dot(plane_norm, w) / dot
        return line_p0 + u * fac
    else:
        return None


def triangle_bounding_rectangle_3D(head_point, edge_points):
    """ The 3D bounding rectangle from the 3D triangle's 3 vertices.

    :param head_point: The 3 vertices of the triangle, shape is [3], which is xyz.
    :type head_point: numpy
    :param edge_points: The 2 vertices of the edge, shape is [2, 3], each row is xyz
    :type edge_points: numpy
    :return: the 4 vertiec of the rectangle, shape is [4,3]
    :rtype: numpy
    """
    edge_point_A = edge_points[0, :]
    edge_point_B = edge_points[1, :]

    edge_points_mid = 0.5 * (edge_point_A + edge_point_B)
    mid_head_vec = head_point - edge_points_mid

    edge_point_AH = edge_point_A + mid_head_vec
    edge_point_BH = edge_point_B + mid_head_vec

    # return rectangle points
    rect_points = np.zeros((4, 3), dtype=np.float32)
    rect_points[0, :] = edge_point_AH
    rect_points[1, :] = edge_point_A
    rect_points[2, :] = edge_point_B
    rect_points[3, :] = edge_point_BH

    return rect_points



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/projection_icosahedron.py
================================================
import copy
import numpy as np
from scipy import ndimage

import gnomonic_projection as gp
import spherical_coordinates as sc
import polygon

from logger import Logger

log = Logger(__name__)
log.logger.propagate = False

"""
Implement icosahedron projection and stitch with the Gnomonic projection (forward and reverse projection).
Reference:
[1]: https://mathworld.wolfram.com/GnomonicProjection.html
"""


def get_icosahedron_parameters(triangle_index, padding_size=0.0):
    """
    Get icosahedron's tangent face's paramters.
    Get the tangent point theta and phi. Known as the theta_0 and phi_0.
    The erp image origin as top-left corner

    :return the tangent face's tangent point and 3 vertices's location.
    """
    # reference: https://en.wikipedia.org/wiki/Regular_icosahedron
    radius_circumscribed = np.sin(2 * np.pi / 5.0)
    radius_inscribed = np.sqrt(3) / 12.0 * (3 + np.sqrt(5))
    radius_midradius = np.cos(np.pi / 5.0)

    # the tangent point
    theta_0 = None
    phi_0 = None

    # the 3 points of tangent triangle in spherical coordinate
    triangle_point_00_theta = None
    triangle_point_00_phi = None
    triangle_point_01_theta = None
    triangle_point_01_phi = None
    triangle_point_02_theta = None
    triangle_point_02_phi = None

    # triangles' row/col range in the erp image
    # erp_image_row_start = None
    # erp_image_row_stop = None
    # erp_image_col_start = None
    # erp_image_col_stop = None

    theta_step = 2.0 * np.pi / 5.0
    # 1) the up 5 triangles
    if 0 <= triangle_index <= 4:
        # tangent point of inscribed spheric
        theta_0 = - np.pi + theta_step / 2.0 + triangle_index * theta_step
        phi_0 = np.pi / 2 - np.arccos(radius_inscribed / radius_circumscribed)
        # the tangent triangle points coordinate in tangent image
        triangle_point_00_theta = -np.pi + triangle_index * theta_step
        triangle_point_00_phi = np.arctan(0.5)
        triangle_point_01_theta = -np.pi + np.pi * 2.0 / 5.0 / 2.0 + triangle_index * theta_step
        triangle_point_01_phi = np.pi / 2.0
        triangle_point_02_theta = -np.pi + (triangle_index + 1) * theta_step
        triangle_point_02_phi = np.arctan(0.5)

        # # availied area of ERP image
        # erp_image_row_start = 0
        # erp_image_row_stop = (np.pi / 2 - np.arctan(0.5)) / np.pi
        # erp_image_col_start = 1.0 / 5.0 * triangle_index_temp
        # erp_image_col_stop = 1.0 / 5.0 * (triangle_index_temp + 1)

    # 2) the middle 10 triangles
    # 2-0) middle-up triangles
    if 5 <= triangle_index <= 9:
        triangle_index_temp = triangle_index - 5
        # tangent point of inscribed spheric
        theta_0 = - np.pi + theta_step / 2.0 + triangle_index_temp * theta_step
        phi_0 = np.pi / 2.0 - np.arccos(radius_inscribed / radius_circumscribed) - 2 * np.arccos(radius_inscribed / radius_midradius)
        # the tangent triangle points coordinate in tangent image
        triangle_point_00_theta = -np.pi + triangle_index_temp * theta_step
        triangle_point_00_phi = np.arctan(0.5)
        triangle_point_01_theta = -np.pi + (triangle_index_temp + 1) * theta_step
        triangle_point_01_phi = np.arctan(0.5)
        triangle_point_02_theta = -np.pi + theta_step / 2.0 + triangle_index_temp * theta_step
        triangle_point_02_phi = -np.arctan(0.5)

        # # availied area of ERP image
        # erp_image_row_start = (np.arccos(radius_inscribed / radius_circumscribed) + np.arccos(radius_inscribed / radius_midradius)) / np.pi
        # erp_image_row_stop = (np.pi / 2.0 + np.arctan(0.5)) / np.pi
        # erp_image_col_start = 1 / 5.0 * triangle_index_temp
        # erp_image_col_stop = 1 / 5.0 * (triangle_index_temp + 1)

    # 2-1) the middle-down triangles
    if 10 <= triangle_index <= 14:
        triangle_index_temp = triangle_index - 10
        # tangent point of inscribed spheric
        theta_0 = - np.pi + triangle_index_temp * theta_step
        phi_0 = -(np.pi / 2.0 - np.arccos(radius_inscribed / radius_circumscribed) - 2 * np.arccos(radius_inscribed / radius_midradius))
        # the tangent triangle points coordinate in tangent image
        triangle_point_00_phi = -np.arctan(0.5)
        triangle_point_00_theta = - np.pi - theta_step / 2.0 + triangle_index_temp * theta_step
        if triangle_index_temp == 10:
            # cross the ERP image boundary
            triangle_point_00_theta = triangle_point_00_theta + 2 * np.pi
        triangle_point_01_theta = -np.pi + triangle_index_temp * theta_step
        triangle_point_01_phi = np.arctan(0.5)
        triangle_point_02_theta = - np.pi + theta_step / 2.0 + triangle_index_temp * theta_step
        triangle_point_02_phi = -np.arctan(0.5)

        # # availied area of ERP image
        # erp_image_row_start = (np.pi / 2.0 - np.arctan(0.5)) / np.pi
        # erp_image_row_stop = (np.pi - np.arccos(radius_inscribed / radius_circumscribed) - np.arccos(radius_inscribed / radius_midradius)) / np.pi
        # erp_image_col_start = 1.0 / 5.0 * triangle_index_temp - 1.0 / 5.0 / 2.0
        # erp_image_col_stop = 1.0 / 5.0 * triangle_index_temp + 1.0 / 5.0 / 2.0

    # 3) the down 5 triangles
    if 15 <= triangle_index <= 19:
        triangle_index_temp = triangle_index - 15
        # tangent point of inscribed spheric
        theta_0 = - np.pi + triangle_index_temp * theta_step
        phi_0 = - (np.pi / 2 - np.arccos(radius_inscribed / radius_circumscribed))
        # the tangent triangle points coordinate in tangent image
        triangle_point_00_theta = - np.pi - theta_step / 2.0 + triangle_index_temp * theta_step
        triangle_point_00_phi = -np.arctan(0.5)
        triangle_point_01_theta = - np.pi + theta_step / 2.0 + triangle_index_temp * theta_step
        # cross the ERP image boundary
        if triangle_index_temp == 15:
            triangle_point_01_theta = triangle_point_01_theta + 2 * np.pi
        triangle_point_01_phi = -np.arctan(0.5)
        triangle_point_02_theta = - np.pi + triangle_index_temp * theta_step
        triangle_point_02_phi = -np.pi / 2.0

        # # spherical coordinate (0,0) is in the center of ERP image
        # erp_image_row_start = (np.pi / 2.0 + np.arctan(0.5)) / np.pi
        # erp_image_row_stop = 1.0
        # erp_image_col_start = 1.0 / 5.0 * triangle_index_temp - 1.0 / 5.0 / 2.0
        # erp_image_col_stop = 1.0 / 5.0 * triangle_index_temp + 1.0 / 5.0 / 2.0

    tangent_point = [theta_0, phi_0]

    # the 3 vertices in tangent image's gnomonic coordinate
    triangle_points_tangent = []
    triangle_points_tangent.append(gp.gnomonic_projection(triangle_point_00_theta, triangle_point_00_phi, theta_0, phi_0))
    triangle_points_tangent.append(gp.gnomonic_projection(triangle_point_01_theta, triangle_point_01_phi, theta_0, phi_0))
    triangle_points_tangent.append(gp.gnomonic_projection(triangle_point_02_theta, triangle_point_02_phi, theta_0, phi_0))

    # pading the tangent image
    triangle_points_tangent_no_pading = copy.deepcopy(triangle_points_tangent)  # Needed for NN blending
    triangle_points_tangent_pading = polygon.enlarge_polygon(triangle_points_tangent, padding_size)

    # if padding_size != 0.0:
    triangle_points_tangent = copy.deepcopy(triangle_points_tangent_pading)

    # the points in spherical location
    triangle_points_sph = []
    for index in range(3):
        tri_pading_x, tri_pading_y = triangle_points_tangent_pading[index]
        triangle_point_theta, triangle_point_phi = gp.reverse_gnomonic_projection(tri_pading_x, tri_pading_y, theta_0, phi_0)
        triangle_points_sph.append([triangle_point_theta, triangle_point_phi])

    # compute bounding box of the face in spherical coordinate
    availied_sph_area = []
    availied_sph_area = np.array(copy.deepcopy(triangle_points_sph))
    triangle_points_tangent_pading = np.array(triangle_points_tangent_pading)
    point_insert_x = np.sort(triangle_points_tangent_pading[:, 0])[1]
    point_insert_y = np.sort(triangle_points_tangent_pading[:, 1])[1]
    availied_sph_area = np.append(availied_sph_area, [gp.reverse_gnomonic_projection(point_insert_x, point_insert_y, theta_0, phi_0)], axis=0)
    # the bounding box of the face with spherical coordinate
    availied_ERP_area_sph = []  # [min_longitude, max_longitude, min_latitude, max_lantitude]

    if 0 <= triangle_index <= 4:
        if padding_size > 0.0:
            availied_ERP_area_sph.append(-np.pi)
            availied_ERP_area_sph.append(np.pi)
        else:
            availied_ERP_area_sph.append(np.amin(availied_sph_area[:, 0]))
            availied_ERP_area_sph.append(np.amax(availied_sph_area[:, 0]))
        availied_ERP_area_sph.append(np.pi / 2.0)
        availied_ERP_area_sph.append(np.amin(availied_sph_area[:, 1]))  # the ERP Y axis direction as down
    elif 15 <= triangle_index <= 19:
        if padding_size > 0.0:
            availied_ERP_area_sph.append(-np.pi)
            availied_ERP_area_sph.append(np.pi)
        else:
            availied_ERP_area_sph.append(np.amin(availied_sph_area[:, 0]))
            availied_ERP_area_sph.append(np.amax(availied_sph_area[:, 0]))
        availied_ERP_area_sph.append(np.amax(availied_sph_area[:, 1]))
        availied_ERP_area_sph.append(-np.pi / 2.0)
    else:
        availied_ERP_area_sph.append(np.amin(availied_sph_area[:, 0]))
        availied_ERP_area_sph.append(np.amax(availied_sph_area[:, 0]))
        availied_ERP_area_sph.append(np.amax(availied_sph_area[:, 1]))
        availied_ERP_area_sph.append(np.amin(availied_sph_area[:, 1]))

    # else:
    #     triangle_points_sph.append([triangle_point_00_theta, triangle_point_00_theta])
    #     triangle_points_sph.append([triangle_point_01_theta, triangle_point_01_theta])
    #     triangle_points_sph.append([triangle_point_02_theta, triangle_point_02_theta])

    #     availied_ERP_area.append(erp_image_row_start)
    #     availied_ERP_area.append(erp_image_row_stop)
    #     availied_ERP_area.append(erp_image_col_start)
    #     availied_ERP_area.append(erp_image_col_stop)

    return {"tangent_point": tangent_point, "triangle_points_tangent": triangle_points_tangent,
            "triangle_points_sph": triangle_points_sph,
            "triangle_points_tangent_nopad": triangle_points_tangent_no_pading, "availied_ERP_area": availied_ERP_area_sph}


def erp2ico_image(erp_image, tangent_image_width, padding_size=0.0, full_face_image=False):
    """Project the equirectangular image to 20 triangle images.

    Project the equirectangular image to level-0 icosahedron.

    :param erp_image: the input equirectangular image, RGB image should be 3 channel [H,W,3], depth map' shape should be [H,W].
    :type erp_image: numpy array, [height, width, 3]
    :param tangent_image_width: the output triangle image size, defaults to 480
    :type tangent_image_width: int, optional
    :param padding_size: the output face image' padding size
    :type padding_size: float
    :param full_face_image: If yes project all pixels in the face image, no just project the pixels in the face triangle, defaults to False
    :type full_face_image: bool, optional
    :param depthmap_enable: if project depth map, return the each pixel's 3D points location in current camera coordinate system.
    :type depthmap_enable: bool
    :return: If erp is rgb image:
                1) a list contain 20 triangle images, the image is 4 channels, invalided pixel's alpha is 0, others is 1.
                2)
                3) None.
    
            If erp is depth map:
                1) a list contain 20 triangle images depth maps in tangent coordinate system.  The subimage's depth is 3D point could depth value.
                2) 
                3) 3D point cloud in tangent coordinate system. The pangent point cloud coordinate system is same as the world coordinate system. +y down, +x right and +z forward.
    :rtype: 
    """
    if full_face_image:
        log.debug("Generate rectangle tangent image.")
    else:
        log.debug("Generating triangle tangent image.")
        
    # ERP image size
    depthmap_enable = False
    if len(erp_image.shape) == 3:
        if np.shape(erp_image)[2] == 4:
            log.info("project ERP image is 4 channels RGB map")
            erp_image = erp_image[:, :, 0:3]
        log.info("project ERP image 3 channels RGB map")
    elif len(erp_image.shape) == 2:
        log.info("project ERP image is single channel depth map")
        erp_image = np.expand_dims(erp_image, axis=2)
        depthmap_enable = True

    erp_image_height = np.shape(erp_image)[0]
    erp_image_width = np.shape(erp_image)[1]
    channel_number = np.shape(erp_image)[2]

    if erp_image_width != erp_image_height * 2:
        raise Exception("the ERP image dimession is {}".format(np.shape(erp_image)))

    tangent_image_list = []
    tangent_image_gnomonic_xy = [] # [x[height, width], y[height, width]]
    tangent_3dpoints_list = []
    tangent_sphcoor_list = []

    tangent_image_height = int((tangent_image_width / 2.0) / np.tan(np.radians(30.0)) + 0.5)

    # generate tangent images
    for triangle_index in range(0, 20):
        log.debug("generate the tangent image {}".format(triangle_index))
        triangle_param = get_icosahedron_parameters(triangle_index, padding_size)

        tangent_triangle_vertices = np.array(triangle_param["triangle_points_tangent"])
        # the face gnomonic range in tangent space
        gnomonic_x_min = np.amin(tangent_triangle_vertices[:, 0], axis=0)
        gnomonic_x_max = np.amax(tangent_triangle_vertices[:, 0], axis=0)
        gnomonic_y_min = np.amin(tangent_triangle_vertices[:, 1], axis=0)
        gnomonic_y_max = np.amax(tangent_triangle_vertices[:, 1], axis=0)
        gnom_range_x = np.linspace(gnomonic_x_min, gnomonic_x_max, num=tangent_image_width, endpoint=True)
        gnom_range_y = np.linspace(gnomonic_y_max, gnomonic_y_min, num=tangent_image_height, endpoint=True)
        gnom_range_xv, gnom_range_yv = np.meshgrid(gnom_range_x, gnom_range_y)

        # the tangent triangle points coordinate in tangent image
        inside_list = np.full(gnom_range_xv.shape[:2], True, dtype=bool)
        if not full_face_image:
            gnom_range_xyv = np.stack((gnom_range_xv.flatten(), gnom_range_yv.flatten()), axis=1)
            pixel_eps = (gnomonic_x_max - gnomonic_x_min) / (tangent_image_width)
            inside_list = gp.inside_polygon_2d(gnom_range_xyv, tangent_triangle_vertices, on_line=True, eps=pixel_eps)
            inside_list = inside_list.reshape(gnom_range_xv.shape)

        # project to tangent image
        tangent_point = triangle_param["tangent_point"]
        tangent_triangle_theta_, tangent_triangle_phi_ = gp.reverse_gnomonic_projection(gnom_range_xv[inside_list], gnom_range_yv[inside_list], tangent_point[0], tangent_point[1])

        tangent_sphcoor_list.append(
            np.stack((tangent_triangle_theta_.reshape(gnom_range_xv.shape), tangent_triangle_phi_.reshape(gnom_range_xv.shape)))
        )

        # tansform from spherical coordinate to pixel location
        tangent_triangle_erp_pixel_x, tangent_triangle_erp_pixel_y = sc.sph2erp(tangent_triangle_theta_, tangent_triangle_phi_, erp_image_height, sph_modulo=True)

        # get the tangent image pixels value
        tangent_gnomonic_range = [gnomonic_x_min, gnomonic_x_max, gnomonic_y_min, gnomonic_y_max]
        tangent_image_x, tangent_image_y = gp.gnomonic2pixel(gnom_range_xv[inside_list], gnom_range_yv[inside_list],
                                                             0.0, tangent_image_width, tangent_image_height, tangent_gnomonic_range)

        if depthmap_enable:
            tangent_image = np.full([tangent_image_height, tangent_image_width, channel_number], -1.0)
        else:
            tangent_image = np.full([tangent_image_height, tangent_image_width, channel_number], 255.0)
        for channel in range(0, np.shape(erp_image)[2]):
            tangent_image[tangent_image_y, tangent_image_x, channel] = \
                ndimage.map_coordinates(erp_image[:, :, channel], [tangent_triangle_erp_pixel_y, tangent_triangle_erp_pixel_x], order=1, mode='wrap', cval=255.0)

        # if the ERP image is depth map, get camera coordinate system 3d points
        tangent_3dpoints = None
        if depthmap_enable:
            # convert the spherical depth map value to tangent image coordinate depth value  
            center2pixel_length = np.sqrt(np.square(gnom_range_xv[inside_list])  + np.square(gnom_range_yv[inside_list]) + np.ones_like(gnom_range_yv[inside_list]))
            center2pixel_length = center2pixel_length.reshape((tangent_image_height, tangent_image_width, channel_number))
            tangent_3dpoints_z = np.divide(tangent_image , center2pixel_length)
            tangent_image = tangent_3dpoints_z

            # get x and y
            tangent_3dpoints_x = np.multiply(tangent_3dpoints_z , gnom_range_xv[inside_list].reshape((tangent_image_height, tangent_image_width, channel_number)))
            tangent_3dpoints_y = np.multiply(tangent_3dpoints_z , gnom_range_yv[inside_list].reshape((tangent_image_height, tangent_image_width, channel_number)))
            tangent_3dpoints = np.concatenate([tangent_3dpoints_x, -tangent_3dpoints_y, tangent_3dpoints_z], axis =2)
            
        # set the pixels outside the boundary to transparent
        # tangent_image[:, :, 3] = 0
        # tangent_image[tangent_image_y, tangent_image_x, 3] = 255
        tangent_image_list.append(tangent_image)
        tangent_3dpoints_list.append(tangent_3dpoints)

    # get the tangent image's gnomonic coordinate
    tangent_image_gnomonic_x = gnom_range_xv[inside_list].reshape((tangent_image_height, tangent_image_width))
    tangent_image_gnomonic_xy.append(tangent_image_gnomonic_x)
    tangent_image_gnomonic_y = gnom_range_yv[inside_list].reshape((tangent_image_height, tangent_image_width))
    tangent_image_gnomonic_xy.append(tangent_image_gnomonic_y)

    return tangent_image_list, tangent_sphcoor_list, [tangent_3dpoints_list, tangent_image_gnomonic_xy]


def ico2erp_image(tangent_images, erp_image_height, padding_size=0.0, blender_method=None):
    """Stitch the level-0 icosahedron's tangent image to ERP image.

    blender_method:
        - None: just sample the triangle area;
        - Mean: the mean value on the overlap area.

    TODO there are seam on the stitched erp image.

    :param tangent_images: 20 tangent images in order.
    :type tangent_images: a list of numpy
    :param erp_image_height: the output erp image's height.
    :type erp_image_height: int
    :param padding_size: the face image's padding size
    :type padding_size: float
    :param blender_method: the method used to blend sub-images. 
    :type blender_method: str
    :return: the stitched ERP image
    :type numpy
    """
    if len(tangent_images) != 20:
        log.error("The tangent's images triangle number is {}.".format(len(tangent_images)))

    if len(tangent_images[0].shape) == 3:
        images_channels_number = tangent_images[0].shape[2]
        if images_channels_number == 4:
            log.debug("the face image is RGBA image, convert the output to RGB image.")
            images_channels_number = 3
    elif len(tangent_images[0].shape) == 2:
        log.info("project single channel disp or depth map")
        images_channels_number = 1

    erp_image_width = erp_image_height * 2
    erp_image = np.full([erp_image_height, erp_image_width, images_channels_number], 0, np.float64)

    tangent_image_height = tangent_images[0].shape[0]
    tangent_image_width = tangent_images[0].shape[1]

    erp_weight_mat = np.zeros((erp_image_height, erp_image_width), dtype=np.float64)
    # stitch all tangnet images to ERP image
    for triangle_index in range(0, 20):
        log.debug("stitch the tangent image {}".format(triangle_index))
        triangle_param = get_icosahedron_parameters(triangle_index, padding_size)

        # 1) get all tangent triangle's available pixels coordinate
        availied_ERP_area = triangle_param["availied_ERP_area"]
        erp_image_col_start, erp_image_row_start = sc.sph2erp(availied_ERP_area[0], availied_ERP_area[2], erp_image_height, sph_modulo=False)
        erp_image_col_stop, erp_image_row_stop = sc.sph2erp(availied_ERP_area[1], availied_ERP_area[3], erp_image_height, sph_modulo=False)

        # process the image boundary
        erp_image_col_start = int(erp_image_col_start) if int(erp_image_col_start) > 0 else int(erp_image_col_start - 0.5)
        erp_image_col_stop = int(erp_image_col_stop + 0.5) if int(erp_image_col_stop) > 0 else int(erp_image_col_stop)
        erp_image_row_start = int(erp_image_row_start) if int(erp_image_row_start) > 0 else int(erp_image_row_start - 0.5)
        erp_image_row_stop = int(erp_image_row_stop + 0.5) if int(erp_image_row_stop) > 0 else int(erp_image_row_stop)

        triangle_x_range = np.linspace(erp_image_col_start, erp_image_col_stop, erp_image_col_stop - erp_image_col_start + 1)
        triangle_y_range = np.linspace(erp_image_row_start, erp_image_row_stop, erp_image_row_stop - erp_image_row_start + 1)
        triangle_xv, triangle_yv = np.meshgrid(triangle_x_range, triangle_y_range)
        # process the wrap around
        triangle_xv = np.remainder(triangle_xv, erp_image_width)
        triangle_yv = np.remainder(triangle_yv, erp_image_height)

        # 2) sample the pixel value from tanget image
        # project spherical coordinate to tangent plane
        spherical_uv = sc.erp2sph([triangle_xv, triangle_yv], erp_image_height=erp_image_height, sph_modulo=False)
        theta_0 = triangle_param["tangent_point"][0]
        phi_0 = triangle_param["tangent_point"][1]
        tangent_xv, tangent_yv = gp.gnomonic_projection(spherical_uv[0, :, :], spherical_uv[1, :, :], theta_0, phi_0)

        # the pixels in the tangent triangle
        triangle_points_tangent = np.array(triangle_param["triangle_points_tangent"])
        gnomonic_x_min = np.amin(triangle_points_tangent[:, 0], axis=0)
        gnomonic_x_max = np.amax(triangle_points_tangent[:, 0], axis=0)
        gnomonic_y_min = np.amin(triangle_points_tangent[:, 1], axis=0)
        gnomonic_y_max = np.amax(triangle_points_tangent[:, 1], axis=0)

        tangent_gnomonic_range = [gnomonic_x_min, gnomonic_x_max, gnomonic_y_min, gnomonic_y_max]
        pixel_eps = abs(tangent_xv[0, 0] - tangent_xv[0, 1]) / (2 * tangent_image_width)

        if len(tangent_images[0].shape) == 2:
            tangent_images_subimage = np.expand_dims(tangent_images[triangle_index], axis=2)
        else:
            tangent_images_subimage = tangent_images[triangle_index]

        if blender_method is None:
            available_pixels_list = gp.inside_polygon_2d(np.stack((tangent_xv.flatten(), tangent_yv.flatten()), axis=1),
                                                         triangle_points_tangent, on_line=True, eps=pixel_eps).reshape(tangent_xv.shape)

            # the tangent available gnomonic coordinate sample the pixel from the tangent image
            tangent_xv, tangent_yv = gp.gnomonic2pixel(tangent_xv[available_pixels_list], tangent_yv[available_pixels_list],
                                                       0.0, tangent_image_width, tangent_image_height, tangent_gnomonic_range)

            for channel in range(0, images_channels_number):
                erp_image[triangle_yv[available_pixels_list].astype(np.int32), triangle_xv[available_pixels_list].astype(np.int32), channel] = \
                    ndimage.map_coordinates(tangent_images_subimage[:, :, channel], [tangent_yv, tangent_xv], order=1, mode='constant', cval=255)
        elif blender_method == "mean":
            triangle_points_tangent = [[gnomonic_x_min, gnomonic_y_max],
                                       [gnomonic_x_max, gnomonic_y_max],
                                       [gnomonic_x_max, gnomonic_y_min],
                                       [gnomonic_x_min, gnomonic_y_min]]
            available_pixels_list = gp.inside_polygon_2d(np.stack((tangent_xv.flatten(), tangent_yv.flatten()), axis=1),
                                                         triangle_points_tangent, on_line=True, eps=pixel_eps).reshape(tangent_xv.shape)

            tangent_xv, tangent_yv = gp.gnomonic2pixel(tangent_xv[available_pixels_list], tangent_yv[available_pixels_list],
                                                       0.0, tangent_image_width, tangent_image_height, tangent_gnomonic_range)
            for channel in range(0, images_channels_number):
                erp_face_image = ndimage.map_coordinates(tangent_images_subimage[:, :, channel], [tangent_yv, tangent_xv], order=1, mode='constant', cval=255)
                erp_image[triangle_yv[available_pixels_list].astype(np.int32), triangle_xv[available_pixels_list].astype(np.int32), channel] += erp_face_image.astype(np.float64)

            face_weight_mat = np.ones(erp_face_image.shape, np.float64)
            erp_weight_mat[triangle_yv[available_pixels_list].astype(np.int64), triangle_xv[available_pixels_list].astype(np.int64)] += face_weight_mat

    # compute the final optical flow base on weight
    if blender_method == "mean":
        # erp_flow_weight_mat = np.full(erp_flow_weight_mat.shape, erp_flow_weight_mat.max(), np.float) # debug
        non_zero_weight_list = erp_weight_mat != 0
        if not np.all(non_zero_weight_list):
            log.warn("the optical flow weight matrix contain 0.")
        for channel_index in range(0, images_channels_number):
            erp_image[:, :, channel_index][non_zero_weight_list] = erp_image[:, :, channel_index][non_zero_weight_list] / erp_weight_mat[non_zero_weight_list]

    return erp_image



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/serialization.py
================================================

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

import metrics
import json
import os
import pickle
from logger import Logger
from pathlib import Path

log = Logger(__name__)
log.logger.propagate = False


class NumpyArrayEncoder(json.JSONEncoder):
    """Assistant class for serialize the numpy to json.
    
    Convert numpy to string list.
    """

    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(NumpyArrayEncoder, self).default(obj)


def cam_param_dict2json(camera_param_data, json_file_path):
    """ Save the camera parameters to json file.

    :param camera_param_data: camera parameters.
    :type camera_param_data: dict
    :param json_file_path: output json file's path.
    :type json_file_path: str
    """
    with open(json_file_path, 'w') as fp:
        json.dump(camera_param_data, fp,  cls=NumpyArrayEncoder, indent=4)


def cam_param_json2dict(json_file_path):
    """Load the camera parameters form json file.

    Convert all parameters to numpy array.

    :param json_file_path: the json file path.
    :type json_file_path: str
    :return: camera parameter
    :rtype: dict
    """
    dict_data = None
    with open(json_file_path) as json_file:
        dict_data = json.load(json_file)

    def _cam_param_json2dict(dict_data):
        for key, value in dict_data.items():
            if isinstance(value, list):
                dict_data[key] = np.asarray(value)
            elif isinstance(value, dict):
                dict_data[key] = _cam_param_json2dict(value)
        return dict_data

    # parer dict and translate the list to numpy array
    _cam_param_json2dict(dict_data)

    return dict_data


def save_cam_params(json_file_path, face_index_list, cam_params_list):
    """Save sub-images' camera parameters

    :param face_index_list: The available faces index list.
    :type face_index_list: list
    :param cam_params_list: The 20 faces' camera parameters.
    :type cam_params_list: list
    """
    camera_param_data = {}
    # camera parameter list to dict
    for face_index in face_index_list:
        camera_param_data[face_index] = cam_params_list[face_index]

    # dict to json file
    with open(json_file_path, 'w') as fp:
        json.dump(camera_param_data, fp,  cls=NumpyArrayEncoder, indent=4)


def load_cam_params(json_file_path):
    """Load sub-images; camera parameters form file.
    """
    # load json to dict
    with open(json_file_path) as json_file:
        dict_data = json.load(json_file)

    # dict to list
    cam_params_list = []
    for index in dict_data.keys():
        cam_params_list.append(dict_data[index])

    return cam_params_list


def get_sha256(data):
    """Return a SHA-256 hash of the given data array.

    :param data: the binary data array
    :type data: numpy or str
    """
    if data is None:
        log.warn("get_sha256 input data is None!")
        return None
        
    import hashlib
    if isinstance(data, str):
        return hashlib.sha256(data.encode('utf-8')).hexdigest()
    elif isinstance(data, np.ndarray):
        return hashlib.sha256(data.data).hexdigest()
    else:
        log.error("current do not support hash {}".format(type(data)))


def pixel_corresponding_save(json_file_path,
                             src_image_filename, src_image_data,
                             tar_image_filename, tar_image_data, pixel_corresponding):
    """ The relationship of pixel corresponding.
    The origin point on the top-left of image.

    ```
    {
        "src_image": "001.jpg",
        "src_image_sha256": image_numpy_data_sha256,
        "tar_image": "erp.jpg",
        "tar_image_sha256": image_numpy_data_sha256,
        "pixel_corresponding": [
            [src_row_number_0, src_column_number_0, tar_row_number_0, tar_column_number_0],
            [src_row_number_1, src_column_number_1, tar_row_number_1, tar_column_number_1],
        ]
    }
    ```

    :param json_file_path: output json file's path.
    :type json_file_path: str
    :param src_image_filename: source image filename
    :type src_image_filename: str
    :param tar_image_filename: target image filename
    :type tar_image_filename: str
    :param pixel_corresponding: the pixels corresponding relationship, shape is [corresponding_number, 4]
    :type pixel_corresponding: numpy
    """
    json_data = {}

    json_data["src_image_filename"] = os.path.basename(src_image_filename)
    json_data["src_image_sha256"] = get_sha256(src_image_data)
    json_data["tar_image_filename"] = os.path.basename(tar_image_filename)
    json_data["tar_image_sha256"] = get_sha256(tar_image_data)
    json_data["pixel_corresponding_number"] = pixel_corresponding.shape[0]
    json_data["pixel_corresponding"] = pixel_corresponding

    with open(json_file_path, 'w') as fp:
        json.dump(json_data, fp,  cls=NumpyArrayEncoder, indent=4)


def pixel_corresponding_load(json_file_path):
    """
    Load the pixels corresponding relationship from JSON file.
    """
    dict_data = {}
    with open(json_file_path) as json_file:
        dict_data = json.load(json_file)

    def _cam_param_json2dict(dict_data):
        for key, value in dict_data.items():
            if isinstance(value, list):
                dict_data[key] = np.asarray(value)
            elif isinstance(value, dict):
                dict_data[key] = _cam_param_json2dict(value)
        return dict_data

    # parer dict and translate the list to numpy array
    _cam_param_json2dict(dict_data)

    return dict_data


def save_subimages_data(data_dir, filename_prefix,  subimage_list, cam_param_list, pixels_corr_dict,
                        output_corr2file=True):
    """
    Save all subimages data to file, including image, camera parameters and pixels corresponding.

    :param data_dir: the root directory of output file.
    :type data_dir: str
    :param data_dir: the filename's prefix
    :type data_dir: str
    :param subimage_list: [description]
    :type subimage_list: [type]
    :param cam_param_list: [description]
    :type cam_param_list: [type]
    :param pixels_corr_dict: its structure is {1:{2:np.array, 3:np.array, ....}, 2:{1:array, 3:array, ....}....}
    :type pixels_corr_dict: [type]
    """
    subimage_disp_filepath_expression = filename_prefix + "_disp_{:03d}.pfm"
    subimage_filepath_expression = filename_prefix + "_rgb_{:03d}.jpg"
    subimage_param_filepath_expression = filename_prefix + "_cam_{:03d}.json"
    pixels_corresponding_json_filepath_expression = filename_prefix + "_corr_{:03d}_{:03d}.json"

    subimage_number = len(subimage_list)
    if cam_param_list is None:
        log.warn("Camera parameters is empty!")
    elif len(cam_param_list) != subimage_number:
        log.error("The subimage information is not completetd!")

    for src_image_index in range(0, subimage_number):
        # output subimage
        subimage_filepath = data_dir + subimage_filepath_expression.format(src_image_index)
        Image.fromarray(subimage_list[src_image_index].astype(np.uint8)).save(subimage_filepath)

        log.debug("Output image {} pixel corresponding relationship.".format(src_image_index))

        # output camera parameters
        if cam_param_list is not None:
            camparam_filepath = data_dir + subimage_param_filepath_expression.format(src_image_index)
            cam_param_dict2json(cam_param_list[src_image_index], camparam_filepath)

        # output pixel corresponding
        pixels_corr_list = pixels_corr_dict[src_image_index]
        for ref_image_index in pixels_corr_list.keys():
            pixel_corr_filepath = data_dir + pixels_corresponding_json_filepath_expression.format(src_image_index, ref_image_index)

            subimage_src_filepath = subimage_disp_filepath_expression.format(src_image_index)
            subimage_tar_filepath = subimage_disp_filepath_expression.format(ref_image_index)

            if output_corr2file:
                pixel_corresponding_save(pixel_corr_filepath,
                                         subimage_src_filepath, subimage_list[src_image_index],
                                         subimage_tar_filepath, subimage_list[ref_image_index], pixels_corr_list[ref_image_index])


def load_subimages_data():
    """
    Load all subimage data from file, including image, camera parameters and pixels corresponding.
    """
    pass


def subimage_alignment_params(json_file_path, coeffs_scale, coeffs_offset, submap_index_list):
    """ Save disparity maps alignment coefficients.

    :param json_file_path: Coefficients output json file's path.
    :type json_file_path: str
    :param coeffs_scale: the 20 subimage's scale coefficients list.
    :type coeffs_scale: list
    :param coeffs_offset: the 20 subimage's offset coefficients list.
    :type coeffs_offset: list
    :param submap_index_list: the available subimage's index list.
    :type submap_index_list: list
    """
    if len(coeffs_offset) != len(submap_index_list) or len(coeffs_scale) != len(submap_index_list):
        raise RuntimeError("The alignment coefficient is not ")

    # create coefficients dict
    coeffs_dict = {}
    coeffs_dict["storage_order"] = "row_major"
    for index in range(0, len(submap_index_list)):
        data_term_scale = {}
        data_term_scale["coeff_type"] = "scale"
        data_term_scale["filename"] = "face {} alignment scale matrix".format(submap_index_list[index])
        data_term_scale["mat_width"] = coeffs_scale[index].shape[0]
        data_term_scale["mat_hight"] = coeffs_scale[index].shape[1]
        data_term_scale["mat_data"] = coeffs_scale[index]
        subimage_coeff_mat_name = "coeff_mat_" + str(index * 2)
        coeffs_dict[subimage_coeff_mat_name] = data_term_scale

        data_term_offset = {}
        data_term_offset["coeff_type"] = "offset"
        data_term_offset["filename"] = "face {} alignment offset matrix".format(submap_index_list[index])
        data_term_offset["mat_width"] = coeffs_offset[index].shape[0]
        data_term_offset["mat_hight"] = coeffs_offset[index].shape[1]
        data_term_offset["mat_data"] = coeffs_offset[index]
        subimage_coeff_mat_name = "coeff_mat_" + str(index * 2 + 1)
        coeffs_dict[subimage_coeff_mat_name] = data_term_offset

    # output to json
    with open(json_file_path, 'w') as fp:
        json.dump(coeffs_dict, fp,  cls=NumpyArrayEncoder, indent=4)


def save_dispmapalign_intermediate_data(filepath, file_format, **data):
    """
    Save the data used to align disparity maps to file.

    # TODO support "msgpack" format, which is more safe and secure.
    
    :param filepath: the output file's path.
    :type filepath: str
    :param file_format: the output file format, "pickle", "msg"
    :type file_format: str
    :param data: the data to be serialized.
    :type data: dict
    """
    if file_format == "pickle":
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
    else:
        raise RuntimeError(f"File format '{file_format}' is not supported")


def load_dispmapalign_intermediate_data(filepath, file_format):
    """
    Load the from disk to align disparity maps to file.

    :param filepath: the output file's path.
    :type filepath: str
    :param file_format: the output file format, "pickle", "msg".
    :type file_format: str
    """
    if file_format == "pickle":
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    else:
        raise RuntimeError(f"File format '{file_format}' is not supported")


def save_metrics(output_file, pred_metrics, times, times_header, idx, blending_methods):
    if idx == 0:
        with open(output_file, "w") as f:
            f.write(','.join(list(pred_metrics[0].keys()) + times_header))
            f.write("\n")
        f.close()

    with open(output_file, "a") as f:
        for idx, key in enumerate(blending_methods):
            f.write(','.join(list(np.array(list(pred_metrics[idx].values())).astype(str)) + [str(t) for t in times]))
            f.write("\n")
    f.close()


def save_predictions(output_folder, erp_image_filename, erp_gt_depthmap, erp_rgb_image_data, estimated_depthmap, persp_monodepth, idx=0):
    # Plot error maps
    vmax = None
    vmin = None
    if erp_gt_depthmap is not None:
        mask = (erp_gt_depthmap > 0) & (~np.isinf(erp_gt_depthmap)) & (~np.isnan(erp_gt_depthmap)) & (erp_gt_depthmap <= 10)
        vmax = 10
        vmin = 0

    for key in estimated_depthmap.keys():
        if erp_gt_depthmap is not None:
            pred = metrics.pred2gt_least_squares(estimated_depthmap[key], erp_gt_depthmap, mask)
        else:
            pred = estimated_depthmap[key]

        np.save(os.path.join(output_folder, Path(erp_image_filename).stem+'.npy'), pred)
        plt.imsave(os.path.join(output_folder, "{:03}_360monodepth_{}_{}.png".format(idx, persp_monodepth, key)),
                   pred, cmap="turbo", vmin=vmin, vmax=vmax)

    #plt.imsave(os.path.join(output_folder, "{:03}_GT.png".format(idx)),
               #erp_gt_depthmap, vmin=vmin, vmax=vmax, cmap="turbo")
    #plt.imsave(os.path.join(output_folder, "{:03}_rgb.png".format(idx)), erp_rgb_image_data)

    # metrics.visualize_error_maps(pred, erp_gt_depthmap, mask, idx=idx,
    #                              save=True, input=erp_rgb_image_data,
    #                              filename="{}/{}".format(opt.experiment_name, key))




================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/spherical_coordinates.py
================================================
import numpy as np

from logger import Logger

log = Logger(__name__)
log.logger.propagate = False

def great_circle_distance_uv(points_1_theta, points_1_phi, points_2_theta, points_2_phi, radius=1):
    """
    @see great_circle_distance (haversine distances )
    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html

    :param points_1_theta: theta in radians
    :type points_1_theta : numpy
    :param points_1_phi: phi in radians
    :type points_1_phi : numpy
    :param points_2_theta: radians
    :type points_2_theta: float
    :param points_2_phi: radians
    :type points_2_phi: float
    :return: The geodestic distance from point ot tangent point.
    :rtype: numpy
    """
    delta_theta = points_2_theta - points_1_theta
    delta_phi = points_2_phi - points_1_phi
    a = np.sin(delta_phi * 0.5) ** 2 + np.cos(points_1_phi) * np.cos(points_2_phi) * np.sin(delta_theta * 0.5) ** 2
    central_angle_delta = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))

    if np.isnan(central_angle_delta).any():
        log.warn("the circle angle have NAN")

    return np.abs(radius * central_angle_delta)


def erp_pixel_modulo_0(erp_points_list, image_height):
    """[summary]

    :param erp_points_list: The erp pixel list, [2, points_number]
    :type erp_points_list: numpy
    :param image_height: erp image height
    :type image_height: numpy
    """
    x = erp_points_list[0,:]
    y = erp_points_list[1,:]
    x, y = erp_pixel_modulo(x, y , image_height)
    return np.stack((x,y), axis=0)


def erp_pixel_modulo(x_arrray, y_array, image_height):
    """ Make x,y and ERP pixels coordinate system range.
    """
    image_width = 2 * image_height
    x_arrray_new = np.remainder(x_arrray + 0.5, image_width) - 0.5
    y_array_new = np.remainder(y_array + 0.5, image_height) - 0.5
    return x_arrray_new, y_array_new


def erp_sph_modulo(theta, phi):
    """Modulo of the spherical coordinate for the erp coordinate.
    """
    points_theta = np.remainder(theta + np.pi, 2 * np.pi) - np.pi
    points_phi = -(np.remainder(-phi + 0.5 * np.pi, np.pi) - 0.5 * np.pi)
    return points_theta, points_phi


def erp2sph(erp_points, erp_image_height=None, sph_modulo=False):
    """
    convert the point from erp image pixel location to spherical coordinate.
    The image center is spherical coordinate origin.

    :param erp_points: the point location in ERP image x∊[0, width-1], y∊[0, height-1] , size is [2, :]
    :type erp_points: numpy
    :param erp_image_height: ERP image's height, defaults to None
    :type erp_image_height: int, optional
    :param sph_modulo: if true, process the input points wrap around, .
    :type sph_modulo: bool
    :return: the spherical coordinate points, theta is in the range [-pi, +pi), and phi is in the range [-pi/2, pi/2)
    :rtype: numpy
    """
    # 0) the ERP image size
    if erp_image_height == None:
        height = np.shape(erp_points)[1]
        width = np.shape(erp_points)[2]

        if (height * 2) != width:
            log.error("the ERP image width {} is not two time of height {}".format(width, height))
    else:
        height = erp_image_height
        width = height * 2

    erp_points_x = erp_points[0]
    erp_points_y = erp_points[1]

    # 1) point location to theta and phi
    points_theta = erp_points_x * (2 * np.pi / width) + np.pi / width - np.pi
    points_phi = -(erp_points_y * (np.pi / height) + np.pi / height * 0.5) + 0.5 * np.pi

    if sph_modulo:
        points_theta, points_phi = erp_sph_modulo(points_theta, points_phi)

    points_theta = np.where(points_theta == np.pi,  -np.pi, points_theta)
    points_phi = np.where(points_phi == -0.5 * np.pi, 0.5 * np.pi, points_phi)

    return np.stack((points_theta, points_phi))


def sph2erp_0(sph_points, erp_image_height=None, sph_modulo=False):
    theta = sph_points[0, :]
    phi = sph_points[1, :]
    erp_x, erp_y = sph2erp(theta, phi, erp_image_height, sph_modulo)
    return np.stack((erp_x, erp_y), axis=0)


def sph2erp(theta, phi, erp_image_height, sph_modulo=False):
    """ 
    Transform the spherical coordinate location to ERP image pixel location.

    :param theta: longitude is radian
    :type theta: numpy
    :param phi: latitude is radian
    :type phi: numpy
    :param image_height: the height of the ERP image. the image width is 2 times of image height
    :type image_height: [type]
    :param sph_modulo: if yes process the wrap around case, if no do not.
    :type sph_modulo: bool, optional
    :return: the pixel location in the ERP image.
    :rtype: numpy
    """
    if sph_modulo:
        theta, phi = erp_sph_modulo(theta, phi)

    erp_image_width = 2 * erp_image_height
    erp_x = (theta + np.pi) / (2.0 * np.pi / erp_image_width) - 0.5
    erp_y = (-phi + 0.5 * np.pi) / (np.pi / erp_image_height) - 0.5
    return erp_x, erp_y


def car2sph(points_car, min_radius=1e-10):
    """
    Transform the 3D point from cartesian to unit spherical coordinate.

    :param points_car: The 3D point array, is [point_number, 3], first column is x, second is y, third is z
    :type points_car: numpy
    :return: the points spherical coordinate, (theta, phi)
    :rtype: numpy
    """
    radius = np.linalg.norm(points_car, axis=1)

    valid_list = radius > min_radius  # set the 0 radius to origin.

    theta = np.zeros((points_car.shape[0]), np.float64)
    theta[valid_list] = np.arctan2(points_car[:, 0][valid_list], points_car[:, 2][valid_list])

    phi = np.zeros((points_car.shape[0]), np.float64)
    phi[valid_list] = -np.arcsin(np.divide(points_car[:, 1][valid_list], radius[valid_list]))

    return np.stack((theta, phi), axis=1)


def sph2car(theta, phi, radius=1.0):
    """
    Transform the spherical coordinate to cartesian 3D point.

    :param theta: longitude
    :type theta: numpy
    :param phi: latitude
    :type phi: numpy
    :param radius: the radius of projection sphere
    :type radius: float
    :return: +x right, +y down, +z is froward, shape is [3, point_number]
    :rtype: numpy
    """
    # points_cartesian_3d = np.array.zeros((theta.shape[0],3),np.float)
    x = radius * np.cos(phi) * np.sin(theta)
    z = radius * np.cos(phi) * np.cos(theta)
    y = -radius * np.sin(phi)

    return np.stack((x, y, z), axis=0)



================================================
FILE: depth-estimation/360monodepth/code/python/src/utility/subimage.py
================================================
import spherical_coordinates
import projection_icosahedron as proj_ico
import gnomonic_projection as gp

from scipy.spatial.transform import Rotation as R
from PIL import Image, ImageDraw
import numpy as np
from colorsys import hsv_to_rgb

from logger import Logger

log = Logger(__name__)
log.logger.propagate = False


def draw_corresponding(src_image_data, tar_image_data, pixel_corresponding_array):
    """
    Visualized the pixel corresponding relationship.

    :param src_image_data: source image data
    :type src_image_data: numpy
    :param tar_image_data: target image data
    :type tar_image_data: numpy
    :param pixel_corresponding_array: the pixel corresponding from source, size is [4, :]
    :type pixel_corresponding_array: numpy 
    :return: the marked source and target image, and warped source image
    """
    # 0) prepare the canvas
    src_image_data_image = Image.fromarray(src_image_data.astype(np.uint8))
    # convert the gray image to rgb image
    if len(src_image_data.shape) == 2:
        src_image_data_image =  src_image_data_image.convert("RGB")
    src_image_draw = ImageDraw.Draw(src_image_data_image)

    tar_image_data_image = Image.fromarray(tar_image_data.astype(np.uint8))
    # convert the gray image to rgb image
    if len(src_image_data.shape) == 2:
        tar_image_data_image =  tar_image_data_image.convert("RGB")
    tar_image_draw = ImageDraw.Draw(tar_image_data_image)

    # the corresponding relationship is empty
    if pixel_corresponding_array.size == 0:
        log.warn("The pixel corresponding is empty!")
        src_warp_image = np.zeros_like(src_image_data_image)
        return np.asarray(src_image_data_image), np.asarray(tar_image_data_image), np.asarray(src_warp_image)

    # 1) draw corresponding pixels in source and target images
    index = 0
    eX, eY = 3, 3  # Size of Bounding Box for ellips
    minval = 0
    maxval = pixel_corresponding_array.shape[0] + 10

    for match_pair in pixel_corresponding_array:
        # if index % 100 == 0:
        #     print("plot corresponding {}".format(index))

        # r = index % 256
        # g = int(index % (256 ** 2) / 256)
        # b = int((index % (256 ** 3)) / 256 ** 2)

        h = (float(index-minval) / (maxval-minval)) * 120.0
        # Convert hsv color (h,1,1) to its rgb equivalent.
        # Note: hsv_to_rgb() function expects h to be in the range 0..1 not 0..360
        r, g, b = (int(i * 255) for i in hsv_to_rgb(h/360.0, 1., 1.))

        y = match_pair[0]
        x = match_pair[1]
        bbox = (x - eX, y - eY, x + eX, y + eY)
        src_image_draw.ellipse(bbox, fill=(r, g, b))

        y = match_pair[2]
        x = match_pair[3]
        bbox = (x - eX, y - eY, x + eX, y + eY)
        tar_image_draw.ellipse(bbox, fill=(r, g, b))

        index += 1

    del src_image_draw
    del tar_image_draw

    src_image_data_image_np = np.asarray(src_image_data_image)
    tar_image_data_image_np = np.asarray(tar_image_data_image)

    # 2) warp src image
    src_warp_image = np.zeros(src_image_data.shape, src_image_data.dtype)
    pixel_corresponding_array_temp = pixel_corresponding_array.astype(np.int32)
    src_y = pixel_corresponding_array_temp[:, 0]
    src_x = pixel_corresponding_array_temp[:, 1]
    tar_y = pixel_corresponding_array_temp[:, 2]
    tar_x = pixel_corresponding_array_temp[:, 3]
    from scipy import ndimage
    for channel in range(0, 3):
        src_warp_image[tar_y, tar_x, channel] = \
            ndimage.map_coordinates(src_image_data[:, :, channel], [src_y, src_x], order=1, mode='constant')

    return src_image_data_image_np, tar_image_data_image_np, src_warp_image.astype(np.uint8)


def tangent_image_resolution(erp_image_width, padding_size):
    """Get the the suggest tangent image resolution base on the FoV.

    :param erp_image_width: [description]
    :type erp_image_width: [type]
    :param padding_size: [description]
    :type padding_size: [type]
    :return: recommended tangent image size in pixel.
    :rtype: int
    """
    # camera intrinsic parameters
    ico_param_list = proj_ico.get_icosahedron_parameters(7, padding_size)
    triangle_points_tangent = ico_param_list["triangle_points_tangent"]
    # compute the tangent image resoution.
    tangent_points_x_min = np.amin(np.array(triangle_points_tangent)[:, 0])
    fov_h = np.abs(2 * np.arctan2(tangent_points_x_min, 1.0))
    tangent_image_width = erp_image_width * (fov_h / (2 * np.pi))
    tangent_image_height = 0.5 * tangent_image_width / np.tan(np.radians(30.0))
    return int(tangent_image_width + 0.5), int(tangent_image_height + 0.5)


def erp_ico_cam_intrparams(image_width, padding_size=0):
    """    
    Compuate the camera intrinsic parameters for 20 faces of icosahedron.
    It does not need camera parameters.

    :param image_width: Tangent image's width, the image height derive from image ratio.
    :type image_width: int
    :param padding_size: The tangent face padding size, defaults to 0
    :type padding_size: float, optional
    :return: 20 faces camera parameters.
    :rtype: list
    """
    # camera intrinsic parameters
    ico_param_list = proj_ico.get_icosahedron_parameters(7, padding_size)
    tangent_point = ico_param_list["tangent_point"]
    triangle_points_tangent = ico_param_list["triangle_points_tangent"]

    # use tangent plane
    tangent_points_x_min = np.amin(np.array(triangle_points_tangent)[:, 0])
    tangent_points_y_min = np.amin(np.array(triangle_points_tangent)[:, 1])
    tangent_points_y_max = np.amax(np.array(triangle_points_tangent)[:, 1])
    fov_v = np.abs(np.arctan2(tangent_points_y_min, 1.0)) + np.abs(np.arctan2(tangent_points_y_max, 1.0))
    fov_h = np.abs(2 * np.arctan2(tangent_points_x_min, 1.0))

    log.debug("Pin-hole camera fov_h: {}, fov_v: {}".format(np.degrees(fov_h), np.degrees(fov_v)))

    # image aspect ratio, the triangle is equilateral triangle
    image_height = 0.5 * image_width / np.tan(np.radians(30.0))
    fx = 0.5 * image_width / np.tan(fov_h * 0.5)
    fy = 0.5 * image_height / np.tan(fov_v * 0.5)

    cx = (image_width - 1) / 2.0
    # invert and upright triangle cy
    cy_invert = 0.5 * (image_width - 1.0) * np.tan(np.radians(30.0)) + 10.0
    cy_up = 0.5 * (image_width - 1.0) / np.sin(np.radians(60.0)) + 10.0

    subimage_cam_param_list = []
    for index in range(0, 20):
        # intrinsic parameters
        cy = None
        if 0 <= index <= 4:
            cy = cy_up
        elif 5 <= index <= 9:
            cy = cy_invert
        elif 10 <= index <= 14:
            cy = cy_up
        else:
            cy = cy_invert

        intrinsic_matrix = np.array([[fx, 0, cx],
                                     [0, fy, cy],
                                     [0, 0, 1]])

        # rotation
        ico_param_list = proj_ico.get_icosahedron_parameters(index, padding_size)
        tangent_point = ico_param_list["tangent_point"]
        # print(tangent_point)
        rot_y = tangent_point[0]
        rot_x = tangent_point[1]
        rotation = R.from_euler("zyx", [0.0, -rot_y, -rot_x], degrees=False)
        rotation_mat = rotation.as_matrix()

        params = {'rotation': rotation_mat,
                  'translation': np.array([0, 0, 0]),
                  'intrinsics': {
                      'image_width': image_width,
                      'image_height': image_height,
                      'focal_length_x': fx,
                      'focal_length_y': fy,
                      'principal_point': [cx, cy],
                      'matrix': intrinsic_matrix}
                  }

        subimage_cam_param_list.append(params)

    return subimage_cam_param_list


def erp_ico_pixel_corr(subimage_sphcoor, next_tangent_point, padding_size, tangent_image_width, tangent_image_height, tangent_triangle_vertices_gnom):
    """
    Get the corresponding point between two Ico's face.
    And return the pixel corresponding relationship, 
    [current_pixel_y, current_pixel_x, target_pixel_y, target_pixel_x]

    :param subimage_sphcoor: source subimage each pixel's spherical coordinate.
    :type subimage_sphcoor: numpy
    :param next_tangent_point: the target subimage's tangent point.
    :param tangent_image_width: 
    :param tangent_image_height:
    :param tangent_triangle_vertices_gnom: the 3 vertexes of tangent plane.
    :type tangent_triangle_vertices_gnom: list
    """
    gnom_x, gnom_y = gp.gnomonic_projection(subimage_sphcoor[0, :], subimage_sphcoor[1, :], next_tangent_point[0], next_tangent_point[1])

    # 1)remove the pixels on the another hemisphere, just use the pixel in the same hemisphere.
    gnomonic_x_min = np.amin(tangent_triangle_vertices_gnom[:, 0], axis=0)
    gnomonic_x_max = np.amax(tangent_triangle_vertices_gnom[:, 0], axis=0)
    gnomonic_y_min = np.amin(tangent_triangle_vertices_gnom[:, 1], axis=0)
    gnomonic_y_max = np.amax(tangent_triangle_vertices_gnom[:, 1], axis=0)
    tangent_gnomonic_range = [gnomonic_x_min, gnomonic_x_max, gnomonic_y_min, gnomonic_y_max]
    gnom_image_x, gnom_image_y = gp.gnomonic2pixel(gnom_x, gnom_y, 0.0, tangent_image_width, tangent_image_height, tangent_gnomonic_range)

    points_2_theta = np.full(subimage_sphcoor[0, :].shape, next_tangent_point[0], np.float64)
    points_2_phi = np.full(subimage_sphcoor[0, :].shape, next_tangent_point[1], np.float64)
    central_angle_delta = \
        spherical_coordinates.great_circle_distance_uv(subimage_sphcoor[0, :], subimage_sphcoor[1, :], points_2_theta, points_2_phi, radius=1)

    valid_pixel_index = np.logical_and.reduce((
        gnom_image_x >= 0, gnom_image_x < tangent_image_width,
        gnom_image_y >= 0, gnom_image_y < tangent_image_height,
        np.abs(central_angle_delta) < 0.5 * np.pi))

    # 2) get the src ant tar's subimage pixel coordinate
    src_subimage_x = np.linspace(0, tangent_image_width, tangent_image_width, endpoint=False)
    src_subimage_y = np.linspace(0, tangent_image_height, tangent_image_height, endpoint=False)
    src_subimage_xv, src_subimage_yv = np.meshgrid(src_subimage_x, src_subimage_y)
    pixel_index_src = np.stack((src_subimage_yv[valid_pixel_index], src_subimage_xv[valid_pixel_index])).T
    pixel_index_tar = np.stack((gnom_image_y[valid_pixel_index], gnom_image_x[valid_pixel_index])).T

    # return pixels spherical coordinate 
    pixels_sph = subimage_sphcoor[:, valid_pixel_index]

    return np.hstack((pixel_index_src, pixel_index_tar)), pixels_sph


def erp_ico_proj(erp_image, padding_size, tangent_image_width, corr_downsample_factor, opt = None):
    """
    Using Icosahedron sample the ERP image to generate subimage, pixel corresponding and camera parameter.
    """
    if corr_downsample_factor != 1.0:
        log.info("Down sample the pixels corresponding, keep {}%.".format(corr_downsample_factor * 100))
        
    # 0) generate subimage
    subimage_list, subimage_sphcoor_list, _ = proj_ico.erp2ico_image(erp_image, tangent_image_width, padding_size, full_face_image=True)
    tangent_image_height = subimage_list[0].shape[0]

    # 1) compute current image overlap are with others subimage
    pixels_corr_dict = {}
    ico_param_list = []
    for index in range(0, len(subimage_list)):
        ico_param_list.append(proj_ico.get_icosahedron_parameters(index, padding_size))

    # set the matterport dataset flag
    if opt is None:
        matterport_hexagon_mask_enable = False
        matterport_hexagon_circumradius = -1
    else:
        matterport_hexagon_mask_enable = opt.dataset_matterport_hexagon_mask_enable
        erp_image_height = erp_image.shape[0]
        matterport_circle_phi = np.deg2rad(opt.dataset_matterport_blur_area_height * (180.0 / erp_image_height))
        matterport_hexagon_circumradius = np.tan(matterport_circle_phi)
        log.info(f"The image height is {erp_image_height}, margin height is {opt.dataset_matterport_blur_area_height}, circumradius is {matterport_hexagon_circumradius}")
        matterport_blurarea_shape = opt.dataset_matterport_blurarea_shape   # "hexagon",  "circle"


    # source subimage index
    def subimage_corr_fun(ico_param_list, pixels_corr_dict, subimage_index_src):
        subimage_sphcoor = subimage_sphcoor_list[subimage_index_src]

        # target subimage index, and compute the corresponding computing with multi-threads
        pixels_corr_dict_subimage = {}
        for subimage_index_tar in range(0, len(subimage_list)):
            # find the corresponding pixel from all subimage
            if subimage_index_src == subimage_index_tar:
                pixels_corr_dict_subimage[subimage_index_tar] = np.empty(shape=(0, 0))
                continue
            # compute the corresponding
            ico_param = ico_param_list[subimage_index_tar]
            heightbout_tangent_point = ico_param["tangent_point"]
            tangent_triangle_vertices_gnom = np.array(ico_param["triangle_points_tangent"])
            pixels_corr_src2tar, pixels_sph = erp_ico_pixel_corr(
                subimage_sphcoor, heightbout_tangent_point, padding_size, tangent_image_width, tangent_image_height, tangent_triangle_vertices_gnom)
            pixels_corr_src2tar = pixels_corr_src2tar.astype(np.float64)

            # remove the pixel at top and bottom
            if matterport_hexagon_mask_enable and \
                ((0 <= subimage_index_src <= 4 and 0 <= subimage_index_tar <= 4)
                 or (15 <= subimage_index_src <= 19 and 15 <= subimage_index_tar <= 19)):

                # 1) get the src and tar pixel coordinate on the top/bottom tangent image
                if 0 <= subimage_index_src <= 4:
                    hexagon_subimage_tangent_point_theta = 0.0
                    hexagon_subimage_tangent_point_phi = 0.5 * np.pi
                elif 15 <= subimage_index_tar <= 19:
                    hexagon_subimage_tangent_point_theta = 0.0
                    hexagon_subimage_tangent_point_phi = -0.5 * np.pi

                if matterport_blurarea_shape == "hexagon":
                    import polygon
                    # project to top or bottom tangent image
                    pixels_gnom_xv, pixels_gnom_yv = \
                        gp.gnomonic_projection(pixels_sph[0, :], pixels_sph[1, :], hexagon_subimage_tangent_point_theta, hexagon_subimage_tangent_point_phi)
                    pixels_corr_gnom = np.stack((pixels_gnom_xv, pixels_gnom_yv), axis=1)

                    # check if the pixels in the hexagon
                    hexagon_points_list = polygon.generate_hexagon(matterport_hexagon_circumradius)
                    inside_hexagon = gp.inside_polygon_2d(pixels_corr_gnom, hexagon_points_list)
                    outside_hexagon = np.logical_not(inside_hexagon)
                    pixels_corr_src2tar = pixels_corr_src2tar[outside_hexagon]
                elif matterport_blurarea_shape == "circle":
                    # import ipdb; ipdb.set_trace()
                    # outside_circle = np.logical_and(pixels_sph[1, :] <= np.pi-matterport_circle_phi, pixels_sph[1, :] >= matterport_circle_phi)
                    outside_circle = np.logical_and(pixels_sph[1, :] <= np.pi * 0.5 - matterport_circle_phi, pixels_sph[1, :] >= - np.pi * 0.5 + matterport_circle_phi)
                    pixels_corr_src2tar = pixels_corr_src2tar[outside_circle]

            # assign the value
            pixels_corr_dict_subimage[subimage_index_tar] = pixels_corr_src2tar

            # down-sample the pixel corresponding relationship
            if corr_downsample_factor != 1.0 and pixels_corr_dict_subimage[subimage_index_tar] is not None:
                corr_number = pixels_corr_dict_subimage[subimage_index_tar].shape[0]
                corr_index = np.linspace(0, corr_number -1, num = int(corr_number * corr_downsample_factor)).astype(np.int32)
                corr_index = np.unique(corr_index)
                pixels_corr_dict_subimage[subimage_index_tar]  = pixels_corr_dict_subimage[subimage_index_tar][corr_index,:]
            
        pixels_corr_dict[subimage_index_src] = pixels_corr_dict_subimage
        return subimage_index_src

    # get the corresponding relationship with multi-thread
    from concurrent.futures import ThreadPoolExecutor, as_completed
    with ThreadPoolExecutor(max_workers=opt.dispalign_corr_thread_number) as executor:
        log.debug("Start generating imagepixels corresponding:")
        feature_list = []
        for subimage_index in range(0, len(subimage_list)):
            # log.debug("Generate image {} pixels corresponding: start ".format(subimage_index))
            feature_list.append(executor.submit(subimage_corr_fun, ico_param_list, pixels_corr_dict, subimage_index))

        for future in as_completed(feature_list):
            try:
                result = future.result()
            except Exception as exc:
                print('Computing subimage {} pixels corresponding error! Exception: {}' % (result, exc))
            else:
                log.debug("Generate image {} pixels corresponding: done ".format(result))

    # 2) camera parameters
    subimage_cam_param_list = erp_ico_cam_intrparams(tangent_image_width, padding_size)
    return subimage_list, subimage_cam_param_list, pixels_corr_dict


def erp_ico_stitch(subimage_list, erp_image_height, padding_size):
    """
    Stitch the Ico's subimages to ERP image.
    """
    # the 'mean' is use linear blend
    erp_image = proj_ico.ico2erp_image(subimage_list, erp_image_height, padding_size, "mean")
    return erp_image


def erp_ico_draw_corresponding(src_image_data, tar_image_data, pixel_corresponding_array,
                               src_image_output_path, tar_image_output_path):
    src_image_data_np = np.array(src_image_data)
    # 0) draw corresponding,
    if isinstance(src_image_data, np.ndarray):
        src_image_data_image = Image.fromarray(src_image_data)
    else:
        src_image_data_image = src_image_data
    if isinstance(tar_image_data, np.ndarray):
        tar_image_data_image = Image.fromarray(tar_image_data)
    else:
        tar_image_data_image = tar_image_data

    src_image_draw = ImageDraw.Draw(src_image_data_image)
    tar_image_draw = ImageDraw.Draw(tar_image_data_image)

    index = 0
    eX, eY = 3, 3  # Size of Bounding Box for ellips
    for match_pair in pixel_corresponding_array:
        # if index % 100 == 0:
        #     print("plot corresponding {}".format(index))

        y = match_pair[0]
        x = match_pair[1]
        bbox = (x - eX, y - eY, x + eX, y + eY)
        src_image_draw.ellipse(bbox, fill=(255, 0, 0))

        y = match_pair[2]
        x = match_pair[3]
        bbox = (x - eX, y - eY, x + eX, y + eY)
        tar_image_draw.ellipse(bbox, fill=(255, 0, 0))

        index += 1

    del src_image_draw
    del tar_image_draw

    src_image_data_image.save(src_image_output_path)
    print(src_image_output_path)
    tar_image_data_image.save(tar_image_output_path)
    print(tar_image_output_path)

    # 1) warp src image
    src_warp = np.zeros(src_image_data_np.shape, src_image_data_np.dtype)
    pixel_corresponding_array_temp = pixel_corresponding_array.astype(np.int32)
    src_y = pixel_corresponding_array_temp[:, 0]
    src_x = pixel_corresponding_array_temp[:, 1]
    tar_y = pixel_corresponding_array_temp[:, 2]
    tar_x = pixel_corresponding_array_temp[:, 3]
    from scipy import ndimage
    for channel in range(0, 3):
        src_warp[tar_y, tar_x, channel] = \
            ndimage.map_coordinates(src_image_data_np[:, :, channel], [src_y, src_x], order=1, mode='constant')

    src_warp_image = Image.fromarray(src_warp)
    src_warp_image.save(src_image_output_path + "_features_warp.jpg")



================================================
FILE: depth-estimation/360monodepth/data/erp_00_data.txt
================================================
../../../data/erp_00/0001_rgb.jpg ../../../data/erp_00/0001_depth.dpt


================================================
FILE: depth-estimation/360monodepth/data/erp_00/0001_depth.dpt
================================================
[Non-text file]



================================================
FILE: docs/demo.html
================================================
<!DOCTYPE html>
<html>
<head>
<title>3D Pano Inpainting: Building a VR Environment from a Single Input Panorama</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
<style>
body {
  font-family: -apple-system, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;

margin: 0 auto;
padding: 30px 20px 50px;
max-width: 800px;
}
h1 { 
text-align: center;
}
h2 { 
text-align: center;
}
p{
    text-align: justify;
}

.authors {
text-align: center;
}

.video-container{
    width:100%;
    position: static;
    text-align:center;
}

.links {
text-align: center;
color: #3273dc;
}
a:link {
color: #3273dc;
}

.thumbnails {
    padding: 10px;
    width: 100%;
    margin: auto;
}

.thumbnail {
    width: 100%;
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.082);
    margin: 0px auto 25px;
    border-radius: 10px;
    overflow: hidden;
}

.thumbnail:hover {
    filter: brightness(50%);
}


.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}

.button-wrapper {
    display: flex;
    background:white;

}

figcaption {

  text-align: center;
}



.button-wrapper button {
    padding: 10px 15px;
    font-size: 16px;
    font-family: inherit;
    margin:0;
    background-color: white;
    color: #3273dc;
    border: 1px solid rgb(241, 241, 241);
    width: 100%;
    cursor: pointer;

}
.button-wrapper button:nth-child(1) {
    border-bottom-left-radius: 10px;
}

.button-wrapper button:nth-child(3) {
    border-bottom-right-radius: 10px;
}

.item {
    width: 100%;
    display: block;
    align-items: center;
    justify-content: center;
  
    /* border: 1px solid; */
}
.carousel{
    overflow: hidden;
}


</style>
</head>
<body>
    <h1 class="title">3D Pano Inpainting: Building a VR Environment from a Single Input Panorama</h1>
    <p class=authors>Shivam Ajisa<sup>1</sup>,&nbsp
        Edward Du<sup>1</sup>,&nbsp
        Nam Nguyen<sup>1</sup>,&nbsp
        Stefanie Zollmann<sup>2</sup>,&nbsp
        Jonathan Ventura<sup>1</sup></p>
        <p class=authors>
            <sup>1</sup>California Polytechnic State University, San Luis Obispo, CA, USA<br>
            <sup>2</sup>University of Otago, Dunedin, New Zealand<br><br>
        <h3 class="links is-size-5">
                <a href="assets/abstract.pdf">IEEE VR 2024 Poster Abstract</a>  |
                <!-- <a href="https://youtu.be/AMSUpCLFoTk">Video</a>  | -->
                <a href="https://github.com/jonathanventura/3d-pano-inpainting">Code</a>
        </h2>    
            <hr>
           <br>
           <h2 class="title is-4">Real-time VR Interactive Demo</h2>
           <p>
            You can explore the output mesh generated by our method using our VR viewer. For the best experience, we recommend using Google Chrome on desktop or a compatible VR headset. Our viewer has been thoroughly tested on an Oculus Quest 2 that allows seamless exploration of walkable spaces. You can also explore the space by swiping with your mouse or using touch.
           </p>
           <br>

           <h2 class="title is-4">2K Resolution</h2>

           <section class="section">
			<div class="container" style="width: 100%; margin-left: -1.0rem;margin-top: -2.0rem">
				<!-- Start Carousel -->
				<div id="carousel-demo" class="carousel">
					<div class="thumbnail">
                        <a href="renderer-uv.html?scene=ascheberg_kirchplatz&res=2k">
                            <img src="assets/mesh_thumbnails/ascheberg_kirchplatz.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Kirchplatz, Ascheberg</figcaption>
                            <figcaption>Germany</figcaption>
                          
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=basel_martinsgasse&res=2k">
                            <img src="assets/mesh_thumbnails/basel_martinsgasse.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Martinsgasse, Basel</figcaption>
                            <figcaption>Switzerland</figcaption>
                           
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=basel_stapfelberg&res=2k">
                            <img src="assets/mesh_thumbnails/basel_stapfelberg.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Stapfelberg, Basel</figcaption>
                            <figcaption>Switzerland</figcaption>
                           
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=braunschweig_altstadt&res=2k">
                            <img src="assets/mesh_thumbnails/braunschweig_altstadt.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Braunschweig, Altstadt</figcaption>
                            <figcaption>Germany</figcaption>
                         
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=halde_zollern_nebel&res=2k">
                            <img src="assets/mesh_thumbnails/halde_zollern_nebel.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Halde Zollern, Dortmund</figcaption>
                            <figcaption>Germany</figcaption>
                         
					</div>

				</div>
                
				<!-- End Carousel -->
			</div>
		   </section>

           <h2 class="title is-4">4K Resolution</h2>

           <section class="section">
			<div class="container" style="width: 100%; margin-left: -1.0rem;margin-top: -2.0rem">
				<!-- Start Carousel -->
				<div id="carousel-demo" class="carousel">
					<div class="thumbnail">
                        <a href="renderer-uv.html?scene=ascheberg_kirchplatz&res=4k">
                            <img src="assets/mesh_thumbnails/ascheberg_kirchplatz.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Kirchplatz, Ascheberg</figcaption>
                            <figcaption>Germany</figcaption>
                          
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=basel_martinsgasse&res=4k">
                            <img src="assets/mesh_thumbnails/basel_martinsgasse.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Martinsgasse, Basel</figcaption>
                            <figcaption>Switzerland</figcaption>
                           
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=basel_stapfelberg&res=4k">
                            <img src="assets/mesh_thumbnails/basel_stapfelberg.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Stapfelberg, Basel</figcaption>
                            <figcaption>Switzerland</figcaption>
                           
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=braunschweig_altstadt&res=4k">
                            <img src="assets/mesh_thumbnails/braunschweig_altstadt.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Braunschweig, Altstadt</figcaption>
                            <figcaption>Germany</figcaption>
                         
					</div>
                    <div class="thumbnail">
                        <a href="renderer-uv.html?scene=halde_zollern_nebel&res=4k">
                            <img src="assets/mesh_thumbnails/halde_zollern_nebel.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Halde Zollern, Dortmund</figcaption>
                            <figcaption>Germany</figcaption>
                         
					</div>

				</div>
                
				<!-- End Carousel -->
			</div>
		   </section>

		<script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
		<script>
		bulmaCarousel.attach('#carousel-demo', {
			slidesToScroll: 1,
			slidesToShow: 3,
            initialSlide: 0
            
		});
		</script>



        
</body>



================================================
FILE: docs/index.html
================================================
<!DOCTYPE html>
<html>
<head>
<title>3D Pano Inpainting: Building a VR Environment from a Single Input Panorama</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
<style>
body {
  font-family: -apple-system, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;

margin: 0 auto;
padding: 30px 20px 50px;
max-width: 800px;
}
h1 { 
text-align: center;
}
h2 { 
text-align: center;
}
p{
    text-align: justify;
}

.authors {
text-align: center;
}

.video-container{
    width:100%;
    position: static;
    text-align:center;
}

.links {
text-align: center;
color: #3273dc;
}
a:link {
color: #3273dc;
}

.thumbnails {
    padding: 10px;
    width: 100%;
    margin: auto;
}

.thumbnail {
    width: 100%;
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.082);
    margin: 0px auto 25px;
    border-radius: 10px;
    overflow: hidden;
}

.thumbnail:hover {
    filter: brightness(50%);
}


.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}

.button-wrapper {
    display: flex;
    background:white;

}

figcaption {

  text-align: center;
}



.button-wrapper button {
    padding: 10px 15px;
    font-size: 16px;
    font-family: inherit;
    margin:0;
    background-color: white;
    color: #3273dc;
    border: 1px solid rgb(241, 241, 241);
    width: 100%;
    cursor: pointer;

}
.button-wrapper button:nth-child(1) {
    border-bottom-left-radius: 10px;
}

.button-wrapper button:nth-child(3) {
    border-bottom-right-radius: 10px;
}

.item {
    width: 100%;
    display: block;
    align-items: center;
    justify-content: center;
  
    /* border: 1px solid; */
}
.carousel{
    overflow: hidden;
}


</style>
</head>
<body>
    <h1 class="title">3D Pano Inpainting: Building a VR Environment from a Single Input Panorama</h1>
    <p class=authors>Shivam Ajisa<sup>1</sup>,&nbsp
        Edward Du<sup>1</sup>,&nbsp
        Nam Nguyen<sup>1</sup>,&nbsp
        Stefanie Zollmann<sup>2</sup>,&nbsp
        Jonathan Ventura<sup>1</sup></p>
        <p class=authors>
            <sup>1</sup>California Polytechnic State University, San Luis Obispo, CA, USA<br>
            <sup>2</sup>University of Otago, Dunedin, New Zealand<br><br>
        <h3 class="links is-size-5">
                <a href="assets/abstract.pdf">IEEE VR 2024 Poster Abstract</a>  |
                <!-- <a href="https://youtu.be/AMSUpCLFoTk">Video</a>  | -->
                <a href="https://github.com/jonathanventura/3d-pano-inpainting">Code</a>
        </h2>    
            <hr>
            <figure>
                <img src="assets/model.png" style="max-width: 100%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                <figcaption>Our novel system takes as input a single panoramic image, generates a depth map, and applies an inpainting method adapted for 360◦ content to produce a 3D textured mesh for real-time 6DOF view synthesis in a VR headset. </figcaption>
           </figure>
           <br>
           <h2 class="title is-4">Abstract</h2>
           <p>
            Creating 360-degree 3D content is challenging because it requires either a multi-camera rig or a collection of many images taken from different perspectives. Our approach aims to generate a 360◦ VR scene from a single panoramic image using a learning-based inpainting method adapted for panoramic content. We introduce a pipeline capable of transforming an equirectangular panoramic RGB image into a complete 360◦ 3D virtual reality scene represented as a textured mesh, which is easily rendered on a VR headset using standard graphics rendering pipelines. We qualitatively evaluate our results on a synthetic dataset consisting of 360 panoramas in indoor scenes.
           </p>
           <br>
           <h2 class="title is-4">Spiral 360-Degree Videos</h2>
           <div class = "video-container">
            <iframe style="width:640px;height:360px" src="https://drive.google.com/file/d/1XD4AQdJkkrR2qTDOXW0MTqW_79-N1EUG/preview" width="640" height="360" allow="autoplay"></iframe>
            <iframe style="width:640px;height:360px" src="https://drive.google.com/file/d/17SnQUHGGuT2Z1Gfm-wcfEosJI0-aXJGT/preview" width="640" height="360" allow="autoplay"></iframe>
           </div>
           <br>
           <h2 class="title is-4">Real-time VR Interactive Demo</h2>
           <p>
            You can explore the output mesh generated by our method using our VR viewer. For the best experience, we recommend using Google Chrome on desktop or a compatible VR headset. Our viewer has been thoroughly tested on an Oculus Quest 2 that allows seamless exploration of walkable spaces. You can also explore the space by swiping with your mouse or using touch.
           </p>

           <section class="section">
			<div class="container" style="width: 100%; margin-left: -1.0rem;margin-top: -2.0rem">
				<!-- Start Carousel -->
				<div id="carousel-demo" class="carousel">
                    <div class="thumbnail">
                        <a href="renderer.html?scene=conference_DA_ip_subdiv_opt">
                            <img src="assets/mesh_thumbnails/ieee_vr.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                        <figcaption>IEEE VR 2024</figcaption>
                        <figcaption>Orlando, FL, USA</figcaption>
                         
					</div>
                    <div class="thumbnail">
                        <a href="renderer.html?scene=disney_DA_ip_subdiv_opt">
                            <img src="assets/mesh_thumbnails/magic_kingdom.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                        <figcaption>Magic Kingdom</figcaption>
                        <figcaption>Orlando, FL, USA</figcaption>
                         
					</div>
					<div class="thumbnail">
                        <a href="renderer.html?scene=stapfelberg_DA_ip_subdiv_opt">
                            <img src="assets/mesh_thumbnails/basel_stapfelberg.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                        <figcaption>Stapfelberg, Basel</figcaption>
                        <figcaption>Switzerland</figcaption>
                         
					</div>
					<div class="thumbnail">
                        <a href="renderer.html?scene=ampleben_DA_ip_subdiv_opt">
                            <img src="assets/mesh_thumbnails/ampleben_dorfkirche.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Dorfkirche Ampleben</figcaption>
                            <figcaption>Germany</figcaption>
                            
					</div>
					<div class="thumbnail">
                        <a href="renderer.html?scene=ascheberg_DA_ip_subdiv_opt">
                            <img src="assets/mesh_thumbnails/ascheberg_kirchplatz.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Kirchplatz, Ascheberg</figcaption>
                            <figcaption>Germany</figcaption>
                          
					</div>
                    <div class="thumbnail">
                        <a href="renderer.html?scene=martinsgasse_DA_ip_subdiv_opt">
                            <img src="assets/mesh_thumbnails/basel_martinsgasse.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Martinsgasse, Basel</figcaption>
                            <figcaption>Switzerland</figcaption>
                           
					</div>

                    <div class="thumbnail">
                        <a href="renderer.html?scene=braunschweig_DA_ip_subdiv_opt">
                            <img src="assets/mesh_thumbnails/braunschweig_altstadt.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Braunschweig, Altstadt</figcaption>
                            <figcaption>Germany</figcaption>
                         
					</div>

                    <div class="thumbnail">
                        <a href="renderer.html?scene=soissons_DA">
                            <img src="assets/mesh_thumbnails/soissons_cathedral_interior.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>Soissons Cathedral</figcaption>
                            <figcaption>France</figcaption>
                          
					</div> 

                    <!-- <div class="thumbnail">
                        <a href="renderer.html?scene=stiftskirche_st_maria_magdalena_in_flaesheim">
                            <img src="assets/mesh_thumbnails/stiftskirche_st_maria_magdalena_in_flaesheim.png" style="width: 97%; height: auto; display: block;  margin-left: auto; margin-right: auto;">
                        </a>
                            <figcaption>St. Maria-Magdalena </figcaption>
                            <figcaption>Germany</figcaption>

					</div> -->

				</div>
                
				<!-- End Carousel -->
			</div>
		</section>

		<script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
		<script>
		bulmaCarousel.attach('#carousel-demo', {
			slidesToScroll: 1,
			slidesToShow: 3,
            initialSlide: 0
            
		});
		</script>



        
</body>



================================================
FILE: docs/index_copy.html
================================================



================================================
FILE: docs/links.html
================================================
<html>
  <body>
  <h1> 4K</h1>
  <a href="http://jonathanventura.github.io/3d-pano-inpainting/renderer-uv.html?scene=halde_zollern_nebel&res=4k">Halde Zollern</a>
  <a href="http://jonathanventura.github.io/3d-pano-inpainting/renderer-uv.html?scene=ascheberg_kirchplatz&res=4k">Ascheberg Kirchplatz</a>
  <a href="http://jonathanventura.github.io/3d-pano-inpainting/renderer-uv.html?scene=basel_stapfelberg&res=4k">Basel Stapfelberg</a>
<h1> 1K</h1>
  
    <a href="http://jonathanventura.github.io/3d-pano-inpainting/renderer-uv.html?scene=halde_zollern_nebel&res=1k">Halde Zollern</a>
  <a href="http://jonathanventura.github.io/3d-pano-inpainting/renderer-uv.html?scene=ascheberg_kirchplatz&res=1k">Ascheberg Kirchplatz</a>
  <a href="http://jonathanventura.github.io/3d-pano-inpainting/renderer-uv.html?scene=basel_stapfelberg&res=1k">Basel Stapfelberg</a>
</body>
  </html>



================================================
FILE: docs/main.css
================================================







<!DOCTYPE html>
<html lang="en" data-color-mode="auto" data-light-theme="light" data-dark-theme="dark" data-a11y-animated-images="system">
  <head>
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://github.githubassets.com">
  <link rel="dns-prefetch" href="https://avatars.githubusercontent.com">
  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com">
  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">
  <link rel="preconnect" href="https://github.githubassets.com" crossorigin>
  <link rel="preconnect" href="https://avatars.githubusercontent.com">

  

  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/light-0946cdc16f15.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/dark-3946c959759a.css" /><link data-color-theme="dark_dimmed" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_dimmed-9b9a8c91acc5.css" /><link data-color-theme="dark_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_high_contrast-11302a585e33.css" /><link data-color-theme="dark_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_colorblind-1a4564ab0fbf.css" /><link data-color-theme="light_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_colorblind-12a8b2aa9101.css" /><link data-color-theme="light_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_high_contrast-5924a648f3e7.css" /><link data-color-theme="light_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_tritanopia-05358496cb79.css" /><link data-color-theme="dark_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_tritanopia-aad6b801a158.css" />
  
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-4d8f37cc9d91.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/global-243d3a393d7d.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/github-b717d68e0146.css" />
  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/code-5347859ecdfc.css" />

    <meta name="optimizely-datafile" content="{&quot;groups&quot;: [], &quot;environmentKey&quot;: &quot;production&quot;, &quot;rollouts&quot;: [], &quot;typedAudiences&quot;: [], &quot;projectId&quot;: &quot;16737760170&quot;, &quot;variables&quot;: [], &quot;featureFlags&quot;: [], &quot;experiments&quot;: [], &quot;version&quot;: &quot;4&quot;, &quot;audiences&quot;: [{&quot;conditions&quot;: &quot;[\&quot;or\&quot;, {\&quot;match\&quot;: \&quot;exact\&quot;, \&quot;name\&quot;: \&quot;$opt_dummy_attribute\&quot;, \&quot;type\&quot;: \&quot;custom_attribute\&quot;, \&quot;value\&quot;: \&quot;$opt_dummy_value\&quot;}]&quot;, &quot;id&quot;: &quot;$opt_dummy_audience&quot;, &quot;name&quot;: &quot;Optimizely-Generated Audience for Backwards Compatibility&quot;}], &quot;anonymizeIP&quot;: true, &quot;sdkKey&quot;: &quot;WTc6awnGuYDdG98CYRban&quot;, &quot;attributes&quot;: [{&quot;id&quot;: &quot;16822470375&quot;, &quot;key&quot;: &quot;user_id&quot;}, {&quot;id&quot;: &quot;17143601254&quot;, &quot;key&quot;: &quot;spammy&quot;}, {&quot;id&quot;: &quot;18175660309&quot;, &quot;key&quot;: &quot;organization_plan&quot;}, {&quot;id&quot;: &quot;18813001570&quot;, &quot;key&quot;: &quot;is_logged_in&quot;}, {&quot;id&quot;: &quot;19073851829&quot;, &quot;key&quot;: &quot;geo&quot;}, {&quot;id&quot;: &quot;20175462351&quot;, &quot;key&quot;: &quot;requestedCurrency&quot;}, {&quot;id&quot;: &quot;20785470195&quot;, &quot;key&quot;: &quot;country_code&quot;}, {&quot;id&quot;: &quot;21656311196&quot;, &quot;key&quot;: &quot;opened_downgrade_dialog&quot;}], &quot;botFiltering&quot;: false, &quot;accountId&quot;: &quot;16737760170&quot;, &quot;events&quot;: [{&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;17911811441&quot;, &quot;key&quot;: &quot;hydro_click.dashboard.teacher_toolbox_cta&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18124116703&quot;, &quot;key&quot;: &quot;submit.organizations.complete_sign_up&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18145892387&quot;, &quot;key&quot;: &quot;no_metric.tracked_outside_of_optimizely&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18178755568&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.add_repo&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18180553241&quot;, &quot;key&quot;: &quot;submit.repository_imports.create&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18186103728&quot;, &quot;key&quot;: &quot;click.help.learn_more_about_repository_creation&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18188530140&quot;, &quot;key&quot;: &quot;test_event&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18191963644&quot;, &quot;key&quot;: &quot;click.empty_org_repo_cta.transfer_repository&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18195612788&quot;, &quot;key&quot;: &quot;click.empty_org_repo_cta.import_repository&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18210945499&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.invite_members&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18211063248&quot;, &quot;key&quot;: &quot;click.empty_org_repo_cta.create_repository&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18215721889&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.update_profile&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18224360785&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.dismiss&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18234832286&quot;, &quot;key&quot;: &quot;submit.organization_activation.complete&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18252392383&quot;, &quot;key&quot;: &quot;submit.org_repository.create&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18257551537&quot;, &quot;key&quot;: &quot;submit.org_member_invitation.create&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18259522260&quot;, &quot;key&quot;: &quot;submit.organization_profile.update&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18564603625&quot;, &quot;key&quot;: &quot;view.classroom_select_organization&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18568612016&quot;, &quot;key&quot;: &quot;click.classroom_sign_in_click&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18572592540&quot;, &quot;key&quot;: &quot;view.classroom_name&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18574203855&quot;, &quot;key&quot;: &quot;click.classroom_create_organization&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18582053415&quot;, &quot;key&quot;: &quot;click.classroom_select_organization&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18589463420&quot;, &quot;key&quot;: &quot;click.classroom_create_classroom&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18591323364&quot;, &quot;key&quot;: &quot;click.classroom_create_first_classroom&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18591652321&quot;, &quot;key&quot;: &quot;click.classroom_grant_access&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18607131425&quot;, &quot;key&quot;: &quot;view.classroom_creation&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18831680583&quot;, &quot;key&quot;: &quot;upgrade_account_plan&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19064064515&quot;, &quot;key&quot;: &quot;click.signup&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19075373687&quot;, &quot;key&quot;: &quot;click.view_account_billing_page&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19077355841&quot;, &quot;key&quot;: &quot;click.dismiss_signup_prompt&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19079713938&quot;, &quot;key&quot;: &quot;click.contact_sales&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19120963070&quot;, &quot;key&quot;: &quot;click.compare_account_plans&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19151690317&quot;, &quot;key&quot;: &quot;click.upgrade_account_cta&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19424193129&quot;, &quot;key&quot;: &quot;click.open_account_switcher&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19520330825&quot;, &quot;key&quot;: &quot;click.visit_account_profile&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19540970635&quot;, &quot;key&quot;: &quot;click.switch_account_context&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19730198868&quot;, &quot;key&quot;: &quot;submit.homepage_signup&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19820830627&quot;, &quot;key&quot;: &quot;click.homepage_signup&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19988571001&quot;, &quot;key&quot;: &quot;click.create_enterprise_trial&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20036538294&quot;, &quot;key&quot;: &quot;click.create_organization_team&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20040653299&quot;, &quot;key&quot;: &quot;click.input_enterprise_trial_form&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20062030003&quot;, &quot;key&quot;: &quot;click.continue_with_team&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20068947153&quot;, &quot;key&quot;: &quot;click.create_organization_free&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20086636658&quot;, &quot;key&quot;: &quot;click.signup_continue.username&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20091648988&quot;, &quot;key&quot;: &quot;click.signup_continue.create_account&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20103637615&quot;, &quot;key&quot;: &quot;click.signup_continue.email&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20111574253&quot;, &quot;key&quot;: &quot;click.signup_continue.password&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20120044111&quot;, &quot;key&quot;: &quot;view.pricing_page&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20152062109&quot;, &quot;key&quot;: &quot;submit.create_account&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20165800992&quot;, &quot;key&quot;: &quot;submit.upgrade_payment_form&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20171520319&quot;, &quot;key&quot;: &quot;submit.create_organization&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20222645674&quot;, &quot;key&quot;: &quot;click.recommended_plan_in_signup.discuss_your_needs&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20227443657&quot;, &quot;key&quot;: &quot;submit.verify_primary_user_email&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20234607160&quot;, &quot;key&quot;: &quot;click.recommended_plan_in_signup.try_enterprise&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20238175784&quot;, &quot;key&quot;: &quot;click.recommended_plan_in_signup.team&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20239847212&quot;, &quot;key&quot;: &quot;click.recommended_plan_in_signup.continue_free&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20251097193&quot;, &quot;key&quot;: &quot;recommended_plan&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20438619534&quot;, &quot;key&quot;: &quot;click.pricing_calculator.1_member&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20456699683&quot;, &quot;key&quot;: &quot;click.pricing_calculator.15_members&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20467868331&quot;, &quot;key&quot;: &quot;click.pricing_calculator.10_members&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20476267432&quot;, &quot;key&quot;: &quot;click.trial_days_remaining&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20476357660&quot;, &quot;key&quot;: &quot;click.discover_feature&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20479287901&quot;, &quot;key&quot;: &quot;click.pricing_calculator.custom_members&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20481107083&quot;, &quot;key&quot;: &quot;click.recommended_plan_in_signup.apply_teacher_benefits&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20483089392&quot;, &quot;key&quot;: &quot;click.pricing_calculator.5_members&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20484283944&quot;, &quot;key&quot;: &quot;click.onboarding_task&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20484996281&quot;, &quot;key&quot;: &quot;click.recommended_plan_in_signup.apply_student_benefits&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20486713726&quot;, &quot;key&quot;: &quot;click.onboarding_task_breadcrumb&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20490791319&quot;, &quot;key&quot;: &quot;click.upgrade_to_enterprise&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20491786766&quot;, &quot;key&quot;: &quot;click.talk_to_us&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20494144087&quot;, &quot;key&quot;: &quot;click.dismiss_enterprise_trial&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20499722759&quot;, &quot;key&quot;: &quot;completed_all_tasks&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20500710104&quot;, &quot;key&quot;: &quot;completed_onboarding_tasks&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20513160672&quot;, &quot;key&quot;: &quot;click.read_doc&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20516196762&quot;, &quot;key&quot;: &quot;actions_enabled&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20518980986&quot;, &quot;key&quot;: &quot;click.dismiss_trial_banner&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20535446721&quot;, &quot;key&quot;: &quot;click.issue_actions_prompt.dismiss_prompt&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20557002247&quot;, &quot;key&quot;: &quot;click.issue_actions_prompt.setup_workflow&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20595070227&quot;, &quot;key&quot;: &quot;click.pull_request_setup_workflow&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20626600314&quot;, &quot;key&quot;: &quot;click.seats_input&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20642310305&quot;, &quot;key&quot;: &quot;click.decrease_seats_number&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20662990045&quot;, &quot;key&quot;: &quot;click.increase_seats_number&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20679620969&quot;, &quot;key&quot;: &quot;click.public_product_roadmap&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20761240940&quot;, &quot;key&quot;: &quot;click.dismiss_survey_banner&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20767210721&quot;, &quot;key&quot;: &quot;click.take_survey&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20795281201&quot;, &quot;key&quot;: &quot;click.archive_list&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20966790249&quot;, &quot;key&quot;: &quot;contact_sales.submit&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20996500333&quot;, &quot;key&quot;: &quot;contact_sales.existing_customer&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;20996890162&quot;, &quot;key&quot;: &quot;contact_sales.blank_message_field&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21000470317&quot;, &quot;key&quot;: &quot;contact_sales.personal_email&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21002790172&quot;, &quot;key&quot;: &quot;contact_sales.blank_phone_field&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21354412592&quot;, &quot;key&quot;: &quot;click.dismiss_create_readme&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21366102546&quot;, &quot;key&quot;: &quot;click.dismiss_zero_user_content&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21370252505&quot;, &quot;key&quot;: &quot;account_did_downgrade&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21370840408&quot;, &quot;key&quot;: &quot;click.cta_create_readme&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21375451068&quot;, &quot;key&quot;: &quot;click.cta_create_new_repository&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21385390948&quot;, &quot;key&quot;: &quot;click.zero_user_content&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21467712175&quot;, &quot;key&quot;: &quot;click.downgrade_keep&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21484112202&quot;, &quot;key&quot;: &quot;click.downgrade&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21495292213&quot;, &quot;key&quot;: &quot;click.downgrade_survey_exit&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21508241468&quot;, &quot;key&quot;: &quot;click.downgrade_survey_submit&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21512030356&quot;, &quot;key&quot;: &quot;click.downgrade_support&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21539090022&quot;, &quot;key&quot;: &quot;click.downgrade_exit&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21543640644&quot;, &quot;key&quot;: &quot;click_fetch_upstream&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21646510300&quot;, &quot;key&quot;: &quot;click.move_your_work&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21656151116&quot;, &quot;key&quot;: &quot;click.add_branch_protection_rule&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21663860599&quot;, &quot;key&quot;: &quot;click.downgrade_dialog_open&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21687860483&quot;, &quot;key&quot;: &quot;click.learn_about_protected_branches&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21689050333&quot;, &quot;key&quot;: &quot;click.dismiss_protect_this_branch&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;21864370109&quot;, &quot;key&quot;: &quot;click.sign_in&quot;}], &quot;revision&quot;: &quot;1372&quot;}" />


  <script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/wp-runtime-a59f3775ca39.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_stacktrace-parser_dist_stack-trace-parser_esm_js-node_modules_github_bro-a4c183-ae93d3fba59c.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/ui_packages_failbot_failbot_ts-e38c93eab86e.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/environment-de3997b81651.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-2646a2c533e3.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-99e288659d4f.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_fzy_js_index_js-node_modules_github_markdown-toolbar-element_dist_index_js-e3de700a4c9d.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_auto-complete-element-5b3870-ff38694180c6.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_text-ex-3415a8-7ecc10fb88d0.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-8873b7-5771678648e0.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_primer_view-components_app_components_primer_primer_js-node_modules_gith-3af896-2189f4f604ee.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/github-elements-7b037525f59f.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/element-registry-265f231a8769.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-9d9fe1859ce5.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_alive-client_dist-bf5aa2-424aa982deef.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-ba0e4d5b3207.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_dimensions_js-node_modules_github_hotkey_dist_-269f6d-5d145c7cc849.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_scroll-anchoring_dist_scro-5881a7-44d01ee9e782.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_color-convert_index_js-35b3ae68c408.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_paste-markdown_dist_index_esm_js-node_modules_github_quote-select-973149-7c1c1618332f.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_updatable-content_ts-dadb69f79923.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_keyboard-shortcuts-helper_ts-app_assets_modules_github_be-f5afdb-8cfe1dd0ad56.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_blob-anchor_ts-app_assets_modules_github_code-editor_ts-app_assets_-d384d0-eae4affc5787.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_sticky-scroll-into-view_ts-1d145b63ed56.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_include-2e2258-dae7d38e0248.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-80a9915bf75c.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/behaviors-86791d034ef8.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-623425af41e1.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/notifications-global-4dc6f295cc92.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_optimizely_optimizely-sdk_dist_optimizely_browser_es_min_js-node_modules-089adc-2328ba323205.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/optimizely-1c55a525615e.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_virtualized-list_es_index_js-node_modules_github_template-parts_lib_index_js-c3e624db1d89.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_decorators_js-node_modules_github_remote-form_-e3de2b-93bbe15e6e78.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_filter--b2311f-939ba5085db0.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-8f8b76ecd8d3.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/repositories-cd235e260130.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_clipboard-copy-element_dist_index_esm_js-node_modules_scroll-anch-c93c97-d63d35dd5d0b.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_diffs_blob-lines_ts-app_assets_modules_github_diffs_linkable-line-n-f96c66-3aec21e0f76c.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/diffs-ce6c07838068.js"></script>
  

  <title>three.js/main.css at dev · mrdoob/three.js · GitHub</title>



  <meta name="route-pattern" content="/:user_id/:repository/blob/*name(/*path)">

    
  <meta name="current-catalog-service-hash" content="581425c0eaaa5e5e53c5b736f58a14dbe5d38b0be425901738ad0670bd1d5a33">


  <meta name="request-id" content="DA93:126E:889F3C:A30E5E:644E12C6" data-pjax-transient="true"/><meta name="html-safe-nonce" content="3bb2d6ca7583672a3ab16e719714f140c80f668928eb0c202f5a26971293610b" data-pjax-transient="true"/><meta name="visitor-payload" content="eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJEQTkzOjEyNkU6ODg5RjNDOkEzMEU1RTo2NDRFMTJDNiIsInZpc2l0b3JfaWQiOiI1ODk1NzA5NTIwMjMxNzk3NDQ2IiwicmVnaW9uX2VkZ2UiOiJhdXN0cmFsaWFlYXN0IiwicmVnaW9uX3JlbmRlciI6ImF1c3RyYWxpYWVhc3QifQ==" data-pjax-transient="true"/><meta name="visitor-hmac" content="b141a8e452d8efab18b71993cb20727cd0a83825a80fac73d5ec782e670dfae4" data-pjax-transient="true"/>


    <meta name="hovercard-subject-tag" content="repository:576201" data-turbo-transient>


  <meta name="github-keyboard-shortcuts" content="repository,source-code,file-tree" data-turbo-transient="true" />
  

  <meta name="selected-link" value="repo_source" data-turbo-transient>

    <meta name="google-site-verification" content="c1kuD-K2HIVF635lypcsWPoD4kilo5-jA_wBFyT4uMY">
  <meta name="google-site-verification" content="KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU">
  <meta name="google-site-verification" content="ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA">
  <meta name="google-site-verification" content="GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc">
  <meta name="google-site-verification" content="Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I">

<meta name="octolytics-url" content="https://collector.github.com/github/collect" />

  <meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show" data-turbo-transient="true" />

  




  

    <meta name="user-login" content="">

  

    <meta name="viewport" content="width=device-width">
    
      <meta name="description" content="JavaScript 3D Library. Contribute to mrdoob/three.js development by creating an account on GitHub.">
      <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">
    <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
    <meta property="fb:app_id" content="1401488693436528">
    <meta name="apple-itunes-app" content="app-id=1477376905" />
      <meta name="twitter:image:src" content="https://repository-images.githubusercontent.com/576201/6c52fa00-6238-11eb-8763-f36f6e226bba" /><meta name="twitter:site" content="@github" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="three.js/main.css at dev · mrdoob/three.js" /><meta name="twitter:description" content="JavaScript 3D Library. Contribute to mrdoob/three.js development by creating an account on GitHub." />
      <meta property="og:image" content="https://repository-images.githubusercontent.com/576201/6c52fa00-6238-11eb-8763-f36f6e226bba" /><meta property="og:image:alt" content="JavaScript 3D Library. Contribute to mrdoob/three.js development by creating an account on GitHub." /><meta property="og:site_name" content="GitHub" /><meta property="og:type" content="object" /><meta property="og:title" content="three.js/main.css at dev · mrdoob/three.js" /><meta property="og:url" content="https://github.com/mrdoob/three.js" /><meta property="og:description" content="JavaScript 3D Library. Contribute to mrdoob/three.js development by creating an account on GitHub." />
      
    <link rel="assets" href="https://github.githubassets.com/">


        <meta name="hostname" content="github.com">



        <meta name="expected-hostname" content="github.com">

    <meta name="enabled-features" content="TURBO_EXPERIMENT_RISKY,IMAGE_METRIC_TRACKING,GEOJSON_AZURE_MAPS">


  <meta http-equiv="x-pjax-version" content="18a14a1052a4f18a45251a3c7ed94b6c7c95a483ac456ee0a83d97becc471a00" data-turbo-track="reload">
  <meta http-equiv="x-pjax-csp-version" content="0db263f9a873141d8256f783c35f244c06d490aacc3b680f99794dd8fd59fb59" data-turbo-track="reload">
  <meta http-equiv="x-pjax-css-version" content="d0d1f5be9d5737dd36dc184b20f007cd603f64dae470c6d16fbee094e3cf2c43" data-turbo-track="reload">
  <meta http-equiv="x-pjax-js-version" content="1341c02f6daeff9d8b3d2157e7755cdec2f8684430c910f2b6050fc993f4a041" data-turbo-track="reload">

  <meta name="turbo-cache-control" content="no-preview" data-turbo-transient="">

        <meta data-hydrostats="publish">

  <meta name="go-import" content="github.com/mrdoob/three.js git https://github.com/mrdoob/three.js.git">

  <meta name="octolytics-dimension-user_id" content="97088" /><meta name="octolytics-dimension-user_login" content="mrdoob" /><meta name="octolytics-dimension-repository_id" content="576201" /><meta name="octolytics-dimension-repository_nwo" content="mrdoob/three.js" /><meta name="octolytics-dimension-repository_public" content="true" /><meta name="octolytics-dimension-repository_is_fork" content="false" /><meta name="octolytics-dimension-repository_network_root_id" content="576201" /><meta name="octolytics-dimension-repository_network_root_nwo" content="mrdoob/three.js" />



    <link rel="canonical" href="https://github.com/mrdoob/three.js/blob/dev/examples/main.css" data-turbo-transient>
  <meta name="turbo-body-classes" content="logged-out env-production page-responsive page-blob">


  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">

  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">

  <meta name="browser-optimizely-client-errors-url" content="https://api.github.com/_private/browser/optimizely_client/errors">

  <link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000">
  <link rel="alternate icon" class="js-site-favicon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png">
  <link rel="icon" class="js-site-favicon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg">

<meta name="theme-color" content="#1e2327">
<meta name="color-scheme" content="light dark" />


  <link rel="manifest" href="/manifest.json" crossOrigin="use-credentials">

  </head>

  <body class="logged-out env-production page-responsive page-blob" style="word-wrap: break-word;">
    <div data-turbo-body class="logged-out env-production page-responsive page-blob" style="word-wrap: break-word;">
      


    <div class="position-relative js-header-wrapper ">
      <a href="#start-of-content" class="px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content">Skip to content</a>
      <span data-view-component="true" class="progress-pjax-loader Progress position-fixed width-full">
    <span style="width: 0%;" data-view-component="true" class="Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis"></span>
</span>      
      


        

            <script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_github_memoize_dist_esm_in-687f35-d131f0b6de8e.js"></script>
<script crossorigin="anonymous" defer="defer" type="application/javascript" src="https://github.githubassets.com/assets/sessions-2638decb9ee5.js"></script>
<header class="Header-old header-logged-out js-details-container Details position-relative f4 py-3" role="banner">
  <button type="button" class="Header-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target" aria-label="Toggle navigation">
    <span class="d-none">Toggle navigation</span>
  </button>

  <div class="container-xl d-flex flex-column flex-lg-row flex-items-center p-responsive height-full position-relative z-1">
    <div class="d-flex flex-justify-between flex-items-center width-full width-lg-auto">
      <a class="mr-lg-3 color-fg-inherit flex-order-2" href="https://github.com/" aria-label="Homepage" data-ga-click="(Logged out) Header, go to homepage, icon:logo-wordmark">
        <svg height="32" aria-hidden="true" viewBox="0 0 16 16" version="1.1" width="32" data-view-component="true" class="octicon octicon-mark-github">
    <path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"></path>
</svg>
      </a>

        <div class="flex-1">
          <a href="/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo"
            class="d-inline-block d-lg-none flex-order-1 f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit"
            data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4d9372862166e00b03aed25a21a655c3b551d4add4e28912715b01b8fe5e43d9"
          >
            Sign&nbsp;up
          </a>
        </div>

      <div class="flex-1 flex-order-2 text-right">
          <button aria-label="Toggle navigation" aria-expanded="false" type="button" data-view-component="true" class="js-details-target Button--link Button--medium Button d-lg-none color-fg-inherit p-1">    <span class="Button-content">
      <span class="Button-label"><div class="HeaderMenu-toggle-bar rounded my-1"></div>
            <div class="HeaderMenu-toggle-bar rounded my-1"></div>
            <div class="HeaderMenu-toggle-bar rounded my-1"></div></span>
    </span>
</button>  
      </div>
    </div>


    <div class="HeaderMenu--logged-out p-responsive height-fit position-lg-relative d-lg-flex flex-column flex-auto pt-7 pb-4 top-0">
      <div class="header-menu-wrapper d-flex flex-column flex-self-end flex-lg-row flex-justify-between flex-auto p-3 p-lg-0 rounded rounded-lg-0 mt-3 mt-lg-0">
          <nav class="mt-0 px-3 px-lg-0 mb-3 mb-lg-0" aria-label="Global">
            <ul class="d-lg-flex list-style-none">
                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
        Product
        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
</svg>
      </button>
      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 d-lg-flex dropdown-menu-wide">
          <div class="px-lg-4 border-lg-right mb-4 mb-lg-0 pr-lg-7">
            <ul class="list-style-none f5" >
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="/features/actions">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-workflow color-fg-subtle mr-3">
    <path d="M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Actions</div>
        Automate any workflow
      </div>

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="/features/packages">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-package color-fg-subtle mr-3">
    <path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.748 1.748 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.748 1.748 0 0 1 1.75 0Zm-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.248.248 0 0 0-.25 0Zm.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Packages</div>
        Host and manage packages
      </div>

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="/features/security">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-shield-check color-fg-subtle mr-3">
    <path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z"></path><path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Security</div>
        Find and fix vulnerabilities
      </div>

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="/features/codespaces">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-codespaces color-fg-subtle mr-3">
    <path d="M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z"></path><path d="M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Codespaces</div>
        Instant dev environments
      </div>

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="/features/copilot">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-copilot color-fg-subtle mr-3">
    <path d="M9.75 14a.75.75 0 0 1 .75.75v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 .75-.75Zm4.5 0a.75.75 0 0 1 .75.75v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 .75-.75Z"></path><path d="M12 2c2.214 0 4.248.657 5.747 1.756.136.099.268.204.397.312.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086l.633 1.478.043.022A4.75 4.75 0 0 1 24 15.222v1.028c0 .529-.309.987-.565 1.293-.28.336-.636.653-.966.918a13.84 13.84 0 0 1-1.299.911l-.024.015-.006.004-.039.025c-.223.135-.45.264-.68.386-.46.245-1.122.571-1.941.895C16.845 21.344 14.561 22 12 22c-2.561 0-4.845-.656-6.479-1.303a19.046 19.046 0 0 1-1.942-.894 14.081 14.081 0 0 1-.535-.3l-.144-.087-.04-.025-.006-.004-.024-.015a13.16 13.16 0 0 1-1.299-.911 6.913 6.913 0 0 1-.967-.918C.31 17.237 0 16.779 0 16.25v-1.028a4.75 4.75 0 0 1 2.626-4.248l.043-.022.633-1.478a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.498 1.132-3.368.397-.406.89-.717 1.474-.952.129-.108.261-.213.397-.312C7.752 2.657 9.786 2 12 2Zm-8 9.654v6.669a17.59 17.59 0 0 0 2.073.98C7.595 19.906 9.686 20.5 12 20.5c2.314 0 4.405-.594 5.927-1.197a17.59 17.59 0 0 0 2.073-.98v-6.669l-.038-.09c-.046.061-.095.12-.145.177-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.544-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.344a4.323 4.323 0 0 1-.355.508C10.704 12.456 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a3.026 3.026 0 0 1-.145-.177Zm6.309-1.092c.445-.547.708-1.334.851-2.301.057-.357.087-.718.09-1.079v-.031c-.001-.762-.166-1.26-.43-1.568l-.008-.01c-.341-.391-1.046-.689-2.533-.529-1.505.163-2.347.537-2.824 1.024-.462.473-.705 1.18-.705 2.32 0 .605.044 1.087.135 1.472.092.384.231.672.423.89.365.413 1.084.75 2.657.75.91 0 1.527-.223 1.964-.564.14-.11.268-.235.38-.374Zm2.504-2.497c.136 1.057.403 1.913.878 2.497.442.545 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.151.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.318-.862-2.824-1.025-1.487-.161-2.192.139-2.533.529-.268.308-.437.808-.438 1.578v.02c.002.299.023.598.063.894Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Copilot</div>
        Write better code with AI
      </div>

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="/features/code-review">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-code-review color-fg-subtle mr-3">
    <path d="M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z"></path><path d="M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Code review</div>
        Manage code changes
      </div>

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="/features/issues">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-issue-opened color-fg-subtle mr-3">
    <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Issues</div>
        Plan and track work
      </div>

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="/features/discussions">
      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-comment-discussion color-fg-subtle mr-3">
    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z"></path><path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z"></path>
</svg>
      <div>
        <div class="color-fg-default h4">Discussions</div>
        Collaborate outside of code
      </div>

    
</a></li>

            </ul>
          </div>
          <div class="px-lg-4">
              <span class="d-block h4 color-fg-default my-1" id="product-explore-heading">Explore</span>
            <ul class="list-style-none f5" aria-labelledby="product-explore-heading">
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to All features&quot;,&quot;label&quot;:&quot;ref_cta:All features;&quot;}" href="/features">
      All features

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Documentation&quot;,&quot;label&quot;:&quot;ref_cta:Documentation;&quot;}" href="https://docs.github.com">
      Documentation

    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
</svg>
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Skills&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Skills;&quot;}" href="https://skills.github.com/">
      GitHub Skills

    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
</svg>
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Blog&quot;,&quot;label&quot;:&quot;ref_cta:Blog;&quot;}" href="https://github.blog">
      Blog

    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
</svg>
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
        Solutions
        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
</svg>
      </button>
      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4">
          <div class="border-bottom pb-3 mb-3">
              <span class="d-block h4 color-fg-default my-1" id="solutions-for-heading">For</span>
            <ul class="list-style-none f5" aria-labelledby="solutions-for-heading">
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Enterprise&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise;&quot;}" href="/enterprise">
      Enterprise

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Teams&quot;,&quot;label&quot;:&quot;ref_cta:Teams;&quot;}" href="/team">
      Teams

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Startups&quot;,&quot;label&quot;:&quot;ref_cta:Startups;&quot;}" href="/enterprise/startups">
      Startups

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Education&quot;,&quot;label&quot;:&quot;ref_cta:Education;&quot;}" href="https://education.github.com">
      Education

    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
</svg>
</a></li>

            </ul>
          </div>
          <div class="border-bottom pb-3 mb-3">
              <span class="d-block h4 color-fg-default my-1" id="solutions-by-solution-heading">By Solution</span>
            <ul class="list-style-none f5" aria-labelledby="solutions-by-solution-heading">
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to CI/CD &amp;amp; Automation&quot;,&quot;label&quot;:&quot;ref_cta:CI/CD &amp;amp; Automation;&quot;}" href="/solutions/ci-cd/">
      CI/CD &amp; Automation

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevOps&quot;,&quot;label&quot;:&quot;ref_cta:DevOps;&quot;}" href="https://resources.github.com/devops/">
      DevOps

    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
</svg>
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevSecOps&quot;,&quot;label&quot;:&quot;ref_cta:DevSecOps;&quot;}" href="https://resources.github.com/devops/fundamentals/devsecops/">
      DevSecOps

    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
</svg>
</a></li>

            </ul>
          </div>
          <div class="">
              <span class="d-block h4 color-fg-default my-1" id="solutions-case-studies-heading">Case Studies</span>
            <ul class="list-style-none f5" aria-labelledby="solutions-case-studies-heading">
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}" href="/customer-stories">
      Customer Stories

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Resources&quot;,&quot;label&quot;:&quot;ref_cta:Resources;&quot;}" href="https://resources.github.com/">
      Resources

    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
</svg>
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
        Open Source
        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
</svg>
      </button>
      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4">
          <div class="border-bottom pb-3 mb-3">
            <ul class="list-style-none f5" >
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="/sponsors">
      
      <div>
        <div class="color-fg-default h4">GitHub Sponsors</div>
        Fund open source developers
      </div>

    
</a></li>

            </ul>
          </div>
          <div class="border-bottom pb-3 mb-3">
            <ul class="list-style-none f5" >
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="/readme">
      
      <div>
        <div class="color-fg-default h4">The ReadME Project</div>
        GitHub community articles
      </div>

    
</a></li>

            </ul>
          </div>
          <div class="">
              <span class="d-block h4 color-fg-default my-1" id="open-source-repositories-heading">Repositories</span>
            <ul class="list-style-none f5" aria-labelledby="open-source-repositories-heading">
                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Topics&quot;,&quot;label&quot;:&quot;ref_cta:Topics;&quot;}" href="/topics">
      Topics

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Trending&quot;,&quot;label&quot;:&quot;ref_cta:Trending;&quot;}" href="/trending">
      Trending

    
</a></li>

                <li>
  <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Collections&quot;,&quot;label&quot;:&quot;ref_cta:Collections;&quot;}" href="/collections">
      Collections

    
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
    <a class="HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block" data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div class="d-lg-flex flex-items-center px-3 px-lg-0 mb-3 mb-lg-0 text-center text-lg-left">
            <div class="d-lg-flex min-width-0 mb-2 mb-lg-0">
              



<div class="header-search flex-auto position-relative js-site-search flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to"
>
  <div class="position-relative">
    <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="js-site-search-form" role="search" aria-label="Site" data-scope-type="Repository" data-scope-id="576201" data-scoped-search-url="/mrdoob/three.js/search" data-owner-scoped-search-url="/users/mrdoob/search" data-unscoped-search-url="/search" data-turbo="false" action="/mrdoob/three.js/search" accept-charset="UTF-8" method="get">
      <label class="form-control header-search-wrapper input-sm p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center">
        <input type="text"
          class="form-control js-site-search-focus header-search-input jump-to-field js-jump-to-field js-site-search-field is-clearable"
          data-hotkey=s,/
          name="q"
          
          placeholder="Search"
          data-unscoped-placeholder="Search GitHub"
          data-scoped-placeholder="Search"
          autocapitalize="off"
          role="combobox"
          aria-haspopup="listbox"
          aria-expanded="false"
          aria-autocomplete="list"
          aria-controls="jump-to-results"
          aria-label="Search"
          data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations"
          spellcheck="false"
          autocomplete="off"
        >
        <input type="hidden" data-csrf="true" class="js-data-jump-to-suggestions-path-csrf" value="ixCRVpt6nTgQ4MBkjxifra8sZM7UYdM0ShKVlhvADZ+dO5csRiaYb/7h0CXYkeOFPxOTB8woXA7O33NlEjk6Gg==" />
        <input type="hidden" class="js-site-search-type-field" name="type" >
            <svg xmlns="http://www.w3.org/2000/svg" width="22" height="20" aria-hidden="true" class="mr-1 header-search-key-slash"><path fill="none" stroke="#979A9C" opacity=".4" d="M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z"></path><path fill="#979A9C" d="M11.8 6L8 15.1h-.9L10.8 6h1z"></path></svg>


          <div class="Box position-absolute overflow-hidden d-none jump-to-suggestions js-jump-to-suggestions-container">
            
<ul class="d-none js-jump-to-suggestions-template-container">
  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-suggestion" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="" data-item-type="suggestion">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg title="Repository" aria-label="Repository" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo js-jump-to-octicon-repo d-none flex-shrink-0">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
      <svg title="Project" aria-label="Project" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-project js-jump-to-octicon-project d-none flex-shrink-0">
    <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z"></path>
</svg>
      <svg title="Search" aria-label="Search" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search js-jump-to-octicon-search d-none flex-shrink-0">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>

</ul>

<ul class="d-none js-jump-to-no-results-template-container">
  <li class="d-flex flex-justify-center flex-items-center f5 d-none js-jump-to-suggestion p-2">
    <span class="color-fg-muted">No suggested jump to results</span>
  </li>
</ul>

<ul id="jump-to-results" role="listbox" class="p-0 m-0 js-navigation-container jump-to-suggestions-results-container js-jump-to-suggestions-results-container">
  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-scoped-search d-none" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="" data-item-type="scoped_search">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg title="Repository" aria-label="Repository" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo js-jump-to-octicon-repo d-none flex-shrink-0">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
      <svg title="Project" aria-label="Project" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-project js-jump-to-octicon-project d-none flex-shrink-0">
    <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z"></path>
</svg>
      <svg title="Search" aria-label="Search" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search js-jump-to-octicon-search d-none flex-shrink-0">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>

  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-owner-scoped-search d-none" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="" data-item-type="owner_scoped_search">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg title="Repository" aria-label="Repository" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo js-jump-to-octicon-repo d-none flex-shrink-0">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
      <svg title="Project" aria-label="Project" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-project js-jump-to-octicon-project d-none flex-shrink-0">
    <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z"></path>
</svg>
      <svg title="Search" aria-label="Search" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search js-jump-to-octicon-search d-none flex-shrink-0">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this user">
        In this user
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>

  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-global-search d-none" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="" data-item-type="global_search">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg title="Repository" aria-label="Repository" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo js-jump-to-octicon-repo d-none flex-shrink-0">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
      <svg title="Project" aria-label="Project" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-project js-jump-to-octicon-project d-none flex-shrink-0">
    <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z"></path>
</svg>
      <svg title="Search" aria-label="Search" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search js-jump-to-octicon-search d-none flex-shrink-0">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-2 flex-shrink-0 color-bg-subtle px-1 color-fg-muted ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>


</ul>

          </div>
      </label>
</form>  </div>
</div>

            </div>

          <div class="position-relative mr-lg-3 d-lg-inline-block">
            <a href="/login?return_to=https%3A%2F%2Fgithub.com%2Fmrdoob%2Fthree.js%2Fblob%2Fdev%2Fexamples%2Fmain.css"
              class="HeaderMenu-link HeaderMenu-link--sign-in flex-shrink-0 no-underline d-block d-lg-inline-block border border-lg-0 rounded rounded-lg-0 p-2 p-lg-0"
              data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f1969faa47af77c9d62eedf0e33360629b98641aff616f433a407e9dd31de6db"
              data-ga-click="(Logged out) Header, clicked Sign in, text:sign-in">
              Sign in
            </a>
          </div>

            <a href="/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=mrdoob%2Fthree.js"
              class="HeaderMenu-link HeaderMenu-link--sign-up flex-shrink-0 d-none d-lg-inline-block no-underline border color-border-default rounded px-2 py-1"
              data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f1969faa47af77c9d62eedf0e33360629b98641aff616f433a407e9dd31de6db"
              data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}"
            >
              Sign up
            </a>
        </div>
      </div>
    </div>
  </div>
</header>

    </div>

  <div id="start-of-content" class="show-on-focus"></div>







    <div id="js-flash-container" data-turbo-replace>





  <template class="js-flash-template">
    
<div class="flash flash-full   {{ className }}">
  <div class="px-2" >
    <button autofocus class="flash-close js-flash-close" type="button" aria-label="Dismiss this message">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
    </button>
    <div aria-atomic="true" role="alert" class="js-flash-alert">
      
      <div>{{ message }}</div>

    </div>
  </div>
</div>
  </template>
</div>


    
    <include-fragment class="js-notification-shelf-include-fragment" data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>






  <div
    class="application-main "
    data-commit-hovercards-enabled
    data-discussion-hovercards-enabled
    data-issue-and-pr-hovercards-enabled
  >
        <div itemscope itemtype="http://schema.org/SoftwareSourceCode" class="">
    <main id="js-repo-pjax-container" >
      
  


    






  
  <div id="repository-container-header"  class="pt-3 hide-full-screen" style="background-color: var(--color-page-header-bg);" data-turbo-replace>

      <div class="d-flex flex-wrap flex-justify-end mb-3  px-3 px-md-4 px-lg-5" style="gap: 1rem;">

        <div class="flex-auto min-width-0 width-fit mr-3">
            
  <div class=" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo color-fg-muted mr-2">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
    
    <span class="author flex-self-stretch" itemprop="author">
      <a class="url fn" rel="author" data-hovercard-type="user" data-hovercard-url="/users/mrdoob/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/mrdoob">
        mrdoob
</a>    </span>
    <span class="mx-1 flex-self-stretch color-fg-muted">/</span>
    <strong itemprop="name" class="mr-2 flex-self-stretch">
      <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="/mrdoob/three.js">three.js</a>
    </strong>

    <span></span><span class="Label Label--secondary v-align-middle mr-1">Public</span>
  </div>


        </div>

        <div id="repository-details-container" data-turbo-replace>
            <ul class="pagehead-actions flex-shrink-0 d-none d-md-inline" style="padding: 2px 0;">
    
        <li>
          <include-fragment src="/mrdoob/three.js/sponsor_button"></include-fragment>
        </li>

      

  <li>
            <a href="/login?return_to=%2Fmrdoob%2Fthree.js" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="0da7c054dc30eb011a41b1a8ec3d2b5350a36f3a61edb79fdaa2da113fbb282b" aria-label="You must be signed in to change notification settings" data-view-component="true" class="tooltipped tooltipped-s btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bell mr-2">
    <path d="M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z"></path>
</svg>Notifications
</a>  </li>

  <li>
          <a icon="repo-forked" id="fork-button" href="/login?return_to=%2Fmrdoob%2Fthree.js" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:576201,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d41767c873734bd6b9307da76f83f15ff8a466031edd1feeb2334b975917c08d" data-view-component="true" class="btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo-forked mr-2">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>Fork
    <span id="repo-network-counter" data-pjax-replace="true" data-turbo-replace="true" title="34,555" data-view-component="true" class="Counter">34.6k</span>
</a>
  </li>

  <li>
        <div data-view-component="true" class="BtnGroup d-flex">
        <a href="/login?return_to=%2Fmrdoob%2Fthree.js" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:576201,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5094df148bdf8b4f99fbbda45f992fc73cf6489edf1be21e1e6453c98c369c97" aria-label="You must be signed in to star a repository" data-view-component="true" class="tooltipped tooltipped-s btn-sm btn BtnGroup-item">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-star v-align-text-bottom d-inline-block mr-2">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg><span data-view-component="true" class="d-inline">
          Star
</span>          <span id="repo-stars-counter-star" aria-label="91145 users starred this repository" data-singular-suffix="user starred this repository" data-plural-suffix="users starred this repository" data-turbo-replace="true" title="91,145" data-view-component="true" class="Counter js-social-count">91.1k</span>
</a>        <button disabled="disabled" aria-label="You must be signed in to add this repository to a list" type="button" data-view-component="true" class="btn-sm btn BtnGroup-item px-2">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-triangle-down">
    <path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z"></path>
</svg>
</button></div>
  </li>


    

</ul>

        </div>
      </div>

        <div id="responsive-meta-container" data-turbo-replace>
</div>


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true" class="js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5">

  <ul data-view-component="true" class="UnderlineNav-body list-style-none">
      <li data-view-component="true" class="d-inline-flex">
  <a id="code-tab" href="/mrdoob/three.js" data-tab-item="i0code-tab" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments /mrdoob/three.js" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g c" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" aria-current="page" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code UnderlineNav-octicon d-none d-sm-inline">
    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
        <span data-content="Code">Code</span>
          <span id="code-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="issues-tab" href="/mrdoob/three.js/issues" data-tab-item="i1issues-tab" data-selected-links="repo_issues repo_labels repo_milestones /mrdoob/three.js/issues" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g i" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline">
    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
</svg>
        <span data-content="Issues">Issues</span>
          <span id="issues-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="368" data-view-component="true" class="Counter">368</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="pull-requests-tab" href="/mrdoob/three.js/pulls" data-tab-item="i2pull-requests-tab" data-selected-links="repo_pulls checks /mrdoob/three.js/pulls" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g p" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline">
    <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z"></path>
</svg>
        <span data-content="Pull requests">Pull requests</span>
          <span id="pull-requests-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="137" data-view-component="true" class="Counter">137</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="actions-tab" href="/mrdoob/three.js/actions" data-tab-item="i3actions-tab" data-selected-links="repo_actions /mrdoob/three.js/actions" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g a" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play UnderlineNav-octicon d-none d-sm-inline">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
</svg>
        <span data-content="Actions">Actions</span>
          <span id="actions-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="wiki-tab" href="/mrdoob/three.js/wiki" data-tab-item="i4wiki-tab" data-selected-links="repo_wiki /mrdoob/three.js/wiki" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g w" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Wiki&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-book UnderlineNav-octicon d-none d-sm-inline">
    <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path>
</svg>
        <span data-content="Wiki">Wiki</span>
          <span id="wiki-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="security-tab" href="/mrdoob/three.js/security" data-tab-item="i5security-tab" data-selected-links="security overview alerts policy token_scanning code_scanning /mrdoob/three.js/security" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g s" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline">
    <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
        <span data-content="Security">Security</span>
          <include-fragment src="/mrdoob/three.js/security/overall-count" accept="text/fragment+html"></include-fragment>

    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="insights-tab" href="/mrdoob/three.js/pulse" data-tab-item="i6insights-tab" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /mrdoob/three.js/pulse" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline">
    <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
        <span data-content="Insights">Insights</span>
          <span id="insights-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
</ul>
    <div style="visibility:hidden;" data-view-component="true" class="UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0">      <details data-view-component="true" class="details-overlay details-reset position-relative">
  <summary role="button" data-view-component="true">          <div class="UnderlineNav-item mr-0 border-0">
            <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-kebab-horizontal">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg>
            <span class="sr-only">More</span>
          </div>
</summary>
  <details-menu role="menu" data-view-component="true" class="dropdown-menu dropdown-menu-sw">          <ul>
              <li data-menu-item="i0code-tab" hidden>
                <a role="menuitem" class="js-selected-navigation-item selected dropdown-item" aria-current="page" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments /mrdoob/three.js" href="/mrdoob/three.js">
                  Code
</a>              </li>
              <li data-menu-item="i1issues-tab" hidden>
                <a role="menuitem" class="js-selected-navigation-item dropdown-item" data-selected-links="repo_issues repo_labels repo_milestones /mrdoob/three.js/issues" href="/mrdoob/three.js/issues">
                  Issues
</a>              </li>
              <li data-menu-item="i2pull-requests-tab" hidden>
                <a role="menuitem" class="js-selected-navigation-item dropdown-item" data-selected-links="repo_pulls checks /mrdoob/three.js/pulls" href="/mrdoob/three.js/pulls">
                  Pull requests
</a>              </li>
              <li data-menu-item="i3actions-tab" hidden>
                <a role="menuitem" class="js-selected-navigation-item dropdown-item" data-selected-links="repo_actions /mrdoob/three.js/actions" href="/mrdoob/three.js/actions">
                  Actions
</a>              </li>
              <li data-menu-item="i4wiki-tab" hidden>
                <a role="menuitem" class="js-selected-navigation-item dropdown-item" data-selected-links="repo_wiki /mrdoob/three.js/wiki" href="/mrdoob/three.js/wiki">
                  Wiki
</a>              </li>
              <li data-menu-item="i5security-tab" hidden>
                <a role="menuitem" class="js-selected-navigation-item dropdown-item" data-selected-links="security overview alerts policy token_scanning code_scanning /mrdoob/three.js/security" href="/mrdoob/three.js/security">
                  Security
</a>              </li>
              <li data-menu-item="i6insights-tab" hidden>
                <a role="menuitem" class="js-selected-navigation-item dropdown-item" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /mrdoob/three.js/pulse" href="/mrdoob/three.js/pulse">
                  Insights
</a>              </li>
          </ul>
</details-menu>
</details></div>
</nav>

  </div>

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance" class="">
    <div id="repo-content-pjax-container" class="repository-content " >
    


    
      
  <div class="clearfix container-xl px-3 px-md-4 px-lg-5 mt-4">
    
    
<div >
  

  



    
<a class="d-none js-permalink-shortcut" data-hotkey="y" href="/mrdoob/three.js/blob/00a427c48e2a5339cf8a1e68f1da44cc0065ab21/examples/main.css">Permalink</a>

<div class="d-flex flex-items-start flex-shrink-0 pb-3 flex-wrap flex-md-nowrap flex-justify-between flex-md-justify-start">
  
<div class="position-relative">
  <details
    class="js-branch-select-menu details-reset details-overlay mr-0 mb-0 "
    id="branch-select-menu"
    data-hydro-click-payload="{&quot;event_type&quot;:&quot;repository.click&quot;,&quot;payload&quot;:{&quot;target&quot;:&quot;REFS_SELECTOR_MENU&quot;,&quot;repository_id&quot;:576201,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="193df5d13eadd2f7d1bc8231f81b663568e10c63ab3042cc76a6bebda44a2963">
    <summary class="btn css-truncate"
            data-hotkey="w"
            title="Switch branches or tags">
      <svg text="gray" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-branch">
    <path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z"></path>
</svg>
      <span class="css-truncate-target" data-menu-button>dev</span>
      <span class="dropdown-caret"></span>
    </summary>

    
<div class="SelectMenu">
  <div class="SelectMenu-modal">
    <header class="SelectMenu-header">
      <span class="SelectMenu-title">Switch branches/tags</span>
      <button class="SelectMenu-closeButton" type="button" data-toggle-for="branch-select-menu"><svg aria-label="Close menu" aria-hidden="false" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg></button>
    </header>

    <input-demux data-action="tab-container-change:input-demux#storeInput tab-container-changed:input-demux#updateInput">
      <tab-container class="d-flex flex-column js-branches-tags-tabs" style="min-height: 0;">
        <div class="SelectMenu-filter">
          <input data-target="input-demux.source"
                 id="context-commitish-filter-field"
                 class="SelectMenu-input form-control"
                 aria-owns="ref-list-branches"
                 data-controls-ref-menu-id="ref-list-branches"
                 autofocus
                 autocomplete="off"
                 aria-label="Filter branches/tags"
                 placeholder="Filter branches/tags"
                 type="text"
          >
        </div>

        <div class="SelectMenu-tabs" role="tablist" data-target="input-demux.control" >
          <button class="SelectMenu-tab" type="button" role="tab" aria-selected="true">Branches</button>
          <button class="SelectMenu-tab" type="button" role="tab">Tags</button>
        </div>

        <div role="tabpanel" id="ref-list-branches" data-filter-placeholder="Filter branches/tags" tabindex="" class="d-flex flex-column flex-auto overflow-auto">
          <ref-selector
            type="branch"
            data-targets="input-demux.sinks"
            data-action="
              input-entered:ref-selector#inputEntered
              tab-selected:ref-selector#tabSelected
              focus-list:ref-selector#focusFirstListMember
            "
            query-endpoint="/mrdoob/three.js/refs"
            
            cache-key="v0:1682719900.0"
            current-committish="ZGV2"
            default-branch="ZGV2"
            name-with-owner="bXJkb29iL3RocmVlLmpz"
            prefetch-on-mouseover
          >

            <template data-target="ref-selector.fetchFailedTemplate">
              <div class="SelectMenu-message" data-index="{{ index }}">Could not load branches</div>
            </template>

              <template data-target="ref-selector.noMatchTemplate">
    <div class="SelectMenu-message">Nothing to show</div>
</template>


            <div data-target="ref-selector.listContainer" role="menu" class="SelectMenu-list " data-turbo-frame="repo-content-turbo-frame">
              <div class="SelectMenu-loading pt-3 pb-0 overflow-hidden" aria-label="Menu is loading">
                <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="32" height="32" viewBox="0 0 16 16" fill="none" data-view-component="true" class="anim-rotate">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" />
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke" />
</svg>
              </div>
            </div>

              

<template data-target="ref-selector.itemTemplate">
  <a href="https://github.com/mrdoob/three.js/blob/{{ urlEncodedRefName }}/examples/main.css" class="SelectMenu-item" role="menuitemradio" rel="nofollow" aria-checked="{{ isCurrent }}" data-index="{{ index }}" >
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check SelectMenu-icon SelectMenu-icon--check">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    <span class="flex-1 css-truncate css-truncate-overflow {{ isFilteringClass }}">{{ refName }}</span>
    <span hidden="{{ isNotDefault }}" class="Label Label--secondary flex-self-start">default</span>
  </a>
</template>


              <footer class="SelectMenu-footer"><a href="/mrdoob/three.js/branches">View all branches</a></footer>
          </ref-selector>

        </div>

        <div role="tabpanel" id="tags-menu" data-filter-placeholder="Find a tag" tabindex="" hidden class="d-flex flex-column flex-auto overflow-auto">
          <ref-selector
            type="tag"
            data-action="
              input-entered:ref-selector#inputEntered
              tab-selected:ref-selector#tabSelected
              focus-list:ref-selector#focusFirstListMember
            "
            data-targets="input-demux.sinks"
            query-endpoint="/mrdoob/three.js/refs"
            cache-key="v0:1682719900.0"
            current-committish="ZGV2"
            default-branch="ZGV2"
            name-with-owner="bXJkb29iL3RocmVlLmpz"
          >

            <template data-target="ref-selector.fetchFailedTemplate">
              <div class="SelectMenu-message" data-index="{{ index }}">Could not load tags</div>
            </template>

            <template data-target="ref-selector.noMatchTemplate">
              <div class="SelectMenu-message" data-index="{{ index }}">Nothing to show</div>
            </template>

              

<template data-target="ref-selector.itemTemplate">
  <a href="https://github.com/mrdoob/three.js/blob/{{ urlEncodedRefName }}/examples/main.css" class="SelectMenu-item" role="menuitemradio" rel="nofollow" aria-checked="{{ isCurrent }}" data-index="{{ index }}" >
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check SelectMenu-icon SelectMenu-icon--check">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    <span class="flex-1 css-truncate css-truncate-overflow {{ isFilteringClass }}">{{ refName }}</span>
    <span hidden="{{ isNotDefault }}" class="Label Label--secondary flex-self-start">default</span>
  </a>
</template>


            <div data-target="ref-selector.listContainer" role="menu" class="SelectMenu-list" data-turbo-frame="repo-content-turbo-frame">
              <div class="SelectMenu-loading pt-3 pb-0 overflow-hidden" aria-label="Menu is loading">
                <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="32" height="32" viewBox="0 0 16 16" fill="none" data-view-component="true" class="anim-rotate">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" />
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke" />
</svg>
              </div>
            </div>
              <footer class="SelectMenu-footer"><a href="/mrdoob/three.js/tags">View all tags</a></footer>
          </ref-selector>
        </div>
      </tab-container>
    </input-demux>
  </div>
</div>

  </details>

</div>


<div class="Overlay--hidden Overlay-backdrop--center" data-modal-dialog-overlay>
  <modal-dialog role="dialog" id="warn-tag-match-create-branch-dialog" aria-modal="true" aria-labelledby="warn-tag-match-create-branch-dialog-header" data-view-component="true" class="Overlay Overlay--width-large Overlay--height-auto Overlay--motion-scaleFade">
      <header class="Overlay-header Overlay-header--large Overlay-header--divided">
        <div class="Overlay-headerContentWrap">
          <div class="Overlay-titleWrap">
            <h1 id="warn-tag-match-create-branch-dialog-header" class="Overlay-title">Name already in use</h1>
          </div>
          <div class="Overlay-actionWrap">
            <button data-close-dialog-id="warn-tag-match-create-branch-dialog" aria-label="Close" type="button" data-view-component="true" class="close-button Overlay-closeButton"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg></button>
          </div>
        </div>
      </header>
    <div class="Overlay-body ">
      
          <div data-view-component="true">      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?
</div>

    </div>
      <footer class="Overlay-footer Overlay-footer--alignEnd">
            <button data-close-dialog-id="warn-tag-match-create-branch-dialog" type="button" data-view-component="true" class="btn">    Cancel
</button>
            <button data-submit-dialog-id="warn-tag-match-create-branch-dialog" type="button" data-view-component="true" class="btn-danger btn">    Create
</button>
      </footer>
</modal-dialog></div>


  <h2 id="blob-path" class="breadcrumb flex-auto flex-self-center min-width-0 text-normal mx-2 width-full width-md-auto flex-order-1 flex-md-order-none mt-3 mt-md-0">
    <span class="js-repo-root text-bold"><span class="js-path-segment d-inline-block wb-break-all"><a data-turbo-frame="repo-content-turbo-frame" href="/mrdoob/three.js"><span>three.js</span></a></span></span><span class="separator">/</span><span class="js-path-segment d-inline-block wb-break-all"><a data-turbo-frame="repo-content-turbo-frame" href="/mrdoob/three.js/tree/dev/examples"><span>examples</span></a></span><span class="separator">/</span><strong class="final-path">main.css</strong>
  </h2>
    <a href="/mrdoob/three.js/find/dev" data-pjax="" data-hotkey="t" data-view-component="true" class="btn mr-2 d-none d-md-block">    Go to file
</a>
  <details id="blob-more-options-details" data-view-component="true" class="details-overlay details-reset position-relative">
    <summary role="button" data-view-component="true" class="btn">    <svg aria-label="More options" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-kebab-horizontal">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg>
</summary>
  <div data-view-component="true">      <ul class="dropdown-menu dropdown-menu-sw">
        <li class="d-block d-md-none">
          <a class="dropdown-item d-flex flex-items-baseline" data-hydro-click="{&quot;event_type&quot;:&quot;repository.click&quot;,&quot;payload&quot;:{&quot;target&quot;:&quot;FIND_FILE_BUTTON&quot;,&quot;repository_id&quot;:576201,&quot;originating_url&quot;:&quot;https://github.com/mrdoob/three.js/blob/dev/examples/main.css&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4179258304fea8248cb0b9459a4f2cc3f8394ad66064610694d346971af91b43" data-ga-click="Repository, find file, location:repo overview" data-hotkey="t" href="/mrdoob/three.js/find/dev">
            <span class="flex-auto">Go to file</span>
            <span class="text-small color-fg-muted" aria-hidden="true">T</span>
</a>        </li>
        <li data-toggle-for="blob-more-options-details">
            <button data-toggle-for="jumpto-line-details-dialog" type="button" data-view-component="true" class="dropdown-item btn-link">    <span class="d-flex flex-items-baseline">
              <span class="flex-auto">Go to line</span>
              <span class="text-small color-fg-muted" aria-hidden="true">L</span>
            </span>
</button>        </li>
        <li class="dropdown-divider" role="none"></li>
        <li>
          <clipboard-copy data-toggle-for="blob-more-options-details" aria-label="Copy path" value="examples/main.css" data-view-component="true" class="dropdown-item cursor-pointer">
    
            Copy path

</clipboard-copy>        </li>
        <li>
          <clipboard-copy data-toggle-for="blob-more-options-details" aria-label="Copy permalink" value="https://github.com/mrdoob/three.js/blob/00a427c48e2a5339cf8a1e68f1da44cc0065ab21/examples/main.css" data-view-component="true" class="dropdown-item cursor-pointer">
    
            <span class="d-flex flex-items-baseline">
              <span class="flex-auto">Copy permalink</span>
            </span>

</clipboard-copy>        </li>
      </ul>
</div>
</details></div>





    <div id="spoof-warning" class="mt-0 pb-3" hidden aria-hidden>
  <div data-view-component="true" class="flash flash-warn mt-0 clearfix">
  
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert float-left mt-1">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>

      <div class="overflow-hidden">This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.</div>


  
</div></div>

    <include-fragment src="/mrdoob/three.js/spoofed_commit_check/00a427c48e2a5339cf8a1e68f1da44cc0065ab21" data-test-selector="spoofed-commit-check"></include-fragment>

    <div class="Box d-flex flex-column flex-shrink-0 mb-3">
  <include-fragment src="/mrdoob/three.js/contributors/dev/examples/main.css" class="commit-loader">
    <div class="Box-header d-flex flex-items-center">
      <div class="Skeleton avatar avatar-user flex-shrink-0 ml-n1 mr-n1 mt-n1 mb-n1" style="width:24px;height:24px;"></div>
      <div class="Skeleton Skeleton--text col-5 ml-2">&nbsp;</div>
    </div>

    <div class="Box-body d-flex flex-items-center" >
      <div class="Skeleton Skeleton--text col-1">&nbsp;</div>
      <span class="color-fg-danger h6 loader-error">Cannot retrieve contributors at this time</span>
    </div>
</include-fragment></div>








  
    <div data-target="readme-toc.content" class="Box mt-3 position-relative">
      
  <div
    class="Box-header js-blob-header py-2 pr-2 d-flex flex-shrink-0 flex-md-row flex-items-center"
    
  >


  <div class="text-mono f6 flex-auto pr-3 flex-order-2 flex-md-order-1">

      91 lines (81 sloc)
      <span class="file-info-divider"></span>
    1.37 KB
  </div>

  <div class="d-flex py-1 py-md-0 flex-auto flex-order-1 flex-md-order-2 flex-sm-grow-0 flex-justify-between hide-sm hide-md">
      

    <div class="BtnGroup">
        <a data-permalink-href="/mrdoob/three.js/raw/00a427c48e2a5339cf8a1e68f1da44cc0065ab21/examples/main.css" href="/mrdoob/three.js/raw/dev/examples/main.css" id="raw-url" group_item="true" data-view-component="true" class="js-permalink-replaceable-link Button--secondary Button--small Button">    <span class="Button-content">
      <span class="Button-label">Raw</span>
    </span>
</a>  
          <a data-permalink-href="/mrdoob/three.js/blame/00a427c48e2a5339cf8a1e68f1da44cc0065ab21/examples/main.css" href="/mrdoob/three.js/blame/dev/examples/main.css" group_item="true" data-hotkey="b" data-view-component="true" class="js-update-url-with-hash js-permalink-replaceable-link Button--secondary Button--small Button">    <span class="Button-content">
      <span class="Button-label">Blame</span>
    </span>
</a>  
    </div>

    <div class="d-flex">
        
<div class="ml-1" >
  <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="BtnGroup-parent js-update-url-with-hash " data-turbo="false" action="/mrdoob/three.js/edit/dev/examples/main.css" accept-charset="UTF-8" method="post"><input type="hidden" name="authenticity_token" value="l8FZaFmxMUH5u73tMnu1VZ5sm5ge1hlM1Mc4wT2Y6jjPIdb-sHDadSAsWNmL9nh3lQCktc1JeMkU7r_EFaW5Pg" autocomplete="off" />
      <button disabled="disabled" title="You must be signed in to make or propose changes" data-hotkey="e" data-disable-with="" type="submit" data-view-component="true" class="btn-sm BtnGroup-item btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-pencil">
    <path d="M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z"></path>
</svg>
</button></form>
  <details class="details-reset details-overlay select-menu BtnGroup-parent d-inline-block position-relative">
      <summary data-disable-invalid="" data-disable-with="" data-dropdown-tracking="{&quot;type&quot;:&quot;blob_edit_dropdown.more_options_click&quot;,&quot;context&quot;:{&quot;repository_id&quot;:576201,&quot;actor_id&quot;:null,&quot;github_dev_enabled&quot;:false,&quot;edit_enabled&quot;:false,&quot;small_screen&quot;:false}}" aria-label="Select additional options" data-view-component="true" class="js-blob-dropdown-click select-menu-button btn-sm btn BtnGroup-item float-none px-2">
</summary>    <div class="SelectMenu right-0">
      <div class="SelectMenu-modal width-full">
        <div class="SelectMenu-list SelectMenu-list--borderless py-2">
          <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="SelectMenu-item js-update-url-with-hash " data-turbo="false" action="/mrdoob/three.js/edit/dev/examples/main.css" accept-charset="UTF-8" method="post"><input type="hidden" name="authenticity_token" value="l5YdqnutomVeT6yoj9fs4psiZTvpUqHfF_Q_lriwAI_PdpI8kmxJUYfYSZw2WiHAkE5aFjrNwFrX3biTkI1TiQ" autocomplete="off" />
              <button disabled="disabled" title="You must be signed in to make or propose changes" type="submit" data-view-component="true" class="btn-invisible btn width-full d-flex flex-justify-between color-fg-muted text-normal p-0">    <div class="mr-5">Edit this file</div>
              <div class="color-fg-muted">E</div>
</button></form>

            <a data-platforms="windows,mac" aria-label="Open this file in GitHub Desktop" href="https://desktop.github.com" data-view-component="true" class="SelectMenu-item no-wrap js-remove-unless-platform width-full text-normal color-fg-default f5">
              Open in GitHub Desktop
</a>        </div>
      </div>
    </div>
  </details>
</div>


        
<div >
  </div>


          <button class="btn-octicon btn-octicon-danger disabled tooltipped tooltipped-nw" disabled
            aria-label="You must be signed in to make or propose changes" type="button">
            <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-trash">
    <path d="M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z"></path>
</svg>
          </button>
    </div>
  </div>

    <div class="d-flex hide-lg hide-xl flex-order-2 flex-grow-0">
      <details class="dropdown details-reset details-overlay d-inline-block">
        <summary
          class="js-blob-dropdown-click btn-octicon"
          aria-haspopup="true"
          aria-label="Possible actions"
          
          data-dropdown-tracking="{&quot;type&quot;:&quot;blob_edit_dropdown.more_options_click&quot;,&quot;context&quot;:{&quot;repository_id&quot;:576201,&quot;actor_id&quot;:null,&quot;github_dev_enabled&quot;:false,&quot;edit_enabled&quot;:false,&quot;small_screen&quot;:true}}"
        >
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-kebab-horizontal">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg>
        </summary>

        <ul class="dropdown-menu dropdown-menu-sw" style="width: 175px">
            <li>
                <a class="dropdown-item tooltipped tooltipped-nw js-remove-unless-platform"
                   data-platforms="windows,mac"
                   href="https://desktop.github.com">
                  Open with Desktop
                </a>
            </li>
          <li>
            <a class="dropdown-item" href="/mrdoob/three.js/raw/dev/examples/main.css">
              View raw
            </a>
          </li>
            <li>
                          </li>
            <li>
              <a class="dropdown-item" href="/mrdoob/three.js/blame/dev/examples/main.css">
                View blame
              </a>
            </li>

        </ul>
      </details>
    </div>
</div>


      
    <div itemprop="text" class="Box-body p-0 blob-wrapper data type-css  gist-border-0">

        
<div class="js-check-bidi js-blob-code-container blob-code-content">

  <template class="js-file-alert-template">
  <div data-view-component="true" class="flash flash-warn flash-full d-flex flex-items-center">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <span>
      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      <a href="https://github.co/hiddenchars" target="_blank">Learn more about bidirectional Unicode characters</a>
    </span>


  <div data-view-component="true" class="flash-action">        <a href="{{ revealButtonHref }}" data-view-component="true" class="btn-sm btn">    Show hidden characters
</a>
</div>
</div></template>
<template class="js-line-alert-template">
  <span aria-label="This line has hidden Unicode characters" data-view-component="true" class="line-alert tooltipped tooltipped-e">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
</span></template>

  <table data-hpc class="highlight tab-size js-file-line-container js-code-nav-container js-tagsearch-file" data-tab-size="8" data-paste-markdown-skip data-tagsearch-lang="CSS" data-tagsearch-path="examples/main.css">
        <tr>
          <td id="L1" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="1"></td>
          <td id="LC1" class="blob-code blob-code-inner js-file-line"><span class=pl-ent>body</span> {</td>
        </tr>
        <tr>
          <td id="L2" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="2"></td>
          <td id="LC2" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>margin</span><span class=pl-kos>:</span> <span class=pl-c1>0</span>;</td>
        </tr>
        <tr>
          <td id="L3" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="3"></td>
          <td id="LC3" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>background-color</span><span class=pl-kos>:</span> <span class=pl-pds><span class=pl-kos>#</span>000</span>;</td>
        </tr>
        <tr>
          <td id="L4" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="4"></td>
          <td id="LC4" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>color</span><span class=pl-kos>:</span> <span class=pl-pds><span class=pl-kos>#</span>fff</span>;</td>
        </tr>
        <tr>
          <td id="L5" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="5"></td>
          <td id="LC5" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>font-family</span><span class=pl-kos>:</span> Monospace;</td>
        </tr>
        <tr>
          <td id="L6" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="6"></td>
          <td id="LC6" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>font-size</span><span class=pl-kos>:</span> <span class=pl-c1>13<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L7" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="7"></td>
          <td id="LC7" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>line-height</span><span class=pl-kos>:</span> <span class=pl-c1>24<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L8" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="8"></td>
          <td id="LC8" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>overscroll-behavior</span><span class=pl-kos>:</span> none;</td>
        </tr>
        <tr>
          <td id="L9" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="9"></td>
          <td id="LC9" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L10" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="10"></td>
          <td id="LC10" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L11" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="11"></td>
          <td id="LC11" class="blob-code blob-code-inner js-file-line"><span class=pl-ent>a</span> {</td>
        </tr>
        <tr>
          <td id="L12" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="12"></td>
          <td id="LC12" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>color</span><span class=pl-kos>:</span> <span class=pl-pds><span class=pl-kos>#</span>ff0</span>;</td>
        </tr>
        <tr>
          <td id="L13" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="13"></td>
          <td id="LC13" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>text-decoration</span><span class=pl-kos>:</span> none;</td>
        </tr>
        <tr>
          <td id="L14" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="14"></td>
          <td id="LC14" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L15" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="15"></td>
          <td id="LC15" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L16" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="16"></td>
          <td id="LC16" class="blob-code blob-code-inner js-file-line"><span class=pl-ent>a</span><span class=pl-kos>:</span><span class=pl-c1>hover</span> {</td>
        </tr>
        <tr>
          <td id="L17" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="17"></td>
          <td id="LC17" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>text-decoration</span><span class=pl-kos>:</span> underline;</td>
        </tr>
        <tr>
          <td id="L18" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="18"></td>
          <td id="LC18" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L19" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="19"></td>
          <td id="LC19" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L20" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="20"></td>
          <td id="LC20" class="blob-code blob-code-inner js-file-line"><span class=pl-ent>button</span> {</td>
        </tr>
        <tr>
          <td id="L21" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="21"></td>
          <td id="LC21" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>cursor</span><span class=pl-kos>:</span> pointer;</td>
        </tr>
        <tr>
          <td id="L22" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="22"></td>
          <td id="LC22" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>text-transform</span><span class=pl-kos>:</span> uppercase;</td>
        </tr>
        <tr>
          <td id="L23" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="23"></td>
          <td id="LC23" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L24" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="24"></td>
          <td id="LC24" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L25" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="25"></td>
          <td id="LC25" class="blob-code blob-code-inner js-file-line"><span class=pl-kos>#</span><span class=pl-c1>info</span> {</td>
        </tr>
        <tr>
          <td id="L26" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="26"></td>
          <td id="LC26" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>position</span><span class=pl-kos>:</span> absolute;</td>
        </tr>
        <tr>
          <td id="L27" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="27"></td>
          <td id="LC27" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>top</span><span class=pl-kos>:</span> <span class=pl-c1>0<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L28" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="28"></td>
          <td id="LC28" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>width</span><span class=pl-kos>:</span> <span class=pl-c1>100<span class=pl-smi>%</span></span>;</td>
        </tr>
        <tr>
          <td id="L29" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="29"></td>
          <td id="LC29" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>padding</span><span class=pl-kos>:</span> <span class=pl-c1>10<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L30" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="30"></td>
          <td id="LC30" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>box-sizing</span><span class=pl-kos>:</span> border-box;</td>
        </tr>
        <tr>
          <td id="L31" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="31"></td>
          <td id="LC31" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>text-align</span><span class=pl-kos>:</span> center;</td>
        </tr>
        <tr>
          <td id="L32" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="32"></td>
          <td id="LC32" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>-moz-user-select</span><span class=pl-kos>:</span> none;</td>
        </tr>
        <tr>
          <td id="L33" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="33"></td>
          <td id="LC33" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>-webkit-user-select</span><span class=pl-kos>:</span> none;</td>
        </tr>
        <tr>
          <td id="L34" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="34"></td>
          <td id="LC34" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>-ms-user-select</span><span class=pl-kos>:</span> none;</td>
        </tr>
        <tr>
          <td id="L35" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="35"></td>
          <td id="LC35" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>user-select</span><span class=pl-kos>:</span> none;</td>
        </tr>
        <tr>
          <td id="L36" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="36"></td>
          <td id="LC36" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>pointer-events</span><span class=pl-kos>:</span> none;</td>
        </tr>
        <tr>
          <td id="L37" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="37"></td>
          <td id="LC37" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>z-index</span><span class=pl-kos>:</span> <span class=pl-c1>1</span>; <span class=pl-c>/* TODO Solve this in HTML */</span></td>
        </tr>
        <tr>
          <td id="L38" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="38"></td>
          <td id="LC38" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L39" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="39"></td>
          <td id="LC39" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L40" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="40"></td>
          <td id="LC40" class="blob-code blob-code-inner js-file-line"><span class=pl-ent>a</span><span class=pl-kos>,</span> <span class=pl-ent>button</span><span class=pl-kos>,</span> <span class=pl-ent>input</span><span class=pl-kos>,</span> <span class=pl-ent>select</span> {</td>
        </tr>
        <tr>
          <td id="L41" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="41"></td>
          <td id="LC41" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>pointer-events</span><span class=pl-kos>:</span> auto;</td>
        </tr>
        <tr>
          <td id="L42" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="42"></td>
          <td id="LC42" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L43" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="43"></td>
          <td id="LC43" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L44" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="44"></td>
          <td id="LC44" class="blob-code blob-code-inner js-file-line">.<span class=pl-c1>lil-gui</span> {</td>
        </tr>
        <tr>
          <td id="L45" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="45"></td>
          <td id="LC45" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>z-index</span><span class=pl-kos>:</span> <span class=pl-c1>2</span> <span class=pl-k>!important</span>; <span class=pl-c>/* TODO Solve this in HTML */</span></td>
        </tr>
        <tr>
          <td id="L46" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="46"></td>
          <td id="LC46" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L47" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="47"></td>
          <td id="LC47" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L48" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="48"></td>
          <td id="LC48" class="blob-code blob-code-inner js-file-line"><span class=pl-k>@media</span> all <span class=pl-c1>and</span> ( <span class=pl-c1>max-width</span><span class=pl-kos>:</span> <span class=pl-c1>640<span class=pl-smi>px</span></span> ) {</td>
        </tr>
        <tr>
          <td id="L49" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="49"></td>
          <td id="LC49" class="blob-code blob-code-inner js-file-line">	.<span class=pl-c1>lil-gui</span>.<span class=pl-c1>root</span> { </td>
        </tr>
        <tr>
          <td id="L50" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="50"></td>
          <td id="LC50" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>right</span><span class=pl-kos>:</span> auto;</td>
        </tr>
        <tr>
          <td id="L51" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="51"></td>
          <td id="LC51" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>top</span><span class=pl-kos>:</span> auto;</td>
        </tr>
        <tr>
          <td id="L52" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="52"></td>
          <td id="LC52" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>max-height</span><span class=pl-kos>:</span> <span class=pl-c1>50<span class=pl-smi>%</span></span>;</td>
        </tr>
        <tr>
          <td id="L53" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="53"></td>
          <td id="LC53" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>max-width</span><span class=pl-kos>:</span> <span class=pl-c1>80<span class=pl-smi>%</span></span>;</td>
        </tr>
        <tr>
          <td id="L54" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="54"></td>
          <td id="LC54" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>bottom</span><span class=pl-kos>:</span> <span class=pl-c1>0</span>;</td>
        </tr>
        <tr>
          <td id="L55" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="55"></td>
          <td id="LC55" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>left</span><span class=pl-kos>:</span> <span class=pl-c1>0</span>;</td>
        </tr>
        <tr>
          <td id="L56" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="56"></td>
          <td id="LC56" class="blob-code blob-code-inner js-file-line">	}</td>
        </tr>
        <tr>
          <td id="L57" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="57"></td>
          <td id="LC57" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L58" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="58"></td>
          <td id="LC58" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L59" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="59"></td>
          <td id="LC59" class="blob-code blob-code-inner js-file-line"><span class=pl-kos>#</span><span class=pl-c1>overlay</span> {</td>
        </tr>
        <tr>
          <td id="L60" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="60"></td>
          <td id="LC60" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>position</span><span class=pl-kos>:</span> absolute;</td>
        </tr>
        <tr>
          <td id="L61" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="61"></td>
          <td id="LC61" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>font-size</span><span class=pl-kos>:</span> <span class=pl-c1>16<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L62" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="62"></td>
          <td id="LC62" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>z-index</span><span class=pl-kos>:</span> <span class=pl-c1>2</span>;</td>
        </tr>
        <tr>
          <td id="L63" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="63"></td>
          <td id="LC63" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>top</span><span class=pl-kos>:</span> <span class=pl-c1>0</span>;</td>
        </tr>
        <tr>
          <td id="L64" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="64"></td>
          <td id="LC64" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>left</span><span class=pl-kos>:</span> <span class=pl-c1>0</span>;</td>
        </tr>
        <tr>
          <td id="L65" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="65"></td>
          <td id="LC65" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>width</span><span class=pl-kos>:</span> <span class=pl-c1>100<span class=pl-smi>%</span></span>;</td>
        </tr>
        <tr>
          <td id="L66" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="66"></td>
          <td id="LC66" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>height</span><span class=pl-kos>:</span> <span class=pl-c1>100<span class=pl-smi>%</span></span>;</td>
        </tr>
        <tr>
          <td id="L67" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="67"></td>
          <td id="LC67" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>display</span><span class=pl-kos>:</span> flex;</td>
        </tr>
        <tr>
          <td id="L68" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="68"></td>
          <td id="LC68" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>align-items</span><span class=pl-kos>:</span> center;</td>
        </tr>
        <tr>
          <td id="L69" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="69"></td>
          <td id="LC69" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>justify-content</span><span class=pl-kos>:</span> center;</td>
        </tr>
        <tr>
          <td id="L70" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="70"></td>
          <td id="LC70" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>flex-direction</span><span class=pl-kos>:</span> column;</td>
        </tr>
        <tr>
          <td id="L71" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="71"></td>
          <td id="LC71" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>background</span><span class=pl-kos>:</span> <span class=pl-en>rgba</span>(<span class=pl-c1>0</span><span class=pl-kos>,</span><span class=pl-c1>0</span><span class=pl-kos>,</span><span class=pl-c1>0</span><span class=pl-kos>,</span><span class=pl-c1>0.7</span>);</td>
        </tr>
        <tr>
          <td id="L72" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="72"></td>
          <td id="LC72" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
        <tr>
          <td id="L73" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="73"></td>
          <td id="LC73" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L74" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="74"></td>
          <td id="LC74" class="blob-code blob-code-inner js-file-line">	<span class=pl-kos>#</span><span class=pl-c1>overlay</span> <span class=pl-ent>button</span> {</td>
        </tr>
        <tr>
          <td id="L75" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="75"></td>
          <td id="LC75" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>background</span><span class=pl-kos>:</span> transparent;</td>
        </tr>
        <tr>
          <td id="L76" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="76"></td>
          <td id="LC76" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>border</span><span class=pl-kos>:</span> <span class=pl-c1>0</span>;</td>
        </tr>
        <tr>
          <td id="L77" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="77"></td>
          <td id="LC77" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>border</span><span class=pl-kos>:</span> <span class=pl-c1>1<span class=pl-smi>px</span></span> solid <span class=pl-en>rgb</span>(<span class=pl-c1>255</span><span class=pl-kos>,</span> <span class=pl-c1>255</span><span class=pl-kos>,</span> <span class=pl-c1>255</span>);</td>
        </tr>
        <tr>
          <td id="L78" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="78"></td>
          <td id="LC78" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>border-radius</span><span class=pl-kos>:</span> <span class=pl-c1>4<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L79" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="79"></td>
          <td id="LC79" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>color</span><span class=pl-kos>:</span> <span class=pl-pds><span class=pl-kos>#</span>ffffff</span>;</td>
        </tr>
        <tr>
          <td id="L80" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="80"></td>
          <td id="LC80" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>padding</span><span class=pl-kos>:</span> <span class=pl-c1>12<span class=pl-smi>px</span></span> <span class=pl-c1>18<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L81" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="81"></td>
          <td id="LC81" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>text-transform</span><span class=pl-kos>:</span> uppercase;</td>
        </tr>
        <tr>
          <td id="L82" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="82"></td>
          <td id="LC82" class="blob-code blob-code-inner js-file-line">		<span class=pl-c1>cursor</span><span class=pl-kos>:</span> pointer;</td>
        </tr>
        <tr>
          <td id="L83" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="83"></td>
          <td id="LC83" class="blob-code blob-code-inner js-file-line">	}</td>
        </tr>
        <tr>
          <td id="L84" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="84"></td>
          <td id="LC84" class="blob-code blob-code-inner js-file-line">
</td>
        </tr>
        <tr>
          <td id="L85" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="85"></td>
          <td id="LC85" class="blob-code blob-code-inner js-file-line"><span class=pl-kos>#</span><span class=pl-c1>notSupported</span> {</td>
        </tr>
        <tr>
          <td id="L86" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="86"></td>
          <td id="LC86" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>width</span><span class=pl-kos>:</span> <span class=pl-c1>50<span class=pl-smi>%</span></span>;</td>
        </tr>
        <tr>
          <td id="L87" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="87"></td>
          <td id="LC87" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>margin</span><span class=pl-kos>:</span> auto;</td>
        </tr>
        <tr>
          <td id="L88" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="88"></td>
          <td id="LC88" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>background-color</span><span class=pl-kos>:</span> <span class=pl-pds><span class=pl-kos>#</span>f00</span>;</td>
        </tr>
        <tr>
          <td id="L89" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="89"></td>
          <td id="LC89" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>margin-top</span><span class=pl-kos>:</span> <span class=pl-c1>20<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L90" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="90"></td>
          <td id="LC90" class="blob-code blob-code-inner js-file-line">	<span class=pl-c1>padding</span><span class=pl-kos>:</span> <span class=pl-c1>10<span class=pl-smi>px</span></span>;</td>
        </tr>
        <tr>
          <td id="L91" class="blob-num js-line-number js-code-nav-line-number js-blob-rnum" data-line-number="91"></td>
          <td id="LC91" class="blob-code blob-code-inner js-file-line">}</td>
        </tr>
  </table>
</div>

  <details class="details-reset details-overlay BlobToolbar position-absolute js-file-line-actions dropdown d-none" aria-hidden="true">
    <summary class="btn-octicon ml-0 px-2 p-0 color-bg-default border color-border-default rounded-2" aria-label="Inline file action toolbar">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-kebab-horizontal">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg>
    </summary>
    <details-menu>

      <ul class="BlobToolbar-dropdown dropdown-menu dropdown-menu-se ml-2 mt-2"
      style="width:185px"
      >
        <li>
          <clipboard-copy role="menuitem" class="dropdown-item" id="js-copy-lines" style="cursor:pointer;" aria-label="Copy lines">
            Copy lines
          </clipboard-copy>
        </li>
        <li>
          <clipboard-copy role="menuitem" class="dropdown-item" id="js-copy-permalink" style="cursor:pointer;" aria-label="Copy permalink">
            Copy permalink
          </clipboard-copy>
        </li>
        <li><a class="dropdown-item js-update-url-with-hash" id="js-view-git-blame" role="menuitem" href="/mrdoob/three.js/blame/00a427c48e2a5339cf8a1e68f1da44cc0065ab21/examples/main.css">View git blame</a></li>
          <li><a class="dropdown-item" id="js-new-issue" role="menuitem" href="/mrdoob/three.js/issues/new">Reference in new issue</a></li>
      </ul>
    </details-menu>
  </details>

    </div>

    </div>


  

  <details class="details-reset details-overlay details-overlay-dark" id="jumpto-line-details-dialog">
    <summary data-hotkey="l" aria-label="Jump to line"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast linejump overflow-hidden" aria-label="Jump to line">
      <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="js-jump-to-line-form Box-body d-flex" data-turbo="false" action="" accept-charset="UTF-8" method="get">
        <input class="form-control flex-auto mr-3 linejump-input js-jump-to-line-field" type="text" placeholder="Jump to line&hellip;" aria-label="Jump to line" autofocus>
          <button data-close-dialog="" type="submit" data-view-component="true" class="btn">    Go
</button>
</form>    </details-dialog>
  </details>



</div>

  </div>


  </div>

</turbo-frame>


    </main>
  </div>

  </div>

          <footer class="footer width-full container-xl p-responsive" role="contentinfo">
  <h2 class='sr-only'>Footer</h2>

  <div class="position-relative d-flex flex-items-center pb-2 f6 color-fg-muted border-top color-border-muted flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap mt-6 pt-6">
    <div class="list-style-none d-flex flex-wrap col-0 col-lg-2 flex-justify-start flex-lg-justify-between mb-2 mb-lg-0">
      <div class="mt-2 mt-lg-0 d-flex flex-items-center">
        <a aria-label="Homepage" title="GitHub" class="footer-octicon mr-2" href="https://github.com">
          <svg aria-hidden="true" height="24" viewBox="0 0 16 16" version="1.1" width="24" data-view-component="true" class="octicon octicon-mark-github">
    <path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"></path>
</svg>
</a>        <span>
        &copy; 2023 GitHub, Inc.
        </span>
      </div>
    </div>

    <nav aria-label='Footer' class="col-12 col-lg-8">
      <h3 class='sr-only' id='sr-footer-heading'>Footer navigation</h3>
      <ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0" aria-labelledby='sr-footer-heading'>
          <li class="mr-3 mr-lg-0"><a href="https://docs.github.com/site-policy/github-terms/github-terms-of-service" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}">Terms</a></li>
          <li class="mr-3 mr-lg-0"><a href="https://docs.github.com/site-policy/privacy-policies/github-privacy-statement" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}">Privacy</a></li>
          <li class="mr-3 mr-lg-0"><a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}" href="https://github.com/security">Security</a></li>
          <li class="mr-3 mr-lg-0"><a href="https://www.githubstatus.com/" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}">Status</a></li>
          <li class="mr-3 mr-lg-0"><a data-ga-click="Footer, go to help, text:Docs" href="https://docs.github.com">Docs</a></li>
          <li class="mr-3 mr-lg-0"><a href="https://support.github.com?tags=dotcom-footer" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}">Contact GitHub</a></li>
          <li class="mr-3 mr-lg-0"><a href="https://github.com/pricing" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Pricing&quot;,&quot;label&quot;:&quot;text:Pricing&quot;}">Pricing</a></li>
        <li class="mr-3 mr-lg-0"><a href="https://docs.github.com" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to api&quot;,&quot;label&quot;:&quot;text:api&quot;}">API</a></li>
        <li class="mr-3 mr-lg-0"><a href="https://services.github.com" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to training&quot;,&quot;label&quot;:&quot;text:training&quot;}">Training</a></li>
          <li class="mr-3 mr-lg-0"><a href="https://github.blog" data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to blog&quot;,&quot;label&quot;:&quot;text:blog&quot;}">Blog</a></li>
          <li><a data-ga-click="Footer, go to about, text:about" href="https://github.com/about">About</a></li>
      </ul>
    </nav>
  </div>

  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 color-fg-muted"></span>
  </div>
</footer>




  <div id="ajax-error-message" class="ajax-error-message flash flash-error" hidden>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
    </button>
    You can’t perform that action at this time.
  </div>

  <div class="js-stale-session-flash flash flash-warn flash-banner" hidden
    >
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <span class="js-stale-session-flash-signed-in" hidden>You signed in with another tab or window. <a href="">Reload</a> to refresh your session.</span>
    <span class="js-stale-session-flash-signed-out" hidden>You signed out in another tab or window. <a href="">Reload</a> to refresh your session.</span>
  </div>
    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open>
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog>
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;" tabindex="0">
  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large" style="width:360px;">
  </div>
</div>

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div>

    <div id="js-global-screen-reader-notice" class="sr-only" aria-live="polite" ></div>
  </body>
</html>




================================================
FILE: docs/meshopt_decoder.js
================================================
// This file is part of meshoptimizer library and is distributed under the terms of MIT License.
// Copyright (C) 2016-2024, by Arseny Kapoulkine (arseny.kapoulkine@gmail.com)
var MeshoptDecoder = (function () {
	'use strict';
	// Built with clang version 18.1.2
	// Built from meshoptimizer 0.21
	var wasm_base =
		'b9H79Tebbbe8Fv9Gbb9Gvuuuuueu9Giuuub9Geueu9Giuuueuikqbeeedddillviebeoweuec:q;iekr;leDo9TW9T9VV95dbH9F9F939H79T9F9J9H229F9Jt9VV7bb8A9TW79O9V9Wt9F9KW9J9V9KW9wWVtW949c919M9MWVbeY9TW79O9V9Wt9F9KW9J9V9KW69U9KW949c919M9MWVbdE9TW79O9V9Wt9F9KW9J9V9KW69U9KW949tWG91W9U9JWbiL9TW79O9V9Wt9F9KW9J9V9KWS9P2tWV9p9JtblK9TW79O9V9Wt9F9KW9J9V9KWS9P2tWV9r919HtbvL9TW79O9V9Wt9F9KW9J9V9KWS9P2tWVT949Wbol79IV9Rbrq;w8Wqdbk;esezu8Jjjjjbcj;eb9Rgv8Kjjjjbc9:hodnadcefal0mbcuhoaiRbbc:Ge9hmbavaialfgrad9Radz1jjjbhwcj;abad9Uc;WFbGgocjdaocjd6EhDaicefhocbhqdnindndndnaeaq9nmbaDaeaq9RaqaDfae6Egkcsfglcl4cifcd4hxalc9WGgmTmecbhPawcjdfhsaohzinaraz9Rax6mvarazaxfgo9RcK6mvczhlcbhHinalgic9WfgOawcj;cbffhldndndndndnazaOco4fRbbaHcoG4ciGPlbedibkal9cb83ibalcwf9cb83ibxikalaoRblaoRbbgOco4gAaAciSgAE86bbawcj;cbfaifglcGfaoclfaAfgARbbaOcl4ciGgCaCciSgCE86bbalcVfaAaCfgARbbaOcd4ciGgCaCciSgCE86bbalc7faAaCfgARbbaOciGgOaOciSgOE86bbalctfaAaOfgARbbaoRbegOco4gCaCciSgCE86bbalc91faAaCfgARbbaOcl4ciGgCaCciSgCE86bbalc4faAaCfgARbbaOcd4ciGgCaCciSgCE86bbalc93faAaCfgARbbaOciGgOaOciSgOE86bbalc94faAaOfgARbbaoRbdgOco4gCaCciSgCE86bbalc95faAaCfgARbbaOcl4ciGgCaCciSgCE86bbalc96faAaCfgARbbaOcd4ciGgCaCciSgCE86bbalc97faAaCfgARbbaOciGgOaOciSgOE86bbalc98faAaOfgORbbaoRbigoco4gAaAciSgAE86bbalc99faOaAfgORbbaocl4ciGgAaAciSgAE86bbalc9:faOaAfgORbbaocd4ciGgAaAciSgAE86bbalcufaOaAfglRbbaociGgoaociSgoE86bbalaofhoxdkalaoRbwaoRbbgOcl4gAaAcsSgAE86bbawcj;cbfaifglcGfaocwfaAfgARbbaOcsGgOaOcsSgOE86bbalcVfaAaOfgORbbaoRbegAcl4gCaCcsSgCE86bbalc7faOaCfgORbbaAcsGgAaAcsSgAE86bbalctfaOaAfgORbbaoRbdgAcl4gCaCcsSgCE86bbalc91faOaCfgORbbaAcsGgAaAcsSgAE86bbalc4faOaAfgORbbaoRbigAcl4gCaCcsSgCE86bbalc93faOaCfgORbbaAcsGgAaAcsSgAE86bbalc94faOaAfgORbbaoRblgAcl4gCaCcsSgCE86bbalc95faOaCfgORbbaAcsGgAaAcsSgAE86bbalc96faOaAfgORbbaoRbvgAcl4gCaCcsSgCE86bbalc97faOaCfgORbbaAcsGgAaAcsSgAE86bbalc98faOaAfgORbbaoRbogAcl4gCaCcsSgCE86bbalc99faOaCfgORbbaAcsGgAaAcsSgAE86bbalc9:faOaAfgORbbaoRbrgocl4gAaAcsSgAE86bbalcufaOaAfglRbbaocsGgoaocsSgoE86bbalaofhoxekalao8Pbb83bbalcwfaocwf8Pbb83bbaoczfhokdnaiam9pmbaHcdfhHaiczfhlarao9RcL0mekkaiam6mvaoTmvdnakTmbawaPfRbbhHawcj;cbfhlashiakhOinaialRbbgzce4cbazceG9R7aHfgH86bbaiadfhialcefhlaOcufgOmbkkascefhsaohzaPcefgPad9hmbxikkcbc99arao9Radcaadca0ESEhoxlkaoaxad2fhCdnakmbadhlinaoTmlarao9Rax6mlaoaxfhoalcufglmbkaChoxekcbhmawcjdfhAinarao9Rax6miawamfRbbhHawcj;cbfhlaAhiakhOinaialRbbgzce4cbazceG9R7aHfgH86bbaiadfhialcefhlaOcufgOmbkaAcefhAaoaxfhoamcefgmad9hmbkaChokabaqad2fawcjdfakad2z1jjjb8Aawawcjdfakcufad2fadz1jjjb8Aakaqfhqaombkc9:hoxekc9:hokavcj;ebf8Kjjjjbaok;cseHu8Jjjjjbc;ae9Rgv8Kjjjjbc9:hodnaeci9UgrcHfal0mbcuhoaiRbbgwc;WeGc;Ge9hmbawcsGgwce0mbavc;abfcFecjez:jjjjb8AavcUf9cu83ibavc8Wf9cu83ibavcyf9cu83ibavcaf9cu83ibavcKf9cu83ibavczf9cu83ibav9cu83iwav9cu83ibaialfc9WfhDaicefgqarfhidnaeTmbcmcsawceSEhkcbhxcbhmcbhPcbhwcbhlindnaiaD9nmbc9:hoxikdndnaqRbbgoc;Ve0mbavc;abfalaocu7gscl4fcsGcitfgzydlhrazydbhzdnaocsGgHak9pmbavawasfcsGcdtfydbaxaHEhoaHThsdndnadcd9hmbabaPcetfgHaz87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHazBdbaHcwfaoBdbaHclfarBdbkaxasfhxcdhHavawcdtfaoBdbawasfhwcehsalhOxdkdndnaHcsSmbaHc987aHamffcefhoxekaicefhoai8SbbgHcFeGhsdndnaHcu9mmbaohixekaicvfhiascFbGhscrhHdninao8SbbgOcFbGaHtasVhsaOcu9kmeaocefhoaHcrfgHc8J9hmbxdkkaocefhikasce4cbasceG9R7amfhokdndnadcd9hmbabaPcetfgHaz87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHazBdbaHcwfaoBdbaHclfarBdbkcdhHavawcdtfaoBdbcehsawcefhwalhOaohmxekdnaocpe0mbaxcefgHavawaDaocsGfRbbgocl49RcsGcdtfydbaocz6gzEhravawao9RcsGcdtfydbaHazfgAaocsGgHEhoaHThCdndnadcd9hmbabaPcetfgHax87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHaxBdbaHcwfaoBdbaHclfarBdbkcdhsavawcdtfaxBdbavawcefgwcsGcdtfarBdbcihHavc;abfalcitfgOaxBdlaOarBdbavawazfgwcsGcdtfaoBdbalcefcsGhOawaCfhwaxhzaAaCfhxxekaxcbaiRbbgOEgzaoc;:eSgHfhraOcsGhCaOcl4hAdndnaOcs0mbarcefhoxekarhoavawaA9RcsGcdtfydbhrkdndnaCmbaocefhxxekaohxavawaO9RcsGcdtfydbhokdndnaHTmbaicefhHxekaicdfhHai8SbegscFeGhzdnascu9kmbaicofhXazcFbGhzcrhidninaH8SbbgscFbGaitazVhzascu9kmeaHcefhHaicrfgic8J9hmbkaXhHxekaHcefhHkazce4cbazceG9R7amfgmhzkdndnaAcsSmbaHhsxekaHcefhsaH8SbbgicFeGhrdnaicu9kmbaHcvfhXarcFbGhrcrhidninas8SbbgHcFbGaitarVhraHcu9kmeascefhsaicrfgic8J9hmbkaXhsxekascefhskarce4cbarceG9R7amfgmhrkdndnaCcsSmbashixekascefhias8SbbgocFeGhHdnaocu9kmbascvfhXaHcFbGhHcrhodninai8SbbgscFbGaotaHVhHascu9kmeaicefhiaocrfgoc8J9hmbkaXhixekaicefhikaHce4cbaHceG9R7amfgmhokdndnadcd9hmbabaPcetfgHaz87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHazBdbaHcwfaoBdbaHclfarBdbkcdhsavawcdtfazBdbavawcefgwcsGcdtfarBdbcihHavc;abfalcitfgXazBdlaXarBdbavawaOcz6aAcsSVfgwcsGcdtfaoBdbawaCTaCcsSVfhwalcefcsGhOkaqcefhqavc;abfaOcitfgOarBdlaOaoBdbavc;abfalasfcsGcitfgraoBdlarazBdbawcsGhwalaHfcsGhlaPcifgPae6mbkkcbc99aiaDSEhokavc;aef8Kjjjjbaok:flevu8Jjjjjbcz9Rhvc9:hodnaecvfal0mbcuhoaiRbbc;:eGc;qe9hmbav9cb83iwaicefhraialfc98fhwdnaeTmbdnadcdSmbcbhDindnaraw6mbc9:skarcefhoar8SbbglcFeGhidndnalcu9mmbaohrxekarcvfhraicFbGhicrhldninao8SbbgdcFbGaltaiVhiadcu9kmeaocefhoalcrfglc8J9hmbxdkkaocefhrkabaDcdtfaic8Etc8F91aicd47avcwfaiceGcdtVgoydbfglBdbaoalBdbaDcefgDae9hmbxdkkcbhDindnaraw6mbc9:skarcefhoar8SbbglcFeGhidndnalcu9mmbaohrxekarcvfhraicFbGhicrhldninao8SbbgdcFbGaltaiVhiadcu9kmeaocefhoalcrfglc8J9hmbxdkkaocefhrkabaDcetfaic8Etc8F91aicd47avcwfaiceGcdtVgoydbfgl87ebaoalBdbaDcefgDae9hmbkkcbc99arawSEhokaok:Lvoeue99dud99eud99dndnadcl9hmbaeTmeindndnabcdfgd8Sbb:Yab8Sbbgi:Ygl:l:tabcefgv8Sbbgo:Ygr:l:tgwJbb;:9cawawNJbbbbawawJbbbb9GgDEgq:mgkaqaicb9iEalMgwawNakaqaocb9iEarMgqaqNMM:r:vglNJbbbZJbbb:;aDEMgr:lJbbb9p9DTmbar:Ohixekcjjjj94hikadai86bbdndnaqalNJbbbZJbbb:;aqJbbbb9GEMgq:lJbbb9p9DTmbaq:Ohdxekcjjjj94hdkavad86bbdndnawalNJbbbZJbbb:;awJbbbb9GEMgw:lJbbb9p9DTmbaw:Ohdxekcjjjj94hdkabad86bbabclfhbaecufgembxdkkaeTmbindndnabclfgd8Ueb:Yab8Uebgi:Ygl:l:tabcdfgv8Uebgo:Ygr:l:tgwJb;:FSawawNJbbbbawawJbbbb9GgDEgq:mgkaqaicb9iEalMgwawNakaqaocb9iEarMgqaqNMM:r:vglNJbbbZJbbb:;aDEMgr:lJbbb9p9DTmbar:Ohixekcjjjj94hikadai87ebdndnaqalNJbbbZJbbb:;aqJbbbb9GEMgq:lJbbb9p9DTmbaq:Ohdxekcjjjj94hdkavad87ebdndnawalNJbbbZJbbb:;awJbbbb9GEMgw:lJbbb9p9DTmbaw:Ohdxekcjjjj94hdkabad87ebabcwfhbaecufgembkkk;oiliui99iue99dnaeTmbcbhiabhlindndnJ;Zl81Zalcof8UebgvciV:Y:vgoal8Ueb:YNgrJb;:FSNJbbbZJbbb:;arJbbbb9GEMgw:lJbbb9p9DTmbaw:OhDxekcjjjj94hDkalclf8Uebhqalcdf8UebhkabaiavcefciGfcetfaD87ebdndnaoak:YNgwJb;:FSNJbbbZJbbb:;awJbbbb9GEMgx:lJbbb9p9DTmbax:OhDxekcjjjj94hDkabaiavciGfgkcd7cetfaD87ebdndnaoaq:YNgoJb;:FSNJbbbZJbbb:;aoJbbbb9GEMgx:lJbbb9p9DTmbax:OhDxekcjjjj94hDkabaiavcufciGfcetfaD87ebdndnJbbjZararN:tawawN:taoaoN:tgrJbbbbarJbbbb9GE:rJb;:FSNJbbbZMgr:lJbbb9p9DTmbar:Ohvxekcjjjj94hvkabakcetfav87ebalcwfhlaiclfhiaecufgembkkk9mbdnadcd4ae2gdTmbinababydbgecwtcw91:Yaece91cjjj98Gcjjj;8if::NUdbabclfhbadcufgdmbkkk9teiucbcbydj1jjbgeabcifc98GfgbBdj1jjbdndnabZbcztgd9nmbcuhiabad9RcFFifcz4nbcuSmekaehikaik;LeeeudndnaeabVciGTmbabhixekdndnadcz9pmbabhixekabhiinaiaeydbBdbaiclfaeclfydbBdbaicwfaecwfydbBdbaicxfaecxfydbBdbaeczfheaiczfhiadc9Wfgdcs0mbkkadcl6mbinaiaeydbBdbaeclfheaiclfhiadc98fgdci0mbkkdnadTmbinaiaeRbb86bbaicefhiaecefheadcufgdmbkkabk;aeedudndnabciGTmbabhixekaecFeGc:b:c:ew2hldndnadcz9pmbabhixekabhiinaialBdbaicxfalBdbaicwfalBdbaiclfalBdbaiczfhiadc9Wfgdcs0mbkkadcl6mbinaialBdbaiclfhiadc98fgdci0mbkkdnadTmbinaiae86bbaicefhiadcufgdmbkkabkkkebcjwklz9Kbb'; // embed! base
	var wasm_simd =
		'b9H79TebbbeKl9Gbb9Gvuuuuueu9Giuuub9Geueuikqbbebeedddilve9Weeeviebeoweuec:q;Aekr;leDo9TW9T9VV95dbH9F9F939H79T9F9J9H229F9Jt9VV7bb8A9TW79O9V9Wt9F9KW9J9V9KW9wWVtW949c919M9MWVbdY9TW79O9V9Wt9F9KW9J9V9KW69U9KW949c919M9MWVblE9TW79O9V9Wt9F9KW9J9V9KW69U9KW949tWG91W9U9JWbvL9TW79O9V9Wt9F9KW9J9V9KWS9P2tWV9p9JtboK9TW79O9V9Wt9F9KW9J9V9KWS9P2tWV9r919HtbrL9TW79O9V9Wt9F9KW9J9V9KWS9P2tWVT949Wbwl79IV9RbDq:p9sqlbzik9:evu8Jjjjjbcz9Rhbcbheincbhdcbhiinabcwfadfaicjuaead4ceGglE86bbaialfhiadcefgdcw9hmbkaec:q:yjjbfai86bbaecitc:q1jjbfab8Piw83ibaecefgecjd9hmbkk:N8JlHud97euo978Jjjjjbcj;kb9Rgv8Kjjjjbc9:hodnadcefal0mbcuhoaiRbbc:Ge9hmbavaialfgrad9Rad;8qbbcj;abad9UhlaicefhodnaeTmbadTmbalc;WFbGglcjdalcjd6EhwcbhDinawaeaD9RaDawfae6Egqcsfglc9WGgkci2hxakcethmalcl4cifcd4hPabaDad2fhsakc;ab6hzcbhHincbhOaohAdndninaraA9RaP6meavcj;cbfaOak2fhCaAaPfhocbhidnazmbarao9Rc;Gb6mbcbhlinaCalfhidndndndndnaAalco4fRbbgXciGPlbedibkaipxbbbbbbbbbbbbbbbbpklbxikaiaopbblaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLgQcdp:meaQpmbzeHdOiAlCvXoQrLpxiiiiiiiiiiiiiiiip9ogLpxiiiiiiiiiiiiiiiip8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spklbaoclfaYpQbfaKc:q:yjjbfRbbfhoxdkaiaopbbwaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLpxssssssssssssssssp9ogLpxssssssssssssssssp8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spklbaocwfaYpQbfaKc:q:yjjbfRbbfhoxekaiaopbbbpklbaoczfhokdndndndndnaXcd4ciGPlbedibkaipxbbbbbbbbbbbbbbbbpklzxikaiaopbblaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLgQcdp:meaQpmbzeHdOiAlCvXoQrLpxiiiiiiiiiiiiiiiip9ogLpxiiiiiiiiiiiiiiiip8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spklzaoclfaYpQbfaKc:q:yjjbfRbbfhoxdkaiaopbbwaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLpxssssssssssssssssp9ogLpxssssssssssssssssp8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spklzaocwfaYpQbfaKc:q:yjjbfRbbfhoxekaiaopbbbpklzaoczfhokdndndndndnaXcl4ciGPlbedibkaipxbbbbbbbbbbbbbbbbpklaxikaiaopbblaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLgQcdp:meaQpmbzeHdOiAlCvXoQrLpxiiiiiiiiiiiiiiiip9ogLpxiiiiiiiiiiiiiiiip8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spklaaoclfaYpQbfaKc:q:yjjbfRbbfhoxdkaiaopbbwaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLpxssssssssssssssssp9ogLpxssssssssssssssssp8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spklaaocwfaYpQbfaKc:q:yjjbfRbbfhoxekaiaopbbbpklaaoczfhokdndndndndnaXco4Plbedibkaipxbbbbbbbbbbbbbbbbpkl8WxikaiaopbblaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLgQcdp:meaQpmbzeHdOiAlCvXoQrLpxiiiiiiiiiiiiiiiip9ogLpxiiiiiiiiiiiiiiiip8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgXcitc:q1jjbfpbibaXc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgXcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spkl8WaoclfaYpQbfaXc:q:yjjbfRbbfhoxdkaiaopbbwaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLpxssssssssssssssssp9ogLpxssssssssssssssssp8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgXcitc:q1jjbfpbibaXc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgXcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spkl8WaocwfaYpQbfaXc:q:yjjbfRbbfhoxekaiaopbbbpkl8Waoczfhokalc;abfhialcjefak0meaihlarao9Rc;Fb0mbkkdnaiak9pmbaici4hlinarao9RcK6miaCaifhXdndndndndnaAaico4fRbbalcoG4ciGPlbedibkaXpxbbbbbbbbbbbbbbbbpkbbxikaXaopbblaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLgQcdp:meaQpmbzeHdOiAlCvXoQrLpxiiiiiiiiiiiiiiiip9ogLpxiiiiiiiiiiiiiiiip8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spkbbaoclfaYpQbfaKc:q:yjjbfRbbfhoxdkaXaopbbwaopbbbgQclp:meaQpmbzeHdOiAlCvXoQrLpxssssssssssssssssp9ogLpxssssssssssssssssp8JgQp5b9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibaKc:q:yjjbfpbbbgYaYpmbbbbbbbbbbbbbbbbaQp5e9cjF;8;4;W;G;ab9:9cU1:NgKcitc:q1jjbfpbibp9UpmbedilvorzHOACXQLpPaLaQp9spkbbaocwfaYpQbfaKc:q:yjjbfRbbfhoxekaXaopbbbpkbbaoczfhokalcdfhlaiczfgiak6mbkkaoTmeaohAaOcefgOclSmdxbkkc9:hoxlkdnakTmbavcjdfaHfhiavaHfpbdbhYcbhXinaiavcj;cbfaXfglpblbgLcep9TaLpxeeeeeeeeeeeeeeeegQp9op9Hp9rgLalakfpblbg8Acep9Ta8AaQp9op9Hp9rg8ApmbzeHdOiAlCvXoQrLgEalamfpblbg3cep9Ta3aQp9op9Hp9rg3alaxfpblbg5cep9Ta5aQp9op9Hp9rg5pmbzeHdOiAlCvXoQrLg8EpmbezHdiOAlvCXorQLgQaQpmbedibedibedibediaYp9UgYp9AdbbaiadfglaYaQaQpmlvorlvorlvorlvorp9UgYp9AdbbaladfglaYaQaQpmwDqkwDqkwDqkwDqkp9UgYp9AdbbaladfglaYaQaQpmxmPsxmPsxmPsxmPsp9UgYp9AdbbaladfglaYaEa8EpmwDKYqk8AExm35Ps8E8FgQaQpmbedibedibedibedip9UgYp9AdbbaladfglaYaQaQpmlvorlvorlvorlvorp9UgYp9AdbbaladfglaYaQaQpmwDqkwDqkwDqkwDqkp9UgYp9AdbbaladfglaYaQaQpmxmPsxmPsxmPsxmPsp9UgYp9AdbbaladfglaYaLa8ApmwKDYq8AkEx3m5P8Es8FgLa3a5pmwKDYq8AkEx3m5P8Es8Fg8ApmbezHdiOAlvCXorQLgQaQpmbedibedibedibedip9UgYp9AdbbaladfglaYaQaQpmlvorlvorlvorlvorp9UgYp9AdbbaladfglaYaQaQpmwDqkwDqkwDqkwDqkp9UgYp9AdbbaladfglaYaQaQpmxmPsxmPsxmPsxmPsp9UgYp9AdbbaladfglaYaLa8ApmwDKYqk8AExm35Ps8E8FgQaQpmbedibedibedibedip9UgYp9AdbbaladfglaYaQaQpmlvorlvorlvorlvorp9UgYp9AdbbaladfglaYaQaQpmwDqkwDqkwDqkwDqkp9UgYp9AdbbaladfglaYaQaQpmxmPsxmPsxmPsxmPsp9UgYp9AdbbaladfhiaXczfgXak6mbkkaHclfgHad6mbkasavcjdfaqad2;8qbbavavcjdfaqcufad2fad;8qbbaqaDfgDae6mbkkcbc99arao9Radcaadca0ESEhokavcj;kbf8Kjjjjbaokwbz:bjjjbk::seHu8Jjjjjbc;ae9Rgv8Kjjjjbc9:hodnaeci9UgrcHfal0mbcuhoaiRbbgwc;WeGc;Ge9hmbawcsGgwce0mbavc;abfcFecje;8kbavcUf9cu83ibavc8Wf9cu83ibavcyf9cu83ibavcaf9cu83ibavcKf9cu83ibavczf9cu83ibav9cu83iwav9cu83ibaialfc9WfhDaicefgqarfhidnaeTmbcmcsawceSEhkcbhxcbhmcbhPcbhwcbhlindnaiaD9nmbc9:hoxikdndnaqRbbgoc;Ve0mbavc;abfalaocu7gscl4fcsGcitfgzydlhrazydbhzdnaocsGgHak9pmbavawasfcsGcdtfydbaxaHEhoaHThsdndnadcd9hmbabaPcetfgHaz87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHazBdbaHcwfaoBdbaHclfarBdbkaxasfhxcdhHavawcdtfaoBdbawasfhwcehsalhOxdkdndnaHcsSmbaHc987aHamffcefhoxekaicefhoai8SbbgHcFeGhsdndnaHcu9mmbaohixekaicvfhiascFbGhscrhHdninao8SbbgOcFbGaHtasVhsaOcu9kmeaocefhoaHcrfgHc8J9hmbxdkkaocefhikasce4cbasceG9R7amfhokdndnadcd9hmbabaPcetfgHaz87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHazBdbaHcwfaoBdbaHclfarBdbkcdhHavawcdtfaoBdbcehsawcefhwalhOaohmxekdnaocpe0mbaxcefgHavawaDaocsGfRbbgocl49RcsGcdtfydbaocz6gzEhravawao9RcsGcdtfydbaHazfgAaocsGgHEhoaHThCdndnadcd9hmbabaPcetfgHax87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHaxBdbaHcwfaoBdbaHclfarBdbkcdhsavawcdtfaxBdbavawcefgwcsGcdtfarBdbcihHavc;abfalcitfgOaxBdlaOarBdbavawazfgwcsGcdtfaoBdbalcefcsGhOawaCfhwaxhzaAaCfhxxekaxcbaiRbbgOEgzaoc;:eSgHfhraOcsGhCaOcl4hAdndnaOcs0mbarcefhoxekarhoavawaA9RcsGcdtfydbhrkdndnaCmbaocefhxxekaohxavawaO9RcsGcdtfydbhokdndnaHTmbaicefhHxekaicdfhHai8SbegscFeGhzdnascu9kmbaicofhXazcFbGhzcrhidninaH8SbbgscFbGaitazVhzascu9kmeaHcefhHaicrfgic8J9hmbkaXhHxekaHcefhHkazce4cbazceG9R7amfgmhzkdndnaAcsSmbaHhsxekaHcefhsaH8SbbgicFeGhrdnaicu9kmbaHcvfhXarcFbGhrcrhidninas8SbbgHcFbGaitarVhraHcu9kmeascefhsaicrfgic8J9hmbkaXhsxekascefhskarce4cbarceG9R7amfgmhrkdndnaCcsSmbashixekascefhias8SbbgocFeGhHdnaocu9kmbascvfhXaHcFbGhHcrhodninai8SbbgscFbGaotaHVhHascu9kmeaicefhiaocrfgoc8J9hmbkaXhixekaicefhikaHce4cbaHceG9R7amfgmhokdndnadcd9hmbabaPcetfgHaz87ebaHclfao87ebaHcdfar87ebxekabaPcdtfgHazBdbaHcwfaoBdbaHclfarBdbkcdhsavawcdtfazBdbavawcefgwcsGcdtfarBdbcihHavc;abfalcitfgXazBdlaXarBdbavawaOcz6aAcsSVfgwcsGcdtfaoBdbawaCTaCcsSVfhwalcefcsGhOkaqcefhqavc;abfaOcitfgOarBdlaOaoBdbavc;abfalasfcsGcitfgraoBdlarazBdbawcsGhwalaHfcsGhlaPcifgPae6mbkkcbc99aiaDSEhokavc;aef8Kjjjjbaok:flevu8Jjjjjbcz9Rhvc9:hodnaecvfal0mbcuhoaiRbbc;:eGc;qe9hmbav9cb83iwaicefhraialfc98fhwdnaeTmbdnadcdSmbcbhDindnaraw6mbc9:skarcefhoar8SbbglcFeGhidndnalcu9mmbaohrxekarcvfhraicFbGhicrhldninao8SbbgdcFbGaltaiVhiadcu9kmeaocefhoalcrfglc8J9hmbxdkkaocefhrkabaDcdtfaic8Etc8F91aicd47avcwfaiceGcdtVgoydbfglBdbaoalBdbaDcefgDae9hmbxdkkcbhDindnaraw6mbc9:skarcefhoar8SbbglcFeGhidndnalcu9mmbaohrxekarcvfhraicFbGhicrhldninao8SbbgdcFbGaltaiVhiadcu9kmeaocefhoalcrfglc8J9hmbxdkkaocefhrkabaDcetfaic8Etc8F91aicd47avcwfaiceGcdtVgoydbfgl87ebaoalBdbaDcefgDae9hmbkkcbc99arawSEhokaok:wPliuo97eue978Jjjjjbca9Rhiaec98Ghldndnadcl9hmbdnalTmbcbhvabhdinadadpbbbgocKp:RecKp:Sep;6egraocwp:RecKp:Sep;6earp;Geaoczp:RecKp:Sep;6egwp;Gep;Kep;LegDpxbbbbbbbbbbbbbbbbp:2egqarpxbbbjbbbjbbbjbbbjgkp9op9rp;Kegrpxbb;:9cbb;:9cbb;:9cbb;:9cararp;MeaDaDp;Meawaqawakp9op9rp;Kegrarp;Mep;Kep;Kep;Jep;Negwp;Mepxbbn0bbn0bbn0bbn0gqp;KepxFbbbFbbbFbbbFbbbp9oaopxbbbFbbbFbbbFbbbFp9op9qarawp;Meaqp;Kecwp:RepxbFbbbFbbbFbbbFbbp9op9qaDawp;Meaqp;Keczp:RepxbbFbbbFbbbFbbbFbp9op9qpkbbadczfhdavclfgval6mbkkalaeSmeaipxbbbbbbbbbbbbbbbbgqpklbaiabalcdtfgdaeciGglcdtgv;8qbbdnalTmbaiaipblbgocKp:RecKp:Sep;6egraocwp:RecKp:Sep;6earp;Geaoczp:RecKp:Sep;6egwp;Gep;Kep;LegDaqp:2egqarpxbbbjbbbjbbbjbbbjgkp9op9rp;Kegrpxbb;:9cbb;:9cbb;:9cbb;:9cararp;MeaDaDp;Meawaqawakp9op9rp;Kegrarp;Mep;Kep;Kep;Jep;Negwp;Mepxbbn0bbn0bbn0bbn0gqp;KepxFbbbFbbbFbbbFbbbp9oaopxbbbFbbbFbbbFbbbFp9op9qarawp;Meaqp;Kecwp:RepxbFbbbFbbbFbbbFbbp9op9qaDawp;Meaqp;Keczp:RepxbbFbbbFbbbFbbbFbp9op9qpklbkadaiav;8qbbskdnalTmbcbhvabhdinadczfgxaxpbbbgopxbbbbbbFFbbbbbbFFgkp9oadpbbbgDaopmbediwDqkzHOAKY8AEgwczp:Reczp:Sep;6egraDaopmlvorxmPsCXQL358E8FpxFubbFubbFubbFubbp9op;6eawczp:Sep;6egwp;Gearp;Gep;Kep;Legopxbbbbbbbbbbbbbbbbp:2egqarpxbbbjbbbjbbbjbbbjgmp9op9rp;Kegrpxb;:FSb;:FSb;:FSb;:FSararp;Meaoaop;Meawaqawamp9op9rp;Kegrarp;Mep;Kep;Kep;Jep;Negwp;Mepxbbn0bbn0bbn0bbn0gqp;KepxFFbbFFbbFFbbFFbbp9oaoawp;Meaqp;Keczp:Rep9qgoarawp;Meaqp;KepxFFbbFFbbFFbbFFbbp9ogrpmwDKYqk8AExm35Ps8E8Fp9qpkbbadaDakp9oaoarpmbezHdiOAlvCXorQLp9qpkbbadcafhdavclfgval6mbkkalaeSmbaiaeciGgvcitgdfcbcaad9R;8kbaiabalcitfglad;8qbbdnavTmbaiaipblzgopxbbbbbbFFbbbbbbFFgkp9oaipblbgDaopmbediwDqkzHOAKY8AEgwczp:Reczp:Sep;6egraDaopmlvorxmPsCXQL358E8FpxFubbFubbFubbFubbp9op;6eawczp:Sep;6egwp;Gearp;Gep;Kep;Legopxbbbbbbbbbbbbbbbbp:2egqarpxbbbjbbbjbbbjbbbjgmp9op9rp;Kegrpxb;:FSb;:FSb;:FSb;:FSararp;Meaoaop;Meawaqawamp9op9rp;Kegrarp;Mep;Kep;Kep;Jep;Negwp;Mepxbbn0bbn0bbn0bbn0gqp;KepxFFbbFFbbFFbbFFbbp9oaoawp;Meaqp;Keczp:Rep9qgoarawp;Meaqp;KepxFFbbFFbbFFbbFFbbp9ogrpmwDKYqk8AExm35Ps8E8Fp9qpklzaiaDakp9oaoarpmbezHdiOAlvCXorQLp9qpklbkalaiad;8qbbkk;4wllue97euv978Jjjjjbc8W9Rhidnaec98GglTmbcbhvabhoinaiaopbbbgraoczfgwpbbbgDpmlvorxmPsCXQL358E8Fgqczp:Segkclp:RepklbaopxbbjZbbjZbbjZbbjZpx;Zl81Z;Zl81Z;Zl81Z;Zl81Zakpxibbbibbbibbbibbbp9qp;6ep;NegkaraDpmbediwDqkzHOAKY8AEgrczp:Reczp:Sep;6ep;MegDaDp;Meakarczp:Sep;6ep;Megxaxp;Meakaqczp:Reczp:Sep;6ep;Megqaqp;Mep;Kep;Kep;Lepxbbbbbbbbbbbbbbbbp:4ep;Jepxb;:FSb;:FSb;:FSb;:FSgkp;Mepxbbn0bbn0bbn0bbn0grp;KepxFFbbFFbbFFbbFFbbgmp9oaxakp;Mearp;Keczp:Rep9qgxaDakp;Mearp;Keamp9oaqakp;Mearp;Keczp:Rep9qgkpmbezHdiOAlvCXorQLgrp5baipblbpEb:T:j83ibaocwfarp5eaipblbpEe:T:j83ibawaxakpmwDKYqk8AExm35Ps8E8Fgkp5baipblbpEd:T:j83ibaocKfakp5eaipblbpEi:T:j83ibaocafhoavclfgval6mbkkdnalaeSmbaiaeciGgvcitgofcbcaao9R;8kbaiabalcitfgwao;8qbbdnavTmbaiaipblbgraipblzgDpmlvorxmPsCXQL358E8Fgqczp:Segkclp:RepklaaipxbbjZbbjZbbjZbbjZpx;Zl81Z;Zl81Z;Zl81Z;Zl81Zakpxibbbibbbibbbibbbp9qp;6ep;NegkaraDpmbediwDqkzHOAKY8AEgrczp:Reczp:Sep;6ep;MegDaDp;Meakarczp:Sep;6ep;Megxaxp;Meakaqczp:Reczp:Sep;6ep;Megqaqp;Mep;Kep;Kep;Lepxbbbbbbbbbbbbbbbbp:4ep;Jepxb;:FSb;:FSb;:FSb;:FSgkp;Mepxbbn0bbn0bbn0bbn0grp;KepxFFbbFFbbFFbbFFbbgmp9oaxakp;Mearp;Keczp:Rep9qgxaDakp;Mearp;Keamp9oaqakp;Mearp;Keczp:Rep9qgkpmbezHdiOAlvCXorQLgrp5baipblapEb:T:j83ibaiarp5eaipblapEe:T:j83iwaiaxakpmwDKYqk8AExm35Ps8E8Fgkp5baipblapEd:T:j83izaiakp5eaipblapEi:T:j83iKkawaiao;8qbbkk:Pddiue978Jjjjjbc;ab9Rhidnadcd4ae2glc98GgvTmbcbheabhdinadadpbbbgocwp:Recwp:Sep;6eaocep:SepxbbjFbbjFbbjFbbjFp9opxbbjZbbjZbbjZbbjZp:Uep;Mepkbbadczfhdaeclfgeav6mbkkdnavalSmbaialciGgecdtgdVcbc;abad9R;8kbaiabavcdtfgvad;8qbbdnaeTmbaiaipblbgocwp:Recwp:Sep;6eaocep:SepxbbjFbbjFbbjFbbjFp9opxbbjZbbjZbbjZbbjZp:Uep;Mepklbkavaiad;8qbbkk9teiucbcbydj1jjbgeabcifc98GfgbBdj1jjbdndnabZbcztgd9nmbcuhiabad9RcFFifcz4nbcuSmekaehikaikkkebcjwklz9Tbb'; // embed! simd

	var detector = new Uint8Array([
		0, 97, 115, 109, 1, 0, 0, 0, 1, 4, 1, 96, 0, 0, 3, 3, 2, 0, 0, 5, 3, 1, 0, 1, 12, 1, 0, 10, 22, 2, 12, 0, 65, 0, 65, 0, 65, 0, 252, 10, 0, 0,
		11, 7, 0, 65, 0, 253, 15, 26, 11,
	]);
	var wasmpack = new Uint8Array([
		32, 0, 65, 2, 1, 106, 34, 33, 3, 128, 11, 4, 13, 64, 6, 253, 10, 7, 15, 116, 127, 5, 8, 12, 40, 16, 19, 54, 20, 9, 27, 255, 113, 17, 42, 67,
		24, 23, 146, 148, 18, 14, 22, 45, 70, 69, 56, 114, 101, 21, 25, 63, 75, 136, 108, 28, 118, 29, 73, 115,
	]);

	if (typeof WebAssembly !== 'object') {
		return {
			supported: false,
		};
	}

	var wasm = WebAssembly.validate(detector) ? unpack(wasm_simd) : unpack(wasm_base);

	var instance;

	var ready = WebAssembly.instantiate(wasm, {}).then(function (result) {
		instance = result.instance;
		instance.exports.__wasm_call_ctors();
	});

	function unpack(data) {
		var result = new Uint8Array(data.length);
		for (var i = 0; i < data.length; ++i) {
			var ch = data.charCodeAt(i);
			result[i] = ch > 96 ? ch - 97 : ch > 64 ? ch - 39 : ch + 4;
		}
		var write = 0;
		for (var i = 0; i < data.length; ++i) {
			result[write++] = result[i] < 60 ? wasmpack[result[i]] : (result[i] - 60) * 64 + result[++i];
		}
		return result.buffer.slice(0, write);
	}

	function decode(instance, fun, target, count, size, source, filter) {
		var sbrk = instance.exports.sbrk;
		var count4 = (count + 3) & ~3;
		var tp = sbrk(count4 * size);
		var sp = sbrk(source.length);
		var heap = new Uint8Array(instance.exports.memory.buffer);
		heap.set(source, sp);
		var res = fun(tp, count, size, sp, source.length);
		if (res == 0 && filter) {
			filter(tp, count4, size);
		}
		target.set(heap.subarray(tp, tp + count * size));
		sbrk(tp - sbrk(0));
		if (res != 0) {
			throw new Error('Malformed buffer data: ' + res);
		}
	}

	var filters = {
		NONE: '',
		OCTAHEDRAL: 'meshopt_decodeFilterOct',
		QUATERNION: 'meshopt_decodeFilterQuat',
		EXPONENTIAL: 'meshopt_decodeFilterExp',
	};

	var decoders = {
		ATTRIBUTES: 'meshopt_decodeVertexBuffer',
		TRIANGLES: 'meshopt_decodeIndexBuffer',
		INDICES: 'meshopt_decodeIndexSequence',
	};

	var workers = [];
	var requestId = 0;

	function createWorker(url) {
		var worker = {
			object: new Worker(url),
			pending: 0,
			requests: {},
		};

		worker.object.onmessage = function (event) {
			var data = event.data;

			worker.pending -= data.count;
			worker.requests[data.id][data.action](data.value);
			delete worker.requests[data.id];
		};

		return worker;
	}

	function initWorkers(count) {
		var source =
			'self.ready = WebAssembly.instantiate(new Uint8Array([' +
			new Uint8Array(wasm) +
			']), {})' +
			'.then(function(result) { result.instance.exports.__wasm_call_ctors(); return result.instance; });' +
			'self.onmessage = ' +
			workerProcess.name +
			';' +
			decode.toString() +
			workerProcess.toString();

		var blob = new Blob([source], { type: 'text/javascript' });
		var url = URL.createObjectURL(blob);

		for (var i = workers.length; i < count; ++i) {
			workers[i] = createWorker(url);
		}

		for (var i = count; i < workers.length; ++i) {
			workers[i].object.postMessage({});
		}

		workers.length = count;

		URL.revokeObjectURL(url);
	}

	function decodeWorker(count, size, source, mode, filter) {
		var worker = workers[0];

		for (var i = 1; i < workers.length; ++i) {
			if (workers[i].pending < worker.pending) {
				worker = workers[i];
			}
		}

		return new Promise(function (resolve, reject) {
			var data = new Uint8Array(source);
			var id = ++requestId;

			worker.pending += count;
			worker.requests[id] = { resolve: resolve, reject: reject };
			worker.object.postMessage({ id: id, count: count, size: size, source: data, mode: mode, filter: filter }, [data.buffer]);
		});
	}

	function workerProcess(event) {
		var data = event.data;
		if (!data.id) {
			return self.close();
		}
		self.ready.then(function (instance) {
			try {
				var target = new Uint8Array(data.count * data.size);
				decode(instance, instance.exports[data.mode], target, data.count, data.size, data.source, instance.exports[data.filter]);
				self.postMessage({ id: data.id, count: data.count, action: 'resolve', value: target }, [target.buffer]);
			} catch (error) {
				self.postMessage({ id: data.id, count: data.count, action: 'reject', value: error });
			}
		});
	}

	return {
		ready: ready,
		supported: true,
		useWorkers: function (count) {
			initWorkers(count);
		},
		decodeVertexBuffer: function (target, count, size, source, filter) {
			decode(instance, instance.exports.meshopt_decodeVertexBuffer, target, count, size, source, instance.exports[filters[filter]]);
		},
		decodeIndexBuffer: function (target, count, size, source) {
			decode(instance, instance.exports.meshopt_decodeIndexBuffer, target, count, size, source);
		},
		decodeIndexSequence: function (target, count, size, source) {
			decode(instance, instance.exports.meshopt_decodeIndexSequence, target, count, size, source);
		},
		decodeGltfBuffer: function (target, count, size, source, mode, filter) {
			decode(instance, instance.exports[decoders[mode]], target, count, size, source, instance.exports[filters[filter]]);
		},
		decodeGltfBufferAsync: function (count, size, source, mode, filter) {
			if (workers.length > 0) {
				return decodeWorker(count, size, source, decoders[mode], filters[filter]);
			}

			return ready.then(function () {
				var target = new Uint8Array(count * size);
				decode(instance, instance.exports[decoders[mode]], target, count, size, source, instance.exports[filters[filter]]);
				return target;
			});
		},
	};
})();

// export! MeshoptDecoder
if (typeof exports === 'object' && typeof module === 'object') module.exports = MeshoptDecoder;
else if (typeof define === 'function' && define['amd'])
	define([], function () {
		return MeshoptDecoder;
	});
else if (typeof exports === 'object') exports['MeshoptDecoder'] = MeshoptDecoder;
else (typeof self !== 'undefined' ? self : this).MeshoptDecoder = MeshoptDecoder;



================================================
FILE: docs/renderer-uv.html
================================================
<!DOCTYPE html>
<html lang="en">
	<head>
		<title>three.js vr - panorama with depth</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
		<link type="text/css" rel="stylesheet" href="main.css">
        <style>
            #progressBar {
                width: 500px;
                height: 24px;
                position: absolute;
                left: 50%;
                top: 10px;
                margin-left: -250px;
            }
			#info {
				position: absolute;
				top: 10px;
				width: 100%;
				text-align: center;
				z-index: 100;
				display:block;
				color:white;
			}
        </style>
        <script async src="https://unpkg.com/es-module-shims@1.6.3/dist/es-module-shims.js"></script>
        <script src="meshopt_decoder.js"></script>

        <script type="importmap">
          {
            "imports": {
              "three": "https://unpkg.com/three@0.152.2/build/three.module.js",
              "three/addons/": "https://unpkg.com/three@0.152.2/examples/jsm/"
            }
          }
        </script>
	</head>
	<body>
        <progress value="0" max="100" id="progressBar"></progress>
		<div id="container"></div>
		<!--
		<div id="info">FPS: </div>
		<canvas id="fps" width="256" height="256"></canvas> -->

		<script type="module">

            import * as THREE from 'three';
			import { VRButton } from 'three/addons/webxr/VRButton.js';
            import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
			import { PLYLoader } from 'three/addons/loaders/PLYLoader.js';
			import { FBXLoader } from 'three/addons/loaders/FBXLoader.js';
            import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
			import Stats from 'three/addons/libs/stats.module.js';

			let camera, scene, renderer, sphere, clock, controls, canvas, fpsMaterial, fpsPlane;
			let frames = 0, prevTime = performance.now();
			let parent = new THREE.Group();
			let cameraoffsetnode = new THREE.Group();
            const stats = new Stats();
            container.appendChild( stats.dom );

			init();
			animate();

			function init() {

				const container = document.getElementById( 'container' );

				clock = new THREE.Clock();

				scene = new THREE.Scene();
				scene.background = new THREE.Color( 0x000000 );
				// scene.background = new THREE.Color( 0xffffff );

				const light = new THREE.AmbientLight( 0xffffff, 1 );
				scene.add( light );

				camera = new THREE.PerspectiveCamera( 70, window.innerWidth / window.innerHeight, .1, 200 );
				camera.position.set( 0.0, 1.6, 0.0 );
				// camera.position.set( 0.0, 0.0, 10.0 );
				scene.add( camera );

				scene.add(cameraoffsetnode);
				cameraoffsetnode.translateY(-1.7);
				cameraoffsetnode.add(parent);

				// var fpsCanvas = document.getElementById('fps');
				// var fpsContext = fpsCanvas.getContext('2d');
				// fpsCanvas.width = 128;
				// fpsCanvas.height = 128;
				// fpsContext.fillStyle = '#fffffff';
				// fpsContext.fillRect(0, 0, fpsCanvas.width, fpsCanvas.height);
				// fpsContext.fillStyle = '#ff00ff';
				// fpsContext.font = "24px sans-serif";
				// fpsContext.fillText("FPS: ", 16, 32, fpsCanvas.width);

				// const fpsGeometry = new THREE.PlaneGeometry( 1, 1 );
				// const texture = new THREE.CanvasTexture(fpsCanvas);
				// fpsMaterial = new THREE.MeshBasicMaterial( {color: 0xffffff, map: texture, side: THREE.DoubleSide} );
				// fpsPlane = new THREE.Mesh( fpsGeometry, fpsMaterial );
				// camera.add( fpsPlane );
				// fpsPlane.position.set(0,0,-4);

				const urlParams = new URLSearchParams(window.location.search);
				const res = urlParams.get('res');
				const scene_name = urlParams.get('scene') + '_' + res + '_inpaint_opt.glb';
				const scale = 0.75; //parseFloat(urlParams.get('scale'));
				const loader = new GLTFLoader();
				loader.setMeshoptDecoder(MeshoptDecoder);
				// const url = 'https://3dpanoinpainting.s3.us-west-1.amazonaws.com/'
				const url = 'https://3d-pano-inpainting.s3.us-west-2.amazonaws.com/';
				// const url = 'assets/4k/';
				console.log(url+scene_name);
                loader.load(  url + scene_name,
                    function ( glb ) {
                        console.log(glb.scene);
                        glb.scene.traverse(function(child) {
                            if (child.isMesh) {
								console.log(child);
                                const geometry = child.geometry;
								const material = new THREE.MeshBasicMaterial( { map: child.material.map } );
								material.map.minFilter = THREE.NearestFilter;
		                        const mesh = new THREE.Mesh( geometry, material );
								console.log(mesh.scale);
								mesh.scale.set(scale,scale,scale);
								console.log(mesh.scale);
        		                scene.add(mesh);
                            }
                        });
                        //const material = new THREE.MeshBasicMaterial( { vertexColors: true } );
                        //const mesh = new THREE.Mesh( geometry, material );

                        //parent.add(mesh);
                        progressBar.style.display = 'none'

                    },


                    function ( xhr ) {
                        if ( xhr.lengthComputable ) {
                            var percentComplete = (xhr.loaded / xhr.total) * 100;
                            progressBar.value = percentComplete;
                            progressBar.style.display = 'block';
                        }
                    },
                    function ( error ) {
                        console.log( 'An error happened' );
                        console.log( error );
                    }
                );
				
				renderer = new THREE.WebGLRenderer();
				renderer.setPixelRatio( window.devicePixelRatio );
				renderer.setSize( window.innerWidth, window.innerHeight );
				renderer.xr.enabled = true;

				//renderer.xr.setReferenceSpaceType( 'local-floor' );
												function resetView(){

													//const controller = event.target;
													var cameraVR = renderer.xr.getCamera(camera);
													let ref = cameraVR.cameras[0].matrix;
													let refpos = new THREE.Vector3();
													refpos.setFromMatrixPosition(ref);
													console.log(refpos);
													let pos = parent.position;
													parent.translateX(refpos.x-pos.x);
													parent.translateY(refpos.y-pos.y);
													parent.translateZ(refpos.z-pos.z);
												}


				                var controller1, controller2;
				                function onSelectEnd( event ) {
				                   resetView();
				                }
				                function onSqueezeEnd( event ) {
				                    //sceneNum = (sceneNum + 1)%10;
				                    //updateScene(mode,sceneNum);
				                }

				                controller1 = renderer.xr.getController( 0 );
				                //controller1.addEventListener( 'selectstart', onSelectStart );
				                controller1.addEventListener( 'selectend', onSelectEnd );
				                scene.add( controller1 );

				                controller2 = renderer.xr.getController( 1 );
				                //controller2.addEventListener( 'selectstart', onSelectStart );
				                controller2.addEventListener( 'selectend', onSelectEnd );
				                scene.add( controller2 );

				                controller1 = renderer.xr.getController( 0 );
				                controller1.addEventListener( 'squeezeend', onSqueezeEnd );
				                scene.add( controller1 );

				                controller2 = renderer.xr.getController( 1 );
				                controller2.addEventListener( 'squeezend', onSqueezeEnd );
				                scene.add( controller2 );


				// renderer.outputColorSpace = THREE.LinearSRGBColorSpace;
				container.appendChild( renderer.domElement );
				// renderer.antialias = true;

				container.appendChild( VRButton.createButton( renderer ) );

                controls = new OrbitControls( camera, renderer.domElement );
                controls.target = new THREE.Vector3(0,2,0);
                controls.autoRotate = true;
				controls.listenToKeyEvents( window ); // optional
                controls.keyPanSpeed = 100;
				controls.enableDamping = true; // an animation loop is required when either damping or auto-rotation are enabled
				controls.dampingFactor = 0.05;

				controls.screenSpacePanning = false;

				controls.minDistance = 1;
				controls.maxDistance = 3;

                controls.addEventListener( 'end', onInteractionEnd );

				window.addEventListener( 'resize', onWindowResize );


				renderer.xr.addEventListener('sessionstart', () => {
	    const cameraVR = renderer.xr.getCamera(camera);

	    // Wait for a short delay to ensure everything is initialized (if needed)
	    setTimeout(() => {
	        if (cameraVR.cameras && cameraVR.cameras.length > 0) {
	            console.log('VR Cameras:', cameraVR.cameras); // Should contain left and right eye cameras
	        } else {
	            console.error('VR Cameras array is empty or not yet initialized.');
	        }
	    }, 100); // Adjust delay as needed
	});



			}

            function onInteractionEnd() {
                controls.autoRotate = false;
            }

			function onWindowResize() {

				camera.aspect = window.innerWidth / window.innerHeight;
				camera.updateProjectionMatrix();

				renderer.setSize( window.innerWidth, window.innerHeight );


			}

			function animate() {

				renderer.setAnimationLoop( render );

			}

			function render() {
				if ( renderer.xr.isPresenting === false ) {
	                controls.update();
				}

				stats.update();
				frames++;
				// const time = performance.now();
				
				// if ( time >= prevTime + 1000 ) {
				// 	const info = document.getElementById( 'info' );
				// 	const fps = Math.round( ( frames * 1000 ) / ( time - prevTime ) );
				// 	// info.textContent = "FPS: " + fps;
				// 	var fpsCanvas = document.getElementById('fps');
				// 	var fpsContext = fpsCanvas.getContext('2d');
				// 	fpsContext.fillStyle = '#000000';
				// 	fpsContext.fillRect(0, 0, fpsCanvas.width, fpsCanvas.height);
				// 	fpsContext.fillStyle = '#ff00ff';
				// 	fpsContext.font = "24px sans-serif";
				// 	fpsContext.fillText("FPS: " + fps, 16, 32, fpsCanvas.width);
				// 	// const texture = new THREE.CanvasTexture(canvas);
				// 	// fpsMaterial = new THREE.MeshBasicMaterial( {color: 0xffffff, map: texture, side: THREE.DoubleSide} );
				// 	// fpsPlane.material  = fpsMaterial;//.map.needsUpdate = true;
				// 	fpsPlane.material.map.needsUpdate = true;
				// 	fpsPlane.material.needsUpdate = true;
				// 	frames = 0;
				// 	prevTime = time;
				// }

				renderer.render( scene, camera );

			}

		</script>
	</body>
</html>



================================================
FILE: docs/renderer.html
================================================
<!DOCTYPE html>
<html lang="en">
	<head>
		<title>three.js vr - panorama with depth</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
		<link type="text/css" rel="stylesheet" href="main.css">
        <style>
            #progressBar {
                width: 500px;
                height: 24px;
                position: absolute;
                left: 50%;
                top: 10px;
                margin-left: -250px;
            }
        </style>
        <script async src="https://unpkg.com/es-module-shims@1.6.3/dist/es-module-shims.js"></script>
        <script src="meshopt_decoder.js"></script>

        <script type="importmap">
          {
            "imports": {
              "three": "https://unpkg.com/three@0.152.2/build/three.module.js",
              "three/addons/": "https://unpkg.com/three@0.152.2/examples/jsm/"
            }
          }
        </script>
	</head>
	<body>
        <progress value="0" max="100" id="progressBar"></progress>
		<div id="container"></div>

		<script type="module">

            import * as THREE from 'three';
			import { VRButton } from 'three/addons/webxr/VRButton.js';
            import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
			import { PLYLoader } from 'three/addons/loaders/PLYLoader.js';
			import { FBXLoader } from 'three/addons/loaders/FBXLoader.js';
            import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
			import Stats from 'three/addons/libs/stats.module.js';

			let camera, scene, renderer, sphere, clock, controls;
			let parent = new THREE.Group();
			let cameraoffsetnode = new THREE.Group();
            const stats = new Stats();
            container.appendChild( stats.dom );

			init();
			animate();

			function init() {

				const container = document.getElementById( 'container' );

				clock = new THREE.Clock();

				scene = new THREE.Scene();
				scene.background = new THREE.Color( 0x101010 );

				const light = new THREE.AmbientLight( 0xffffff, 1 );
				scene.add( light );

				camera = new THREE.PerspectiveCamera( 70, window.innerWidth / window.innerHeight, .1, 200 );
				camera.position.set( 0.0, 1.6, 0.0 );
				scene.add( camera );

				scene.add(cameraoffsetnode);
				cameraoffsetnode.translateY(-1.7);
				cameraoffsetnode.add(parent);


				const urlParams = new URLSearchParams(window.location.search);
				const scene_name = urlParams.get('scene') + '.glb';

				const loader = new GLTFLoader();
				loader.setMeshoptDecoder(MeshoptDecoder);
				const url = 'https://3dpanoinpainting.s3.us-west-1.amazonaws.com/'
				// const url = 'assets/';
                loader.load(  url + scene_name,
                    function ( glb ) {
                        console.log(glb.scene);
                        glb.scene.traverse(function(child) {
                            if (child.isMesh) {
								console.log(child);
                                const geometry = child.geometry;
								const material = new THREE.MeshBasicMaterial( { vertexColors: true } );
		                        const mesh = new THREE.Mesh( geometry, material );
        		                scene.add(mesh);
                            }
                        });
                        //const material = new THREE.MeshBasicMaterial( { vertexColors: true } );
                        //const mesh = new THREE.Mesh( geometry, material );

                        //parent.add(mesh);
                        progressBar.style.display = 'none'

                    },


                    function ( xhr ) {
                        if ( xhr.lengthComputable ) {
                            var percentComplete = (xhr.loaded / xhr.total) * 100;
                            progressBar.value = percentComplete;
                            progressBar.style.display = 'block';
                        }
                    },
                    function ( error ) {
                        console.log( 'An error happened' );
                        console.log( error );
                    }
                );

				renderer = new THREE.WebGLRenderer();
				renderer.setPixelRatio( window.devicePixelRatio );
				renderer.setSize( window.innerWidth, window.innerHeight );
				renderer.xr.enabled = true;

				//renderer.xr.setReferenceSpaceType( 'local-floor' );
												function resetView(){

													//const controller = event.target;
													var cameraVR = renderer.xr.getCamera(camera);
													let ref = cameraVR.cameras[0].matrix;
													let refpos = new THREE.Vector3();
													refpos.setFromMatrixPosition(ref);
													console.log(refpos);
													let pos = parent.position;
													parent.translateX(refpos.x-pos.x);
													parent.translateY(refpos.y-pos.y);
													parent.translateZ(refpos.z-pos.z);
												}


				                var controller1, controller2;
				                function onSelectEnd( event ) {
				                   resetView();
				                }
				                function onSqueezeEnd( event ) {
				                    //sceneNum = (sceneNum + 1)%10;
				                    //updateScene(mode,sceneNum);
				                }

				                controller1 = renderer.xr.getController( 0 );
				                //controller1.addEventListener( 'selectstart', onSelectStart );
				                controller1.addEventListener( 'selectend', onSelectEnd );
				                scene.add( controller1 );

				                controller2 = renderer.xr.getController( 1 );
				                //controller2.addEventListener( 'selectstart', onSelectStart );
				                controller2.addEventListener( 'selectend', onSelectEnd );
				                scene.add( controller2 );

				                controller1 = renderer.xr.getController( 0 );
				                controller1.addEventListener( 'squeezeend', onSqueezeEnd );
				                scene.add( controller1 );

				                controller2 = renderer.xr.getController( 1 );
				                controller2.addEventListener( 'squeezend', onSqueezeEnd );
				                scene.add( controller2 );


				renderer.outputColorSpace = THREE.LinearSRGBColorSpace;
				container.appendChild( renderer.domElement );
				// renderer.antialias = true;

				container.appendChild( VRButton.createButton( renderer ) );

                controls = new OrbitControls( camera, renderer.domElement );
                controls.target = new THREE.Vector3(0,2,0);
                controls.autoRotate = true;
				controls.listenToKeyEvents( window ); // optional
                controls.keyPanSpeed = 100;
				controls.enableDamping = true; // an animation loop is required when either damping or auto-rotation are enabled
				controls.dampingFactor = 0.05;

				controls.screenSpacePanning = false;

				controls.minDistance = 1;
				controls.maxDistance = 3;

                controls.addEventListener( 'end', onInteractionEnd );

				window.addEventListener( 'resize', onWindowResize );


				renderer.xr.addEventListener('sessionstart', () => {
	    const cameraVR = renderer.xr.getCamera(camera);

	    // Wait for a short delay to ensure everything is initialized (if needed)
	    setTimeout(() => {
	        if (cameraVR.cameras && cameraVR.cameras.length > 0) {
	            console.log('VR Cameras:', cameraVR.cameras); // Should contain left and right eye cameras
	        } else {
	            console.error('VR Cameras array is empty or not yet initialized.');
	        }
	    }, 100); // Adjust delay as needed
	});



			}

            function onInteractionEnd() {
                controls.autoRotate = false;
            }

			function onWindowResize() {

				camera.aspect = window.innerWidth / window.innerHeight;
				camera.updateProjectionMatrix();

				renderer.setSize( window.innerWidth, window.innerHeight );


			}

			function animate() {

				renderer.setAnimationLoop( render );

			}

			function render() {
				if ( renderer.xr.isPresenting === false ) {
	                controls.update();
				}

				stats.update();

				renderer.render( scene, camera );

			}

		</script>
	</body>
</html>



================================================
FILE: docs/study.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
	<title>three.js vr - panorama with depth</title>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
	<link type="text/css" rel="stylesheet" href="main.css">
	<style>
		#progressBar {
			width: 500px;
			height: 24px;
			position: absolute;
			left: 50%;
			top: 10px;
			margin-left: -250px;
		}
	</style>
	<script async src="https://unpkg.com/es-module-shims@1.6.3/dist/es-module-shims.js"></script>
	<script>


	</script>

	<script type="importmap">
		{
		  "imports": {
			"three": "https://unpkg.com/three@0.152.2/build/three.module.js",
			"three/addons/": "https://unpkg.com/three@0.152.2/examples/jsm/"
		  }
		}
	</script>
</head>
<body>
	<progress value="0" max="100" id="progressBar"></progress>
	<div id="container"></div>

	<script type="module">

		import * as THREE from 'three';
		import { VRButton } from 'three/addons/webxr/VRButton.js';
		import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
		import { PLYLoader } from 'three/addons/loaders/PLYLoader.js';
		import { FBXLoader } from 'three/addons/loaders/FBXLoader.js';
		import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
		import Stats from 'three/addons/libs/stats.module.js';

		let camera, scene, renderer, sphere, clock, controls;
		let parent = new THREE.Group();
		let cameraoffsetnode = new THREE.Group();
		const stats = new Stats();
		container.appendChild(stats.dom);

		init();
		animate();

		function init() {

			const container = document.getElementById('container');

			clock = new THREE.Clock();

			scene = new THREE.Scene();
			scene.background = new THREE.Color(0x101010);

			const light = new THREE.AmbientLight(0xffffff, 1);
			scene.add(light);

			camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, .1, 200);
			camera.position.set(0.0, 2.0, 0.0);
			scene.add(camera);

			scene.add(cameraoffsetnode);
			cameraoffsetnode.translateY(-2.0);
			cameraoffsetnode.add(parent);


			const urlParams = new URLSearchParams(window.location.search);
			const scene_name = urlParams.get('scene') + '.glb';

			const loader = new GLTFLoader();
			const url = 'assets/'
			loader.load(url + scene_name,
				function (glb) {
					console.log(glb.scene);
					var geometry;
					glb.scene.traverse(function (child) {
						if (child.isMesh) {
							geometry = child.geometry;
						}
					});
					const material = new THREE.MeshBasicMaterial({ vertexColors: true });
					const mesh = new THREE.Mesh(geometry, material);

					parent.add(mesh);
					progressBar.style.display = 'none'
					

				},


				function (xhr) {
					if (xhr.lengthComputable) {
						var percentComplete = (xhr.loaded / xhr.total) * 100;
						progressBar.value = percentComplete;
						progressBar.style.display = 'block';
					}
				},
				function (error) {
					console.log('An error happened');
					console.log(error);
				}
			);

			renderer = new THREE.WebGLRenderer();
			renderer.setPixelRatio(window.devicePixelRatio);
			renderer.setSize(window.innerWidth, window.innerHeight);
			renderer.xr.enabled = true;

			//renderer.xr.setReferenceSpaceType( 'local-floor' );
			function resetView() {

				//const controller = event.target;
				var cameraVR = renderer.xr.getCamera(camera);
				let ref = cameraVR.cameras[0].matrix;
				let refpos = new THREE.Vector3();
				refpos.setFromMatrixPosition(ref);
				console.log(refpos);
				let pos = parent.position;
				parent.translateX(refpos.x - pos.x);
				parent.translateY(refpos.y - pos.y);
				parent.translateZ(refpos.z - pos.z);
			}


			var controller1, controller2;
			function onSelectEnd(event) {
				resetView();
			}
			function onSqueezeEnd(event) {
				//sceneNum = (sceneNum + 1)%10;
				//updateScene(mode,sceneNum);
			}

			controller1 = renderer.xr.getController(0);
			//controller1.addEventListener( 'selectstart', onSelectStart );
			controller1.addEventListener('selectend', onSelectEnd);
			scene.add(controller1);

			controller2 = renderer.xr.getController(1);
			//controller2.addEventListener( 'selectstart', onSelectStart );
			controller2.addEventListener('selectend', onSelectEnd);
			scene.add(controller2);

			controller1 = renderer.xr.getController(0);
			controller1.addEventListener('squeezeend', onSqueezeEnd);
			scene.add(controller1);

			controller2 = renderer.xr.getController(1);
			controller2.addEventListener('squeezend', onSqueezeEnd);
			scene.add(controller2);


			renderer.outputColorSpace = THREE.LinearSRGBColorSpace;
			container.appendChild(renderer.domElement);

			container.appendChild(VRButton.createButton(renderer));

			controls = new OrbitControls(camera, renderer.domElement);
			controls.target = new THREE.Vector3(0, 0, 0);
			controls.autoRotate = false;
			controls.listenToKeyEvents(window); // optional
			controls.keyPanSpeed = 100;
			controls.enableDamping = true; // an animation loop is required when either damping or auto-rotation are enabled
			controls.dampingFactor = 0.05;

			controls.screenSpacePanning = false;

			controls.minDistance = 1;
			controls.maxDistance = 3;

			controls.addEventListener('end', onInteractionEnd);

			window.addEventListener('resize', onWindowResize);


			renderer.xr.addEventListener('sessionstart', () => {
				const cameraVR = renderer.xr.getCamera(camera);

				// Wait for a short delay to ensure everything is initialized (if needed)
				setTimeout(() => {
					if (cameraVR.cameras && cameraVR.cameras.length > 0) {
						console.log('VR Cameras:', cameraVR.cameras); // Should contain left and right eye cameras
						resetView();
					} else {
						console.error('VR Cameras array is empty or not yet initialized.');
					}
				}, 100); // Adjust delay as needed
			});



		}

		function onInteractionEnd() {
			controls.autoRotate = false;
		}

		function onWindowResize() {

			camera.aspect = window.innerWidth / window.innerHeight;
			camera.updateProjectionMatrix();

			renderer.setSize(window.innerWidth, window.innerHeight);


		}

		function animate() {

			renderer.setAnimationLoop(render);

		}

		function render() {
			if (renderer.xr.isPresenting === false) {
				controls.update();
			}

			stats.update();

			renderer.render(scene, camera);

		}

	</script>
</body>
</html>




================================================
FILE: inpainting/README.md
================================================
# [CVPR 2020] 3D Photography using Context-aware Layered Depth Inpainting

[![Open 3DPhotoInpainting in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)

### [[Paper](https://arxiv.org/abs/2004.04727)] [[Project Website](https://shihmengli.github.io/3D-Photo-Inpainting/)] [[Google Colab](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)]

<p align='center'>
<img src='https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_teaser.jpg' width='900'/>
</p>

We propose a method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view. We use a Layered Depth Image with explicit pixel connectivity as underlying representation, and present a learning-based inpainting model that iteratively synthesizes new local color-and-depth content into the occluded region in a spatial context-aware manner. The resulting 3D photos can be efficiently rendered with motion parallax using standard graphics engines. We validate the effectiveness of our method on a wide range of challenging everyday scenes and show fewer artifacts when compared with the state-of-the-arts.
<br/>

**3D Photography using Context-aware Layered Depth Inpainting**
<br/>
[Meng-Li Shih](https://shihmengli.github.io/), 
[Shih-Yang Su](https://lemonatsu.github.io/), 
[Johannes Kopf](https://johanneskopf.de/), and
[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/)
<br/>
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.


## Prerequisites

- Linux (tested on Ubuntu 18.04.4 LTS)
- Anaconda
- Python 3.7 (tested on 3.7.4)
- PyTorch 1.4.0 (tested on 1.4.0 for execution)

and the Python dependencies listed in [requirements.txt](requirements.txt)
- To get started, please run the following commands:
    ```bash
    conda create -n 3DP python=3.7 anaconda
    conda activate 3DP
    pip install -r requirements.txt
    conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit==10.1.243 -c pytorch
    ```
- Next, please download the model weight using the following command:
    ```bash
    chmod +x download.sh
    ./download.sh
    ```    

## Quick start
Please follow the instructions in this section. 
This should allow to execute our results.
For more detailed instructions, please refer to [`DOCUMENTATION.md`](DOCUMENTATION.md).

## Execute
1. Put ```.jpg``` files (e.g., test.jpg) into the ```image``` folder. 
    - E.g., `image/moon.jpg`
2. Run the following command
    ```bash
    python main.py --config argument.yml
    ```
    - Note: The 3D photo generation process usually takes about 2-3 minutes depending on the available computing resources.
3. The results are stored in the following directories:
    - Corresponding depth map estimated by [MiDaS](https://github.com/intel-isl/MiDaS.git) 
        - E.g. ```depth/moon.npy```, ```depth/moon.png```
        - User could edit ```depth/moon.png``` manually. 
            - Remember to set the following two flags as listed below if user wants to use manually edited ```depth/moon.png``` as input for 3D Photo.
                - `depth_format: '.png'`
                - `require_midas: False`
    - Inpainted 3D mesh (Optional: User need to switch on the flag `save_ply`)
        - E.g. ```mesh/moon.ply```
    - Rendered videos with zoom-in motion
        - E.g. ```video/moon_zoom-in.mp4```
    - Rendered videos with swing motion
        - E.g. ```video/moon_swing.mp4```
    - Rendered videos with circle motion
        - E.g. ```video/moon_circle.mp4```         
    - Rendered videos with dolly zoom-in effect
        - E.g. ```video/moon_dolly-zoom-in.mp4```
        - Note: We assume that the object of focus is located at the center of the image.
4. (Optional) If you want to change the default configuration. Please read [`DOCUMENTATION.md`](DOCUMENTATION.md) and modified ```argument.yml```.


## License
This work is licensed under MIT License. See [LICENSE](LICENSE) for details. 

If you find our code/models useful, please consider citing our paper:
```
@inproceedings{Shih3DP20,
  author = {Shih, Meng-Li and Su, Shih-Yang and Kopf, Johannes and Huang, Jia-Bin},
  title = {3D Photography using Context-aware Layered Depth Inpainting},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2020}
}
```

## Acknowledgments
- We thank Pratul Srinivasan for providing clarification of the method [Srinivasan et al. CVPR 2019](https://people.eecs.berkeley.edu/~pratul/publication/mpi_extrapolation/).
- We thank the author of [Zhou et al. 2018](https://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/), [Choi et al. 2019](https://github.com/NVlabs/extreme-view-synth/), [Mildenhall et al. 2019](https://github.com/Fyusion/LLFF), [Srinivasan et al. 2019](https://github.com/google-research/google-research/tree/ac9b04e1dbdac468fda53e798a326fe9124e49fe/mpi_extrapolation), [Wiles et al. 2020](http://www.robots.ox.ac.uk/~ow/synsin.html), [Niklaus et al. 2019](https://github.com/sniklaus/3d-ken-burns) for providing their implementations online.
- Our code builds upon [EdgeConnect](https://github.com/knazeri/edge-connect), [MiDaS](https://github.com/intel-isl/MiDaS.git) and [pytorch-inpainting-with-partial-conv](https://github.com/naoto0804/pytorch-inpainting-with-partial-conv)



================================================
FILE: inpainting/argument.yml
================================================
depth_edge_model_ckpt: checkpoints/edge-model.pth
depth_feat_model_ckpt: checkpoints/depth-model.pth
rgb_feat_model_ckpt: checkpoints/color-model.pth
MiDaS_model_ckpt: MiDaS/model.pt
use_boostmonodepth: False
fps: 40
num_frames: 240
x_shift_range: [0.00, 0.00, -0.015, -0.015]
y_shift_range: [0.00, 0.00, -0.015, -0.00]
z_shift_range: [-0.05, -0.05, -0.05, -0.05]
traj_types: ['double-straight-line', 'double-straight-line', 'circle', 'circle']
video_postfix: ['dolly-zoom-in', 'zoom-in', 'circle', 'swing']
specific: ''
longer_side_len: 2048
src_folder: ./data
depth_folder: ./results
mesh_folder: ./results
video_folder: ./results
load_ply: False
save_ply: True
inference_video: True
gpu_ids: 0
offscreen_rendering: False
img_format: '.png'
depth_format: '.npy'
require_midas: False
depth_threshold: 0.04
ext_edge_threshold: 0.002
sparse_iter: 5
filter_size: [7, 7, 5, 5, 5]
sigma_s: 4.0
sigma_r: 0.5
redundant_number: 12
background_thickness: 70
context_thickness: 140
background_thickness_2: 70
context_thickness_2: 70
discount_factor: 1.00
log_depth: True
largest_size: 512
depth_edge_dilate: 10
depth_edge_dilate_2: 5
extrapolate_border: False
extrapolation_thickness: 0
repeat_inpaint_edge: True
crop_border: [0.03, 0.03, 0.05, 0.03]
anti_flickering: True
use_stable_diffusion: False
stable_diffusion_version: 1.5
use_controlnet: False
resize_patch: False
use_real_depth: False



================================================
FILE: inpainting/argument_p2m.yml
================================================
depth_edge_model_ckpt: checkpoints/edge-model.pth
depth_feat_model_ckpt: checkpoints/depth-model.pth
rgb_feat_model_ckpt: checkpoints/color-model.pth
MiDaS_model_ckpt: MiDaS/model.pt
use_boostmonodepth: False
fps: 40
num_frames: 240
x_shift_range: [0.00, 0.00, -0.015, -0.015]
y_shift_range: [0.00, 0.00, -0.015, -0.00]
z_shift_range: [-0.05, -0.05, -0.05, -0.05]
traj_types: ['double-straight-line', 'double-straight-line', 'circle', 'circle']
video_postfix: ['dolly-zoom-in', 'zoom-in', 'circle', 'swing']
specific: ''
longer_side_len: 2048
src_folder: ./data
depth_folder: ./results
mesh_folder: ./results
video_folder: ./results
load_ply: False
save_ply: True
inference_video: True
gpu_ids: 0
offscreen_rendering: False
img_format: '.png'
depth_format: '.npy'
require_midas: False
depth_threshold: 0.04
ext_edge_threshold: 0.002
sparse_iter: 5
filter_size: [7, 7, 5, 5, 5]
sigma_s: 4.0
sigma_r: 0.5
redundant_number: 12
background_thickness: 70
context_thickness: 140
background_thickness_2: 70
context_thickness_2: 70
discount_factor: 1.00
log_depth: True
largest_size: 512
depth_edge_dilate: 10
depth_edge_dilate_2: 5
extrapolate_border: False
extrapolation_thickness: 0
repeat_inpaint_edge: True
crop_border: [0.03, 0.03, 0.05, 0.03]
anti_flickering: True
use_stable_diffusion: False
stable_diffusion_version: 1.5
use_controlnet: False
resize_patch: False
use_real_depth: False
tear_edges: True



================================================
FILE: inpainting/bilateral_filtering.py
================================================
import numpy as np
from functools import reduce

def sparse_bilateral_filtering(
    depth, image, config, HR=False, mask=None, gsHR=True, edge_id=None, num_iter=None, num_gs_iter=None, spdb=False
):
    """
    config:
    - filter_size
    """
    import time

    save_images = []
    save_depths = []
    save_discontinuities = []
    vis_depth = depth.copy()
    backup_vis_depth = vis_depth.copy()

    depth_max = vis_depth.max()
    depth_min = vis_depth.min()
    vis_image = image.copy()
    for i in range(num_iter):
        if isinstance(config["filter_size"], list):
            window_size = config["filter_size"][i]
        else:
            window_size = config["filter_size"]
        vis_image = image.copy()
        save_images.append(vis_image)
        save_depths.append(vis_depth)
        u_over, b_over, l_over, r_over = vis_depth_discontinuity(vis_depth, config, mask=mask)
        vis_image[u_over > 0] = np.array([0, 0, 0])
        vis_image[b_over > 0] = np.array([0, 0, 0])
        vis_image[l_over > 0] = np.array([0, 0, 0])
        vis_image[r_over > 0] = np.array([0, 0, 0])

        discontinuity_map = (u_over + b_over + l_over + r_over).clip(0.0, 1.0)
        discontinuity_map[depth == 0] = 1
        save_discontinuities.append(discontinuity_map)
        if mask is not None:
            discontinuity_map[mask == 0] = 0
        vis_depth = bilateral_filter(
            vis_depth, config, discontinuity_map=discontinuity_map, HR=HR, mask=mask, window_size=window_size
        )

    return save_images, save_depths


def vis_depth_discontinuity(depth, config, vis_diff=False, label=False, mask=None):
    """
    config:
    - 
    """
    if label == False:
        disp = 1./depth
        u_diff = (disp[1:, :] - disp[:-1, :])[:-1, 1:-1]
        b_diff = (disp[:-1, :] - disp[1:, :])[1:, 1:-1]
        l_diff = (disp[:, 1:] - disp[:, :-1])[1:-1, :-1]
        r_diff = (disp[:, :-1] - disp[:, 1:])[1:-1, 1:]
        if mask is not None:
            u_mask = (mask[1:, :] * mask[:-1, :])[:-1, 1:-1]
            b_mask = (mask[:-1, :] * mask[1:, :])[1:, 1:-1]
            l_mask = (mask[:, 1:] * mask[:, :-1])[1:-1, :-1]
            r_mask = (mask[:, :-1] * mask[:, 1:])[1:-1, 1:]
            u_diff = u_diff * u_mask
            b_diff = b_diff * b_mask
            l_diff = l_diff * l_mask
            r_diff = r_diff * r_mask
        u_over = (np.abs(u_diff) > config['depth_threshold']).astype(np.float32)
        b_over = (np.abs(b_diff) > config['depth_threshold']).astype(np.float32)
        l_over = (np.abs(l_diff) > config['depth_threshold']).astype(np.float32)
        r_over = (np.abs(r_diff) > config['depth_threshold']).astype(np.float32)
    else:
        disp = depth
        u_diff = (disp[1:, :] * disp[:-1, :])[:-1, 1:-1]
        b_diff = (disp[:-1, :] * disp[1:, :])[1:, 1:-1]
        l_diff = (disp[:, 1:] * disp[:, :-1])[1:-1, :-1]
        r_diff = (disp[:, :-1] * disp[:, 1:])[1:-1, 1:]
        if mask is not None:
            u_mask = (mask[1:, :] * mask[:-1, :])[:-1, 1:-1]
            b_mask = (mask[:-1, :] * mask[1:, :])[1:, 1:-1]
            l_mask = (mask[:, 1:] * mask[:, :-1])[1:-1, :-1]
            r_mask = (mask[:, :-1] * mask[:, 1:])[1:-1, 1:]
            u_diff = u_diff * u_mask
            b_diff = b_diff * b_mask
            l_diff = l_diff * l_mask
            r_diff = r_diff * r_mask
        u_over = (np.abs(u_diff) > 0).astype(np.float32)
        b_over = (np.abs(b_diff) > 0).astype(np.float32)
        l_over = (np.abs(l_diff) > 0).astype(np.float32)
        r_over = (np.abs(r_diff) > 0).astype(np.float32)
    u_over = np.pad(u_over, 1, mode='constant')
    b_over = np.pad(b_over, 1, mode='constant')
    l_over = np.pad(l_over, 1, mode='constant')
    r_over = np.pad(r_over, 1, mode='constant')
    u_diff = np.pad(u_diff, 1, mode='constant')
    b_diff = np.pad(b_diff, 1, mode='constant')
    l_diff = np.pad(l_diff, 1, mode='constant')
    r_diff = np.pad(r_diff, 1, mode='constant')

    if vis_diff:
        return [u_over, b_over, l_over, r_over], [u_diff, b_diff, l_diff, r_diff]
    else:
        return [u_over, b_over, l_over, r_over]

def bilateral_filter(depth, config, discontinuity_map=None, HR=False, mask=None, window_size=False):
    sort_time = 0
    replace_time = 0
    filter_time = 0
    init_time = 0
    filtering_time = 0
    sigma_s = config['sigma_s']
    sigma_r = config['sigma_r']
    if window_size == False:
        window_size = config['filter_size']
    midpt = window_size//2
    ax = np.arange(-midpt, midpt+1.)
    xx, yy = np.meshgrid(ax, ax)
    if discontinuity_map is not None:
        spatial_term = np.exp(-(xx**2 + yy**2) / (2. * sigma_s**2))

    # padding
    depth = depth[1:-1, 1:-1]
    depth = np.pad(depth, ((1,1), (1,1)), 'edge')
    pad_depth = np.pad(depth, (midpt,midpt), 'edge')
    if discontinuity_map is not None:
        discontinuity_map = discontinuity_map[1:-1, 1:-1]
        discontinuity_map = np.pad(discontinuity_map, ((1,1), (1,1)), 'edge')
        pad_discontinuity_map = np.pad(discontinuity_map, (midpt,midpt), 'edge')
        pad_discontinuity_hole = 1 - pad_discontinuity_map
    # filtering
    output = depth.copy()
    pad_depth_patches = rolling_window(pad_depth, [window_size, window_size], [1,1])
    if discontinuity_map is not None:
        pad_discontinuity_patches = rolling_window(pad_discontinuity_map, [window_size, window_size], [1,1])
        pad_discontinuity_hole_patches = rolling_window(pad_discontinuity_hole, [window_size, window_size], [1,1])

    if mask is not None:
        pad_mask = np.pad(mask, (midpt,midpt), 'constant')
        pad_mask_patches = rolling_window(pad_mask, [window_size, window_size], [1,1])
    from itertools import product
    if discontinuity_map is not None:
        pH, pW = pad_depth_patches.shape[:2]
        for pi in range(pH):
            for pj in range(pW):
                if mask is not None and mask[pi, pj] == 0:
                    continue
                if discontinuity_map is not None:
                    if bool(pad_discontinuity_patches[pi, pj].any()) is False:
                        continue
                    discontinuity_patch = pad_discontinuity_patches[pi, pj]
                    discontinuity_holes = pad_discontinuity_hole_patches[pi, pj]
                depth_patch = pad_depth_patches[pi, pj]
                depth_order = depth_patch.ravel().argsort()
                patch_midpt = depth_patch[window_size//2, window_size//2]
                if discontinuity_map is not None:
                    coef = discontinuity_holes.astype(np.float32)
                    if mask is not None:
                        coef = coef * pad_mask_patches[pi, pj]
                else:
                    range_term = np.exp(-(depth_patch-patch_midpt)**2 / (2. * sigma_r**2))
                    coef = spatial_term * range_term
                if coef.max() == 0:
                    output[pi, pj] = patch_midpt
                    continue
                if discontinuity_map is not None and (coef.max() == 0):
                    output[pi, pj] = patch_midpt
                else:
                    coef = coef/(coef.sum())
                    coef_order = coef.ravel()[depth_order]
                    cum_coef = np.cumsum(coef_order)
                    ind = np.digitize(0.5, cum_coef)
                    output[pi, pj] = depth_patch.ravel()[depth_order][ind]
    else:
        pH, pW = pad_depth_patches.shape[:2]
        for pi in range(pH):
            for pj in range(pW):
                if discontinuity_map is not None:
                    if pad_discontinuity_patches[pi, pj][window_size//2, window_size//2] == 1:
                        continue
                    discontinuity_patch = pad_discontinuity_patches[pi, pj]
                    discontinuity_holes = (1. - discontinuity_patch)
                depth_patch = pad_depth_patches[pi, pj]
                depth_order = depth_patch.ravel().argsort()
                patch_midpt = depth_patch[window_size//2, window_size//2]
                range_term = np.exp(-(depth_patch-patch_midpt)**2 / (2. * sigma_r**2))
                if discontinuity_map is not None:
                    coef = spatial_term * range_term * discontinuity_holes
                else:
                    coef = spatial_term * range_term
                if coef.sum() == 0:
                    output[pi, pj] = patch_midpt
                    continue
                if discontinuity_map is not None and (coef.sum() == 0):
                    output[pi, pj] = patch_midpt
                else:
                    coef = coef/(coef.sum())
                    coef_order = coef.ravel()[depth_order]
                    cum_coef = np.cumsum(coef_order)
                    ind = np.digitize(0.5, cum_coef)
                    output[pi, pj] = depth_patch.ravel()[depth_order][ind]

    return output

def rolling_window(a, window, strides):
    assert len(a.shape)==len(window)==len(strides), "\'a\', \'window\', \'strides\' dimension mismatch"
    shape_fn = lambda i,w,s: (a.shape[i]-w)//s + 1
    shape = [shape_fn(i,w,s) for i,(w,s) in enumerate(zip(window, strides))] + list(window)
    def acc_shape(i):
        if i+1>=len(a.shape):
            return 1
        else:
            return reduce(lambda x,y:x*y, a.shape[i+1:])
    _strides = [acc_shape(i)*s*a.itemsize for i,s in enumerate(strides)] + list(a.strides)

    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=_strides)



================================================
FILE: inpainting/boostmonodepth_utils.py
================================================
import os
import cv2
import glob
import numpy as np
import imageio
from MiDaS.MiDaS_utils import write_depth

BOOST_BASE = 'BoostingMonocularDepth'

BOOST_INPUTS = 'inputs'
BOOST_OUTPUTS = 'outputs'

def run_boostmonodepth(img_names, src_folder, depth_folder):

    if not isinstance(img_names, list):
        img_names = [img_names]

    # remove irrelevant files first
    clean_folder(os.path.join(BOOST_BASE, BOOST_INPUTS))
    clean_folder(os.path.join(BOOST_BASE, BOOST_OUTPUTS))

    tgt_names = []
    for img_name in img_names:
        base_name = os.path.basename(img_name)
        tgt_name = os.path.join(BOOST_BASE, BOOST_INPUTS, base_name)
        os.system(f'cp {img_name} {tgt_name}')

        # keep only the file name here.
        # they save all depth as .png file
        tgt_names.append(os.path.basename(tgt_name).replace('.jpg', '.png'))

    os.system(f'cd {BOOST_BASE} && python run.py --Final --data_dir {BOOST_INPUTS}/  --output_dir {BOOST_OUTPUTS} --depthNet 0')

    for i, (img_name, tgt_name) in enumerate(zip(img_names, tgt_names)):
        img = imageio.imread(img_name)
        H, W = img.shape[:2]
        scale = 640. / max(H, W)

        # resize and save depth
        target_height, target_width = int(round(H * scale)), int(round(W * scale))
        depth = imageio.imread(os.path.join(BOOST_BASE, BOOST_OUTPUTS, tgt_name))
        depth = np.array(depth).astype(np.float32)
        depth = resize_depth(depth, target_width, target_height)
        np.save(os.path.join(depth_folder, tgt_name.replace('.png', '.npy')), depth / 32768. - 1.)
        write_depth(os.path.join(depth_folder, tgt_name.replace('.png', '')), depth)

def clean_folder(folder, img_exts=['.png', '.jpg', '.npy']):

    for img_ext in img_exts:
        paths_to_check = os.path.join(folder, f'*{img_ext}')
        if len(glob.glob(paths_to_check)) == 0:
            continue
        print(paths_to_check)
        os.system(f'rm {paths_to_check}')

def resize_depth(depth, width, height):
    """Resize numpy (or image read by imageio) depth map

    Args:
        depth (numpy): depth
        width (int): image width
        height (int): image height

    Returns:
        array: processed depth
    """
    depth = cv2.blur(depth, (3, 3))
    return cv2.resize(depth, (width, height), interpolation=cv2.INTER_AREA)



================================================
FILE: inpainting/Dockerfile
================================================
FROM nvidia/cuda:12.3.0-devel-ubuntu20.04
RUN apt-get update

ARG DEBIAN_FRONTEND=noninteractive
ENV TZ=Europe/London

RUN apt-get install --no-install-recommends -y \
        git \
        wget \
        python3 \
        python3-pip

RUN pip install torch torchvision opencv-python-headless transforms3d networkx scikit-image tqdm pyyaml matplotlib
RUN pip install "jax[cuda12_pip]==0.4.13" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
RUN pip install -qq -U diffusers transformers ftfy gradio accelerate

WORKDIR "/inpainting"
COPY . ./
RUN sh download.sh
RUN chmod -R a+rwX ./


================================================
FILE: inpainting/DOCUMENTATION.md
================================================
# Documentation

## Python scripts

These files are for our monocular 3D Tracking pipeline:

`main.py` Execute 3D photo inpainting

`mesh.py` Functions about context-aware depth inpainting

`mesh_tools.py` Some common functions used in `mesh.py`

`utils.py` Some common functions used in image preprocessing, data loading

`networks.py` Network architectures of inpainting model


MiDaS/

`run.py` Execute depth estimation

`monodepth_net.py` Network architecture of depth estimation model

`MiDaS_utils.py` Some common functions in depth estimation


## Configuration

```bash
argument.yml
```

- `depth_edge_model_ckpt: checkpoints/EdgeModel.pth`
    - Pretrained model of depth-edge inpainting
- `depth_feat_model_ckpt: checkpoints/DepthModel.pth`
    - Pretrained model of depth inpainting
- `rgb_feat_model_ckpt: checkpoints/ColorModel.pth`
    - Pretrained model of color inpainting
- `MiDaS_model_ckpt: MiDaS/model.pt`
    - Pretrained model of depth estimation
- `use_boostmonodepth: True`
    - Use [BoostMonocularDepth](https://github.com/compphoto/BoostingMonocularDepth) to get sharper monocular depth estimation
- `fps: 40`
    - Frame per second of output rendered video
- `num_frames: 240`
    - Total number of frames in output rendered video
- `x_shift_range: [-0.03, -0.03, -0.03]`
    - The translations on x-axis of output rendered videos.
    - This parameter is a list. Each element corresponds to a specific camera motion.
- `y_shift_range: [-0.00, -0.00, -0.03]`
    - The translations on y-axis of output rendered videos.
    - This parameter is a list. Each element corresponds to a specific camera motion.
- `z_shift_range: [-0.07, -0.07, -0.07]`
    - The translations on z-axis of output rendered videos.
    - This parameter is a list. Each element corresponds to a specific camera motion.
- `traj_types: ['straight-line', 'circle', 'circle']`
    - The type of camera trajectory.
    - This parameter is a list.
    - Currently, we only privode `straight-line` and `circle`.
-  `video_postfix: ['zoom-in', 'swing', 'circle']`
    - The postfix of video.
    - This parameter is a list.
- Note that the number of elements in `x_shift_range`,  `y_shift_range`, `z_shift_range`, `traj_types` and `video_postfix` should be equal.
- `specific: '' `
    - The specific image name, use this to specify the image to be executed. By default, all the image in the folder will    be executed.
- `longer_side_len: 960`
    - The length of larger dimension in output resolution.
- `src_folder: image`
    - Input image directory. 
- `depth_folder: depth`
    - Estimated depth directory.
- `mesh_folder: mesh`
    - Output 3-D mesh directory.
- `video_folder: video`
    - Output rendered video directory
- `load_ply: False`
    - Action to load existed mesh (.ply) file
- `save_ply: True`
    - Action to store the output mesh (.ply) file
    - Disable this option `save_ply: False` to reduce the computational time.
- `inference_video: True`
    - Action to rendered the output video
- `gpu_ids: 0`
    - The ID of working GPU. Leave it blank or negative to use CPU.
- `offscreen_rendering: True`
    - If you're executing the process in a remote server (via ssh), please switch on this flag. 
    - Sometimes, using off-screen rendering result in longer execution time.
- `img_format: '.jpg'`
    - Input image format.
- `depth_format: '.npy'`
    - Input depth (disparity) format. Use NumPy array file as default.
    - If the user wants to edit the depth (disparity) map manually, we provide `.png` format depth (disparity) map.
        - Remember to switch this parameter from `.npy` to `.png` when using depth (disparity) map with `.png` format.
- `require_midas: True`
    - Set it to `True` if the user wants to use depth map estimated by `MiDaS`.
    - Set it to `False` if the user wants to use manually edited depth map.
    - If the user wants to edit the depth (disparity) map manually, we provide `.png` format depth (disparity) map.
        - Remember to switch this parameter from `True` to `False` when using manually edited depth map.
- `depth_threshold: 0.04`
    - A threshold in disparity, adjacent two pixels are discontinuity pixels 
      if the difference between them excceed this number.
- `ext_edge_threshold: 0.002`
    - The threshold to define inpainted depth edge. A pixel in inpainted edge 
      map belongs to extended depth edge if the value of that pixel exceeds this number,
- `sparse_iter: 5`
    - Total iteration numbers of bilateral median filter
- `filter_size: [7, 7, 5, 5, 5]`
    - Window size of bilateral median filter in each iteration.
- `sigma_s: 4.0`
    - Intensity term of bilateral median filter
- `sigma_r: 0.5`
    - Spatial term of bilateral median filter
- `redundant_number: 12`
    - The number defines short segments. If a depth edge is shorter than this number, 
      it is a short segment and removed.
- `background_thickness: 70`
    - The thickness of synthesis area.
- `context_thickness: 140`
    - The thickness of context area.
- `background_thickness_2: 70`
    - The thickness of synthesis area when inpaint second time.
- `context_thickness_2: 70`
    - The thickness of context area when inpaint second time.
- `discount_factor: 1.00`
- `log_depth: True`
    - The scale of depth inpainting. If true, performing inpainting in log scale. 
      Otherwise, performing in linear scale.
- `largest_size: 512`
    - The largest size of inpainted image patch.
- `depth_edge_dilate: 10`
    - The thickness of dilated synthesis area.
- `depth_edge_dilate_2: 5`
    - The thickness of dilated synthesis area when inpaint second time.
- `extrapolate_border: True`
    - Action to extrapolate out-side the border.
- `extrapolation_thickness: 60`
    - The thickness of extrapolated area.
- `repeat_inpaint_edge: True`
    - Action to apply depth edge inpainting model repeatedly. Sometimes inpainting depth 
      edge once results in short inpinated edge, apply depth edge inpainting repeatedly 
      could help you prolong the inpainted depth edge. 
- `crop_border: [0.03, 0.03, 0.05, 0.03]`
    - The fraction of pixels to crop out around the borders `[top, left, bottom, right]`.
- `anti_flickering: True`
    - Action to avoid flickering effect in the output video. 
    - This may result in longer computational time in rendering phase.



================================================
FILE: inpainting/download.sh
================================================
#!/bin/sh
fb_status=$(wget --spider -S https://filebox.ece.vt.edu/ 2>&1 | grep  "HTTP/1.1 200 OK")

mkdir checkpoints

echo "downloading from filebox ..."
wget https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/model/color-model.pth
wget https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/model/depth-model.pth
wget https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/model/edge-model.pth
wget https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/model/model.pt

mv color-model.pth checkpoints/.
mv depth-model.pth checkpoints/.
mv edge-model.pth checkpoints/.
mv model.pt MiDaS/.

echo "cloning from BoostingMonocularDepth ..."
git clone https://github.com/compphoto/BoostingMonocularDepth.git
mkdir -p BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/

echo "downloading mergenet weights ..."
wget https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/model/latest_net_G.pth
mv latest_net_G.pth BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/
wget https://github.com/intel-isl/MiDaS/releases/download/v2/model-f46da743.pt
mv model-f46da743.pt BoostingMonocularDepth/midas/model.pt



================================================
FILE: inpainting/LICENSE
================================================

MIT License

Copyright (c) 2020 Virginia Tech Vision and Learning Lab

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

------------------ LICENSE FOR MiDaS --------------------

MIT License

Copyright (c) 2019 Intel ISL (Intel Intelligent Systems Lab)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------- LICENSE FOR EdgeConnect --------------------------------

Attribution-NonCommercial 4.0 International


================================================
FILE: inpainting/main.py
================================================
import numpy as np
import argparse
import glob
import os
from functools import partial
#import vispy
import scipy.misc as misc
from tqdm import tqdm
import yaml
import time
import sys
from mesh import write_ply, read_ply, output_3d_photo
from utils import get_MiDaS_samples, read_MiDaS_depth, read_real_depth
import torch
import cv2
from skimage.transform import resize
import imageio
import copy
from networks import Inpaint_Color_Net, Inpaint_Depth_Net, Inpaint_Edge_Net
from MiDaS.run import run_depth
from boostmonodepth_utils import run_boostmonodepth
from MiDaS.monodepth_net import MonoDepthNet
import MiDaS.MiDaS_utils as MiDaS_utils
from bilateral_filtering import sparse_bilateral_filtering
from skimage import color
from diffusers import StableDiffusionInpaintPipeline, StableDiffusionControlNetInpaintPipeline, ControlNetModel

parser = argparse.ArgumentParser()
parser.add_argument('--config', type=str, default='argument.yml',help='Configure of post processing')
args = parser.parse_args()
config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)
#if config['offscreen_rendering'] is True:
    #vispy.use(app='egl')
os.makedirs(config['mesh_folder'], exist_ok=True)
os.makedirs(config['video_folder'], exist_ok=True)
os.makedirs(config['depth_folder'], exist_ok=True)
sample_list = get_MiDaS_samples(config['src_folder'], config['depth_folder'], config, config['specific'])
normal_canvas, all_canvas = None, None

if isinstance(config["gpu_ids"], int) and (config["gpu_ids"] >= 0):
    device = config["gpu_ids"]
else:
    device = "cpu"

print(f"running on device {device}")

for idx in tqdm(range(len(sample_list))):
    depth = None
    sample = sample_list[idx]
    print("Current Source ==> ", sample['src_pair_name'])
    mesh_fi = os.path.join(config['mesh_folder'], sample['src_pair_name'] +'.ply')
    image = imageio.imread(sample['ref_img_fi'])[:,:,:3]

    print(f"Running depth extraction at {time.time()}")
    if config['use_boostmonodepth'] is True:
        run_boostmonodepth(sample['ref_img_fi'], config['src_folder'], config['depth_folder'])
    elif config['require_midas'] is True:
        run_depth([sample['ref_img_fi']], config['src_folder'], config['depth_folder'],
                  config['MiDaS_model_ckpt'], MonoDepthNet, MiDaS_utils, target_w=640)
    print(sample['depth_fi'])
    print("Depth shape", np.load(sample['depth_fi']).shape)

    if 'npy' in config['depth_format']:
        config['output_h'], config['output_w'] = np.load(sample['depth_fi']).shape[:2]
    else:
        config['output_h'], config['output_w'] = imageio.imread(sample['depth_fi']).shape[:2]
    frac = config['longer_side_len'] / max(config['output_h'], config['output_w'])
    config['output_h'], config['output_w'] = int(config['output_h'] * frac), int(config['output_w'] * frac)
    config['original_h'], config['original_w'] = config['output_h'], config['output_w']
    if image.ndim == 2:
        image = image[..., None].repeat(3, -1)
    if np.sum(np.abs(image[..., 0] - image[..., 1])) == 0 and np.sum(np.abs(image[..., 1] - image[..., 2])) == 0:
        config['gray_image'] = True
    else:
        config['gray_image'] = False
    print("Width, height:", config['output_w'],config['output_h'])
    image = cv2.resize(image, (config['output_w'], config['output_h']), interpolation=cv2.INTER_AREA)
    if config["use_real_depth"] is False:
        depth = read_MiDaS_depth(sample['depth_fi'], 3.0, config['output_h'], config['output_w'])
    else:
        depth = read_real_depth(sample['depth_fi'], h=config["output_h"], w=config['output_w'])
    mean_loc_depth = depth[depth.shape[0]//2, depth.shape[1]//2]
    if not(config['load_ply'] is True and os.path.exists(mesh_fi)):
        vis_photos, vis_depths = sparse_bilateral_filtering(depth.copy(), image.copy(), config, num_iter=config['sparse_iter'], spdb=False)
        depth = vis_depths[-1]
        model = None
        torch.cuda.empty_cache()
        print("Start Running 3D_Photo ...")
        print(f"Loading edge model at {time.time()}")
        depth_edge_model = Inpaint_Edge_Net(init_weights=True)
        depth_edge_weight = torch.load(config['depth_edge_model_ckpt'],
                                       map_location=torch.device(device))
        depth_edge_model.load_state_dict(depth_edge_weight)
        depth_edge_model = depth_edge_model.to(device)
        depth_edge_model.eval()

        print(f"Loading depth model at {time.time()}")
        depth_feat_model = Inpaint_Depth_Net()
        depth_feat_weight = torch.load(config['depth_feat_model_ckpt'],
                                       map_location=torch.device(device))
        depth_feat_model.load_state_dict(depth_feat_weight, strict=True)
        depth_feat_model = depth_feat_model.to(device)
        depth_feat_model.eval()
        depth_feat_model = depth_feat_model.to(device)
        print(f"Loading rgb model at {time.time()}")
        if config['use_stable_diffusion']:
            device = "cuda"
            if config["use_controlnet"]:
                model_path = "runwayml/stable-diffusion-inpainting"
                controlnet_path =  "fusing/stable-diffusion-v1-5-controlnet-depth"
                controlnet = ControlNetModel.from_pretrained(
                                                        controlnet_path, torch_dtype=torch.float16
                                                    ).to(device)
                rgb_model = StableDiffusionControlNetInpaintPipeline.from_pretrained(
                                                                        model_path,
                                                                        controlnet = controlnet,
                                                                        torch_dtype=torch.float16,
                                                                    ).to(device)
            else:
                if config["stable_diffusion_version"] == 2:
                    model_path = "stabilityai/stable-diffusion-2-inpainting"
                else:
                     model_path = "runwayml/stable-diffusion-inpainting"
                rgb_model = StableDiffusionInpaintPipeline.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                    ).to(device)
        else:
            rgb_model = Inpaint_Color_Net()
            rgb_feat_weight = torch.load(config['rgb_feat_model_ckpt'],
                                        map_location=torch.device(device))
            rgb_model.load_state_dict(rgb_feat_weight)
            rgb_model.eval()
            rgb_model = rgb_model.to(device)
        graph = None


        print(f"Writing depth ply (and basically doing everything) at {time.time()}")
        rt_info = write_ply(image,
                              depth,
                              sample['int_mtx'],
                              mesh_fi,
                              config,
                              rgb_model,
                              depth_edge_model,
                              depth_edge_model,
                              depth_feat_model)

        if rt_info is False:
            continue
        rgb_model = None
        color_feat_model = None
        depth_edge_model = None
        depth_feat_model = None
        torch.cuda.empty_cache()
    #if config['save_ply'] is True or config['load_ply'] is True:
        #verts, colors, faces, Height, Width, hFov, vFov = read_ply(mesh_fi)
    #else:
        #verts, colors, faces, Height, Width, hFov, vFov = rt_info


    # print(f"Making video at {time.time()}")
    # videos_poses, video_basename = copy.deepcopy(sample['tgts_poses']), sample['tgt_name']
    # top = (config.get('original_h') // 2 - sample['int_mtx'][1, 2] * config['output_h'])
    # left = (config.get('original_w') // 2 - sample['int_mtx'][0, 2] * config['output_w'])
    # down, right = top + config['output_h'], left + config['output_w']
    # border = [int(xx) for xx in [top, down, left, right]]
    # normal_canvas, all_canvas = output_3d_photo(verts.copy(), colors.copy(), faces.copy(), copy.deepcopy(Height), copy.deepcopy(Width), copy.deepcopy(hFov), copy.deepcopy(vFov),
    #                     copy.deepcopy(sample['tgt_pose']), sample['video_postfix'], copy.deepcopy(sample['ref_pose']), copy.deepcopy(config['video_folder']),
    #                     image.copy(), copy.deepcopy(sample['int_mtx']), config, image,
    #                     videos_poses, video_basename, config.get('original_h'), config.get('original_w'), border=border, depth=depth, normal_canvas=normal_canvas, all_canvas=all_canvas,
    #                     mean_loc_depth=mean_loc_depth)



================================================
FILE: inpainting/mesh.py
================================================
import os
import csv
import numpy as np
try:
    import cynetworkx as netx
except ImportError:
    import networkx as netx
import matplotlib.pyplot as plt
from functools import partial
#from vispy import scene, io
#from vispy.scene import visuals
#from vispy.visuals.filters import Alpha
import cv2
#from moviepy.editor import ImageSequenceClip
from skimage.transform import resize
import time
import copy
import torch
import os
from utils import path_planning, open_small_mask, clean_far_edge, refine_depth_around_edge
from utils import refine_color_around_edge, filter_irrelevant_edge_new, require_depth_edge, clean_far_edge_new
from utils import create_placeholder, refresh_node, find_largest_rect
from mesh_tools import get_depth_from_maps, get_map_from_ccs, get_edge_from_nodes, get_depth_from_nodes, get_rgb_from_nodes, crop_maps_by_size, convert2tensor, recursive_add_edge, update_info, filter_edge, relabel_node, depth_inpainting
from mesh_tools import refresh_bord_depth, enlarge_border, fill_dummy_bord, extrapolate, fill_missing_node, incomplete_node, get_valid_size, dilate_valid_size, size_operation
import transforms3d
import random
from functools import reduce
import math
from diffusers import StableDiffusionInpaintPipeline, StableDiffusionControlNetInpaintPipeline, ControlNetModel


def create_mesh(depth, image, int_mtx, config):
    H, W, C = image.shape
    ext_H, ext_W = H + 2 * config['extrapolation_thickness'], W + 2 * config['extrapolation_thickness']
    LDI = netx.Graph(H=ext_H, W=ext_W, noext_H=H, noext_W=W, cam_param=int_mtx)
    xy2depth = {}
    int_mtx_pix = int_mtx * np.array([[W], [H], [1.]])
    LDI.graph['cam_param_pix'], LDI.graph['cam_param_pix_inv'] = int_mtx_pix, np.linalg.inv(int_mtx_pix)
    disp = 1. / (-depth)
    LDI.graph['hoffset'], LDI.graph['woffset'] = config['extrapolation_thickness'], config['extrapolation_thickness']
    LDI.graph['bord_up'], LDI.graph['bord_down'] = LDI.graph['hoffset'] + 0, LDI.graph['hoffset'] + H
    LDI.graph['bord_left'], LDI.graph['bord_right'] = LDI.graph['woffset'] + 0, LDI.graph['woffset'] + W
    for idx in range(H):
        for idy in range(W):
            x, y = idx + LDI.graph['hoffset'], idy + LDI.graph['woffset']
            LDI.add_node((x, y, -depth[idx, idy]),
                         color=image[idx, idy],
                         disp=disp[idx, idy],
                         synthesis=False,
                         cc_id=set())
            xy2depth[(x, y)] = [-depth[idx, idy]]

    for x, y, d in LDI.nodes:
        two_nes = [ne for ne in [(x+1, y % LDI.graph['bord_right']), (x, (y+1) % LDI.graph['bord_right'])] if ne[0] < LDI.graph['bord_down'] and ne[1] <= LDI.graph['bord_right']]
        # two_nes = [ne if ne[0] < LDI.graph['bord_down'] and ne[1] < LDI.graph['bord_right'] else [(LDI.graph['hoffset'], y), (x, LDI.graph['woffset'])] for ne in [(x+1, y), (x, y+1)]]
        # two_nes = [ne for ne in [((x+1) if (x+1) < LDI.graph['bord_down'] else LDI.graph['hoffset'] + ((x+1) % LDI.graph['bord_down']), y), (x, (y+1) if (y+1) < LDI.graph['bord_right'] else LDI.graph['woffset'] + ((y+1) % LDI.graph['bord_right']))]]
        [LDI.add_edge((ne[0], ne[1], xy2depth[ne][0]), (x, y, d)) for ne in two_nes]
    LDI = calculate_fov(LDI)
    image = np.pad(image,
                    pad_width=((config['extrapolation_thickness'], config['extrapolation_thickness']),
                               (config['extrapolation_thickness'], config['extrapolation_thickness']),
                               (0, 0)),
                    mode='constant')
    depth = np.pad(depth,
                    pad_width=((config['extrapolation_thickness'], config['extrapolation_thickness']),
                               (config['extrapolation_thickness'], config['extrapolation_thickness'])),
                    mode='constant')

    return LDI, xy2depth, image, depth


def tear_edges(mesh, threshold = 0.00025, xy2depth=None):
    remove_edge_list = []
    remove_horizon, remove_vertical = np.zeros((2, mesh.graph['H'], mesh.graph['W']))
    mesh_nodes = mesh.nodes
    for edge in mesh.edges:
        if abs(mesh_nodes[edge[0]]['disp'] - mesh_nodes[edge[1]]['disp']) > threshold:
            remove_edge_list.append((edge[0], edge[1]))

            near, far = edge if abs(edge[0][2]) < abs(edge[1][2]) else edge[::-1]

            mesh_nodes[far]['near'] = [] if mesh_nodes[far].get('near') is None else mesh_nodes[far]['near'].append(near)
            mesh_nodes[near]['far'] = [] if mesh_nodes[near].get('far') is None else mesh_nodes[near]['far'].append(far)

            if near[0] == far[0]:
                remove_horizon[near[0], np.minimum(near[1], far[1])] = 1
            elif near[1] == far[1]:
                remove_vertical[np.minimum(near[0], far[0]), near[1]] = 1
    mesh.remove_edges_from(remove_edge_list)

    remove_edge_list = []

    dang_horizon = np.where(np.roll(remove_horizon, 1, 0) + np.roll(remove_horizon, -1, 0) - remove_horizon == 2)
    dang_vertical = np.where(np.roll(remove_vertical, 1, 1) + np.roll(remove_vertical, -1, 1) - remove_vertical == 2)

    horizon_condition = lambda x, y: mesh.graph['bord_up'] + 1 <= x < mesh.graph['bord_down'] - 1
    vertical_condition = lambda x, y: mesh.graph['bord_left'] + 1 <= y < mesh.graph['bord_right'] - 1

    prjto3d = lambda x, y: (x, y, xy2depth[(x, y)][0])

    node_existence = lambda x, y: mesh.has_node(prjto3d(x, y))

    for x, y in zip(dang_horizon[0], dang_horizon[1]):
        if horizon_condition(x, y) and node_existence(x, y) and node_existence(x, y+1):
            remove_edge_list.append((prjto3d(x, y), prjto3d(x, y+1)))
    for x, y in zip(dang_vertical[0], dang_vertical[1]):
        if vertical_condition(x, y) and node_existence(x, y) and node_existence(x+1, y):
            remove_edge_list.append((prjto3d(x, y), prjto3d(x+1, y)))
    mesh.remove_edges_from(remove_edge_list)

    return mesh


def calculate_fov(mesh):
    k = mesh.graph['cam_param']
    mesh.graph['hFov'] = 2 * np.arctan(1. / (2*k[0, 0]))
    mesh.graph['vFov'] = 2 * np.arctan(1. / (2*k[1, 1]))
    mesh.graph['aspect'] = mesh.graph['noext_H'] / mesh.graph['noext_W']

    return mesh

def calculate_fov_FB(mesh):
    mesh.graph['aspect'] = mesh.graph['H'] / mesh.graph['W']
    if mesh.graph['H'] > mesh.graph['W']:
        mesh.graph['hFov'] = 0.508015513
        half_short = np.tan(mesh.graph['hFov']/2.0)
        half_long = half_short * mesh.graph['aspect']
        mesh.graph['vFov'] = 2.0 * np.arctan(half_long)
    else:
        mesh.graph['vFov'] = 0.508015513
        half_short = np.tan(mesh.graph['vFov']/2.0)
        half_long = half_short / mesh.graph['aspect']
        mesh.graph['hFov'] = 2.0 * np.arctan(half_long)

    return mesh

def map_range(value, in_min, in_max, out_min, out_max):
    return (value - in_min) * (out_max - out_min) / (in_max - in_min) + out_min

def reproject_3d_int_detail(sx, sy, z, k_00, k_02, k_11, k_12, w_offset, h_offset, H, W):
    abs_z = abs(z)

    phi = (sy+0.5) * 2 * np.pi / W
    theta = (sx+0.5) * np.pi / H
    X = -np.sin(phi)*np.sin(theta)
    Y = np.cos(theta)
    Z = np.cos(phi)*np.sin(theta)

    ret = [abs_z * X, abs_z * Y, abs_z * Z]

    return ret




def reproject_3d_int_detail_FB(sx, sy, z, w_offset, h_offset, mesh):
    if mesh.graph.get('tan_hFov') is None:
        mesh.graph['tan_hFov'] = np.tan(mesh.graph['hFov'] / 2.)
    if mesh.graph.get('tan_vFov') is None:
        mesh.graph['tan_vFov'] = np.tan(mesh.graph['vFov'] / 2.)

    ray = np.array([(-1. + 2. * ((sy+0.5-w_offset)/(mesh.graph['W'] - 1))) * mesh.graph['tan_hFov'],
                    (1. - 2. * (sx+0.5-h_offset)/(mesh.graph['H'] - 1)) * mesh.graph['tan_vFov'],
                    -1])
    point_3d = ray * np.abs(z)

    return point_3d


def reproject_3d_int(sx, sy, z, mesh):
    k = mesh.graph['cam_param_pix_inv'].copy()
    if k[0, 2] > 0:
        k = np.linalg.inv(k)
    ray = np.dot(k, np.array([sy-mesh.graph['woffset'], sx-mesh.graph['hoffset'], 1]).reshape(3, 1))

    point_3d = ray * np.abs(z)
    point_3d = point_3d.flatten()

    return point_3d

def generate_init_node(mesh, config, min_node_in_cc):
    mesh_nodes = mesh.nodes

    info_on_pix = {}

    ccs = sorted(netx.connected_components(mesh), key = len, reverse=True)
    remove_nodes = []

    for cc in ccs:

        remove_flag = True if len(cc) < min_node_in_cc else False
        if remove_flag is False:
            for (nx, ny, nd) in cc:
                info_on_pix[(nx, ny)] = [{'depth':nd,
                                          'color':mesh_nodes[(nx, ny, nd)]['color'],
                                          'synthesis':False,
                                          'disp':mesh_nodes[(nx, ny, nd)]['disp']}]
        else:
            [remove_nodes.append((nx, ny, nd)) for (nx, ny, nd) in cc]

    for node in remove_nodes:
        far_nodes = [] if mesh_nodes[node].get('far') is None else mesh_nodes[node]['far']
        for far_node in far_nodes:
            if mesh.has_node(far_node) and mesh_nodes[far_node].get('near') is not None and node in mesh_nodes[far_node]['near']:
                mesh_nodes[far_node]['near'].remove(node)
        near_nodes = [] if mesh_nodes[node].get('near') is None else mesh_nodes[node]['near']
        for near_node in near_nodes:
            if mesh.has_node(near_node) and mesh_nodes[near_node].get('far') is not None and node in mesh_nodes[near_node]['far']:
                mesh_nodes[near_node]['far'].remove(node)

    [mesh.remove_node(node) for node in remove_nodes]

    return mesh, info_on_pix

def get_neighbors(mesh, node):
    return [*mesh.neighbors(node)]

def generate_face(mesh, info_on_pix, config):
    H, W = mesh.graph['H'], mesh.graph['W']
    str_faces = []
    num_node = len(mesh.nodes)
    ply_flag = config.get('save_ply')
    def out_fmt(input, cur_id_b, cur_id_self, cur_id_a, ply_flag):
        if ply_flag is True:
            input.append(' '.join(['3', cur_id_b, cur_id_self, cur_id_a]) + '\n')
        else:
            input.append([cur_id_b, cur_id_self, cur_id_a])
    mesh_nodes = mesh.nodes
    for node in mesh_nodes:
        cur_id_self = mesh_nodes[node]['cur_id']
        ne_nodes = get_neighbors(mesh, node)
        four_dir_nes = {'up': [], 'left': [],
                        'down': [], 'right': []}
        for ne_node in ne_nodes:
            store_tuple = [ne_node, mesh_nodes[ne_node]['cur_id']]
            if ne_node[0] == node[0]:
                if ne_node[1] == ne_node[1] - 1:
                    four_dir_nes['left'].append(store_tuple)
                else:
                    four_dir_nes['right'].append(store_tuple)
            else:
                if ne_node[0] == ne_node[0] - 1:
                    four_dir_nes['up'].append(store_tuple)
                else:
                    four_dir_nes['down'].append(store_tuple)
        for node_a, cur_id_a in four_dir_nes['up']:
            for node_b, cur_id_b in four_dir_nes['right']:
                out_fmt(str_faces, cur_id_b, cur_id_self, cur_id_a, ply_flag)
        for node_a, cur_id_a in four_dir_nes['right']:
            for node_b, cur_id_b in four_dir_nes['down']:
                out_fmt(str_faces, cur_id_b, cur_id_self, cur_id_a, ply_flag)
        for node_a, cur_id_a in four_dir_nes['down']:
            for node_b, cur_id_b in four_dir_nes['left']:
                out_fmt(str_faces, cur_id_b, cur_id_self, cur_id_a, ply_flag)
        for node_a, cur_id_a in four_dir_nes['left']:
            for node_b, cur_id_b in four_dir_nes['up']:
                out_fmt(str_faces, cur_id_b, cur_id_self, cur_id_a, ply_flag)

    return str_faces

def reassign_floating_island(mesh, info_on_pix, image, depth):
    H, W = mesh.graph['H'], mesh.graph['W'],
    mesh_nodes = mesh.nodes
    bord_up, bord_down = mesh.graph['bord_up'], mesh.graph['bord_down']
    bord_left, bord_right = mesh.graph['bord_left'], mesh.graph['bord_right']
    W = mesh.graph['W']
    lost_map = np.zeros((H, W))

    '''
    (5) is_inside(x, y, xmin, xmax, ymin, ymax) : Check if a pixel(x, y) is inside the border.
    (6) get_cross_nes(x, y) : Get the four cross neighbors of pixel(x, y).
    '''
    key_exist = lambda d, k: k in d
    is_inside = lambda x, y, xmin, xmax, ymin, ymax: xmin <= x < xmax and ymin <= y < ymax
    get_cross_nes = lambda x, y: [(x + 1, y), (x - 1, y), (x, y - 1), (x, y + 1)]
    '''
    (A) Highlight the pixels on isolated floating island.
    (B) Number those isolated floating islands with connected component analysis.
    (C) For each isolated island:
        (1) Find its longest surrounded depth edge.
        (2) Propogate depth from that depth edge to the pixels on the isolated island.
        (3) Build the connection between the depth edge and that isolated island.
    '''
    for x in range(H):
        for y in range(W):
            if is_inside(x, y, bord_up, bord_down, bord_left, bord_right) and not(key_exist(info_on_pix, (x, y))):
                lost_map[x, y] = 1
    _, label_lost_map = cv2.connectedComponents(lost_map.astype(np.uint8), connectivity=4)
    mask = np.zeros((H, W))
    mask[bord_up:bord_down, bord_left:bord_right] = 1
    label_lost_map = (label_lost_map * mask).astype(np.int32)

    for i in range(1, label_lost_map.max()+1):
        lost_xs, lost_ys = np.where(label_lost_map == i)
        surr_edge_ids = {}
        for lost_x, lost_y in zip(lost_xs, lost_ys):
            if (lost_x, lost_y) == (295, 389) or (lost_x, lost_y) == (296, 389):
                import pdb; pdb.set_trace()
            for ne in get_cross_nes(lost_x, lost_y):
                if key_exist(info_on_pix, ne):
                    for info in info_on_pix[ne]:
                        ne_node = (ne[0], ne[1], info['depth'])
                        if key_exist(mesh_nodes[ne_node], 'edge_id'):
                            edge_id = mesh_nodes[ne_node]['edge_id']
                            surr_edge_ids[edge_id] = surr_edge_ids[edge_id] + [ne_node] if \
                                                key_exist(surr_edge_ids, edge_id) else [ne_node]
        if len(surr_edge_ids) == 0:
            continue
        edge_id, edge_nodes = sorted([*surr_edge_ids.items()], key=lambda x: len(x[1]), reverse=True)[0]
        edge_depth_map = np.zeros((H, W))
        for node in edge_nodes:
            edge_depth_map[node[0], node[1]] = node[2]
        lost_xs, lost_ys = np.where(label_lost_map == i)
        while lost_xs.shape[0] > 0:
            lost_xs, lost_ys = np.where(label_lost_map == i)
            for lost_x, lost_y in zip(lost_xs, lost_ys):
                propagated_depth = []
                real_nes = []
                for ne in get_cross_nes(lost_x, lost_y):
                    if not(is_inside(ne[0], ne[1], bord_up, bord_down, bord_left, bord_right)) or \
                       edge_depth_map[ne[0], ne[1]] == 0:
                        continue
                    propagated_depth.append(edge_depth_map[ne[0], ne[1]])
                    real_nes.append(ne)
                if len(real_nes) == 0:
                    continue
                reassign_depth = np.mean(propagated_depth)
                label_lost_map[lost_x, lost_y] = 0
                edge_depth_map[lost_x, lost_y] = reassign_depth
                depth[lost_x, lost_y] = -reassign_depth
                mesh.add_node((lost_x, lost_y, reassign_depth), color=image[lost_x, lost_y],
                                                            synthesis=False,
                                                            disp=1./reassign_depth,
                                                            cc_id=set())
                info_on_pix[(lost_x, lost_y)] = [{'depth':reassign_depth,
                                                  'color':image[lost_x, lost_y],
                                                  'synthesis':False,
                                                  'disp':1./reassign_depth}]
                new_connections = [((lost_x, lost_y, reassign_depth),
                                    (ne[0], ne[1], edge_depth_map[ne[0], ne[1]])) for ne in real_nes]
                mesh.add_edges_from(new_connections)

    return mesh, info_on_pix, depth

def remove_node_feat(mesh, *feats):
    mesh_nodes = mesh.nodes
    for node in mesh_nodes:
        for feat in feats:
            mesh_nodes[node][feat] = None

    return mesh

def update_status(mesh, info_on_pix, depth=None):
    '''
    (2) clear_node_feat(G, *fts) : Clear all the node feature on graph G.
    (6) get_cross_nes(x, y) : Get the four cross neighbors of pixel(x, y).
    '''
    key_exist = lambda d, k: d.get(k) is not None
    is_inside = lambda x, y, xmin, xmax, ymin, ymax: xmin <= x < xmax and ymin <= y < ymax
    get_cross_nes = lambda x, y: [(x + 1, y), (x - 1, y), (x, y - 1), (x, y + 1)]
    append_element = lambda d, k, x: d[k] + [x] if key_exist(d, k) else [x]

    def clear_node_feat(G, fts):
        le_nodes = G.nodes
        for k in le_nodes:
            v = le_nodes[k]
            for ft in fts:
                if ft in v:
                    v[ft] = None

    clear_node_feat(mesh, ['edge_id', 'far', 'near'])
    bord_up, bord_down = mesh.graph['bord_up'], mesh.graph['bord_down']
    bord_left, bord_right = mesh.graph['bord_left'], mesh.graph['bord_right']

    le_nodes = mesh.nodes

    for node_key in le_nodes:
        if mesh.neighbors(node_key).__length_hint__() == 4:
            continue
        four_nes = [xx for xx in get_cross_nes(node_key[0], node_key[1]) if
                    is_inside(xx[0], xx[1], bord_up, bord_down, bord_left, bord_right) and
                    xx in info_on_pix]
        # print(mesh.neighbors(node_key).__length_hint__())
        # print(four_nes)
        [four_nes.remove((ne_node[0], ne_node[1])) for ne_node in mesh.neighbors(node_key) if (ne_node[0], ne_node[1]) in four_nes]
        # remove the above for some reason
        for ne in four_nes:
            for info in info_on_pix[ne]:
                assert mesh.has_node((ne[0], ne[1], info['depth'])), "No node_key"
                ind_node = le_nodes[node_key]
                if abs(node_key[2]) > abs(info['depth']):
                    ind_node['near'] = append_element(ind_node, 'near', (ne[0], ne[1], info['depth']))
                else:
                    ind_node['far'] = append_element(ind_node, 'far', (ne[0], ne[1], info['depth']))
    if depth is not None:
        for key, value in info_on_pix.items():
            if depth[key[0], key[1]] != abs(value[0]['depth']):
                value[0]['disp'] = 1. / value[0]['depth']
                depth[key[0], key[1]] = abs(value[0]['depth'])

        return mesh, depth, info_on_pix
    else:
        return mesh

def group_edges(LDI, config, image, remove_conflict_ordinal, spdb=False):

    '''
    (1) add_new_node(G, node) : add "node" to graph "G"
    (2) add_new_edge(G, node_a, node_b) : add edge "node_a--node_b" to graph "G"
    (3) exceed_thre(x, y, thre) : Check if difference between "x" and "y" exceed threshold "thre"
    (4) key_exist(d, k) : Check if key "k' exists in dictionary "d"
    (5) comm_opp_bg(G, x, y) : Check if node "x" and "y" in graph "G" treat the same opposite node as background
    (6) comm_opp_fg(G, x, y) : Check if node "x" and "y" in graph "G" treat the same opposite node as foreground
    '''
    add_new_node = lambda G, node: None if G.has_node(node) else G.add_node(node)
    add_new_edge = lambda G, node_a, node_b: None if G.has_edge(node_a, node_b) else G.add_edge(node_a, node_b)
    exceed_thre = lambda x, y, thre: (abs(x) - abs(y)) > thre
    key_exist = lambda d, k: d.get(k) is not None
    comm_opp_bg = lambda G, x, y: key_exist(G.nodes[x], 'far') and key_exist(G.nodes[y], 'far') and \
                                    not(set(G.nodes[x]['far']).isdisjoint(set(G.nodes[y]['far'])))
    comm_opp_fg = lambda G, x, y: key_exist(G.nodes[x], 'near') and key_exist(G.nodes[y], 'near') and \
                                    not(set(G.nodes[x]['near']).isdisjoint(set(G.nodes[y]['near'])))
    discont_graph = netx.Graph()
    '''
    (A) Skip the pixel at image boundary, we don't want to deal with them.
    (B) Identify discontinuity by the number of its neighbor(degree).
        If the degree < 4(up/right/buttom/left). We will go through following steps:
        (1) Add the discontinuity pixel "node" to graph "discont_graph".
        (2) Find "node"'s cross neighbor(up/right/buttom/left) "ne_node".
            - If the cross neighbor "ne_node" is a discontinuity pixel(degree("ne_node") < 4),
                (a) add it to graph "discont_graph" and build the connection between "ne_node" and "node".
                (b) label its cross neighbor as invalid pixels "inval_diag_candi" to avoid building
                    connection between original discontinuity pixel "node" and "inval_diag_candi".
            - Otherwise, find "ne_node"'s cross neighbors, called diagonal candidate "diag_candi".
                - The "diag_candi" is diagonal to the original discontinuity pixel "node".
                - If "diag_candi" exists, go to step(3).
        (3) A diagonal candidate "diag_candi" will be :
            - added to the "discont_graph" if its degree < 4.
            - connected to the original discontinuity pixel "node" if it satisfied either
                one of following criterion:
                (a) the difference of disparity between "diag_candi" and "node" is smaller than default threshold.
                (b) the "diag_candi" and "node" face the same opposite pixel. (See. function "tear_edges")
                (c) Both of "diag_candi" and "node" must_connect to each other. (See. function "combine_end_node")
    (C) Aggregate each connected part in "discont_graph" into "discont_ccs" (A.K.A. depth edge).
    '''
    for node in LDI.nodes:
        if not(LDI.graph['bord_up'] + 1 <= node[0] <= LDI.graph['bord_down'] - 2 and \
               LDI.graph['bord_left'] + 1 <= node[1] <= LDI.graph['bord_right'] - 2):
            continue
        neighbors = [*LDI.neighbors(node)]
        if len(neighbors) < 4:
            add_new_node(discont_graph, node)
            diag_candi_anc, inval_diag_candi, discont_nes = set(), set(), set()
            for ne_node in neighbors:
                if len([*LDI.neighbors(ne_node)]) < 4:
                    add_new_node(discont_graph, ne_node)
                    add_new_edge(discont_graph, ne_node, node)
                    discont_nes.add(ne_node)
                else:
                    diag_candi_anc.add(ne_node)
            inval_diag_candi = set([inval_diagonal for ne_node in discont_nes for inval_diagonal in LDI.neighbors(ne_node) if \
                                     abs(inval_diagonal[0] - node[0]) < 2 and abs(inval_diagonal[1] - node[1]) < 2])
            for ne_node in diag_candi_anc:
                if ne_node[0] == node[0]:
                    diagonal_xys = [[ne_node[0] + 1, ne_node[1]], [ne_node[0] - 1, ne_node[1]]]
                elif ne_node[1] == node[1]:
                    diagonal_xys = [[ne_node[0], ne_node[1] + 1], [ne_node[0], ne_node[1] - 1]]
                for diag_candi in LDI.neighbors(ne_node):
                    if [diag_candi[0], diag_candi[1]] in diagonal_xys and LDI.degree(diag_candi) < 4:
                        if diag_candi not in inval_diag_candi:
                            if not exceed_thre(1./node[2], 1./diag_candi[2], config['depth_threshold']) or \
                               (comm_opp_bg(LDI, diag_candi, node) and comm_opp_fg(LDI, diag_candi, node)):
                                add_new_node(discont_graph, diag_candi)
                                add_new_edge(discont_graph, diag_candi, node)
                        if key_exist(LDI.nodes[diag_candi], 'must_connect') and node in LDI.nodes[diag_candi]['must_connect'] and \
                            key_exist(LDI.nodes[node], 'must_connect') and diag_candi in LDI.nodes[node]['must_connect']:
                            add_new_node(discont_graph, diag_candi)
                            add_new_edge(discont_graph, diag_candi, node)
    if spdb == True:
        import pdb; pdb.set_trace()
    discont_ccs = [*netx.connected_components(discont_graph)]
    '''
    In some corner case, a depth edge "discont_cc" will contain both
    foreground(FG) and background(BG) pixels. This violate the assumption that
    a depth edge can only composite by one type of pixel(FG or BG).
    We need to further divide this depth edge into several sub-part so that the
    assumption is satisfied.
    (A) A depth edge is invalid if both of its "far_flag"(BG) and
        "near_flag"(FG) are True.
    (B) If the depth edge is invalid, we need to do:
        (1) Find the role("oridinal") of each pixel on the depth edge.
            "-1" --> Its opposite pixels has smaller depth(near) than it.
                     It is a backgorund pixel.
            "+1" --> Its opposite pixels has larger depth(far) than it.
                     It is a foregorund pixel.
            "0"  --> Some of opposite pixels has larger depth(far) than it,
                     and some has smaller pixel than it.
                     It is an ambiguous pixel.
        (2) For each pixel "discont_node", check if its neigbhors' roles are consistent.
            - If not, break the connection between the neighbor "ne_node" that has a role
              different from "discont_node".
            - If yes, remove all the role that are inconsistent to its neighbors "ne_node".
        (3) Connected component analysis to re-identified those divided depth edge.
    (C) Aggregate each connected part in "discont_graph" into "discont_ccs" (A.K.A. depth edge).
    '''
    if remove_conflict_ordinal:
        new_discont_ccs = []
        num_new_cc = 0
        for edge_id, discont_cc in enumerate(discont_ccs):
            near_flag = False
            far_flag = False
            for discont_node in discont_cc:
                near_flag = True if key_exist(LDI.nodes[discont_node], 'far') else near_flag
                far_flag = True if key_exist(LDI.nodes[discont_node], 'near') else far_flag
                if far_flag and near_flag:
                    break
            if far_flag and near_flag:
                for discont_node in discont_cc:
                    discont_graph.nodes[discont_node]['ordinal'] = \
                        np.array([key_exist(LDI.nodes[discont_node], 'far'),
                                  key_exist(LDI.nodes[discont_node], 'near')]) * \
                        np.array([-1, 1])
                    discont_graph.nodes[discont_node]['ordinal'] = \
                        np.sum(discont_graph.nodes[discont_node]['ordinal'])
                remove_nodes, remove_edges = [], []
                for discont_node in discont_cc:
                    ordinal_relation = np.sum([discont_graph.nodes[xx]['ordinal'] \
                                               for xx in discont_graph.neighbors(discont_node)])
                    near_side = discont_graph.nodes[discont_node]['ordinal'] <= 0
                    if abs(ordinal_relation) < len([*discont_graph.neighbors(discont_node)]):
                        remove_nodes.append(discont_node)
                        for ne_node in discont_graph.neighbors(discont_node):
                            remove_flag = (near_side and not(key_exist(LDI.nodes[ne_node], 'far'))) or \
                                          (not near_side and not(key_exist(LDI.nodes[ne_node], 'near')))
                            remove_edges += [(discont_node, ne_node)] if remove_flag else []
                    else:
                        if near_side and key_exist(LDI.nodes[discont_node], 'near'):
                            LDI.nodes[discont_node].pop('near')
                        elif not(near_side) and key_exist(LDI.nodes[discont_node], 'far'):
                            LDI.nodes[discont_node].pop('far')
                discont_graph.remove_edges_from(remove_edges)
                sub_mesh = discont_graph.subgraph(list(discont_cc)).copy()
                sub_discont_ccs = [*netx.connected_components(sub_mesh)]
                is_redun_near = lambda xx: len(xx) == 1 and xx[0] in remove_nodes and key_exist(LDI.nodes[xx[0]], 'far')
                for sub_discont_cc in sub_discont_ccs:
                    if is_redun_near(list(sub_discont_cc)):
                        LDI.nodes[list(sub_discont_cc)[0]].pop('far')
                    new_discont_ccs.append(sub_discont_cc)
            else:
                new_discont_ccs.append(discont_cc)
        discont_ccs = new_discont_ccs
        new_discont_ccs = None
    if spdb == True:
        import pdb; pdb.set_trace()

    for edge_id, edge_cc in enumerate(discont_ccs):
        for node in edge_cc:
            LDI.nodes[node]['edge_id'] = edge_id

    return discont_ccs, LDI, discont_graph

def combine_end_node(mesh, edge_mesh, edge_ccs, depth):
    import collections
    mesh_nodes = mesh.nodes
    connect_dict = dict()
    for valid_edge_id, valid_edge_cc in enumerate(edge_ccs):
        connect_info = []
        for valid_edge_node in valid_edge_cc:
            single_connect = set()
            for ne_node in mesh.neighbors(valid_edge_node):
                if mesh_nodes[ne_node].get('far') is not None:
                    for fn in mesh_nodes[ne_node].get('far'):
                        if mesh.has_node(fn) and mesh_nodes[fn].get('edge_id') is not None:
                            single_connect.add(mesh_nodes[fn]['edge_id'])
                if mesh_nodes[ne_node].get('near') is not None:
                    for fn in mesh_nodes[ne_node].get('near'):
                        if mesh.has_node(fn) and mesh_nodes[fn].get('edge_id') is not None:
                            single_connect.add(mesh_nodes[fn]['edge_id'])
            connect_info.extend([*single_connect])
        connect_dict[valid_edge_id] = collections.Counter(connect_info)

    end_maps = np.zeros((mesh.graph['H'], mesh.graph['W']))
    edge_maps = np.zeros((mesh.graph['H'], mesh.graph['W'])) - 1
    for valid_edge_id, valid_edge_cc in enumerate(edge_ccs):
        for valid_edge_node in valid_edge_cc:
            edge_maps[valid_edge_node[0], valid_edge_node[1]] = valid_edge_id
            if len([*edge_mesh.neighbors(valid_edge_node)]) == 1:
                num_ne = 1
                if num_ne == 1:
                    end_maps[valid_edge_node[0], valid_edge_node[1]] = valid_edge_node[2]
    nxs, nys = np.where(end_maps != 0)
    invalid_nodes = set()
    for nx, ny in zip(nxs, nys):
        if mesh.has_node((nx, ny, end_maps[nx, ny])) is False:
            invalid_nodes.add((nx, ny))
            continue
        four_nes = [xx for xx in [(nx - 1, ny), (nx + 1, ny), (nx, ny - 1), (nx, ny + 1)] \
                        if 0 <= xx[0] < mesh.graph['H'] and 0 <= xx[1] < mesh.graph['W'] and \
                        end_maps[xx[0], xx[1]] != 0]
        mesh_nes = [*mesh.neighbors((nx, ny, end_maps[nx, ny]))]
        remove_num = 0
        for fne in four_nes:
            if (fne[0], fne[1], end_maps[fne[0], fne[1]]) in mesh_nes:
                remove_num += 1
        if remove_num == len(four_nes):
            invalid_nodes.add((nx, ny))
    for invalid_node in invalid_nodes:
        end_maps[invalid_node[0], invalid_node[1]] = 0

    nxs, nys = np.where(end_maps != 0)
    invalid_nodes = set()
    for nx, ny in zip(nxs, nys):
        if mesh_nodes[(nx, ny, end_maps[nx, ny])].get('edge_id') is None:
            continue
        else:
            self_id = mesh_nodes[(nx, ny, end_maps[nx, ny])].get('edge_id')
            self_connect = connect_dict[self_id] if connect_dict.get(self_id) is not None else dict()
        four_nes = [xx for xx in [(nx - 1, ny), (nx + 1, ny), (nx, ny - 1), (nx, ny + 1)] \
                        if 0 <= xx[0] < mesh.graph['H'] and 0 <= xx[1] < mesh.graph['W'] and \
                        end_maps[xx[0], xx[1]] != 0]
        for fne in four_nes:
            if mesh_nodes[(fne[0], fne[1], end_maps[fne[0], fne[1]])].get('edge_id') is None:
                continue
            else:
                ne_id = mesh_nodes[(fne[0], fne[1], end_maps[fne[0], fne[1]])]['edge_id']
                if self_connect.get(ne_id) is None or self_connect.get(ne_id) == 1:
                    continue
                else:
                    invalid_nodes.add((nx, ny))
    for invalid_node in invalid_nodes:
        end_maps[invalid_node[0], invalid_node[1]] = 0
    nxs, nys = np.where(end_maps != 0)
    invalid_nodes = set()
    for nx, ny in zip(nxs, nys):
        four_nes = [xx for xx in [(nx - 1, ny), (nx + 1, ny), (nx, ny - 1), (nx, ny + 1)] \
                        if 0 <= xx[0] < mesh.graph['H'] and 0 <= xx[1] < mesh.graph['W'] and \
                        end_maps[xx[0], xx[1]] != 0]
        for fne in four_nes:
            if mesh.has_node((fne[0], fne[1], end_maps[fne[0], fne[1]])):
                node_a, node_b = (fne[0], fne[1], end_maps[fne[0], fne[1]]), (nx, ny, end_maps[nx, ny])
                mesh.add_edge(node_a, node_b)
                mesh_nodes[node_b]['must_connect'] = set() if mesh_nodes[node_b].get('must_connect') is None else mesh_nodes[node_b]['must_connect']
                mesh_nodes[node_b]['must_connect'].add(node_a)
                mesh_nodes[node_b]['must_connect'] |= set([xx for xx in [*edge_mesh.neighbors(node_a)] if \
                                                            (xx[0] - node_b[0]) < 2 and (xx[1] - node_b[1]) < 2])
                mesh_nodes[node_a]['must_connect'] = set() if mesh_nodes[node_a].get('must_connect') is None else mesh_nodes[node_a]['must_connect']
                mesh_nodes[node_a]['must_connect'].add(node_b)
                mesh_nodes[node_a]['must_connect'] |= set([xx for xx in [*edge_mesh.neighbors(node_b)] if \
                                                            (xx[0] - node_a[0]) < 2 and (xx[1] - node_a[1]) < 2])
                invalid_nodes.add((nx, ny))
    for invalid_node in invalid_nodes:
        end_maps[invalid_node[0], invalid_node[1]] = 0

    return mesh

def remove_redundant_edge(mesh, edge_mesh, edge_ccs, info_on_pix, config, redundant_number=1000, invalid=False, spdb=False):
    point_to_amount = {}
    point_to_id = {}
    end_maps = np.zeros((mesh.graph['H'], mesh.graph['W'])) - 1
    for valid_edge_id, valid_edge_cc in enumerate(edge_ccs):
        for valid_edge_node in valid_edge_cc:
            point_to_amount[valid_edge_node] = len(valid_edge_cc)
            point_to_id[valid_edge_node] = valid_edge_id
            if edge_mesh.has_node(valid_edge_node) is True:
                if len([*edge_mesh.neighbors(valid_edge_node)]) == 1:
                    end_maps[valid_edge_node[0], valid_edge_node[1]] = valid_edge_id
    nxs, nys = np.where(end_maps > -1)
    point_to_adjoint = {}
    for nx, ny in zip(nxs, nys):
        adjoint_edges = set([end_maps[x, y] for x, y in [(nx + 1, ny), (nx - 1, ny), (nx, ny + 1), (nx, ny - 1)] if end_maps[x, y] != -1])
        point_to_adjoint[end_maps[nx, ny]] = (point_to_adjoint[end_maps[nx, ny]] | adjoint_edges) if point_to_adjoint.get(end_maps[nx, ny]) is not None else adjoint_edges
    valid_edge_ccs = filter_edge(mesh, edge_ccs, config, invalid=invalid)
    edge_canvas = np.zeros((mesh.graph['H'], mesh.graph['W'])) - 1
    for valid_edge_id, valid_edge_cc in enumerate(valid_edge_ccs):
        for valid_edge_node in valid_edge_cc:
            edge_canvas[valid_edge_node[0], valid_edge_node[1]] = valid_edge_id
    if spdb is True:
        plt.imshow(edge_canvas); plt.show()
        import pdb; pdb.set_trace()
    for valid_edge_id, valid_edge_cc in enumerate(valid_edge_ccs):
        end_number = 0
        four_end_number = 0
        eight_end_number = 0
        db_eight_end_number = 0
        if len(valid_edge_cc) > redundant_number:
            continue
        for valid_edge_node in valid_edge_cc:
            if len([*edge_mesh.neighbors(valid_edge_node)]) == 3:
                break
            elif len([*edge_mesh.neighbors(valid_edge_node)]) == 1:
                hx, hy, hz = valid_edge_node
                if invalid is False:
                    eight_nes = [(x, y) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1),
                                                     (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)] \
                                            if info_on_pix.get((x, y)) is not None and edge_canvas[x, y] != -1 and edge_canvas[x, y] != valid_edge_id]
                    if len(eight_nes) == 0:
                        end_number += 1
                if invalid is True:
                    four_nes = []; eight_nes = []; db_eight_nes = []
                    four_nes = [(x, y) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] \
                                            if info_on_pix.get((x, y)) is not None and edge_canvas[x, y] != -1 and edge_canvas[x, y] != valid_edge_id]
                    eight_nes = [(x, y) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1), \
                                                    (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)] \
                                            if info_on_pix.get((x, y)) is not None and edge_canvas[x, y] != -1 and edge_canvas[x, y] != valid_edge_id]
                    db_eight_nes = [(x, y) for x in range(hx - 2, hx + 3) for y in range(hy - 2, hy + 3) \
                                    if info_on_pix.get((x, y)) is not None and edge_canvas[x, y] != -1 and edge_canvas[x, y] != valid_edge_id and (x, y) != (hx, hy)]
                    if len(four_nes) == 0 or len(eight_nes) == 0:
                        end_number += 1
                        if len(four_nes) == 0:
                            four_end_number += 1
                        if len(eight_nes) == 0:
                            eight_end_number += 1
                        if len(db_eight_nes) == 0:
                            db_eight_end_number += 1
            elif len([*edge_mesh.neighbors(valid_edge_node)]) == 0:
                hx, hy, hz = valid_edge_node
                four_nes = [(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] \
                                if info_on_pix.get((x, y)) is not None and \
                                    mesh.has_edge(valid_edge_node, (x, y, info_on_pix[(x, y)][0]['depth'])) is False]
                for ne in four_nes:
                    try:
                        if invalid is True or (point_to_amount.get(ne) is None or point_to_amount[ne] < redundant_number) or \
                            point_to_id[ne] in point_to_adjoint.get(point_to_id[valid_edge_node], set()):
                            mesh.add_edge(valid_edge_node, ne)
                    except:
                        import pdb; pdb.set_trace()
        if (invalid is not True and end_number >= 1) or (invalid is True and end_number >= 2 and eight_end_number >= 1 and db_eight_end_number >= 1):
            for valid_edge_node in valid_edge_cc:
                hx, hy, _ = valid_edge_node
                four_nes = [(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] \
                                if info_on_pix.get((x, y)) is not None and \
                                    mesh.has_edge(valid_edge_node, (x, y, info_on_pix[(x, y)][0]['depth'])) is False and \
                                    (edge_canvas[x, y] == -1 or edge_canvas[x, y] == valid_edge_id)]
                for ne in four_nes:
                    if invalid is True or (point_to_amount.get(ne) is None or point_to_amount[ne] < redundant_number) or \
                        point_to_id[ne] in point_to_adjoint.get(point_to_id[valid_edge_node], set()):
                        mesh.add_edge(valid_edge_node, ne)

    return mesh

def judge_dangle(mark, mesh, node):
    if not (1 <= node[0] < mesh.graph['H']-1) or not(1 <= node[1] < mesh.graph['W']-1):
        return mark
    mesh_neighbors = [*mesh.neighbors(node)]
    mesh_neighbors = [xx for xx in mesh_neighbors if 0 < xx[0] < mesh.graph['H'] - 1 and 0 < xx[1] < mesh.graph['W'] - 1]
    if len(mesh_neighbors) >= 3:
        return mark
    elif len(mesh_neighbors) <= 1:
        mark[node[0], node[1]] = (len(mesh_neighbors) + 1)
    else:
        dan_ne_node_a = mesh_neighbors[0]
        dan_ne_node_b = mesh_neighbors[1]
        if abs(dan_ne_node_a[0] - dan_ne_node_b[0]) > 1 or \
            abs(dan_ne_node_a[1] - dan_ne_node_b[1]) > 1:
            mark[node[0], node[1]] = 3

    return mark

def remove_dangling(mesh, edge_ccs, edge_mesh, info_on_pix, image, depth, config):

    tmp_edge_ccs = copy.deepcopy(edge_ccs)
    for edge_cc_id, valid_edge_cc in enumerate(tmp_edge_ccs):
        if len(valid_edge_cc) > 1 or len(valid_edge_cc) == 0:
            continue
        single_edge_node = [*valid_edge_cc][0]
        hx, hy, hz = single_edge_node
        eight_nes = set([(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1),
                         (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)] \
                         if info_on_pix.get((x, y)) is not None])
        four_nes = [(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] \
                    if info_on_pix.get((x, y)) is not None]
        sub_mesh = mesh.subgraph(eight_nes).copy()
        ccs = netx.connected_components(sub_mesh)
        four_ccs = []
        for cc_id, _cc in enumerate(ccs):
            four_ccs.append(set())
            for cc_node in _cc:
                if abs(cc_node[0] - hx) + abs(cc_node[1] - hy) < 2:
                    four_ccs[cc_id].add(cc_node)
        largest_cc = sorted(four_ccs, key=lambda x: (len(x), -np.sum([abs(xx[2] - hz) for xx in x])))[-1]
        if len(largest_cc) < 2:
            for ne in four_nes:
                mesh.add_edge(single_edge_node, ne)
        else:
            mesh.remove_edges_from([(single_edge_node, ne) for ne in mesh.neighbors(single_edge_node)])
            new_depth = np.mean([xx[2] for xx in largest_cc])
            info_on_pix[(hx, hy)][0]['depth'] = new_depth
            info_on_pix[(hx, hy)][0]['disp'] = 1./new_depth
            new_node = (hx, hy, new_depth)
            mesh = refresh_node(single_edge_node, mesh._node[single_edge_node], new_node, dict(), mesh)
            edge_ccs[edge_cc_id] = set([new_node])
            for ne in largest_cc:
                mesh.add_edge(new_node, ne)

    mark = np.zeros((mesh.graph['H'], mesh.graph['W']))
    for edge_idx, edge_cc in enumerate(edge_ccs):
        for edge_node in edge_cc:
            if not (mesh.graph['bord_up'] <= edge_node[0] < mesh.graph['bord_down']-1) or \
               not (mesh.graph['bord_left'] <= edge_node[1] < mesh.graph['bord_right']-1):
                continue
            mesh_neighbors = [*mesh.neighbors(edge_node)]
            mesh_neighbors = [xx for xx in mesh_neighbors \
                                if mesh.graph['bord_up'] < xx[0] < mesh.graph['bord_down'] - 1 and \
                                   mesh.graph['bord_left'] < xx[1] < mesh.graph['bord_right'] - 1]
            if len([*mesh.neighbors(edge_node)]) >= 3:
                continue
            elif len([*mesh.neighbors(edge_node)]) <= 1:
                mark[edge_node[0], edge_node[1]] += (len([*mesh.neighbors(edge_node)]) + 1)
            else:
                dan_ne_node_a = [*mesh.neighbors(edge_node)][0]
                dan_ne_node_b = [*mesh.neighbors(edge_node)][1]
                if abs(dan_ne_node_a[0] - dan_ne_node_b[0]) > 1 or \
                    abs(dan_ne_node_a[1] - dan_ne_node_b[1]) > 1:
                    mark[edge_node[0], edge_node[1]] += 3
    mxs, mys = np.where(mark == 1)
    conn_0_nodes = [(x[0], x[1], info_on_pix[(x[0], x[1])][0]['depth']) for x in zip(mxs, mys) \
                        if mesh.has_node((x[0], x[1], info_on_pix[(x[0], x[1])][0]['depth']))]
    mxs, mys = np.where(mark == 2)
    conn_1_nodes = [(x[0], x[1], info_on_pix[(x[0], x[1])][0]['depth']) for x in zip(mxs, mys) \
                        if mesh.has_node((x[0], x[1], info_on_pix[(x[0], x[1])][0]['depth']))]
    for node in conn_0_nodes:
        hx, hy = node[0], node[1]
        four_nes = [(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] \
                     if info_on_pix.get((x, y)) is not None]
        re_depth = {'value' : 0, 'count': 0}
        for ne in four_nes:
            mesh.add_edge(node, ne)
            re_depth['value'] += cc_node[2]
            re_depth['count'] += 1.
        re_depth = re_depth['value'] / re_depth['count']
        mapping_dict = {node: (node[0], node[1], re_depth)}
        info_on_pix, mesh, edge_mesh = update_info(mapping_dict, info_on_pix, mesh, edge_mesh)
        depth[node[0], node[1]] = abs(re_depth)
        mark[node[0], node[1]] = 0
    for node in conn_1_nodes:
        hx, hy = node[0], node[1]
        eight_nes = set([(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1),
                                                                           (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)] \
                        if info_on_pix.get((x, y)) is not None])
        self_nes = set([ne2 for ne1 in mesh.neighbors(node) for ne2 in mesh.neighbors(ne1) if ne2 in eight_nes])
        eight_nes = [*(eight_nes - self_nes)]
        sub_mesh = mesh.subgraph(eight_nes).copy()
        ccs = netx.connected_components(sub_mesh)
        largest_cc = sorted(ccs, key=lambda x: (len(x), -np.sum([abs(xx[0] - node[0]) + abs(xx[1] - node[1]) for xx in x])))[-1]

        mesh.remove_edges_from([(xx, node) for xx in mesh.neighbors(node)])
        re_depth = {'value' : 0, 'count': 0}
        for cc_node in largest_cc:
            if cc_node[0] == node[0] and cc_node[1] == node[1]:
                continue
            re_depth['value'] += cc_node[2]
            re_depth['count'] += 1.
            if abs(cc_node[0] - node[0]) + abs(cc_node[1] - node[1]) < 2:
                mesh.add_edge(cc_node, node)
        try:
            re_depth = re_depth['value'] / re_depth['count']
        except:
            re_depth = node[2]
        renode = (node[0], node[1], re_depth)
        mapping_dict = {node: renode}
        info_on_pix, mesh, edge_mesh = update_info(mapping_dict, info_on_pix, mesh, edge_mesh)
        depth[node[0], node[1]] = abs(re_depth)
        mark[node[0], node[1]] = 0
        edge_mesh, mesh, mark, info_on_pix = recursive_add_edge(edge_mesh, mesh, info_on_pix, renode, mark)
    mxs, mys = np.where(mark == 3)
    conn_2_nodes = [(x[0], x[1], info_on_pix[(x[0], x[1])][0]['depth']) for x in zip(mxs, mys) \
                        if mesh.has_node((x[0], x[1], info_on_pix[(x[0], x[1])][0]['depth'])) and \
                            mesh.degree((x[0], x[1], info_on_pix[(x[0], x[1])][0]['depth'])) == 2]
    sub_mesh = mesh.subgraph(conn_2_nodes).copy()
    ccs = netx.connected_components(sub_mesh)
    for cc in ccs:
        candidate_nodes = [xx for xx in cc if sub_mesh.degree(xx) == 1]
        for node in candidate_nodes:
            if mesh.has_node(node) is False:
                continue
            ne_node = [xx for xx in mesh.neighbors(node) if xx not in cc][0]
            hx, hy = node[0], node[1]
            eight_nes = set([(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1),
                                                                            (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)] \
                              if info_on_pix.get((x, y)) is not None and (x, y, info_on_pix[(x, y)][0]['depth']) not in cc])
            ne_sub_mesh = mesh.subgraph(eight_nes).copy()
            ne_ccs = netx.connected_components(ne_sub_mesh)
            try:
                ne_cc = [ne_cc for ne_cc in ne_ccs if ne_node in ne_cc][0]
            except:
                import pdb; pdb.set_trace()
            largest_cc = [xx for xx in ne_cc if abs(xx[0] - node[0]) + abs(xx[1] - node[1]) == 1]
            mesh.remove_edges_from([(xx, node) for xx in mesh.neighbors(node)])
            re_depth = {'value' : 0, 'count': 0}
            for cc_node in largest_cc:
                re_depth['value'] += cc_node[2]
                re_depth['count'] += 1.
                mesh.add_edge(cc_node, node)
            try:
                re_depth = re_depth['value'] / re_depth['count']
            except:
                re_depth = node[2]
            renode = (node[0], node[1], re_depth)
            mapping_dict = {node: renode}
            info_on_pix, mesh, edge_mesh = update_info(mapping_dict, info_on_pix, mesh, edge_mesh)
            depth[node[0], node[1]] = abs(re_depth)
            mark[node[0], node[1]] = 0
            edge_mesh, mesh, mark, info_on_pix = recursive_add_edge(edge_mesh, mesh, info_on_pix, renode, mark)
            break
        if len(cc) == 1:
            node = [node for node in cc][0]
            hx, hy = node[0], node[1]
            nine_nes = set([(x, y, info_on_pix[(x, y)][0]['depth']) for x, y in [(hx, hy), (hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1),
                                                                                  (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)] \
                                if info_on_pix.get((x, y)) is not None and mesh.has_node((x, y, info_on_pix[(x, y)][0]['depth']))])
            ne_sub_mesh = mesh.subgraph(nine_nes).copy()
            ne_ccs = netx.connected_components(ne_sub_mesh)
            for ne_cc in ne_ccs:
                if node in ne_cc:
                    re_depth = {'value' : 0, 'count': 0}
                    for ne in ne_cc:
                        if abs(ne[0] - node[0]) + abs(ne[1] - node[1]) == 1:
                            mesh.add_edge(node, ne)
                            re_depth['value'] += ne[2]
                            re_depth['count'] += 1.
                    re_depth = re_depth['value'] / re_depth['count']
                    mapping_dict = {node: (node[0], node[1], re_depth)}
                    info_on_pix, mesh, edge_mesh = update_info(mapping_dict, info_on_pix, mesh, edge_mesh)
                    depth[node[0], node[1]] = abs(re_depth)
                    mark[node[0], node[1]] = 0


    return mesh, info_on_pix, edge_mesh, depth, mark

def context_and_holes(mesh, edge_ccs, config, specific_edge_id, specific_edge_loc, depth_feat_model,
                      connect_points_ccs=None, inpaint_iter=0, filter_edge=False, vis_edge_id=None):
    edge_maps = np.zeros((mesh.graph['H'], mesh.graph['W'])) - 1
    mask_info = {}
    for edge_id, edge_cc in enumerate(edge_ccs):
        for edge_node in edge_cc:
            edge_maps[edge_node[0], edge_node[1]] = edge_id

    context_ccs = [set() for x in range(len(edge_ccs))]
    extend_context_ccs = [set() for x in range(len(edge_ccs))]
    extend_erode_context_ccs = [set() for x in range(len(edge_ccs))]
    extend_edge_ccs = [set() for x in range(len(edge_ccs))]
    accomp_extend_context_ccs = [set() for x in range(len(edge_ccs))]
    erode_context_ccs = [set() for x in range(len(edge_ccs))]
    broken_mask_ccs = [set() for x in range(len(edge_ccs))]
    invalid_extend_edge_ccs = [set() for x in range(len(edge_ccs))]
    intouched_ccs = [set() for x in range(len(edge_ccs))]
    redundant_ccs = [set() for x in range(len(edge_ccs))]
    if inpaint_iter == 0:
        background_thickness = config['background_thickness']
        context_thickness = config['context_thickness']
    else:
        background_thickness = config['background_thickness_2']
        context_thickness = config['context_thickness_2']

    mesh_nodes = mesh.nodes
    for edge_id, edge_cc in enumerate(edge_ccs):
        if context_thickness == 0 or (len(specific_edge_id) > 0 and edge_id not in specific_edge_id):
            continue
        edge_group = {}
        for edge_node in edge_cc:
            far_nodes = mesh_nodes[edge_node].get('far')
            if far_nodes is None:
                continue
            for far_node in far_nodes:
                if far_node in edge_cc:
                    continue
                context_ccs[edge_id].add(far_node)
                if mesh_nodes[far_node].get('edge_id') is not None:
                    if edge_group.get(mesh_nodes[far_node]['edge_id']) is None:
                        edge_group[mesh_nodes[far_node]['edge_id']] = set()
                    edge_group[mesh_nodes[far_node]['edge_id']].add(far_node)
        if len(edge_cc) > 2:
            for edge_key in [*edge_group.keys()]:
                if len(edge_group[edge_key]) == 1:
                    context_ccs[edge_id].remove([*edge_group[edge_key]][0])
    for edge_id, edge_cc in enumerate(edge_ccs):
        if inpaint_iter != 0:
            continue
        tmp_intouched_nodes = set()
        for edge_node in edge_cc:
            raw_intouched_nodes = set(mesh_nodes[edge_node].get('near')) if mesh_nodes[edge_node].get('near') is not None else set()
            tmp_intouched_nodes |= set([xx for xx in raw_intouched_nodes if mesh_nodes[xx].get('edge_id') is not None and \
                                                                         len(context_ccs[mesh_nodes[xx].get('edge_id')]) > 0])
        intouched_ccs[edge_id] |= tmp_intouched_nodes
        tmp_intouched_nodes = None
    mask_ccs = copy.deepcopy(edge_ccs)
    forbidden_len = 3
    forbidden_map = np.ones((mesh.graph['H'] - forbidden_len, mesh.graph['W'] - forbidden_len))
    forbidden_map = np.pad(forbidden_map, ((forbidden_len, forbidden_len), (forbidden_len, forbidden_len)), mode='constant').astype(bool)
    cur_tmp_mask_map = np.zeros_like(forbidden_map).astype(bool)
    passive_background = 10 if 10 is not None else background_thickness
    passive_context = 1 if 1 is not None else context_thickness

    for edge_id, edge_cc in enumerate(edge_ccs):
        cur_mask_cc = None; cur_mask_cc = []
        cur_context_cc = None; cur_context_cc = []
        cur_accomp_near_cc = None; cur_accomp_near_cc = []
        cur_invalid_extend_edge_cc = None; cur_invalid_extend_edge_cc = []
        cur_comp_far_cc = None; cur_comp_far_cc = []
        tmp_erode = []
        if len(context_ccs[edge_id]) == 0 or (len(specific_edge_id) > 0 and edge_id not in specific_edge_id):
            continue
        for i in range(max(background_thickness, context_thickness)):
            cur_tmp_mask_map.fill(False)
            if i == 0:
                tmp_mask_nodes = copy.deepcopy(mask_ccs[edge_id])
                tmp_intersect_nodes = []
                tmp_intersect_context_nodes = []
                mask_map = np.zeros((mesh.graph['H'], mesh.graph['W']), dtype=bool)
                context_depth = np.zeros((mesh.graph['H'], mesh.graph['W']))
                comp_cnt_depth = np.zeros((mesh.graph['H'], mesh.graph['W']))
                connect_map = np.zeros((mesh.graph['H'], mesh.graph['W']))
                for node in tmp_mask_nodes:
                    mask_map[node[0], node[1]] = True
                    depth_count = 0
                    if mesh_nodes[node].get('far') is not None:
                        for comp_cnt_node in mesh_nodes[node]['far']:
                            comp_cnt_depth[node[0], node[1]] += abs(comp_cnt_node[2])
                            depth_count += 1
                    if depth_count > 0:
                        comp_cnt_depth[node[0], node[1]] = comp_cnt_depth[node[0], node[1]] / depth_count
                    connect_node = []
                    if mesh_nodes[node].get('connect_point_id') is not None:
                        connect_node.append(mesh_nodes[node]['connect_point_id'])
                    connect_point_id = np.bincount(connect_node).argmax() if len(connect_node) > 0 else -1
                    if connect_point_id > -1 and connect_points_ccs is not None:
                        for xx in connect_points_ccs[connect_point_id]:
                            if connect_map[xx[0], xx[1]] == 0:
                                connect_map[xx[0], xx[1]] = xx[2]
                    if mesh_nodes[node].get('connect_point_exception') is not None:
                        for xx in mesh_nodes[node]['connect_point_exception']:
                            if connect_map[xx[0], xx[1]] == 0:
                                connect_map[xx[0], xx[1]] = xx[2]
                tmp_context_nodes = [*context_ccs[edge_id]]
                tmp_erode.append([*context_ccs[edge_id]])
                context_map = np.zeros((mesh.graph['H'], mesh.graph['W']), dtype=bool)
                if (context_map.astype(np.uint8) * mask_map.astype(np.uint8)).max() > 0:
                    import pdb; pdb.set_trace()
                for node in tmp_context_nodes:
                    context_map[node[0], node[1]] = True
                    context_depth[node[0], node[1]] = node[2]
                context_map[mask_map == True] = False
                if (context_map.astype(np.uint8) * mask_map.astype(np.uint8)).max() > 0:
                    import pdb; pdb.set_trace()
                tmp_intouched_nodes = [*intouched_ccs[edge_id]]
                intouched_map = np.zeros((mesh.graph['H'], mesh.graph['W']), dtype=bool)
                for node in tmp_intouched_nodes: intouched_map[node[0], node[1]] = True
                intouched_map[mask_map == True] = False
                tmp_redundant_nodes = set()
                tmp_noncont_nodes = set()
                noncont_map = np.zeros((mesh.graph['H'], mesh.graph['W']), dtype=bool)
                intersect_map = np.zeros((mesh.graph['H'], mesh.graph['W']), dtype=bool)
                intersect_context_map = np.zeros((mesh.graph['H'], mesh.graph['W']), dtype=bool)
            if i > passive_background and inpaint_iter == 0:
                new_tmp_intersect_nodes = None
                new_tmp_intersect_nodes = []
                for node in tmp_intersect_nodes:
                    nes = mesh.neighbors(node)
                    for ne in nes:
                        if bool(context_map[ne[0], ne[1]]) is False and \
                        bool(mask_map[ne[0], ne[1]]) is False and \
                        bool(forbidden_map[ne[0], ne[1]]) is True and \
                        bool(intouched_map[ne[0], ne[1]]) is False and\
                        bool(intersect_map[ne[0], ne[1]]) is False and\
                        bool(intersect_context_map[ne[0], ne[1]]) is False:
                            break_flag = False
                            if (i - passive_background) % 2 == 0 and (i - passive_background) % 8 != 0:
                                four_nes = [xx for xx in[[ne[0] - 1, ne[1]], [ne[0] + 1, ne[1]], [ne[0], ne[1] - 1], [ne[0], ne[1] + 1]] \
                                                if 0 <= xx[0] < mesh.graph['H'] and 0 <= xx[1] < mesh.graph['W']]
                                for fne in four_nes:
                                    if bool(mask_map[fne[0], fne[1]]) is True:
                                        break_flag = True
                                        break
                                if break_flag is True:
                                    continue
                            intersect_map[ne[0], ne[1]] = True
                            new_tmp_intersect_nodes.append(ne)
                tmp_intersect_nodes = None
                tmp_intersect_nodes = new_tmp_intersect_nodes

            if i > passive_context and inpaint_iter == 1:
                new_tmp_intersect_context_nodes = None
                new_tmp_intersect_context_nodes = []
                for node in tmp_intersect_context_nodes:
                    nes = mesh.neighbors(node)
                    for ne in nes:
                        if bool(context_map[ne[0], ne[1]]) is False and \
                        bool(mask_map[ne[0], ne[1]]) is False and \
                        bool(forbidden_map[ne[0], ne[1]]) is True and \
                        bool(intouched_map[ne[0], ne[1]]) is False and\
                        bool(intersect_map[ne[0], ne[1]]) is False and \
                        bool(intersect_context_map[ne[0], ne[1]]) is False:
                            intersect_context_map[ne[0], ne[1]] = True
                            new_tmp_intersect_context_nodes.append(ne)
                tmp_intersect_context_nodes = None
                tmp_intersect_context_nodes = new_tmp_intersect_context_nodes

            new_tmp_mask_nodes = None
            new_tmp_mask_nodes = []
            for node in tmp_mask_nodes:
                four_nes = {xx:[] for xx in [(node[0] - 1, node[1]), (node[0] + 1, node[1]), (node[0], node[1] - 1), (node[0], node[1] + 1)] if \
                            0 <= xx[0] < connect_map.shape[0] and 0 <= xx[1] < connect_map.shape[1]}
                if inpaint_iter > 0:
                    for ne in four_nes.keys():
                        if connect_map[ne[0], ne[1]] == True:
                            tmp_context_nodes.append((ne[0], ne[1], connect_map[ne[0], ne[1]]))
                            context_map[ne[0], ne[1]] = True
                nes = mesh.neighbors(node)
                if inpaint_iter > 0:
                    for ne in nes: four_nes[(ne[0], ne[1])].append(ne[2])
                    nes = []
                    for kfne, vfnes in four_nes.items(): vfnes.sort(key = lambda xx: abs(xx), reverse=True)
                    for kfne, vfnes in four_nes.items():
                        for vfne in vfnes: nes.append((kfne[0], kfne[1], vfne))
                for ne in nes:
                    if bool(context_map[ne[0], ne[1]]) is False and \
                       bool(mask_map[ne[0], ne[1]]) is False and \
                       bool(forbidden_map[ne[0], ne[1]]) is True and \
                       bool(intouched_map[ne[0], ne[1]]) is False and \
                       bool(intersect_map[ne[0], ne[1]]) is False and \
                       bool(intersect_context_map[ne[0], ne[1]]) is False:
                        if i == passive_background and inpaint_iter == 0:
                            if np.any(context_map[max(ne[0] - 1, 0):min(ne[0] + 2, mesh.graph['H']), max(ne[1] - 1, 0):min(ne[1] + 2, mesh.graph['W'])]) == True:
                                intersect_map[ne[0], ne[1]] = True
                                tmp_intersect_nodes.append(ne)
                                continue
                        if i < background_thickness:
                            if inpaint_iter == 0:
                                cur_mask_cc.append(ne)
                            elif mesh_nodes[ne].get('inpaint_id') == 1:
                                cur_mask_cc.append(ne)
                            else:
                                continue
                            mask_ccs[edge_id].add(ne)
                            if inpaint_iter == 0:
                                if comp_cnt_depth[node[0], node[1]] > 0 and comp_cnt_depth[ne[0], ne[1]] == 0:
                                    comp_cnt_depth[ne[0], ne[1]] = comp_cnt_depth[node[0], node[1]]
                                if mesh_nodes[ne].get('far') is not None:
                                    for comp_far_node in mesh_nodes[ne]['far']:
                                        cur_comp_far_cc.append(comp_far_node)
                                        cur_accomp_near_cc.append(ne)
                                        cur_invalid_extend_edge_cc.append(comp_far_node)
                                if mesh_nodes[ne].get('edge_id') is not None and \
                                    len(context_ccs[mesh_nodes[ne].get('edge_id')]) > 0:
                                    intouched_fars = set(mesh_nodes[ne].get('far')) if mesh_nodes[ne].get('far') is not None else set()
                                    accum_intouched_fars = set(intouched_fars)
                                    for intouched_far in intouched_fars:
                                        accum_intouched_fars |= set([*mesh.neighbors(intouched_far)])
                                    for intouched_far in accum_intouched_fars:
                                        if bool(mask_map[intouched_far[0], intouched_far[1]]) is True or \
                                        bool(context_map[intouched_far[0], intouched_far[1]]) is True:
                                            continue
                                        tmp_redundant_nodes.add(intouched_far)
                                        intouched_map[intouched_far[0], intouched_far[1]] = True
                                if mesh_nodes[ne].get('near') is not None:
                                    intouched_nears = set(mesh_nodes[ne].get('near'))
                                    for intouched_near in intouched_nears:
                                        if bool(mask_map[intouched_near[0], intouched_near[1]]) is True or \
                                        bool(context_map[intouched_near[0], intouched_near[1]]) is True:
                                            continue
                                        tmp_redundant_nodes.add(intouched_near)
                                        intouched_map[intouched_near[0], intouched_near[1]] = True
                        if not (mesh_nodes[ne].get('inpaint_id') != 1 and inpaint_iter == 1):
                            new_tmp_mask_nodes.append(ne)
                            mask_map[ne[0], ne[1]] = True
            tmp_mask_nodes = new_tmp_mask_nodes

            new_tmp_context_nodes = None
            new_tmp_context_nodes = []
            for node in tmp_context_nodes:
                nes = mesh.neighbors(node)
                if inpaint_iter > 0:
                    four_nes = {(node[0] - 1, node[1]):[], ((node[0] + 1) % mesh.graph['H'], node[1]):[], (node[0], node[1] - 1):[], (node[0], (node[1] + 1) % mesh.graph['W']):[]}
                    for ne in nes: four_nes[(ne[0], ne[1])].append(ne[2])
                    nes = []
                    for kfne, vfnes in four_nes.items(): vfnes.sort(key = lambda xx: abs(xx), reverse=True)
                    for kfne, vfnes in four_nes.items():
                        for vfne in vfnes: nes.append((kfne[0], kfne[1], vfne))
                for ne in nes:
                    mask_flag = (bool(mask_map[ne[0], ne[1]]) is False)
                    if bool(context_map[ne[0], ne[1]]) is False and mask_flag and \
                       bool(forbidden_map[ne[0], ne[1]]) is True and bool(noncont_map[ne[0], ne[1]]) is False and \
                       bool(intersect_context_map[ne[0], ne[1]]) is False:
                        if i == passive_context and inpaint_iter == 1:
                            mnes = mesh.neighbors(ne)
                            if any([mask_map[mne[0], mne[1]] == True for mne in mnes]) is True:
                                intersect_context_map[ne[0], ne[1]] = True
                                tmp_intersect_context_nodes.append(ne)
                                continue
                        if False and mesh_nodes[ne].get('near') is not None and mesh_nodes[ne].get('edge_id') != edge_id:
                            noncont_nears = set(mesh_nodes[ne].get('near'))
                            for noncont_near in noncont_nears:
                                if bool(context_map[noncont_near[0], noncont_near[1]]) is False:
                                    tmp_noncont_nodes.add(noncont_near)
                                    noncont_map[noncont_near[0], noncont_near[1]] = True
                        new_tmp_context_nodes.append(ne)
                        context_map[ne[0], ne[1]] = True
                        context_depth[ne[0], ne[1]] = ne[2]
            cur_context_cc.extend(new_tmp_context_nodes)
            tmp_erode.append(new_tmp_context_nodes)
            tmp_context_nodes = None
            tmp_context_nodes = new_tmp_context_nodes
            new_tmp_intouched_nodes = None; new_tmp_intouched_nodes = []

            for node in tmp_intouched_nodes:
                if bool(context_map[node[0], node[1]]) is True or bool(mask_map[node[0], node[1]]) is True:
                    continue
                nes = mesh.neighbors(node)

                for ne in nes:
                    if bool(context_map[ne[0], ne[1]]) is False and \
                       bool(mask_map[ne[0], ne[1]]) is False and \
                       bool(intouched_map[ne[0], ne[1]]) is False and \
                       bool(forbidden_map[ne[0], ne[1]]) is True:
                        new_tmp_intouched_nodes.append(ne)
                        intouched_map[ne[0], ne[1]] = True
            tmp_intouched_nodes = None
            tmp_intouched_nodes = set(new_tmp_intouched_nodes)
            new_tmp_redundant_nodes = None; new_tmp_redundant_nodes = []
            for node in tmp_redundant_nodes:
                if bool(context_map[node[0], node[1]]) is True or \
                   bool(mask_map[node[0], node[1]]) is True:
                    continue
                nes = mesh.neighbors(node)

                for ne in nes:
                    if bool(context_map[ne[0], ne[1]]) is False and \
                       bool(mask_map[ne[0], ne[1]]) is False and \
                       bool(intouched_map[ne[0], ne[1]]) is False and \
                       bool(forbidden_map[ne[0], ne[1]]) is True:
                        new_tmp_redundant_nodes.append(ne)
                        intouched_map[ne[0], ne[1]] = True
            tmp_redundant_nodes = None
            tmp_redundant_nodes = set(new_tmp_redundant_nodes)
            new_tmp_noncont_nodes = None; new_tmp_noncont_nodes = []
            for node in tmp_noncont_nodes:
                if bool(context_map[node[0], node[1]]) is True or \
                   bool(mask_map[node[0], node[1]]) is True:
                    continue
                nes = mesh.neighbors(node)
                rmv_flag = False
                for ne in nes:
                    if bool(context_map[ne[0], ne[1]]) is False and \
                       bool(mask_map[ne[0], ne[1]]) is False and \
                       bool(noncont_map[ne[0], ne[1]]) is False and \
                       bool(forbidden_map[ne[0], ne[1]]) is True:
                        patch_context_map = context_map[max(ne[0] - 1, 0):min(ne[0] + 2, context_map.shape[0]),
                                                        max(ne[1] - 1, 0):min(ne[1] + 2, context_map.shape[1])]
                        if bool(np.any(patch_context_map)) is True:
                            new_tmp_noncont_nodes.append(ne)
                            noncont_map[ne[0], ne[1]] = True
            tmp_noncont_nodes = None
            tmp_noncont_nodes = set(new_tmp_noncont_nodes)
        if inpaint_iter == 0:
            depth_dict = get_depth_from_maps(context_map, mask_map, context_depth, mesh.graph['H'], mesh.graph['W'], log_depth=config['log_depth'])
            mask_size = get_valid_size(depth_dict['mask'])
            mask_size = dilate_valid_size(mask_size, depth_dict['mask'], dilate=[20, 20])
            context_size = get_valid_size(depth_dict['context'])
            context_size = dilate_valid_size(context_size, depth_dict['context'], dilate=[20, 20])
            union_size = size_operation(mask_size, context_size, operation='+')
            npz_count = 0
            depth_dict = depth_inpainting(None, None, None, None, mesh, config, union_size, depth_feat_model, None, given_depth_dict=depth_dict, spdb=False)
            near_depth_map, raw_near_depth_map = np.zeros((mesh.graph['H'], mesh.graph['W'])), np.zeros((mesh.graph['H'], mesh.graph['W']))
            filtered_comp_far_cc, filtered_accomp_near_cc = set(), set()
            for node in cur_accomp_near_cc:
                near_depth_map[node[0], node[1]] = depth_dict['output'][node[0], node[1]]
                raw_near_depth_map[node[0], node[1]] = node[2]
            for node in cur_comp_far_cc:
                four_nes = [xx for xx in [(node[0] - 1, node[1]), (node[0] + 1, node[1]), (node[0], node[1] - 1), (node[0], node[1] + 1)] \
                            if 0 <= xx[0] < mesh.graph['H'] and 0 <= xx[1] < mesh.graph['W'] and \
                            near_depth_map[xx[0], xx[1]] != 0 and \
                            abs(near_depth_map[xx[0], xx[1]]) < abs(node[2])]
                if len(four_nes) > 0:
                    filtered_comp_far_cc.add(node)
                for ne in four_nes:
                    filtered_accomp_near_cc.add((ne[0], ne[1], -abs(raw_near_depth_map[ne[0], ne[1]])))
            cur_comp_far_cc, cur_accomp_near_cc = filtered_comp_far_cc, filtered_accomp_near_cc
        mask_ccs[edge_id] |= set(cur_mask_cc)
        context_ccs[edge_id] |= set(cur_context_cc)
        accomp_extend_context_ccs[edge_id] |= set(cur_accomp_near_cc).intersection(cur_mask_cc)
        extend_edge_ccs[edge_id] |= set(cur_accomp_near_cc).intersection(cur_mask_cc)
        extend_context_ccs[edge_id] |= set(cur_comp_far_cc)
        invalid_extend_edge_ccs[edge_id] |= set(cur_invalid_extend_edge_cc)
        erode_size = [0]
        for tmp in tmp_erode:
            erode_size.append(len(tmp))
            if len(erode_size) > 1:
                erode_size[-1] += erode_size[-2]
        if inpaint_iter == 0:
            tmp_width = config['depth_edge_dilate']
        else:
            tmp_width = 0
        while float(erode_size[tmp_width]) / (erode_size[-1] + 1e-6) > 0.3:
            tmp_width = tmp_width - 1
        try:
            if tmp_width == 0:
                erode_context_ccs[edge_id] = set([])
            else:
                erode_context_ccs[edge_id] = set(reduce(lambda x, y : x + y, [] + tmp_erode[:tmp_width]))
        except:
            import pdb; pdb.set_trace()
        erode_context_cc = copy.deepcopy(erode_context_ccs[edge_id])
        for erode_context_node in erode_context_cc:
            if (inpaint_iter != 0 and (mesh_nodes[erode_context_node].get('inpaint_id') is None or
                                        mesh_nodes[erode_context_node].get('inpaint_id') == 0)):
                erode_context_ccs[edge_id].remove(erode_context_node)
            else:
                context_ccs[edge_id].remove(erode_context_node)
        context_map = np.zeros((mesh.graph['H'], mesh.graph['W']))
        for context_node in context_ccs[edge_id]:
            context_map[context_node[0], context_node[1]] = 1
        extend_context_ccs[edge_id] = extend_context_ccs[edge_id] - mask_ccs[edge_id] - accomp_extend_context_ccs[edge_id]
    if inpaint_iter == 0:
        all_ecnt_cc = set()
        for ecnt_id, ecnt_cc in enumerate(extend_context_ccs):
            constraint_context_ids = set()
            constraint_context_cc = set()
            constraint_erode_context_cc = set()
            tmp_mask_cc = set()
            accum_context_cc = None; accum_context_cc = []
            for ecnt_node in accomp_extend_context_ccs[ecnt_id]:
                if edge_maps[ecnt_node[0], ecnt_node[1]] > -1:
                    constraint_context_ids.add(int(round(edge_maps[ecnt_node[0], ecnt_node[1]])))
            constraint_erode_context_cc = erode_context_ccs[ecnt_id]
            for constraint_context_id in constraint_context_ids:
                constraint_context_cc = constraint_context_cc | context_ccs[constraint_context_id] | erode_context_ccs[constraint_context_id]
                constraint_erode_context_cc = constraint_erode_context_cc | erode_context_ccs[constraint_context_id]
            for i in range(background_thickness):
                if i == 0:
                    tmp_context_nodes = copy.deepcopy(ecnt_cc)
                    tmp_invalid_context_nodes = copy.deepcopy(invalid_extend_edge_ccs[ecnt_id])
                    tmp_mask_nodes = copy.deepcopy(accomp_extend_context_ccs[ecnt_id])
                    tmp_context_map = np.zeros((mesh.graph['H'], mesh.graph['W'])).astype(bool)
                    tmp_mask_map = np.zeros((mesh.graph['H'], mesh.graph['W'])).astype(bool)
                    tmp_invalid_context_map = np.zeros((mesh.graph['H'], mesh.graph['W'])).astype(bool)
                    for node in tmp_mask_nodes:
                        tmp_mask_map[node[0], node[1]] = True
                    for node in context_ccs[ecnt_id]:
                        tmp_context_map[node[0], node[1]] = True
                    for node in erode_context_ccs[ecnt_id]:
                        tmp_context_map[node[0], node[1]] = True
                    for node in extend_context_ccs[ecnt_id]:
                        tmp_context_map[node[0], node[1]] = True
                    for node in invalid_extend_edge_ccs[ecnt_id]:
                        tmp_invalid_context_map[node[0], node[1]] = True
                    init_invalid_context_map = tmp_invalid_context_map.copy()
                    init_context_map = tmp
                    if (tmp_mask_map.astype(np.uint8) * tmp_context_map.astype(np.uint8)).max() > 0:
                        import pdb; pdb.set_trace()
                    if vis_edge_id is not None and ecnt_id == vis_edge_id:
                        f, ((ax1, ax2)) = plt.subplots(1, 2, sharex=True, sharey=True)
                        ax1.imshow(tmp_context_map * 1); ax2.imshow(init_invalid_context_map * 1 + tmp_context_map * 2)
                        plt.show()
                        import pdb; pdb.set_trace()
                else:
                    tmp_context_nodes = new_tmp_context_nodes
                    new_tmp_context_nodes = None
                    tmp_mask_nodes = new_tmp_mask_nodes
                    new_tmp_mask_nodes = None
                    tmp_invalid_context_nodes = new_tmp_invalid_context_nodes
                    new_tmp_invalid_context_nodes = None
                new_tmp_context_nodes = None
                new_tmp_context_nodes = []
                new_tmp_invalid_context_nodes = None
                new_tmp_invalid_context_nodes = []
                new_tmp_mask_nodes = set([])
                for node in tmp_context_nodes:
                    for ne in mesh.neighbors(node):
                        if ne in constraint_context_cc and \
                            bool(tmp_mask_map[ne[0], ne[1]]) is False and \
                            bool(tmp_context_map[ne[0], ne[1]]) is False and \
                            bool(forbidden_map[ne[0], ne[1]]) is True:
                            new_tmp_context_nodes.append(ne)
                            tmp_context_map[ne[0], ne[1]] = True
                accum_context_cc.extend(new_tmp_context_nodes)
                for node in tmp_invalid_context_nodes:
                    for ne in mesh.neighbors(node):
                        if bool(tmp_mask_map[ne[0], ne[1]]) is False and \
                           bool(tmp_context_map[ne[0], ne[1]]) is False and \
                           bool(tmp_invalid_context_map[ne[0], ne[1]]) is False and \
                           bool(forbidden_map[ne[0], ne[1]]) is True:
                            tmp_invalid_context_map[ne[0], ne[1]] = True
                            new_tmp_invalid_context_nodes.append(ne)
                for node in tmp_mask_nodes:
                    for ne in mesh.neighbors(node):
                        if bool(tmp_mask_map[ne[0], ne[1]]) is False and \
                           bool(tmp_context_map[ne[0], ne[1]]) is False and \
                           bool(tmp_invalid_context_map[ne[0], ne[1]]) is False and \
                           bool(forbidden_map[ne[0], ne[1]]) is True:
                            new_tmp_mask_nodes.add(ne)
                            tmp_mask_map[ne[0], ne[1]] = True
            init_invalid_context_map[tmp_context_map] = False
            _, tmp_label_map = cv2.connectedComponents((init_invalid_context_map | tmp_context_map).astype(np.uint8), connectivity=8)
            tmp_label_ids = set(np.unique(tmp_label_map[init_invalid_context_map]))
            if (tmp_mask_map.astype(np.uint8) * tmp_context_map.astype(np.uint8)).max() > 0:
                import pdb; pdb.set_trace()
            if vis_edge_id is not None and ecnt_id == vis_edge_id:
                f, ((ax1, ax2)) = plt.subplots(1, 2, sharex=True, sharey=True)
                ax1.imshow(tmp_label_map); ax2.imshow(init_invalid_context_map * 1 + tmp_context_map * 2)
                plt.show()
                import pdb; pdb.set_trace()
            extend_context_ccs[ecnt_id] |= set(accum_context_cc)
            extend_context_ccs[ecnt_id] = extend_context_ccs[ecnt_id] - mask_ccs[ecnt_id]
            extend_erode_context_ccs[ecnt_id] = extend_context_ccs[ecnt_id] & constraint_erode_context_cc
            extend_context_ccs[ecnt_id] = extend_context_ccs[ecnt_id] - extend_erode_context_ccs[ecnt_id] - erode_context_ccs[ecnt_id]
            tmp_context_cc = context_ccs[ecnt_id] - extend_erode_context_ccs[ecnt_id] - erode_context_ccs[ecnt_id]
            if len(tmp_context_cc) > 0:
                context_ccs[ecnt_id] = tmp_context_cc
            tmp_mask_cc = tmp_mask_cc - context_ccs[ecnt_id] - erode_context_ccs[ecnt_id]
            mask_ccs[ecnt_id] = mask_ccs[ecnt_id] | tmp_mask_cc

    return context_ccs, mask_ccs, broken_mask_ccs, edge_ccs, erode_context_ccs, invalid_extend_edge_ccs, edge_maps, extend_context_ccs, extend_edge_ccs, extend_erode_context_ccs

def ceil_modulo(x, mod):
    if x % mod == 0:
        return x
    return (x // mod + 1) * mod

def unpad_img_to_modulo(img, H_original, W_original):
    _,_, H, W = img.shape
    """
    if H != max_pad:
        img = torch.nn.functional.interpolate(img, (max_pad, max_pad),
                                                                 mode='bilinear', align_corners=True)
    """
    img = img[:, :, :H_original, :W_original]
    return img

def pad_img_to_modulo(img, mod, value = 0):
    _, channels, height, width = img.shape
    out_height = ceil_modulo(height, mod)
    out_width = ceil_modulo(width, mod)

    ## Pad width and height to get a squared image of max_pad x max_pad
    img_padded =  torch.nn.functional.pad(
        img,
        (0, out_width -width, 0, out_height - height),
        mode="constant", value = value
    )
    ## If image width and height are higher than 512, downsample the image to match 512x512
    """
    if max_pad > mod:
        img_padded = torch.nn.functional.interpolate(img_padded, (mod, mod),
                                                                 mode='bilinear', align_corners=True)
    """
    return img_padded

def inpaint_Stable_Diffusion_window(hole, context, image, condition_image, pipe, generator, guidance_scale, num_samples, stride, config):
    """
    image_padded_scaled, max_pad = pad_img_to_modulo(image, 512, 0)
    #image_padded_scaled = image_padded * 2.0 - 1.0

    hole_padded, _ = pad_img_to_modulo(hole, 512, 0)

    context_padded, _ =  pad_img_to_modulo(context, 512, 0)
    inpainting_mask_padded = 1.0 - context_padded
    print(hole_padded.shape)
    print(inpainting_mask_padded.shape)
    print(image_padded_scaled.shape)
    images = pipe(
        prompt='',
        image=image_padded_scaled,
        mask_image=inpainting_mask_padded,
        strength=0.95,
        output_type = 'np'
    ).images
    img_inpainted_padded = torch.from_numpy(images)
    img_inpainted_padded = torch.permute(img_inpainted_padded, [0, 3, 1, 2])
    B, C, H_original , W_original = image.shape
    img_inpainted = unpad_img_to_modulo(img_inpainted_padded, max_pad, H_original, W_original)
    img_inpainted = img_inpainted * hole + image*context
    """
    B, C, H, W = image.shape
    chip_size = 512
    stride = 256
    img_chips = []
    context_chips = []
    hole_chips = []
    inpainted_mask_chips = []
    condition_image_chips = []
    ## These clones will be modifed during inpainting step
    image_clone = image.detach().clone()
    # inpainting_mask_clone = inpainting_mask.detach().clone()
    context_clone = context.detach().clone()
    hole_clone = hole.detach().clone()
    condition_image_clone = condition_image.detach().clone()

    generator = generator.manual_seed(0) if generator is not None else  torch.Generator(device="cuda").manual_seed(0) # Reset to seed 0 every single time!!!
    
    # For each window of 512x512
    for row in range(0, H, chip_size - stride):
        for col in range(0, W, chip_size - stride):
            # Get a small window from big image
            hole_chip =  hole_clone[:, :, row: row + chip_size, col:col+chip_size]
            # If there is no region to inpaint, then skip
            if torch.all(hole_chip == 0):
                continue
            context_chip = context_clone[:, :, row: row + chip_size, col:col+chip_size]
            image_chip = image_clone[:, :, row: row + chip_size, col:col+chip_size]
            condition_image_chip =  condition_image_clone[:, :, row: row + chip_size, col:col+chip_size]

            # Initialize new buffers (512x512)
            new_image_chip = torch.zeros((image_clone.shape[0], image.shape[1] , chip_size, chip_size))
            new_hole_chip  =  torch.zeros((hole_clone.shape[0], hole.shape[1] , chip_size, chip_size))
            new_context_chip = torch.zeros((context_clone.shape[0], context.shape[1] , chip_size, chip_size))
            new_condition_image_chip = torch.zeros((condition_image_clone.shape[0], condition_image_clone.shape[1] , chip_size, chip_size))

            # copy data to new buffers
            image_chip_dimension = image_chip.shape[-2:] ## The dimension of the original cut
            new_image_chip[:,:,:image_chip_dimension[0], :image_chip_dimension[1]] = image_chip
            new_hole_chip[:,:,:image_chip_dimension[0], :image_chip_dimension[1]] = hole_chip
            new_context_chip[:,:,:image_chip_dimension[0], :image_chip_dimension[1]] = context_chip
            new_inpainting_mask_chip = 1 - new_context_chip
            new_condition_image_chip[:,:,:image_chip_dimension[0], :image_chip_dimension[1]] = condition_image_chip
            new_condition_image_chip = new_condition_image_chip.expand(-1, 3, -1, -1)


            if config["use_controlnet"]:
                new_inpainted_chip = pipe(
                    prompt='',
                    image=new_image_chip,
                    control_image=new_condition_image_chip,
                    mask_image= new_inpainting_mask_chip,
                    output_type = 'np',
                ).images
            else:
                new_inpainted_chip = pipe(
                    prompt='',
                    image=new_image_chip,
                    mask_image= new_inpainting_mask_chip,
                    generator = generator,
                    num_samples=num_samples,
                    output_type = 'np',
                ).images



            new_inpainted_chip = torch.from_numpy(new_inpainted_chip)
            new_inpainted_chip = torch.permute(new_inpainted_chip, [0, 3, 1, 2])
            new_inpainted_chip = new_inpainted_chip * new_hole_chip + new_image_chip*new_context_chip

            ## Update the clones to reflect the current progress
            image_clone[:, :, row: row +  image_chip_dimension[0], col:col+image_chip_dimension[1]] \
                = new_inpainted_chip[:,:, :image_chip_dimension[0],  :image_chip_dimension[1]]
            context_clone[:, :, row: row +  image_chip_dimension[0], col:col+image_chip_dimension[1]] \
                = new_context_chip[:,:, :image_chip_dimension[0],  :image_chip_dimension[1]] \
                + new_hole_chip[:,:, :image_chip_dimension[0],  :image_chip_dimension[1]]
            hole_clone[:, :, row: row +  image_chip_dimension[0], col:col+image_chip_dimension[1]] \
                = 0
    return image_clone

def inpaint_Stable_Diffusion(hole, context, image, condition_image, pipe, generator, guidance_scale, num_samples, config):
    generator = generator.manual_seed(0) if generator is not None else  torch.Generator(device="cuda").manual_seed(0) ## Reset to seed 0 every single time!!
    image_padded = pad_img_to_modulo(image, 8, 0)
    hole_padded = pad_img_to_modulo(hole, 8, 0)
    context_padded =  pad_img_to_modulo(context, 8, 0)
    inpainting_mask_padded = 1.0 - context_padded

    condition_image = condition_image.expand(-1, 3, -1, -1)

    if config["use_controlnet"]:
        images = pipe(
        height = image_padded.shape[-2],
        width = image_padded.shape[-1],
        prompt='',
        image=image_padded,
        control_image=condition_image,
        mask_image= inpainting_mask_padded,
        generator = generator,
        output_type = 'np',
    ).images
    else:
        images = pipe(
            height = image_padded.shape[-2],
            width = image_padded.shape[-1],
            prompt='',
            image=image_padded,
            mask_image= inpainting_mask_padded,
            generator = generator,
            output_type = 'np',
        ).images

    img_inpainted_padded = torch.from_numpy(images)
    img_inpainted_padded = torch.permute(img_inpainted_padded, [0, 3, 1, 2])
    B, C, H_original , W_original = image.shape
    img_inpainted = unpad_img_to_modulo(img_inpainted_padded, H_original, W_original)
    # img_inpainted = img_inpainted * hole + image*context
    return img_inpainted

def DL_inpaint_edge(mesh,
                    info_on_pix,
                    config,
                    image,
                    depth,
                    context_ccs,
                    erode_context_ccs,
                    extend_context_ccs,
                    extend_erode_context_ccs,
                    mask_ccs,
                    broken_mask_ccs,
                    edge_ccs,
                    extend_edge_ccs,
                    init_mask_connect,
                    edge_maps,
                    rgb_model=None,
                    depth_edge_model=None,
                    depth_edge_model_init=None,
                    depth_feat_model=None,
                    specific_edge_id=-1,
                    specific_edge_loc=None,
                    inpaint_iter=0,
                    generator = None):

    if isinstance(config["gpu_ids"], int) and (config["gpu_ids"] >= 0):
        device = config["gpu_ids"]
    else:
        device = "cpu"

    edge_map = np.zeros_like(depth)
    new_edge_ccs = [set() for _ in range(len(edge_ccs))]
    edge_maps_with_id = edge_maps
    edge_condition = lambda x, m: m.nodes[x].get('far') is not None and len(m.nodes[x].get('far')) > 0
    edge_map = get_map_from_ccs(edge_ccs, mesh.graph['H'], mesh.graph['W'], mesh, edge_condition)
    np_depth, np_image = depth.copy(), image.copy()
    image_c = image.shape[-1]
    image = torch.FloatTensor(image.transpose(2, 0, 1)).unsqueeze(0).to(device)
    if depth.ndim < 3:
        depth = depth[..., None]
    depth = torch.FloatTensor(depth.transpose(2, 0, 1)).unsqueeze(0).to(device)
    mesh.graph['max_edge_id'] = len(edge_ccs)
    connnect_points_ccs = [set() for _ in range(len(edge_ccs))]
    gp_time, tmp_mesh_time, bilateral_time = 0, 0, 0
    edges_infos = dict()
    edges_in_mask = [set() for _ in range(len(edge_ccs))]
    tmp_specific_edge_id = []
    for edge_id, (context_cc, mask_cc, erode_context_cc, extend_context_cc, edge_cc) in enumerate(zip(context_ccs, mask_ccs, erode_context_ccs, extend_context_ccs, edge_ccs)):
        if len(specific_edge_id) > 0:
            if edge_id not in specific_edge_id:
                continue
        if len(context_cc) < 1 or len(mask_cc) < 1:
            continue
        edge_dict = get_edge_from_nodes(context_cc | extend_context_cc, erode_context_cc | extend_erode_context_ccs[edge_id], mask_cc, edge_cc, extend_edge_ccs[edge_id],
                                        mesh.graph['H'], mesh.graph['W'], mesh)
        edge_dict['edge'], end_depth_maps, _ = \
            filter_irrelevant_edge_new(edge_dict['self_edge'], edge_dict['comp_edge'],
                                    edge_map,
                                    edge_maps_with_id,
                                    edge_id,
                                    edge_dict['context'],
                                    edge_dict['depth'], mesh, context_cc | erode_context_cc | extend_context_cc | extend_erode_context_ccs[edge_id], spdb=False)
        if specific_edge_loc is not None and \
            (specific_edge_loc is not None and edge_dict['mask'][specific_edge_loc[0], specific_edge_loc[1]] == 0):
            continue
        mask_size = get_valid_size(edge_dict['mask'])
        mask_size = dilate_valid_size(mask_size, edge_dict['mask'], dilate=[20, 20])
        context_size = get_valid_size(edge_dict['context'])
        context_size = dilate_valid_size(context_size, edge_dict['context'], dilate=[20, 20])
        union_size = size_operation(mask_size, context_size, operation='+')
        patch_edge_dict = dict()
        patch_edge_dict['mask'], patch_edge_dict['context'], patch_edge_dict['rgb'], \
            patch_edge_dict['disp'], patch_edge_dict['edge'] = \
            crop_maps_by_size(union_size, edge_dict['mask'], edge_dict['context'],
                                edge_dict['rgb'], edge_dict['disp'], edge_dict['edge'])
        x_anchor, y_anchor = [union_size['x_min'], union_size['x_max']], [union_size['y_min'], union_size['y_max']]
        tensor_edge_dict = convert2tensor(patch_edge_dict)
        np.save("tensor_edge_dict.npy", patch_edge_dict)
        #print("TENSOR EDGE DICT" + str(tensor_edge_dict))
        input_edge_feat = torch.cat((tensor_edge_dict['rgb'],
                                        tensor_edge_dict['disp'],
                                        tensor_edge_dict['edge'],
                                        1 - tensor_edge_dict['context'],
                                        tensor_edge_dict['mask']), dim=1)
        if require_depth_edge(patch_edge_dict['edge'], patch_edge_dict['mask']) and inpaint_iter == 0:
            with torch.no_grad():
                depth_edge_output = depth_edge_model.forward_3P(tensor_edge_dict['mask'],
                                                                tensor_edge_dict['context'],
                                                                tensor_edge_dict['rgb'],
                                                                tensor_edge_dict['disp'],
                                                                tensor_edge_dict['edge'],
                                                                unit_length=128,
                                                                cuda=device)
                depth_edge_output = depth_edge_output.cpu()
            tensor_edge_dict['output'] = (depth_edge_output> config['ext_edge_threshold']).float() * tensor_edge_dict['mask'] + tensor_edge_dict['edge']
        else:
            tensor_edge_dict['output'] = tensor_edge_dict['edge']
            depth_edge_output = tensor_edge_dict['edge'] + 0
        patch_edge_dict['output'] = tensor_edge_dict['output'].squeeze().data.cpu().numpy()
        edge_dict['output'] = np.zeros((mesh.graph['H'], mesh.graph['W']))
        edge_dict['output'][union_size['x_min']:union_size['x_max'], union_size['y_min']:union_size['y_max']] = \
            patch_edge_dict['output']
        if require_depth_edge(patch_edge_dict['edge'], patch_edge_dict['mask']) and inpaint_iter == 0:
            if ((depth_edge_output> config['ext_edge_threshold']).float() * tensor_edge_dict['mask']).max() > 0:
                try:
                    edge_dict['fpath_map'], edge_dict['npath_map'], break_flag, npaths, fpaths, invalid_edge_id = \
                        clean_far_edge_new(edge_dict['output'], end_depth_maps, edge_dict['mask'], edge_dict['context'], mesh, info_on_pix, edge_dict['self_edge'], inpaint_iter, config)
                except:
                    import pdb; pdb.set_trace()
                pre_npath_map = edge_dict['npath_map'].copy()
                if config.get('repeat_inpaint_edge') is True:
                    for _ in range(2):
                        tmp_input_edge = ((edge_dict['npath_map'] > -1) + edge_dict['edge']).clip(0, 1)
                        patch_tmp_input_edge = crop_maps_by_size(union_size, tmp_input_edge)[0]
                        tensor_input_edge = torch.FloatTensor(patch_tmp_input_edge)[None, None, ...]
                        depth_edge_output = depth_edge_model.forward_3P(tensor_edge_dict['mask'],
                                                    tensor_edge_dict['context'],
                                                    tensor_edge_dict['rgb'],
                                                    tensor_edge_dict['disp'],
                                                    tensor_input_edge,
                                                    unit_length=128,
                                                    cuda=device)
                        depth_edge_output = depth_edge_output.cpu()
                        depth_edge_output = (depth_edge_output> config['ext_edge_threshold']).float() * tensor_edge_dict['mask'] + tensor_edge_dict['edge']
                        depth_edge_output = depth_edge_output.squeeze().data.cpu().numpy()
                        full_depth_edge_output = np.zeros((mesh.graph['H'], mesh.graph['W']))
                        full_depth_edge_output[union_size['x_min']:union_size['x_max'], union_size['y_min']:union_size['y_max']] = \
                            depth_edge_output
                        edge_dict['fpath_map'], edge_dict['npath_map'], break_flag, npaths, fpaths, invalid_edge_id = \
                            clean_far_edge_new(full_depth_edge_output, end_depth_maps, edge_dict['mask'], edge_dict['context'], mesh, info_on_pix, edge_dict['self_edge'], inpaint_iter, config)
                for nid in npaths.keys():
                    npath, fpath = npaths[nid], fpaths[nid]
                    start_mx, start_my, end_mx, end_my = -1, -1, -1, -1
                    if end_depth_maps[npath[0][0], npath[0][1]] != 0:
                        start_mx, start_my = npath[0][0], npath[0][1]
                    if end_depth_maps[npath[-1][0], npath[-1][1]] != 0:
                        end_mx, end_my = npath[-1][0], npath[-1][1]
                    if start_mx == -1:
                        import pdb; pdb.set_trace()
                    valid_end_pt = () if end_mx == -1 else (end_mx, end_my, info_on_pix[(end_mx, end_my)][0]['depth'])
                    new_edge_info = dict(fpath=fpath,
                                         npath=npath,
                                         cont_end_pts=valid_end_pt,
                                         mask_id=edge_id,
                                         comp_edge_id=nid,
                                         depth=end_depth_maps[start_mx, start_my])
                    if edges_infos.get((start_mx, start_my)) is None:
                        edges_infos[(start_mx, start_my)] = []
                    edges_infos[(start_mx, start_my)].append(new_edge_info)
                    edges_in_mask[edge_id].add((start_mx, start_my))
                    if len(valid_end_pt) > 0:
                        new_edge_info = dict(fpath=fpath[::-1],
                                             npath=npath[::-1],
                                             cont_end_pts=(start_mx, start_my, info_on_pix[(start_mx, start_my)][0]['depth']),
                                             mask_id=edge_id,
                                             comp_edge_id=nid,
                                             depth=end_depth_maps[end_mx, end_my])
                        if edges_infos.get((end_mx, end_my)) is None:
                            edges_infos[(end_mx, end_my)] = []
                        edges_infos[(end_mx, end_my)].append(new_edge_info)
                        edges_in_mask[edge_id].add((end_mx, end_my))
    guidance_scale=7.5
    num_samples = 1
    # generator = torch.Generator(device="cuda").manual_seed(0)
    count = 0
    for edge_id, (context_cc, mask_cc, erode_context_cc, extend_context_cc, edge_cc) in enumerate(zip(context_ccs, mask_ccs, erode_context_ccs, extend_context_ccs, edge_ccs)):
        if len(specific_edge_id) > 0:
            if edge_id not in specific_edge_id:
                continue
        if len(context_cc) < 1 or len(mask_cc) < 1:
            continue
        edge_dict = get_edge_from_nodes(context_cc | extend_context_cc, erode_context_cc | extend_erode_context_ccs[edge_id], mask_cc, edge_cc, extend_edge_ccs[edge_id],
                                        mesh.graph['H'], mesh.graph['W'], mesh)
        if specific_edge_loc is not None and \
            (specific_edge_loc is not None and edge_dict['mask'][specific_edge_loc[0], specific_edge_loc[1]] == 0):
            continue
        else:
            tmp_specific_edge_id.append(edge_id)
        edge_dict['edge'], end_depth_maps, _ = \
            filter_irrelevant_edge_new(edge_dict['self_edge'], edge_dict['comp_edge'],
                                    edge_map,
                                    edge_maps_with_id,
                                    edge_id,
                                    edge_dict['context'],
                                    edge_dict['depth'], mesh, context_cc | erode_context_cc | extend_context_cc | extend_erode_context_ccs[edge_id], spdb=False)
        discard_map = np.zeros_like(edge_dict['edge'])
        mask_size = get_valid_size(edge_dict['mask'])
        mask_size = dilate_valid_size(mask_size, edge_dict['mask'], dilate=[20, 20])
        context_size = get_valid_size(edge_dict['context'])
        context_size = dilate_valid_size(context_size, edge_dict['context'], dilate=[20, 20])
        union_size = size_operation(mask_size, context_size, operation='+')
        patch_edge_dict = dict()
        patch_edge_dict['mask'], patch_edge_dict['context'], patch_edge_dict['rgb'], \
            patch_edge_dict['disp'], patch_edge_dict['edge'] = \
            crop_maps_by_size(union_size, edge_dict['mask'], edge_dict['context'],
                                edge_dict['rgb'], edge_dict['disp'], edge_dict['edge'])
        x_anchor, y_anchor = [union_size['x_min'], union_size['x_max']], [union_size['y_min'], union_size['y_max']]
        tensor_edge_dict = convert2tensor(patch_edge_dict)
        input_edge_feat = torch.cat((tensor_edge_dict['rgb'],
                                        tensor_edge_dict['disp'],
                                        tensor_edge_dict['edge'],
                                        1 - tensor_edge_dict['context'],
                                        tensor_edge_dict['mask']), dim=1)
        edge_dict['output'] = edge_dict['edge'].copy()

        if require_depth_edge(patch_edge_dict['edge'], patch_edge_dict['mask']) and inpaint_iter == 0:
            edge_dict['fpath_map'], edge_dict['npath_map'] = edge_dict['fpath_map'] * 0 - 1, edge_dict['npath_map'] * 0 - 1
            end_pts = edges_in_mask[edge_id]
            for end_pt in end_pts:
                cur_edge_infos = edges_infos[(end_pt[0], end_pt[1])]
                cur_info = [xx for xx in cur_edge_infos if xx['mask_id'] == edge_id][0]
                other_infos = [xx for xx in cur_edge_infos if xx['mask_id'] != edge_id and len(xx['cont_end_pts']) > 0]
                if len(cur_info['cont_end_pts']) > 0 or (len(cur_info['cont_end_pts']) == 0 and len(other_infos) == 0):
                    for fnode in cur_info['fpath']:
                        edge_dict['fpath_map'][fnode[0], fnode[1]] = cur_info['comp_edge_id']
                    for fnode in cur_info['npath']:
                        edge_dict['npath_map'][fnode[0], fnode[1]] = cur_info['comp_edge_id']
            fnmap = edge_dict['fpath_map'] * 1
            fnmap[edge_dict['npath_map'] != -1] = edge_dict['npath_map'][edge_dict['npath_map'] != -1]
            for end_pt in end_pts:
                cur_edge_infos = edges_infos[(end_pt[0], end_pt[1])]
                cur_info = [xx for xx in cur_edge_infos if xx['mask_id'] == edge_id][0]
                cur_depth = cur_info['depth']
                other_infos = [xx for xx in cur_edge_infos if xx['mask_id'] != edge_id and len(xx['cont_end_pts']) > 0]
                comp_edge_id = cur_info['comp_edge_id']
                if len(cur_info['cont_end_pts']) == 0 and len(other_infos) > 0:
                    other_infos = sorted(other_infos, key=lambda aa: abs(abs(aa['cont_end_pts'][2]) - abs(cur_depth)))
                    for other_info in other_infos:
                        tmp_fmap, tmp_nmap = np.zeros((mesh.graph['H'], mesh.graph['W'])) - 1, np.zeros((mesh.graph['H'], mesh.graph['W'])) - 1
                        for fnode in other_info['fpath']:
                            if fnmap[fnode[0], fnode[1]] != -1:
                                tmp_fmap = tmp_fmap * 0 - 1
                                break
                            else:
                                tmp_fmap[fnode[0], fnode[1]] = comp_edge_id
                        if fnmap[fnode[0], fnode[1]] != -1:
                            continue
                        for fnode in other_info['npath']:
                            if fnmap[fnode[0], fnode[1]] != -1:
                                tmp_nmap = tmp_nmap * 0 - 1
                                break
                            else:
                                tmp_nmap[fnode[0], fnode[1]] = comp_edge_id
                        if fnmap[fnode[0], fnode[1]] != -1:
                            continue
                        break
                    if min(tmp_fmap.max(), tmp_nmap.max()) != -1:
                        edge_dict['fpath_map'] = tmp_fmap
                        edge_dict['fpath_map'][edge_dict['valid_area'] == 0] = -1
                        edge_dict['npath_map'] = tmp_nmap
                        edge_dict['npath_map'][edge_dict['valid_area'] == 0] = -1
                        discard_map = ((tmp_nmap != -1).astype(np.uint8) + (tmp_fmap != -1).astype(np.uint8)) * edge_dict['mask']
                    else:
                        for fnode in cur_info['fpath']:
                            edge_dict['fpath_map'][fnode[0], fnode[1]] = cur_info['comp_edge_id']
                        for fnode in cur_info['npath']:
                            edge_dict['npath_map'][fnode[0], fnode[1]] = cur_info['comp_edge_id']
            if edge_dict['npath_map'].min() == 0 or edge_dict['fpath_map'].min() == 0:
                import pdb; pdb.set_trace()
            edge_dict['output'] = (edge_dict['npath_map'] > -1) * edge_dict['mask'] + edge_dict['context'] * edge_dict['edge']
            #print(edge_dict['output'])
        mesh, _, _, _ = create_placeholder(edge_dict['context'], edge_dict['mask'],
                                  edge_dict['depth'], edge_dict['fpath_map'],
                                  edge_dict['npath_map'], mesh, inpaint_iter,
                                  edge_ccs,
                                  extend_edge_ccs[edge_id],
                                  edge_maps_with_id,
                                  edge_id)

        dxs, dys = np.where(discard_map != 0)
        for dx, dy in zip(dxs, dys):
            mesh.nodes[(dx, dy)]['inpaint_twice'] = False
        depth_dict = depth_inpainting(context_cc, extend_context_cc, erode_context_cc | extend_erode_context_ccs[edge_id], mask_cc, mesh, config, union_size, depth_feat_model, edge_dict['output'])
        refine_depth_output = depth_dict['output']*depth_dict['mask']
        for near_id in np.unique(edge_dict['npath_map'])[1:]:
            refine_depth_output = refine_depth_around_edge(refine_depth_output.copy(),
                                                            (edge_dict['fpath_map'] == near_id).astype(np.uint8) * edge_dict['mask'],
                                                            (edge_dict['fpath_map'] == near_id).astype(np.uint8),
                                                            (edge_dict['npath_map'] == near_id).astype(np.uint8) * edge_dict['mask'],
                                                            depth_dict['mask'].copy(),
                                                            depth_dict['output'] * depth_dict['context'],
                                                            config)
        depth_dict['output'][depth_dict['mask'] > 0] = refine_depth_output[depth_dict['mask'] > 0]
        rgb_dict = get_rgb_from_nodes(context_cc | extend_context_cc,
                                      erode_context_cc | extend_erode_context_ccs[edge_id], mask_cc, mesh.graph['H'], mesh.graph['W'], mesh)
        if np.all(rgb_dict['mask'] == edge_dict['mask']) is False:
            import pdb; pdb.set_trace()
        rgb_dict['edge'] = edge_dict['output']
        patch_rgb_dict = dict()
        patch_rgb_dict['mask'], patch_rgb_dict['context'], patch_rgb_dict['rgb'], \
            patch_rgb_dict['edge'] = crop_maps_by_size(union_size, rgb_dict['mask'],
                                                        rgb_dict['context'], rgb_dict['rgb'],
                                                        rgb_dict['edge'])
        tensor_rgb_dict = convert2tensor(patch_rgb_dict)
        resize_rgb_dict = {k: v.clone() for k, v in tensor_rgb_dict.items()}
        max_hw = np.array([*patch_rgb_dict['mask'].shape[-2:]]).max()
        if config['resize_patch']: 
            init_frac = config['largest_size'] / (np.array([*patch_rgb_dict['mask'].shape[-2:]]).prod() ** 0.5)
            resize_hw = [patch_rgb_dict['mask'].shape[-2] * init_frac, patch_rgb_dict['mask'].shape[-1] * init_frac]
            resize_max_hw = max(resize_hw)
            frac = (np.floor(resize_max_hw / 128.) * 128.) / max_hw
        else:
            frac = 1 ## test inpainting w no resize
        if frac < 1:
            resize_mark = torch.nn.functional.interpolate(torch.cat((resize_rgb_dict['mask'],
                                                            resize_rgb_dict['context']),
                                                            dim=1),
                                                            scale_factor=frac,
                                                            mode='area')
            resize_rgb_dict['mask'] = (resize_mark[:, 0:1] > 0).float()
            resize_rgb_dict['context'] = (resize_mark[:, 1:2] == 1).float()
            resize_rgb_dict['context'][resize_rgb_dict['mask'] > 0] = 0
            resize_rgb_dict['rgb'] = torch.nn.functional.interpolate(resize_rgb_dict['rgb'],
                                                                        scale_factor=frac,
                                                                        mode='area')
            resize_rgb_dict['rgb'] = resize_rgb_dict['rgb'] * resize_rgb_dict['context']
            resize_rgb_dict['edge'] = torch.nn.functional.interpolate(resize_rgb_dict['edge'],
                                                                        scale_factor=frac,
                                                                        mode='area')
            resize_rgb_dict['edge'] = (resize_rgb_dict['edge'] > 0).float() * 0
            resize_rgb_dict['edge'] = resize_rgb_dict['edge'] * (resize_rgb_dict['context'] + resize_rgb_dict['mask'])
        rgb_input_feat = torch.cat((resize_rgb_dict['rgb'], resize_rgb_dict['edge']), dim=1)
        rgb_input_feat[:, 3] = 1 - rgb_input_feat[:, 3]
        resize_mask = open_small_mask(resize_rgb_dict['mask'], resize_rgb_dict['context'], 3, 41)
        specified_hole = resize_mask
    
   
        stride = 256
        condition_image = depth_dict['condition_image']
    
        if condition_image.shape[-1] != resize_rgb_dict['rgb'].shape[-1] or  condition_image.shape[-2] !=  resize_rgb_dict['rgb'].shape[-2]:
            condition_image = torch.nn.functional.interpolate(condition_image, (resize_rgb_dict['rgb'].shape[-2], resize_rgb_dict['rgb'].shape[-1]),
                                                                        mode='bilinear', align_corners=True)
      
        with torch.no_grad():
            if config['use_stable_diffusion']:
                rgb_output = inpaint_Stable_Diffusion(specified_hole, 
                                                    resize_rgb_dict['context'],
                                                    resize_rgb_dict['rgb'],
                                                    condition_image,
                                                    rgb_model, 
                                                    generator,
                                                    guidance_scale, 
                                                    num_samples,
                                                    config)
            else:
                rgb_output = rgb_model.forward_3P(specified_hole,
                                                resize_rgb_dict['context'],
                                                resize_rgb_dict['rgb'],
                                                resize_rgb_dict['edge'],
                                                unit_length=128,
                                                cuda=device)
            rgb_output = rgb_output.cpu()
            if config.get('gray_image') is True:
                rgb_output = rgb_output.mean(1, keepdim=True).repeat((1,3,1,1))
            rgb_output = rgb_output.cpu()
        resize_rgb_dict['output'] = rgb_output * resize_rgb_dict['mask'] + resize_rgb_dict['rgb']
        tensor_rgb_dict['output'] = resize_rgb_dict['output']
        if frac < 1:
            tensor_rgb_dict['output'] = torch.nn.functional.interpolate(tensor_rgb_dict['output'],
                                                                        size=tensor_rgb_dict['mask'].shape[-2:],
                                                                        mode='bicubic')
            tensor_rgb_dict['output'] = tensor_rgb_dict['output'] * \
                                         tensor_rgb_dict['mask'] + (tensor_rgb_dict['rgb'] * tensor_rgb_dict['context'])
        patch_rgb_dict['output'] = tensor_rgb_dict['output'].data.cpu().numpy().squeeze().transpose(1,2,0)
        rgb_dict['output'] = np.zeros((mesh.graph['H'], mesh.graph['W'], 3))
        rgb_dict['output'][union_size['x_min']:union_size['x_max'], union_size['y_min']:union_size['y_max']] = \
            patch_rgb_dict['output']

        if require_depth_edge(patch_edge_dict['edge'], patch_edge_dict['mask']) or inpaint_iter > 0:
            edge_occlusion = True
        else:
            edge_occlusion = False
        for node in erode_context_cc:
            if rgb_dict['mask'][node[0], node[1]] > 0:
                for info in info_on_pix[(node[0], node[1])]:
                    if abs(info['depth']) == abs(node[2]):
                        info['update_color'] = (rgb_dict['output'][node[0], node[1]] * 255).astype(np.uint8)
        if frac < 1.:
            depth_edge_dilate_2_color_flag = False
        else:
            depth_edge_dilate_2_color_flag = True
        hxs, hys = np.where((rgb_dict['mask'] > 0) & (rgb_dict['erode'] == 0))
        for hx, hy in zip(hxs, hys):
            real_depth = None
            if abs(depth_dict['output'][hx, hy]) <= abs(np_depth[hx, hy]):
                depth_dict['output'][hx, hy] = np_depth[hx, hy] + 0.01
            node = (hx, hy, -depth_dict['output'][hx, hy])
            if info_on_pix.get((node[0], node[1])) is not None:
                for info in info_on_pix.get((node[0], node[1])):
                    if info.get('inpaint_id') is None or abs(info['inpaint_id'] < mesh.nodes[(hx, hy)]['inpaint_id']):
                        pre_depth = info['depth'] if info.get('real_depth') is None else info['real_depth']
                        if abs(node[2]) < abs(pre_depth):
                            node = (node[0], node[1], -(abs(pre_depth) + 0.001))
            if mesh.has_node(node):
                real_depth = node[2]
            while True:
                if mesh.has_node(node):
                    node = (node[0], node[1], -(abs(node[2]) + 0.001))
                else:
                    break
            if real_depth == node[2]:
                real_depth = None
            cur_disp = 1./node[2]
            if not(mesh.has_node(node)):
                if not mesh.has_node((node[0], node[1])):
                    print("2D node not found.")
                    import pdb; pdb.set_trace()
                if inpaint_iter == 1:
                    paint = (rgb_dict['output'][hx, hy] * 255).astype(np.uint8)
                else:
                    paint = (rgb_dict['output'][hx, hy] * 255).astype(np.uint8)
                ndict = dict(color=paint,
                                synthesis=True,
                                disp=cur_disp,
                                cc_id=set([edge_id]),
                                overlap_number=1.0,
                                refine_depth=False,
                                edge_occlusion=edge_occlusion,
                                depth_edge_dilate_2_color_flag=depth_edge_dilate_2_color_flag,
                                real_depth=real_depth)
                mesh, _, _ = refresh_node((node[0], node[1]), mesh.nodes[(node[0], node[1])], node, ndict, mesh, stime=True)
                if inpaint_iter == 0 and mesh.degree(node) < 4:
                    connnect_points_ccs[edge_id].add(node)
            if info_on_pix.get((hx, hy)) is None:
                info_on_pix[(hx, hy)] = []
            new_info = {'depth':node[2],
                        'color': paint,
                        'synthesis':True,
                        'disp':cur_disp,
                        'cc_id':set([edge_id]),
                        'inpaint_id':inpaint_iter + 1,
                        'edge_occlusion':edge_occlusion,
                        'overlap_number':1.0,
                        'real_depth': real_depth}
            info_on_pix[(hx, hy)].append(new_info)
    specific_edge_id = tmp_specific_edge_id
    for erode_id, erode_context_cc in enumerate(erode_context_ccs):
        if len(specific_edge_id) > 0 and erode_id not in specific_edge_id:
            continue
        for erode_node in erode_context_cc:
            for info in info_on_pix[(erode_node[0], erode_node[1])]:
                if info['depth'] == erode_node[2]:
                    info['color'] = info['update_color']
                    mesh.nodes[erode_node]['color'] = info['update_color']
                    np_image[(erode_node[0], erode_node[1])] = info['update_color']
    new_edge_ccs = [set() for _ in range(mesh.graph['max_edge_id'] + 1)]
    for node in mesh.nodes:
        if len(node) == 2:
            mesh.remove_node(node)
            continue
        if mesh.nodes[node].get('edge_id') is not None and mesh.nodes[node].get('inpaint_id') == inpaint_iter + 1:
            if mesh.nodes[node].get('inpaint_twice') is False:
                continue
            try:
                new_edge_ccs[mesh.nodes[node].get('edge_id')].add(node)
            except:
                import pdb; pdb.set_trace()
    specific_mask_nodes = None
    if inpaint_iter == 0:
        mesh, info_on_pix = refine_color_around_edge(mesh, info_on_pix, new_edge_ccs, config, False)

    return mesh, info_on_pix, specific_mask_nodes, new_edge_ccs, connnect_points_ccs, np_image


def write_ply_no_inpainting(image,
                            depth,
                            int_mtx,
                            ply_name,
                            config):
    depth = depth.astype(np.float64)
    input_mesh, xy2depth, image, depth = create_mesh(depth, image, int_mtx, config)
    H, W = input_mesh.graph['H'], input_mesh.graph['W']
    if config['tear_edges']:
        input_mesh = tear_edges(input_mesh, config['depth_threshold'], xy2depth)
    input_mesh, info_on_pix = generate_init_node(input_mesh, config, min_node_in_cc=200)
    vertex_id = 0
    input_mesh.graph['H'], input_mesh.graph['W'] = input_mesh.graph['noext_H'], input_mesh.graph['noext_W']
    background_canvas = np.zeros((input_mesh.graph['H'],
                                  input_mesh.graph['W'],
                                  3))
    ply_flag = config.get('save_ply')
    if ply_flag is True:
        node_str_list = []
    else:
        node_str_color = []
        node_str_point = []
    out_fmt = lambda x, x_flag: str(x) if x_flag is True else x
    point_time = 0
    hlight_time = 0
    cur_id_time = 0
    node_str_time = 0
    generate_face_time = 0
    point_list = []
    k_00, k_02, k_11, k_12 = \
        input_mesh.graph['cam_param_pix_inv'][0, 0], input_mesh.graph['cam_param_pix_inv'][0, 2], \
        input_mesh.graph['cam_param_pix_inv'][1, 1], input_mesh.graph['cam_param_pix_inv'][1, 2]
    w_offset = input_mesh.graph['woffset']
    h_offset = input_mesh.graph['hoffset']
    for pix_xy, pix_list in info_on_pix.items():
        for pix_idx, pix_info in enumerate(pix_list):
            pix_depth = pix_info['depth'] if pix_info.get('real_depth') is None else pix_info['real_depth']
            str_pt = [out_fmt(x, ply_flag) for x in reproject_3d_int_detail(pix_xy[0], pix_xy[1], pix_depth,
                      k_00, k_02, k_11, k_12, w_offset, h_offset, H, W)]
            if input_mesh.has_node((pix_xy[0], pix_xy[1], pix_info['depth'])) is False:
                return False
                continue
            if pix_info.get('overlap_number') is not None:
                str_color = [out_fmt(x, ply_flag) for x in (pix_info['color']/pix_info['overlap_number']).astype(np.uint8).tolist()]
            else:
                str_color = [out_fmt(x, ply_flag) for x in pix_info['color'].tolist()]
            # if pix_info.get('edge_occlusion') is True:
            #     str_color.append(out_fmt(4, ply_flag))
            # else:
            #     if pix_info.get('inpaint_id') is None:
            #         str_color.append(out_fmt(1, ply_flag))
            #     else:
            #         str_color.append(out_fmt(pix_info.get('inpaint_id') + 1, ply_flag))
            if pix_info.get('modified_border') is True or pix_info.get('ext_pixel') is True:
                if len(str_color) == 4:
                    str_color[-1] = out_fmt(5, ply_flag)
                else:
                    str_color.append(out_fmt(5, ply_flag))
            pix_info['cur_id'] = vertex_id
            input_mesh.nodes[(pix_xy[0], pix_xy[1], pix_info['depth'])]['cur_id'] = out_fmt(vertex_id, ply_flag)
            vertex_id += 1
            if ply_flag is True:
                node_str_list.append(' '.join(str_pt) + ' ' + ' '.join(str_color) + ' ' + str(pix_idx) + ' ' + str(pix_xy[0]) + ' ' + str(pix_xy[1]) + '\n')
            else:
                node_str_color.append(str_color)
                node_str_point.append(str_pt)
    str_faces = generate_face(input_mesh, info_on_pix, config)
    if config['save_ply'] is True:
        print("Writing mesh file %s ..." % ply_name)
        with open(ply_name, 'w') as ply_fi:
            ply_fi.write('ply\n' + 'format ascii 1.0\n')
            ply_fi.write('comment H ' + str(int(input_mesh.graph['H'])) + '\n')
            ply_fi.write('comment W ' + str(int(input_mesh.graph['W'])) + '\n')
            ply_fi.write('comment hFov ' + str(float(input_mesh.graph['hFov'])) + '\n')
            ply_fi.write('comment vFov ' + str(float(input_mesh.graph['vFov'])) + '\n')
            ply_fi.write('element vertex ' + str(len(node_str_list)) + '\n')
            ply_fi.write('property float x\n' + \
                         'property float y\n' + \
                         'property float z\n' + \
                         'property uchar red\n' + \
                         'property uchar green\n' + \
                         'property uchar blue\n' + \
                         'property int index\n' + \
                         'property int pix_x\n' + \
                         'property int pix_y\n') 
                        #  + \'property uchar alpha\n')
            ply_fi.write('element face ' + str(len(str_faces)) + '\n')
            ply_fi.write('property list uchar int vertex_index\n')
            ply_fi.write('end_header\n')
            ply_fi.writelines(node_str_list)
            ply_fi.writelines(str_faces)
        ply_fi.close()
        return input_mesh
    else:
        H = int(input_mesh.graph['H'])
        W = int(input_mesh.graph['W'])
        hFov = input_mesh.graph['hFov']
        vFov = input_mesh.graph['vFov']
        node_str_color = np.array(node_str_color).astype(np.float32)
        node_str_color[..., :3] = node_str_color[..., :3] / 255.
        node_str_point = np.array(node_str_point)
        str_faces = np.array(str_faces)

        return node_str_point, node_str_color, str_faces, H, W, hFov, vFov



def write_ply(image,
              depth,
              int_mtx,
              ply_name,
              config,
              rgb_model,
              depth_edge_model,
              depth_edge_model_init,
              depth_feat_model):
    generator = torch.Generator(device="cuda")#.manual_seed(0)
    depth = depth.astype(np.float64)
    input_mesh, xy2depth, image, depth = create_mesh(depth, image, int_mtx, config)
    index = 0

    H, W = input_mesh.graph['H'], input_mesh.graph['W']

    input_mesh = tear_edges(input_mesh, config['depth_threshold'], xy2depth)
 
    
    input_mesh, info_on_pix = generate_init_node(input_mesh, config, min_node_in_cc=200)
    
    edge_ccs, input_mesh, edge_mesh = group_edges(input_mesh, config, image, remove_conflict_ordinal=False)
    edge_canvas = np.zeros((H, W)) - 1

    input_mesh, info_on_pix, depth = reassign_floating_island(input_mesh, info_on_pix, image, depth)

    input_mesh = update_status(input_mesh, info_on_pix)
    specific_edge_id = []
    edge_ccs, input_mesh, edge_mesh = group_edges(input_mesh, config, image, remove_conflict_ordinal=True)
    pre_depth = depth.copy()
    input_mesh, info_on_pix, edge_mesh, depth, aft_mark = remove_dangling(input_mesh, edge_ccs, edge_mesh, info_on_pix, image, depth, config)

    input_mesh, depth, info_on_pix = update_status(input_mesh, info_on_pix, depth)

    edge_ccs, input_mesh, edge_mesh = group_edges(input_mesh, config, image, remove_conflict_ordinal=True)
    edge_canvas = np.zeros((H, W)) - 1

    mesh, info_on_pix, depth = fill_missing_node(input_mesh, info_on_pix, image, depth)

    if config['extrapolate_border'] is True:
        pre_depth = depth.copy()
        input_mesh, info_on_pix, depth = refresh_bord_depth(input_mesh, info_on_pix, image, depth)
        input_mesh = remove_node_feat(input_mesh, 'edge_id')
        aft_depth = depth.copy()
        input_mesh, info_on_pix, depth, image = enlarge_border(input_mesh, info_on_pix, depth, image, config)
        noext_H, noext_W = H, W
        H, W = image.shape[:2]
        input_mesh, info_on_pix = fill_dummy_bord(input_mesh, info_on_pix, image, depth, config)
        edge_ccs, input_mesh, edge_mesh = \
            group_edges(input_mesh, config, image, remove_conflict_ordinal=True)
        input_mesh = combine_end_node(input_mesh, edge_mesh, edge_ccs, depth)
        input_mesh, depth, info_on_pix = update_status(input_mesh, info_on_pix, depth)
        edge_ccs, input_mesh, edge_mesh = \
            group_edges(input_mesh, config, image, remove_conflict_ordinal=True, spdb=False)
        input_mesh = remove_redundant_edge(input_mesh, edge_mesh, edge_ccs, info_on_pix, config, redundant_number=config['redundant_number'], spdb=False)
        input_mesh, depth, info_on_pix = update_status(input_mesh, info_on_pix, depth)
        edge_ccs, input_mesh, edge_mesh = group_edges(input_mesh, config, image, remove_conflict_ordinal=True)
        input_mesh = combine_end_node(input_mesh, edge_mesh, edge_ccs, depth)
        input_mesh = remove_redundant_edge(input_mesh, edge_mesh, edge_ccs, info_on_pix, config, redundant_number=config['redundant_number'], invalid=True, spdb=False)
        input_mesh, depth, info_on_pix = update_status(input_mesh, info_on_pix, depth)
        edge_ccs, input_mesh, edge_mesh = group_edges(input_mesh, config, image, remove_conflict_ordinal=True)
        input_mesh = combine_end_node(input_mesh, edge_mesh, edge_ccs, depth)
        input_mesh, depth, info_on_pix = update_status(input_mesh, info_on_pix, depth)
        edge_ccs, input_mesh, edge_mesh = group_edges(input_mesh, config, image, remove_conflict_ordinal=True)
        edge_condition = lambda x, m: m.nodes[x].get('far') is not None and len(m.nodes[x].get('far')) > 0
        edge_map = get_map_from_ccs(edge_ccs, input_mesh.graph['H'], input_mesh.graph['W'], input_mesh, edge_condition)
        other_edge_with_id = get_map_from_ccs(edge_ccs, input_mesh.graph['H'], input_mesh.graph['W'], real_id=True)
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="up")
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="left")
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="down")
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="right")
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="right-up")
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="right-down")
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="left-up")
        info_on_pix, input_mesh, image, depth, edge_ccs = extrapolate(input_mesh, info_on_pix, image, depth, other_edge_with_id, edge_map, edge_ccs,
                                                depth_edge_model, depth_feat_model, rgb_model, config, direc="left-down")
    specific_edge_loc = None
    specific_edge_id = []
    vis_edge_id = None
    context_ccs, mask_ccs, broken_mask_ccs, edge_ccs, erode_context_ccs, \
        init_mask_connect, edge_maps, extend_context_ccs, extend_edge_ccs, extend_erode_context_ccs = \
                                                                                context_and_holes(input_mesh,
                                                                                            edge_ccs,
                                                                                            config,
                                                                                            specific_edge_id,
                                                                                            specific_edge_loc,
                                                                                            depth_feat_model,
                                                                                            inpaint_iter=0,
                                                                                            vis_edge_id=vis_edge_id)
    edge_canvas = np.zeros((H, W))
    mask = np.zeros((H, W))
    context = np.zeros((H, W))
    vis_edge_ccs = filter_edge(input_mesh, edge_ccs, config)
    edge_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W'])) - 1
    specific_edge_loc = None
    FG_edge_maps = edge_maps.copy()
    edge_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W'])) - 1
    # for cc_id, cc in enumerate(edge_ccs):
    #     for node in cc:
    #         edge_canvas[node[0], node[1]] = cc_id
    # f, ((ax0, ax1, ax2)) = plt.subplots(1, 3, sharex=True, sharey=True); ax0.imshow(1./depth); ax1.imshow(image); ax2.imshow(edge_canvas); plt.show()
    input_mesh, info_on_pix, specific_edge_nodes, new_edge_ccs, connect_points_ccs, image = DL_inpaint_edge(input_mesh,
                                                                                                            info_on_pix,
                                                                                                            config,
                                                                                                            image,
                                                                                                            depth,
                                                                                                            context_ccs,
                                                                                                            erode_context_ccs,
                                                                                                            extend_context_ccs,
                                                                                                            extend_erode_context_ccs,
                                                                                                            mask_ccs,
                                                                                                            broken_mask_ccs,
                                                                                                            edge_ccs,
                                                                                                            extend_edge_ccs,
                                                                                                            init_mask_connect,
                                                                                                            edge_maps,
                                                                                                            rgb_model,
                                                                                                            depth_edge_model,
                                                                                                            depth_edge_model_init,
                                                                                                            depth_feat_model,
                                                                                                            specific_edge_id,
                                                                                                            specific_edge_loc,
                                                                                                            inpaint_iter=0,
                                                                                                            generator = generator)
    
    
    specific_edge_id = []
    edge_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W']))
    connect_points_ccs = [set() for _ in connect_points_ccs]
    context_ccs, mask_ccs, broken_mask_ccs, edge_ccs, erode_context_ccs, init_mask_connect, \
        edge_maps, extend_context_ccs, extend_edge_ccs, extend_erode_context_ccs = \
            context_and_holes(input_mesh, new_edge_ccs, config, specific_edge_id, specific_edge_loc, depth_feat_model, connect_points_ccs, inpaint_iter=1)
    mask_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W']))
    context_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W']))
    erode_context_ccs_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W']))
    edge_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W']))
    # edge_canvas = np.zeros((input_mesh.graph['H'], input_mesh.graph['W'])) - 1
    # for cc_id, cc in enumerate(edge_ccs):
    #     for node in cc:
    #         edge_canvas[node[0], node[1]] = cc_id
    specific_edge_id = []
    input_mesh, info_on_pix, specific_edge_nodes, new_edge_ccs, _, image = DL_inpaint_edge(input_mesh,
                                                                                    info_on_pix,
                                                                                    config,
                                                                                    image,
                                                                                    depth,
                                                                                    context_ccs,
                                                                                    erode_context_ccs,
                                                                                    extend_context_ccs,
                                                                                    extend_erode_context_ccs,
                                                                                    mask_ccs,
                                                                                    broken_mask_ccs,
                                                                                    edge_ccs,
                                                                                    extend_edge_ccs,
                                                                                    init_mask_connect,
                                                                                    edge_maps,
                                                                                    rgb_model,
                                                                                    depth_edge_model,
                                                                                    depth_edge_model_init,
                                                                                    depth_feat_model,
                                                                                    specific_edge_id,
                                                                                    specific_edge_loc,
                                                                                    inpaint_iter=1,
                                                                                    generator = generator)
    vertex_id = 0
    input_mesh.graph['H'], input_mesh.graph['W'] = input_mesh.graph['noext_H'], input_mesh.graph['noext_W']
    background_canvas = np.zeros((input_mesh.graph['H'],
                                  input_mesh.graph['W'],
                                  3))
    ply_flag = config.get('save_ply')
    if ply_flag is True:
        node_str_list = []
    else:
        node_str_color = []
        node_str_point = []
    out_fmt = lambda x, x_flag: str(x) if x_flag is True else x
    point_time = 0
    hlight_time = 0
    cur_id_time = 0
    node_str_time = 0
    generate_face_time = 0
    point_list = []
    k_00, k_02, k_11, k_12 = \
        input_mesh.graph['cam_param_pix_inv'][0, 0], input_mesh.graph['cam_param_pix_inv'][0, 2], \
        input_mesh.graph['cam_param_pix_inv'][1, 1], input_mesh.graph['cam_param_pix_inv'][1, 2]
    w_offset = input_mesh.graph['woffset']
    h_offset = input_mesh.graph['hoffset']
    for pix_xy, pix_list in info_on_pix.items():
        for pix_idx, pix_info in enumerate(pix_list):
            pix_depth = pix_info['depth'] if pix_info.get('real_depth') is None else pix_info['real_depth']
            str_pt = [out_fmt(x, ply_flag) for x in reproject_3d_int_detail(pix_xy[0], pix_xy[1], pix_depth,
                      k_00, k_02, k_11, k_12, w_offset, h_offset, H, W)]
            if input_mesh.has_node((pix_xy[0], pix_xy[1], pix_info['depth'])) is False:
                return False
                continue
            if pix_info.get('overlap_number') is not None:
                str_color = [out_fmt(x, ply_flag) for x in (pix_info['color']/pix_info['overlap_number']).astype(np.uint8).tolist()]
            else:
                str_color = [out_fmt(x, ply_flag) for x in pix_info['color'].tolist()]
            # if pix_info.get('edge_occlusion') is True:
            #     str_color.append(out_fmt(4, ply_flag))
            # else:
            #     if pix_info.get('inpaint_id') is None:
            #         str_color.append(out_fmt(1, ply_flag))
            #     else:
            #         str_color.append(out_fmt(pix_info.get('inpaint_id') + 1, ply_flag))
            if pix_info.get('modified_border') is True or pix_info.get('ext_pixel') is True:
                if len(str_color) == 4:
                    str_color[-1] = out_fmt(5, ply_flag)
                else:
                    str_color.append(out_fmt(5, ply_flag))
            pix_info['cur_id'] = vertex_id
            input_mesh.nodes[(pix_xy[0], pix_xy[1], pix_info['depth'])]['cur_id'] = out_fmt(vertex_id, ply_flag)
            vertex_id += 1
            if ply_flag is True:
                node_str_list.append(' '.join(str_pt) + ' ' + ' '.join(str_color) + ' ' + str(pix_idx) + ' ' + str(pix_xy[0]) + ' ' + str(pix_xy[1]) + '\n')
            else:
                node_str_color.append(str_color)
                node_str_point.append(str_pt)
    str_faces = generate_face(input_mesh, info_on_pix, config)
    if config['save_ply'] is True:
        print("Writing mesh file %s ..." % ply_name)
        with open(ply_name, 'w') as ply_fi:
            ply_fi.write('ply\n' + 'format ascii 1.0\n')
            ply_fi.write('comment H ' + str(int(input_mesh.graph['H'])) + '\n')
            ply_fi.write('comment W ' + str(int(input_mesh.graph['W'])) + '\n')
            ply_fi.write('comment hFov ' + str(float(input_mesh.graph['hFov'])) + '\n')
            ply_fi.write('comment vFov ' + str(float(input_mesh.graph['vFov'])) + '\n')
            ply_fi.write('element vertex ' + str(len(node_str_list)) + '\n')
            ply_fi.write('property float x\n' + \
                         'property float y\n' + \
                         'property float z\n' + \
                         'property uchar red\n' + \
                         'property uchar green\n' + \
                         'property uchar blue\n' + \
                         'property int index\n' + \
                         'property int pix_x\n' + \
                         'property int pix_y\n') 
                        #  + \'property uchar alpha\n')
            ply_fi.write('element face ' + str(len(str_faces)) + '\n')
            ply_fi.write('property list uchar int vertex_index\n')
            ply_fi.write('end_header\n')
            ply_fi.writelines(node_str_list)
            ply_fi.writelines(str_faces)
        ply_fi.close()
        return input_mesh
    else:
        H = int(input_mesh.graph['H'])
        W = int(input_mesh.graph['W'])
        hFov = input_mesh.graph['hFov']
        vFov = input_mesh.graph['vFov']
        node_str_color = np.array(node_str_color).astype(np.float32)
        node_str_color[..., :3] = node_str_color[..., :3] / 255.
        node_str_point = np.array(node_str_point)
        str_faces = np.array(str_faces)

        return node_str_point, node_str_color, str_faces, H, W, hFov, vFov

def read_ply(mesh_fi):
    ply_fi = open(mesh_fi, 'r')
    Height = None
    Width = None
    hFov = None
    vFov = None
    while True:
        line = ply_fi.readline().split('\n')[0]
        if line.startswith('element vertex'):
            num_vertex = int(line.split(' ')[-1])
        elif line.startswith('element face'):
            num_face = int(line.split(' ')[-1])
        elif line.startswith('comment'):
            if line.split(' ')[1] == 'H':
                Height = int(line.split(' ')[-1].split('\n')[0])
            if line.split(' ')[1] == 'W':
                Width = int(line.split(' ')[-1].split('\n')[0])
            if line.split(' ')[1] == 'hFov':
                hFov = float(line.split(' ')[-1].split('\n')[0])
            if line.split(' ')[1] == 'vFov':
                vFov = float(line.split(' ')[-1].split('\n')[0])
        elif line.startswith('end_header'):
            break
    contents = ply_fi.readlines()
    vertex_infos = contents[:num_vertex]
    face_infos = contents[num_vertex:]
    verts = []
    colors = []
    faces = []
    for v_info in vertex_infos:
        str_info = [float(v) for v in v_info.split('\n')[0].split(' ')]
        if len(str_info) == 6:
            vx, vy, vz, r, g, b = str_info
            colors.append([r, g, b])
        else:
            vx, vy, vz, r, g, b, hi = str_info
            colors.append([r, g, b, hi])
        verts.append([vx, vy, vz])
    verts = np.array(verts)
    try:
        colors = np.array(colors)
        colors[..., :3] = colors[..., :3]/255.
    except:
        import pdb
        pdb.set_trace()

    for f_info in face_infos:
        _, v1, v2, v3 = [int(f) for f in f_info.split('\n')[0].split(' ')]
        faces.append([v1, v2, v3])
    faces = np.array(faces)


    return verts, colors, faces, Height, Width, hFov, vFov


class Canvas_view():
    def __init__(self,
                 fov,
                 verts,
                 faces,
                 colors,
                 canvas_size,
                 factor=1,
                 bgcolor='gray',
                 proj='perspective',
                 ):
        self.canvas = scene.SceneCanvas(bgcolor=bgcolor, size=(canvas_size*factor, canvas_size*factor))
        self.view = self.canvas.central_widget.add_view()
        self.view.camera = 'perspective'
        self.view.camera.fov = fov
        self.mesh = visuals.Mesh(shading=None)
        self.mesh.attach(Alpha(1.0))
        self.view.add(self.mesh)
        self.tr = self.view.camera.transform
        self.mesh.set_data(vertices=verts, faces=faces, vertex_colors=colors[:, :3])
        self.translate([0,0,0])
        self.rotate(axis=[1,0,0], angle=180)
        self.view_changed()

    def translate(self, trans=[0,0,0]):
        self.tr.translate(trans)

    def rotate(self, axis=[1,0,0], angle=0):
        self.tr.rotate(axis=axis, angle=angle)

    def view_changed(self):
        self.view.camera.view_changed()

    def render(self):
        return self.canvas.render()

    def reinit_mesh(self, verts, faces, colors):
        self.mesh.set_data(vertices=verts, faces=faces, vertex_colors=colors[:, :3])

    def reinit_camera(self, fov):
        self.view.camera.fov = fov
        self.view.camera.view_changed()


def output_3d_photo(verts, colors, faces, Height, Width, hFov, vFov, tgt_poses, video_traj_types, ref_pose,
                    output_dir, ref_image, int_mtx, config, image, videos_poses, video_basename, original_H=None, original_W=None,
                    border=None, depth=None, normal_canvas=None, all_canvas=None, mean_loc_depth=None):

    cam_mesh = netx.Graph()
    cam_mesh.graph['H'] = Height
    cam_mesh.graph['W'] = Width
    cam_mesh.graph['original_H'] = original_H
    cam_mesh.graph['original_W'] = original_W
    int_mtx_real_x = int_mtx[0] * Width
    int_mtx_real_y = int_mtx[1] * Height
    cam_mesh.graph['hFov'] = 2 * np.arctan((1. / 2.) * ((cam_mesh.graph['original_W']) / int_mtx_real_x[0]))
    cam_mesh.graph['vFov'] = 2 * np.arctan((1. / 2.) * ((cam_mesh.graph['original_H']) / int_mtx_real_y[1]))
    colors = colors[..., :3]

    fov_in_rad = max(cam_mesh.graph['vFov'], cam_mesh.graph['hFov'])
    fov = (fov_in_rad * 180 / np.pi)
    print("fov: " + str(fov))
    init_factor = 1
    if config.get('anti_flickering') is True:
        init_factor = 3
    if (cam_mesh.graph['original_H'] is not None) and (cam_mesh.graph['original_W'] is not None):
        canvas_w = cam_mesh.graph['original_W']
        canvas_h = cam_mesh.graph['original_H']
    else:
        canvas_w = cam_mesh.graph['W']
        canvas_h = cam_mesh.graph['H']
    canvas_size = max(canvas_h, canvas_w)
    if normal_canvas is None:
        normal_canvas = Canvas_view(fov,
                                    verts,
                                    faces,
                                    colors,
                                    canvas_size=canvas_size,
                                    factor=init_factor,
                                    bgcolor='gray',
                                    proj='perspective')
    else:
        normal_canvas.reinit_mesh(verts, faces, colors)
        normal_canvas.reinit_camera(fov)
    img = normal_canvas.render()
    backup_img, backup_all_img, all_img_wo_bound = img.copy(), img.copy() * 0, img.copy() * 0
    img = cv2.resize(img, (int(img.shape[1] / init_factor), int(img.shape[0] / init_factor)), interpolation=cv2.INTER_AREA)
    if border is None:
        border = [0, img.shape[0], 0, img.shape[1]]
    H, W = cam_mesh.graph['H'], cam_mesh.graph['W']
    if (cam_mesh.graph['original_H'] is not None) and (cam_mesh.graph['original_W'] is not None):
        aspect_ratio = cam_mesh.graph['original_H'] / cam_mesh.graph['original_W']
    else:
        aspect_ratio = cam_mesh.graph['H'] / cam_mesh.graph['W']
    if aspect_ratio > 1:
        img_h_len = cam_mesh.graph['H'] if cam_mesh.graph.get('original_H') is None else cam_mesh.graph['original_H']
        img_w_len = img_h_len / aspect_ratio
        anchor = [0,
                  img.shape[0],
                  int(max(0, int((img.shape[1])//2 - img_w_len//2))),
                  int(min(int((img.shape[1])//2 + img_w_len//2), (img.shape[1])-1))]
    elif aspect_ratio <= 1:
        img_w_len = cam_mesh.graph['W'] if cam_mesh.graph.get('original_W') is None else cam_mesh.graph['original_W']
        img_h_len = img_w_len * aspect_ratio
        anchor = [int(max(0, int((img.shape[0])//2 - img_h_len//2))),
                  int(min(int((img.shape[0])//2 + img_h_len//2), (img.shape[0])-1)),
                  0,
                  img.shape[1]]
    anchor = np.array(anchor)
    plane_width = np.tan(fov_in_rad/2.) * np.abs(mean_loc_depth)
    for video_pose, video_traj_type in zip(videos_poses, video_traj_types):
        stereos = []
        tops = []; buttoms = []; lefts = []; rights = []
        for tp_id, tp in enumerate(video_pose):
            rel_pose = np.linalg.inv(np.dot(tp, np.linalg.inv(ref_pose)))
            axis, angle = transforms3d.axangles.mat2axangle(rel_pose[0:3, 0:3])
            normal_canvas.rotate(axis=axis, angle=(angle*180)/np.pi)
            normal_canvas.translate(rel_pose[:3,3])
            new_mean_loc_depth = mean_loc_depth - float(rel_pose[2, 3])
            if 'dolly' in video_traj_type:
                new_fov = float((np.arctan2(plane_width, np.array([np.abs(new_mean_loc_depth)])) * 180. / np.pi) * 2)
                normal_canvas.reinit_camera(new_fov)
            else:
                normal_canvas.reinit_camera(fov)
            normal_canvas.view_changed()
            img = normal_canvas.render()
            img = cv2.GaussianBlur(img,(int(init_factor//2 * 2 + 1), int(init_factor//2 * 2 + 1)), 0)
            img = cv2.resize(img, (int(img.shape[1] / init_factor), int(img.shape[0] / init_factor)), interpolation=cv2.INTER_AREA)
            img = img[anchor[0]:anchor[1], anchor[2]:anchor[3]]
            img = img[int(border[0]):int(border[1]), int(border[2]):int(border[3])]

            if any(np.array(config['crop_border']) > 0.0):
                H_c, W_c, _ = img.shape
                o_t = int(H_c * config['crop_border'][0])
                o_l = int(W_c * config['crop_border'][1])
                o_b = int(H_c * config['crop_border'][2])
                o_r = int(W_c * config['crop_border'][3])
                img = img[o_t:H_c-o_b, o_l:W_c-o_r]
                img = cv2.resize(img, (W_c, H_c), interpolation=cv2.INTER_CUBIC)

            """
            img = cv2.resize(img, (int(img.shape[1] / init_factor), int(img.shape[0] / init_factor)), interpolation=cv2.INTER_CUBIC)
            img = img[anchor[0]:anchor[1], anchor[2]:anchor[3]]
            img = img[int(border[0]):int(border[1]), int(border[2]):int(border[3])]

            if config['crop_border'] is True:
                top, buttom, left, right = find_largest_rect(img, bg_color=(128, 128, 128))
                tops.append(top); buttoms.append(buttom); lefts.append(left); rights.append(right)
            """
            stereos.append(img[..., :3])
            normal_canvas.translate(-rel_pose[:3,3])
            normal_canvas.rotate(axis=axis, angle=-(angle*180)/np.pi)
            normal_canvas.view_changed()
        """
        if config['crop_border'] is True:
            atop, abuttom = min(max(tops), img.shape[0]//2 - 10), max(min(buttoms), img.shape[0]//2 + 10)
            aleft, aright = min(max(lefts), img.shape[1]//2 - 10), max(min(rights), img.shape[1]//2 + 10)
            atop -= atop % 2; abuttom -= abuttom % 2; aleft -= aleft % 2; aright -= aright % 2
        else:
            atop = 0; abuttom = img.shape[0] - img.shape[0] % 2; aleft = 0; aright = img.shape[1] - img.shape[1] % 2
        """
        atop = 0; abuttom = img.shape[0] - img.shape[0] % 2; aleft = 0; aright = img.shape[1] - img.shape[1] % 2
        crop_stereos = []
        for stereo in stereos:
            crop_stereos.append((stereo[atop:abuttom, aleft:aright, :3] * 1).astype(np.uint8))
            stereos = crop_stereos
        clip = ImageSequenceClip(stereos, fps=config['fps'])
        if isinstance(video_basename, list):
            video_basename = video_basename[0]
        clip.write_videofile(os.path.join(output_dir, video_basename + '_' + video_traj_type + '.mp4'), fps=config['fps'])



    return normal_canvas, all_canvas



================================================
FILE: inpainting/mesh_tools.py
================================================
import os
import numpy as np
try:
    import cynetworkx as netx
except ImportError:
    import networkx as netx

import json
import scipy.misc as misc
#import OpenEXR
import scipy.signal as signal
import matplotlib.pyplot as plt
import cv2
import scipy.misc as misc
from skimage import io
from functools import partial
#from vispy import scene, io
#from vispy.scene import visuals
from functools import reduce
# from moviepy.editor import ImageSequenceClip
import scipy.misc as misc
#from vispy.visuals.filters import Alpha
import cv2
from skimage.transform import resize
import copy
import torch
import os
from utils import refine_depth_around_edge, smooth_cntsyn_gap
from utils import require_depth_edge, filter_irrelevant_edge_new, open_small_mask
from skimage.feature import canny
from scipy import ndimage
import time
import transforms3d

def relabel_node(mesh, nodes, cur_node, new_node):
    if cur_node == new_node:
        return mesh
    mesh.add_node(new_node)
    for key, value in nodes[cur_node].items():
        nodes[new_node][key] = value
    for ne in mesh.neighbors(cur_node):
        mesh.add_edge(new_node, ne)
    mesh.remove_node(cur_node)

    return mesh

def filter_edge(mesh, edge_ccs, config, invalid=False):
    context_ccs = [set() for _ in edge_ccs]
    mesh_nodes = mesh.nodes
    for edge_id, edge_cc in enumerate(edge_ccs):
        if config['context_thickness'] == 0:
            continue
        edge_group = {}
        for edge_node in edge_cc:
            far_nodes = mesh_nodes[edge_node].get('far')
            if far_nodes is None:
                continue
            for far_node in far_nodes:
                context_ccs[edge_id].add(far_node)
                if mesh_nodes[far_node].get('edge_id') is not None:
                    if edge_group.get(mesh_nodes[far_node]['edge_id']) is None:
                        edge_group[mesh_nodes[far_node]['edge_id']] = set()
                    edge_group[mesh_nodes[far_node]['edge_id']].add(far_node)
        if len(edge_cc) > 2:
            for edge_key in [*edge_group.keys()]:
                if len(edge_group[edge_key]) == 1:
                    context_ccs[edge_id].remove([*edge_group[edge_key]][0])
    valid_edge_ccs = []
    for xidx, yy in enumerate(edge_ccs):
        if invalid is not True and len(context_ccs[xidx]) > 0:
            # if len(context_ccs[xidx]) > 0:
            valid_edge_ccs.append(yy)
        elif invalid is True and len(context_ccs[xidx]) == 0:
            valid_edge_ccs.append(yy)
        else:
            valid_edge_ccs.append(set())
    # valid_edge_ccs = [yy for xidx, yy in enumerate(edge_ccs) if len(context_ccs[xidx]) > 0]

    return valid_edge_ccs

def extrapolate(global_mesh,
                info_on_pix,
                image,
                depth,
                other_edge_with_id,
                edge_map,
                edge_ccs,
                depth_edge_model,
                depth_feat_model,
                rgb_feat_model,
                config,
                direc='right-up'):
    h_off, w_off = global_mesh.graph['hoffset'], global_mesh.graph['woffset']
    noext_H, noext_W = global_mesh.graph['noext_H'], global_mesh.graph['noext_W']

    if "up" in direc.lower() and "-" not in direc.lower():
        all_anchor = [0, h_off + config['context_thickness'], w_off, w_off + noext_W]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [0, h_off, w_off, w_off + noext_W]
        context_anchor = [h_off, h_off + config['context_thickness'], w_off, w_off + noext_W]
        valid_line_anchor = [h_off, h_off + 1, w_off, w_off + noext_W]
        valid_anchor = [min(mask_anchor[0], context_anchor[0]), max(mask_anchor[1], context_anchor[1]),
                        min(mask_anchor[2], context_anchor[2]), max(mask_anchor[3], context_anchor[3])]
    elif "down" in direc.lower() and "-" not in direc.lower():
        all_anchor = [h_off + noext_H - config['context_thickness'], 2 * h_off + noext_H, w_off, w_off + noext_W]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [h_off + noext_H, 2 * h_off + noext_H, w_off, w_off + noext_W]
        context_anchor = [h_off + noext_H - config['context_thickness'], h_off + noext_H, w_off, w_off + noext_W]
        valid_line_anchor = [h_off + noext_H - 1, h_off + noext_H, w_off, w_off + noext_W]
        valid_anchor = [min(mask_anchor[0], context_anchor[0]), max(mask_anchor[1], context_anchor[1]),
                        min(mask_anchor[2], context_anchor[2]), max(mask_anchor[3], context_anchor[3])]
    elif "left" in direc.lower() and "-" not in direc.lower():
        all_anchor = [h_off, h_off + noext_H, 0, w_off + config['context_thickness']]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [h_off, h_off + noext_H, 0, w_off]
        context_anchor = [h_off, h_off + noext_H, w_off, w_off + config['context_thickness']]
        valid_line_anchor = [h_off, h_off + noext_H, w_off, w_off + 1]
        valid_anchor = [min(mask_anchor[0], context_anchor[0]), max(mask_anchor[1], context_anchor[1]),
                        min(mask_anchor[2], context_anchor[2]), max(mask_anchor[3], context_anchor[3])]
    elif "right" in direc.lower() and "-" not in direc.lower():
        all_anchor = [h_off, h_off + noext_H, w_off + noext_W - config['context_thickness'], 2 * w_off + noext_W]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [h_off, h_off + noext_H, w_off + noext_W, 2 * w_off + noext_W]
        context_anchor = [h_off, h_off + noext_H, w_off + noext_W - config['context_thickness'], w_off + noext_W]
        valid_line_anchor = [h_off, h_off + noext_H, w_off + noext_W - 1, w_off + noext_W]
        valid_anchor = [min(mask_anchor[0], context_anchor[0]), max(mask_anchor[1], context_anchor[1]),
                        min(mask_anchor[2], context_anchor[2]), max(mask_anchor[3], context_anchor[3])]
    elif "left" in direc.lower() and "up" in direc.lower() and "-" in direc.lower():
        all_anchor = [0, h_off + config['context_thickness'], 0, w_off + config['context_thickness']]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [0, h_off, 0, w_off]
        context_anchor = "inv-mask"
        valid_line_anchor = None
        valid_anchor = all_anchor
    elif "left" in direc.lower() and "down" in direc.lower() and "-" in direc.lower():
        all_anchor = [h_off + noext_H - config['context_thickness'], 2 * h_off + noext_H, 0, w_off + config['context_thickness']]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [h_off + noext_H, 2 * h_off + noext_H, 0, w_off]
        context_anchor = "inv-mask"
        valid_line_anchor = None
        valid_anchor = all_anchor
    elif "right" in direc.lower() and "up" in direc.lower() and "-" in direc.lower():
        all_anchor = [0, h_off + config['context_thickness'], w_off + noext_W - config['context_thickness'], 2 * w_off + noext_W]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [0, h_off, w_off + noext_W, 2 * w_off + noext_W]
        context_anchor = "inv-mask"
        valid_line_anchor = None
        valid_anchor = all_anchor
    elif "right" in direc.lower() and "down" in direc.lower() and "-" in direc.lower():
        all_anchor = [h_off + noext_H - config['context_thickness'], 2 * h_off + noext_H, w_off + noext_W - config['context_thickness'], 2 * w_off + noext_W]
        global_shift = [all_anchor[0], all_anchor[2]]
        mask_anchor = [h_off + noext_H, 2 * h_off + noext_H, w_off + noext_W, 2 * w_off + noext_W]
        context_anchor = "inv-mask"
        valid_line_anchor = None
        valid_anchor = all_anchor

    global_mask = np.zeros_like(depth)
    global_mask[mask_anchor[0]:mask_anchor[1],mask_anchor[2]:mask_anchor[3]] = 1
    mask = global_mask[valid_anchor[0]:valid_anchor[1], valid_anchor[2]:valid_anchor[3]] * 1
    context = 1 - mask
    global_context = np.zeros_like(depth)
    global_context[all_anchor[0]:all_anchor[1],all_anchor[2]:all_anchor[3]] = context
    # context = global_context[valid_anchor[0]:valid_anchor[1], valid_anchor[2]:valid_anchor[3]] * 1



    valid_area = mask + context
    input_rgb = image[valid_anchor[0]:valid_anchor[1], valid_anchor[2]:valid_anchor[3]] / 255. * context[..., None]
    input_depth = depth[valid_anchor[0]:valid_anchor[1], valid_anchor[2]:valid_anchor[3]] * context
    log_depth = np.log(input_depth + 1e-8)
    log_depth[mask > 0] = 0
    input_mean_depth = np.mean(log_depth[context > 0])
    input_zero_mean_depth = (log_depth - input_mean_depth) * context
    input_disp = 1./np.abs(input_depth)
    input_disp[mask > 0] = 0
    input_disp = input_disp / input_disp.max()
    valid_line = np.zeros_like(depth)
    if valid_line_anchor is not None:
        valid_line[valid_line_anchor[0]:valid_line_anchor[1], valid_line_anchor[2]:valid_line_anchor[3]] = 1
    valid_line = valid_line[all_anchor[0]:all_anchor[1], all_anchor[2]:all_anchor[3]]
    # f, ((ax1, ax2)) = plt.subplots(1, 2, sharex=True, sharey=True); ax1.imshow(global_context * 1 + global_mask * 2); ax2.imshow(image); plt.show()
    # f, ((ax1, ax2, ax3)) = plt.subplots(1, 3, sharex=True, sharey=True); ax1.imshow(context * 1 + mask * 2); ax2.imshow(input_rgb); ax3.imshow(valid_line); plt.show()
    # import pdb; pdb.set_trace()
    # return
    input_edge_map = edge_map[all_anchor[0]:all_anchor[1], all_anchor[2]:all_anchor[3]] * context
    input_other_edge_with_id = other_edge_with_id[all_anchor[0]:all_anchor[1], all_anchor[2]:all_anchor[3]]
    end_depth_maps = ((valid_line * input_edge_map) > 0) * input_depth


    if isinstance(config["gpu_ids"], int) and (config["gpu_ids"] >= 0):
        device = config["gpu_ids"]
    else:
        device = "cpu"

    valid_edge_ids = sorted(list(input_other_edge_with_id[(valid_line * input_edge_map) > 0]))
    valid_edge_ids = valid_edge_ids[1:] if (len(valid_edge_ids) > 0 and valid_edge_ids[0] == -1) else valid_edge_ids
    edge = reduce(lambda x, y: (x + (input_other_edge_with_id == y).astype(np.uint8)).clip(0, 1), [np.zeros_like(mask)] + list(valid_edge_ids))
    t_edge = torch.FloatTensor(edge).to(device)[None, None, ...]
    t_rgb = torch.FloatTensor(input_rgb).to(device).permute(2,0,1).unsqueeze(0)
    t_mask = torch.FloatTensor(mask).to(device)[None, None, ...]
    t_context = torch.FloatTensor(context).to(device)[None, None, ...]
    t_disp = torch.FloatTensor(input_disp).to(device)[None, None, ...]
    t_depth_zero_mean_depth = torch.FloatTensor(input_zero_mean_depth).to(device)[None, None, ...]

    depth_edge_output = depth_edge_model.forward_3P(t_mask, t_context, t_rgb, t_disp, t_edge, unit_length=128,
                                                    cuda=device)
    t_output_edge = (depth_edge_output> config['ext_edge_threshold']).float() * t_mask + t_edge
    output_raw_edge = t_output_edge.data.cpu().numpy().squeeze()
    # import pdb; pdb.set_trace()
    mesh = netx.Graph()
    hxs, hys = np.where(output_raw_edge * mask > 0)
    valid_map = mask + context
    for hx, hy in zip(hxs, hys):
        node = (hx, hy)
        mesh.add_node((hx, hy))
        eight_nes = [ne for ne in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1), \
                                   (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)]\
                        if 0 <= ne[0] < output_raw_edge.shape[0] and 0 <= ne[1] < output_raw_edge.shape[1] and 0 < output_raw_edge[ne[0], ne[1]]]
        for ne in eight_nes:
            mesh.add_edge(node, ne, length=np.hypot(ne[0] - hx, ne[1] - hy))
            if end_depth_maps[ne[0], ne[1]] != 0:
                mesh.nodes[ne[0], ne[1]]['cnt'] = True
                mesh.nodes[ne[0], ne[1]]['depth'] = end_depth_maps[ne[0], ne[1]]
    ccs = [*netx.connected_components(mesh)]
    end_pts = []
    for cc in ccs:
        end_pts.append(set())
        for node in cc:
            if mesh.nodes[node].get('cnt') is not None:
                end_pts[-1].add((node[0], node[1], mesh.nodes[node]['depth']))
    fpath_map = np.zeros_like(output_raw_edge) - 1
    npath_map = np.zeros_like(output_raw_edge) - 1
    for end_pt, cc in zip(end_pts, ccs):
        sorted_end_pt = []
        if len(end_pt) >= 2:
            continue
        if len(end_pt) == 0:
            continue
        if len(end_pt) == 1:
            sub_mesh = mesh.subgraph(list(cc)).copy()
            pnodes = netx.periphery(sub_mesh)
            ends = [*end_pt]
            edge_id = global_mesh.nodes[(ends[0][0] + all_anchor[0], ends[0][1] + all_anchor[2], -ends[0][2])]['edge_id']
            pnodes = sorted(pnodes,
                            key=lambda x: np.hypot((x[0] - ends[0][0]), (x[1] - ends[0][1])),
                            reverse=True)[0]
            npath = [*netx.shortest_path(sub_mesh, (ends[0][0], ends[0][1]), pnodes, weight='length')]
            for np_node in npath:
                npath_map[np_node[0], np_node[1]] = edge_id
            fpath = []
            if global_mesh.nodes[(ends[0][0] + all_anchor[0], ends[0][1] + all_anchor[2], -ends[0][2])].get('far') is None:
                print("None far")
                import pdb; pdb.set_trace()
            else:
                fnodes = global_mesh.nodes[(ends[0][0] + all_anchor[0], ends[0][1] + all_anchor[2], -ends[0][2])].get('far')
                fnodes = [(xx[0] - all_anchor[0], xx[1] - all_anchor[2], xx[2]) for xx in fnodes]
                dmask = mask + 0
                did = 0
                while True:
                    did += 1
                    dmask = cv2.dilate(dmask, np.ones((3, 3)), iterations=1)
                    if did > 3:
                        break
                    # ffnode = [fnode for fnode in fnodes if (dmask[fnode[0], fnode[1]] > 0)]
                    ffnode = [fnode for fnode in fnodes if (dmask[fnode[0], fnode[1]] > 0 and mask[fnode[0], fnode[1]] == 0)]
                    if len(ffnode) > 0:
                        fnode = ffnode[0]
                        break
                if len(ffnode) == 0:
                    continue
                fpath.append((fnode[0], fnode[1]))
                for step in range(0, len(npath) - 1):
                    parr = (npath[step + 1][0] - npath[step][0], npath[step + 1][1] - npath[step][1])
                    new_loc = (fpath[-1][0] + parr[0], fpath[-1][1] + parr[1])
                    new_loc_nes = [xx for xx in [(new_loc[0] + 1, new_loc[1]), (new_loc[0] - 1, new_loc[1]),
                                                (new_loc[0], new_loc[1] + 1), (new_loc[0], new_loc[1] - 1)]\
                                        if xx[0] >= 0 and xx[0] < fpath_map.shape[0] and xx[1] >= 0 and xx[1] < fpath_map.shape[1]]
                    if np.sum([fpath_map[nlne[0], nlne[1]] for nlne in new_loc_nes]) != -4:
                        break
                    if npath_map[new_loc[0], new_loc[1]] != -1:
                        if npath_map[new_loc[0], new_loc[1]] != edge_id:
                            break
                        else:
                            continue
                    if valid_area[new_loc[0], new_loc[1]] == 0:
                        break
                    new_loc_nes_eight = [xx for xx in [(new_loc[0] + 1, new_loc[1]), (new_loc[0] - 1, new_loc[1]),
                                                        (new_loc[0], new_loc[1] + 1), (new_loc[0], new_loc[1] - 1),
                                                        (new_loc[0] + 1, new_loc[1] + 1), (new_loc[0] + 1, new_loc[1] - 1),
                                                        (new_loc[0] - 1, new_loc[1] - 1), (new_loc[0] - 1, new_loc[1] + 1)]\
                                        if xx[0] >= 0 and xx[0] < fpath_map.shape[0] and xx[1] >= 0 and xx[1] < fpath_map.shape[1]]
                    if np.sum([int(npath_map[nlne[0], nlne[1]] == edge_id) for nlne in new_loc_nes_eight]) == 0:
                        break
                    fpath.append((fpath[-1][0] + parr[0], fpath[-1][1] + parr[1]))
                if step != len(npath) - 2:
                    for xx in npath[step+1:]:
                        if npath_map[xx[0], xx[1]] == edge_id:
                            npath_map[xx[0], xx[1]] = -1
            if len(fpath) > 0:
                for fp_node in fpath:
                    fpath_map[fp_node[0], fp_node[1]] = edge_id
    # import pdb; pdb.set_trace()
    far_edge = (fpath_map > -1).astype(np.uint8)
    update_edge = (npath_map > -1) * mask + edge
    t_update_edge = torch.FloatTensor(update_edge).to(device)[None, None, ...]
    depth_output = depth_feat_model.forward_3P(t_mask, t_context, t_depth_zero_mean_depth, t_update_edge, unit_length=128,
                                               cuda=device)
    depth_output = depth_output.cpu().data.numpy().squeeze()
    depth_output = np.exp(depth_output + input_mean_depth) * mask # + input_depth * context
    # if "right" in direc.lower() and "-" not in direc.lower():
    #     plt.imshow(depth_output); plt.show()
    #     import pdb; pdb.set_trace()
    #     f, ((ax1, ax2)) = plt.subplots(1, 2, sharex=True, sharey=True); ax1.imshow(depth_output); ax2.imshow(npath_map + fpath_map); plt.show()
    for near_id in np.unique(npath_map[npath_map > -1]):
        depth_output = refine_depth_around_edge(depth_output.copy(),
                                                (fpath_map == near_id).astype(np.uint8) * mask, # far_edge_map_in_mask,
                                                (fpath_map == near_id).astype(np.uint8), # far_edge_map,
                                                (npath_map == near_id).astype(np.uint8) * mask,
                                                mask.copy(),
                                                np.zeros_like(mask),
                                                config)
    # if "right" in direc.lower() and "-" not in direc.lower():
    #     plt.imshow(depth_output); plt.show()
    #     import pdb; pdb.set_trace()
    #     f, ((ax1, ax2)) = plt.subplots(1, 2, sharex=True, sharey=True); ax1.imshow(depth_output); ax2.imshow(npath_map + fpath_map); plt.show()
    rgb_output = rgb_feat_model.forward_3P(t_mask, t_context, t_rgb, t_update_edge, unit_length=128,
                                           cuda=device)

    # rgb_output = rgb_feat_model.forward_3P(t_mask, t_context, t_rgb, t_update_edge, unit_length=128, cuda=config['gpu_ids'])
    if config.get('gray_image') is True:
        rgb_output = rgb_output.mean(1, keepdim=True).repeat((1,3,1,1))
    rgb_output = ((rgb_output.squeeze().data.cpu().permute(1,2,0).numpy() * mask[..., None] + input_rgb) * 255).astype(np.uint8)
    image[all_anchor[0]:all_anchor[1], all_anchor[2]:all_anchor[3]][mask > 0] = rgb_output[mask > 0] # np.array([255,0,0]) # rgb_output[mask > 0]
    depth[all_anchor[0]:all_anchor[1], all_anchor[2]:all_anchor[3]][mask > 0] = depth_output[mask > 0]
    # nxs, nys = np.where(mask > -1)
    # for nx, ny in zip(nxs, nys):
    #     info_on_pix[(nx, ny)][0]['color'] = rgb_output[]


    nxs, nys = np.where((npath_map > -1))
    for nx, ny in zip(nxs, nys):
        n_id = npath_map[nx, ny]
        four_nes = [xx for xx in [(nx + 1, ny), (nx - 1, ny), (nx, ny + 1), (nx, ny - 1)]\
                        if 0 <= xx[0] < fpath_map.shape[0] and 0 <= xx[1] < fpath_map.shape[1]]
        for nex, ney in four_nes:
            if fpath_map[nex, ney] == n_id:
                na, nb = (nx + all_anchor[0], ny + all_anchor[2], info_on_pix[(nx + all_anchor[0], ny + all_anchor[2])][0]['depth']), \
                        (nex + all_anchor[0], ney + all_anchor[2], info_on_pix[(nex + all_anchor[0], ney + all_anchor[2])][0]['depth'])
                if global_mesh.has_edge(na, nb):
                    global_mesh.remove_edge(na, nb)
    nxs, nys = np.where((fpath_map > -1))
    for nx, ny in zip(nxs, nys):
        n_id = fpath_map[nx, ny]
        four_nes = [xx for xx in [(nx + 1, ny), (nx - 1, ny), (nx, ny + 1), (nx, ny - 1)]\
                        if 0 <= xx[0] < npath_map.shape[0] and 0 <= xx[1] < npath_map.shape[1]]
        for nex, ney in four_nes:
            if npath_map[nex, ney] == n_id:
                na, nb = (nx + all_anchor[0], ny + all_anchor[2], info_on_pix[(nx + all_anchor[0], ny + all_anchor[2])][0]['depth']), \
                        (nex + all_anchor[0], ney + all_anchor[2], info_on_pix[(nex + all_anchor[0], ney + all_anchor[2])][0]['depth'])
                if global_mesh.has_edge(na, nb):
                    global_mesh.remove_edge(na, nb)
    nxs, nys = np.where(mask > 0)
    for x, y in zip(nxs, nys):
        x = x + all_anchor[0]
        y = y + all_anchor[2]
        cur_node = (x, y, 0)
        new_node = (x, y, -abs(depth[x, y]))
        disp = 1. / -abs(depth[x, y])
        mapping_dict = {cur_node: new_node}
        info_on_pix, global_mesh = update_info(mapping_dict, info_on_pix, global_mesh)
        global_mesh.nodes[new_node]['color'] = image[x, y]
        global_mesh.nodes[new_node]['old_color'] = image[x, y]
        global_mesh.nodes[new_node]['disp'] = disp
        info_on_pix[(x, y)][0]['depth'] = -abs(depth[x, y])
        info_on_pix[(x, y)][0]['disp'] = disp
        info_on_pix[(x, y)][0]['color'] = image[x, y]


    nxs, nys = np.where((npath_map > -1))
    for nx, ny in zip(nxs, nys):
        self_node = (nx + all_anchor[0], ny + all_anchor[2], info_on_pix[(nx + all_anchor[0], ny + all_anchor[2])][0]['depth'])
        if global_mesh.has_node(self_node) is False:
            break
        n_id = int(round(npath_map[nx, ny]))
        four_nes = [xx for xx in [(nx + 1, ny), (nx - 1, ny), (nx, ny + 1), (nx, ny - 1)]\
                        if 0 <= xx[0] < fpath_map.shape[0] and 0 <= xx[1] < fpath_map.shape[1]]
        for nex, ney in four_nes:
            ne_node = (nex + all_anchor[0], ney + all_anchor[2], info_on_pix[(nex + all_anchor[0], ney + all_anchor[2])][0]['depth'])
            if global_mesh.has_node(ne_node) is False:
                continue
            if fpath_map[nex, ney] == n_id:
                if global_mesh.nodes[self_node].get('edge_id') is None:
                    global_mesh.nodes[self_node]['edge_id'] = n_id
                    edge_ccs[n_id].add(self_node)
                    info_on_pix[(self_node[0], self_node[1])][0]['edge_id'] = n_id
                if global_mesh.has_edge(self_node, ne_node) is True:
                    global_mesh.remove_edge(self_node, ne_node)
                if global_mesh.nodes[self_node].get('far') is None:
                    global_mesh.nodes[self_node]['far'] = []
                global_mesh.nodes[self_node]['far'].append(ne_node)

    global_fpath_map = np.zeros_like(other_edge_with_id) - 1
    global_fpath_map[all_anchor[0]:all_anchor[1], all_anchor[2]:all_anchor[3]] = fpath_map
    fpath_ids = np.unique(global_fpath_map)
    fpath_ids = fpath_ids[1:] if fpath_ids.shape[0] > 0 and fpath_ids[0] == -1 else []
    fpath_real_id_map = np.zeros_like(global_fpath_map) - 1
    for fpath_id in fpath_ids:
        fpath_real_id = np.unique(((global_fpath_map == fpath_id).astype(np.int) * (other_edge_with_id + 1)) - 1)
        fpath_real_id = fpath_real_id[1:] if fpath_real_id.shape[0] > 0 and fpath_real_id[0] == -1 else []
        fpath_real_id = fpath_real_id.astype(np.int)
        fpath_real_id = np.bincount(fpath_real_id).argmax()
        fpath_real_id_map[global_fpath_map == fpath_id] = fpath_real_id
    nxs, nys = np.where((fpath_map > -1))
    for nx, ny in zip(nxs, nys):
        self_node = (nx + all_anchor[0], ny + all_anchor[2], info_on_pix[(nx + all_anchor[0], ny + all_anchor[2])][0]['depth'])
        n_id = fpath_map[nx, ny]
        four_nes = [xx for xx in [(nx + 1, ny), (nx - 1, ny), (nx, ny + 1), (nx, ny - 1)]\
                        if 0 <= xx[0] < npath_map.shape[0] and 0 <= xx[1] < npath_map.shape[1]]
        for nex, ney in four_nes:
            ne_node = (nex + all_anchor[0], ney + all_anchor[2], info_on_pix[(nex + all_anchor[0], ney + all_anchor[2])][0]['depth'])
            if global_mesh.has_node(ne_node) is False:
                continue
            if npath_map[nex, ney] == n_id or global_mesh.nodes[ne_node].get('edge_id') == n_id:
                if global_mesh.has_edge(self_node, ne_node) is True:
                    global_mesh.remove_edge(self_node, ne_node)
                if global_mesh.nodes[self_node].get('near') is None:
                    global_mesh.nodes[self_node]['near'] = []
                if global_mesh.nodes[self_node].get('edge_id') is None:
                    f_id = int(round(fpath_real_id_map[self_node[0], self_node[1]]))
                    global_mesh.nodes[self_node]['edge_id'] = f_id
                    info_on_pix[(self_node[0], self_node[1])][0]['edge_id'] = f_id
                    edge_ccs[f_id].add(self_node)
                global_mesh.nodes[self_node]['near'].append(ne_node)

    return info_on_pix, global_mesh, image, depth, edge_ccs
    # for edge_cc in edge_ccs:
    #     for edge_node in edge_cc:
    #         edge_ccs
    # context_ccs, mask_ccs, broken_mask_ccs, edge_ccs, erode_context_ccs, init_mask_connect, edge_maps, extend_context_ccs, extend_edge_ccs

def get_valid_size(imap):
    x_max = np.where(imap.sum(1).squeeze() > 0)[0].max() + 1
    x_min = np.where(imap.sum(1).squeeze() > 0)[0].min()
    y_max = np.where(imap.sum(0).squeeze() > 0)[0].max() + 1
    y_min = np.where(imap.sum(0).squeeze() > 0)[0].min()
    size_dict = {'x_max':x_max, 'y_max':y_max, 'x_min':x_min, 'y_min':y_min}

    return size_dict

def dilate_valid_size(isize_dict, imap, dilate=[0, 0]):
    osize_dict = copy.deepcopy(isize_dict)
    osize_dict['x_min'] = max(0, osize_dict['x_min'] - dilate[0])
    osize_dict['x_max'] = min(imap.shape[0], osize_dict['x_max'] + dilate[0])
    osize_dict['y_min'] = max(0, osize_dict['y_min'] - dilate[0])
    osize_dict['y_max'] = min(imap.shape[1], osize_dict['y_max'] + dilate[1])

    return osize_dict

def size_operation(size_a, size_b, operation):
    assert operation == '+' or operation == '-', "Operation must be '+' (union) or '-' (exclude)"
    osize = {}
    if operation == '+':
        osize['x_min'] = min(size_a['x_min'], size_b['x_min'])
        osize['y_min'] = min(size_a['y_min'], size_b['y_min'])
        osize['x_max'] = max(size_a['x_max'], size_b['x_max'])
        osize['y_max'] = max(size_a['y_max'], size_b['y_max'])
    assert operation != '-', "Operation '-' is undefined !"

    return osize

def fill_dummy_bord(mesh, info_on_pix, image, depth, config):
    context = np.zeros_like(depth).astype(np.uint8)
    context[mesh.graph['hoffset']:mesh.graph['hoffset'] + mesh.graph['noext_H'],
            mesh.graph['woffset']:mesh.graph['woffset'] + mesh.graph['noext_W']] = 1
    mask = 1 - context
    xs, ys = np.where(mask > 0)
    depth = depth * context
    image = image * context[..., None]
    cur_depth = 0
    cur_disp = 0
    color = [0, 0, 0]
    for x, y in zip(xs, ys):
        cur_node = (x, y, cur_depth)
        mesh.add_node(cur_node, color=color,
                        synthesis=False,
                        disp=cur_disp,
                        cc_id=set(),
                        ext_pixel=True)
        info_on_pix[(x, y)] = [{'depth':cur_depth,
                    'color':mesh.nodes[(x, y, cur_depth)]['color'],
                    'synthesis':False,
                    'disp':mesh.nodes[cur_node]['disp'],
                    'ext_pixel':True}]
        # for x, y in zip(xs, ys):
        # four_nes = [(xx, yy) for xx, yy in [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)] if\
        #             0 <= x < mesh.graph['H'] and 0 <= y < mesh.graph['W'] and info_on_pix.get((xx, yy)) is not None]
        four_nes = [(xx, yy) for xx, yy in [(x + 1, y % mesh.graph['W']), (x - 1, y % mesh.graph['W']), (x, (y + 1) % mesh.graph['W']), (x, (y - 1) % mesh.graph['W'])] if\
                    0 <= x < mesh.graph['H'] and 0 <= y <= mesh.graph['W'] and info_on_pix.get((xx, yy)) is not None]
        # four_nes = []
        # for xx, yy in [(x + 1, y % mesh.graph['W']), (x - 1, y % mesh.graph['W']), (x, (y + 1) % mesh.graph['W']), (x, (y - 1) % mesh.graph['W'])]:
        #     if 0 <= x < mesh.graph['H'] and 0 <= y <= mesh.graph['W'] and info_on_pix.get((xx, yy)) is not None:
        #         four_nes.append((xx, yy))
        #     if four_nes:
        #         if info_on_pix.get(four_nes[-1]) is None:
        #             print(four_nes[-1])
        for ne in four_nes:
            # if (ne[0] - x) + (ne[1] - y) == 1 and info_on_pix.get((ne[0], ne[1])) is not None:
            mesh.add_edge(cur_node, (ne[0], ne[1], info_on_pix[(ne[0], ne[1])][0]['depth']))

    return mesh, info_on_pix


def enlarge_border(mesh, info_on_pix, depth, image, config):
    mesh.graph['hoffset'], mesh.graph['woffset'] = config['extrapolation_thickness'], config['extrapolation_thickness']
    mesh.graph['bord_up'], mesh.graph['bord_left'], mesh.graph['bord_down'], mesh.graph['bord_right'] = \
        0, 0, mesh.graph['H'], mesh.graph['W']
    # new_image = np.pad(image,
    #                    pad_width=((config['extrapolation_thickness'], config['extrapolation_thickness']),
    #                               (config['extrapolation_thickness'], config['extrapolation_thickness']), (0, 0)),
    #                    mode='constant')
    # new_depth = np.pad(depth,
    #                    pad_width=((config['extrapolation_thickness'], config['extrapolation_thickness']),
    #                               (config['extrapolation_thickness'], config['extrapolation_thickness'])),
    #                    mode='constant')

    return mesh, info_on_pix, depth, image

def fill_missing_node(mesh, info_on_pix, image, depth):
    for x in range(mesh.graph['bord_up'], mesh.graph['bord_down']):
        for y in range(mesh.graph['bord_left'], mesh.graph['bord_right']):
            if info_on_pix.get((x, y)) is None:
                print("fill missing node = ", x, y)
                import pdb; pdb.set_trace()
                re_depth, re_count = 0, 0
                for ne in [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)]:
                    if info_on_pix.get(ne) is not None:
                        re_depth += info_on_pix[ne][0]['depth']
                        re_count += 1
                if re_count == 0:
                    re_depth = -abs(depth[x, y])
                else:
                    re_depth = re_depth / re_count
                depth[x, y] = abs(re_depth)
                info_on_pix[(x, y)] = [{'depth':re_depth,
                                            'color':image[x, y],
                                            'synthesis':False,
                                            'disp':1./re_depth}]
                mesh.add_node((x, y, re_depth), color=image[x, y],
                                                synthesis=False,
                                                disp=1./re_depth,
                                                cc_id=set())
    return mesh, info_on_pix, depth



def refresh_bord_depth(mesh, info_on_pix, image, depth):
    H, W = mesh.graph['H'], mesh.graph['W']
    corner_nodes = [(mesh.graph['bord_up'], mesh.graph['bord_left']),
                    (mesh.graph['bord_up'], mesh.graph['bord_right'] - 1),
                    (mesh.graph['bord_down'] - 1, mesh.graph['bord_left']),
                    (mesh.graph['bord_down'] - 1, mesh.graph['bord_right'] - 1)]
                    # (0, W - 1), (H - 1, 0), (H - 1, W - 1)]
    bord_nodes = []
    bord_nodes += [(mesh.graph['bord_up'], xx) for xx in range(mesh.graph['bord_left'] + 1, mesh.graph['bord_right'] - 1)]
    bord_nodes += [(mesh.graph['bord_down'] - 1, xx) for xx in range(mesh.graph['bord_left'] + 1, mesh.graph['bord_right'] - 1)]
    bord_nodes += [(xx, mesh.graph['bord_left']) for xx in range(mesh.graph['bord_up'] + 1, mesh.graph['bord_down'] - 1)]
    bord_nodes += [(xx, mesh.graph['bord_right'] - 1) for xx in range(mesh.graph['bord_up'] + 1, mesh.graph['bord_down'] - 1)]
    for xy in bord_nodes:
        tgt_loc = None
        if xy[0] == mesh.graph['bord_up']:
            tgt_loc = (xy[0] + 1, xy[1])# (1, xy[1])
        elif xy[0] == mesh.graph['bord_down'] - 1:
            tgt_loc = (xy[0] - 1, xy[1]) # (H - 2, xy[1])
        elif xy[1] == mesh.graph['bord_left']:
            tgt_loc = (xy[0], xy[1] + 1)
        elif xy[1] == mesh.graph['bord_right'] - 1:
            tgt_loc = (xy[0], xy[1] - 1)
        if tgt_loc is not None:
            ne_infos = info_on_pix.get(tgt_loc)
            if ne_infos is None:
                import pdb; pdb.set_trace()
            # if ne_infos is not None and len(ne_infos) == 1:
            tgt_depth = ne_infos[0]['depth']
            tgt_disp = ne_infos[0]['disp']
            new_node = (xy[0], xy[1], tgt_depth)
            src_node = (tgt_loc[0], tgt_loc[1], tgt_depth)
            tgt_nes_loc = [(xx[0], xx[1]) \
                            for xx in mesh.neighbors(src_node)]
            tgt_nes_loc = [(xx[0] - tgt_loc[0] + xy[0], xx[1] - tgt_loc[1] + xy[1]) for xx in tgt_nes_loc \
                            if abs(xx[0] - xy[0]) == 1 and abs(xx[1] - xy[1]) == 1]
            tgt_nes_loc = [xx for xx in tgt_nes_loc if info_on_pix.get(xx) is not None]
            tgt_nes_loc.append(tgt_loc)
            # if (xy[0], xy[1]) == (559, 60):
            #     import pdb; pdb.set_trace()
            if info_on_pix.get(xy) is not None and len(info_on_pix.get(xy)) > 0:
                old_depth = info_on_pix[xy][0].get('depth')
                old_node = (xy[0], xy[1], old_depth)
                mesh.remove_edges_from([(old_ne, old_node) for old_ne in mesh.neighbors(old_node)])
                mesh.add_edges_from([((zz[0], zz[1], info_on_pix[zz][0]['depth']), old_node) for zz in tgt_nes_loc])
                mapping_dict = {old_node: new_node}
                # if old_node[2] == new_node[2]:
                #     print("mapping_dict = ", mapping_dict)
                info_on_pix, mesh = update_info(mapping_dict, info_on_pix, mesh)
            else:
                info_on_pix[xy] = []
                info_on_pix[xy][0] = info_on_pix[tgt_loc][0]
                info_on_pix['color'] = image[xy[0], xy[1]]
                info_on_pix['old_color'] = image[xy[0], xy[1]]
                mesh.add_node(new_node)
                mesh.add_edges_from([((zz[0], zz[1], info_on_pix[zz][0]['depth']), new_node) for zz in tgt_nes_loc])
            mesh.nodes[new_node]['far'] = None
            mesh.nodes[new_node]['near'] = None
            if mesh.nodes[src_node].get('far') is not None:
                redundant_nodes = [ne for ne in mesh.nodes[src_node]['far'] if (ne[0], ne[1]) == xy]
                [mesh.nodes[src_node]['far'].remove(aa) for aa in redundant_nodes]
            if mesh.nodes[src_node].get('near') is not None:
                redundant_nodes = [ne for ne in mesh.nodes[src_node]['near'] if (ne[0], ne[1]) == xy]
                [mesh.nodes[src_node]['near'].remove(aa) for aa in redundant_nodes]
    for xy in corner_nodes:
        hx, hy = xy
        four_nes = [xx for xx in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] if \
                        mesh.graph['bord_up'] <= xx[0] < mesh.graph['bord_down'] and \
                            mesh.graph['bord_left'] <= xx[1] < mesh.graph['bord_right']]
        ne_nodes = []
        ne_depths = []
        for ne_loc in four_nes:
            if info_on_pix.get(ne_loc) is not None:
                ne_depths.append(info_on_pix[ne_loc][0]['depth'])
                ne_nodes.append((ne_loc[0], ne_loc[1], info_on_pix[ne_loc][0]['depth']))
        new_node = (xy[0], xy[1], float(np.mean(ne_depths)))
        if info_on_pix.get(xy) is not None and len(info_on_pix.get(xy)) > 0:
            old_depth = info_on_pix[xy][0].get('depth')
            old_node = (xy[0], xy[1], old_depth)
            mesh.remove_edges_from([(old_ne, old_node) for old_ne in mesh.neighbors(old_node)])
            mesh.add_edges_from([(zz, old_node) for zz in ne_nodes])
            mapping_dict = {old_node: new_node}
            info_on_pix, mesh = update_info(mapping_dict, info_on_pix, mesh)
        else:
            info_on_pix[xy] = []
            info_on_pix[xy][0] = info_on_pix[ne_loc[-1]][0]
            info_on_pix['color'] = image[xy[0], xy[1]]
            info_on_pix['old_color'] = image[xy[0], xy[1]]
            mesh.add_node(new_node)
            mesh.add_edges_from([(zz, new_node) for zz in ne_nodes])
        mesh.nodes[new_node]['far'] = None
        mesh.nodes[new_node]['near'] = None
    for xy in bord_nodes + corner_nodes:
        # if (xy[0], xy[1]) == (559, 60):
        #     import pdb; pdb.set_trace()
        depth[xy[0], xy[1]] = abs(info_on_pix[xy][0]['depth'])
    for xy in bord_nodes:
        cur_node = (xy[0], xy[1], info_on_pix[xy][0]['depth'])
        nes = mesh.neighbors(cur_node)
        four_nes = set([(xy[0] + 1, xy[1]), (xy[0] - 1, xy[1]), (xy[0], xy[1] + 1), (xy[0], xy[1] - 1)]) - \
                   set([(ne[0], ne[1]) for ne in nes])
        four_nes = [ne for ne in four_nes if mesh.graph['bord_up'] <= ne[0] < mesh.graph['bord_down'] and \
                                             mesh.graph['bord_left'] <= ne[1] < mesh.graph['bord_right']]
        four_nes = [(ne[0], ne[1], info_on_pix[(ne[0], ne[1])][0]['depth']) for ne in four_nes]
        mesh.nodes[cur_node]['far'] = []
        mesh.nodes[cur_node]['near'] = []
        for ne in four_nes:
            if abs(ne[2]) >= abs(cur_node[2]):
                mesh.nodes[cur_node]['far'].append(ne)
            else:
                mesh.nodes[cur_node]['near'].append(ne)

    return mesh, info_on_pix, depth

def get_union_size(mesh, dilate, *alls_cc):
    all_cc = reduce(lambda x, y: x | y, [set()] + [*alls_cc])
    min_x, min_y, max_x, max_y = mesh.graph['H'], mesh.graph['W'], 0, 0
    H, W = mesh.graph['H'], mesh.graph['W']
    for node in all_cc:
        if node[0] < min_x:
            min_x = node[0]
        if node[0] > max_x:
            max_x = node[0]
        if node[1] < min_y:
            min_y = node[1]
        if node[1] > max_y:
            max_y = node[1]
    max_x = max_x + 1
    max_y = max_y + 1
    # mask_size = dilate_valid_size(mask_size, edge_dict['mask'], dilate=[20, 20])
    osize_dict = dict()
    osize_dict['x_min'] = max(0, min_x - dilate[0])
    osize_dict['x_max'] = min(H, max_x + dilate[0])
    osize_dict['y_min'] = max(0, min_y - dilate[1])
    osize_dict['y_max'] = min(W, max_y + dilate[1])

    return osize_dict

def incomplete_node(mesh, edge_maps, info_on_pix):
    vis_map = np.zeros((mesh.graph['H'], mesh.graph['W']))

    for node in mesh.nodes:
        if mesh.nodes[node].get('synthesis') is not True:
            connect_all_flag = False
            nes = [xx for xx in mesh.neighbors(node) if mesh.nodes[xx].get('synthesis') is not True]
            if len(nes) < 3 and 0 < node[0] < mesh.graph['H'] - 1 and 0 < node[1] < mesh.graph['W'] - 1:
                if len(nes) <= 1:
                    connect_all_flag = True
                else:
                    dan_ne_node_a = nes[0]
                    dan_ne_node_b = nes[1]
                    if abs(dan_ne_node_a[0] - dan_ne_node_b[0]) > 1 or \
                        abs(dan_ne_node_a[1] - dan_ne_node_b[1]) > 1:
                        connect_all_flag = True
            if connect_all_flag == True:
                vis_map[node[0], node[1]] = len(nes)
                four_nes = [(node[0] - 1, node[1]), (node[0] + 1, node[1]), (node[0], node[1] - 1), (node[0], node[1] + 1)]
                for ne in four_nes:
                    for info in info_on_pix[(ne[0], ne[1])]:
                        ne_node = (ne[0], ne[1], info['depth'])
                        if info.get('synthesis') is not True and mesh.has_node(ne_node):
                            mesh.add_edge(node, ne_node)
                            break

    return mesh

def edge_inpainting(edge_id, context_cc, erode_context_cc, mask_cc, edge_cc, extend_edge_cc,
                    mesh, edge_map, edge_maps_with_id, config, union_size, depth_edge_model, inpaint_iter):
    edge_dict = get_edge_from_nodes(context_cc, erode_context_cc, mask_cc, edge_cc, extend_edge_cc,
                                        mesh.graph['H'], mesh.graph['W'], mesh)
    edge_dict['edge'], end_depth_maps, _ = \
        filter_irrelevant_edge_new(edge_dict['self_edge'] + edge_dict['comp_edge'],
                                edge_map,
                                edge_maps_with_id,
                                edge_id,
                                edge_dict['context'],
                                edge_dict['depth'], mesh, context_cc | erode_context_cc, spdb=True)
    patch_edge_dict = dict()
    patch_edge_dict['mask'], patch_edge_dict['context'], patch_edge_dict['rgb'], \
        patch_edge_dict['disp'], patch_edge_dict['edge'] = \
        crop_maps_by_size(union_size, edge_dict['mask'], edge_dict['context'],
                            edge_dict['rgb'], edge_dict['disp'], edge_dict['edge'])
    tensor_edge_dict = convert2tensor(patch_edge_dict)
    if require_depth_edge(patch_edge_dict['edge'], patch_edge_dict['mask']) and inpaint_iter == 0:
        with torch.no_grad():
            device = config["gpu_ids"] if isinstance(config["gpu_ids"], int) and config["gpu_ids"] >= 0 else "cpu"
            depth_edge_output = depth_edge_model.forward_3P(tensor_edge_dict['mask'],
                                                            tensor_edge_dict['context'],
                                                            tensor_edge_dict['rgb'],
                                                            tensor_edge_dict['disp'],
                                                            tensor_edge_dict['edge'],
                                                            unit_length=128,
                                                            cuda=device)
            depth_edge_output = depth_edge_output.cpu()
        tensor_edge_dict['output'] = (depth_edge_output > config['ext_edge_threshold']).float() * tensor_edge_dict['mask'] + tensor_edge_dict['edge']
    else:
        tensor_edge_dict['output'] = tensor_edge_dict['edge']
        depth_edge_output = tensor_edge_dict['edge'] + 0
    patch_edge_dict['output'] = tensor_edge_dict['output'].squeeze().data.cpu().numpy()
    edge_dict['output'] = np.zeros((mesh.graph['H'], mesh.graph['W']))
    edge_dict['output'][union_size['x_min']:union_size['x_max'], union_size['y_min']:union_size['y_max']] = \
        patch_edge_dict['output']

    return edge_dict, end_depth_maps

def depth_inpainting(context_cc, extend_context_cc, erode_context_cc, mask_cc, mesh, config, union_size, depth_feat_model, edge_output, given_depth_dict=False, spdb=False):
    if given_depth_dict is False:
        depth_dict = get_depth_from_nodes(context_cc | extend_context_cc, erode_context_cc, mask_cc, mesh.graph['H'], mesh.graph['W'], mesh, config['log_depth'])
        if edge_output is not None:
            depth_dict['edge'] = edge_output
    else:
        depth_dict = given_depth_dict
    patch_depth_dict = dict()
    patch_depth_dict['mask'], patch_depth_dict['context'], patch_depth_dict['depth'], \
        patch_depth_dict['zero_mean_depth'], patch_depth_dict['edge'] = \
            crop_maps_by_size(union_size, depth_dict['mask'], depth_dict['context'],
                                depth_dict['real_depth'], depth_dict['zero_mean_depth'], depth_dict['edge'])
    tensor_depth_dict = convert2tensor(patch_depth_dict)
    resize_mask = open_small_mask(tensor_depth_dict['mask'], tensor_depth_dict['context'], 3, 41)
    with torch.no_grad():
        device = config["gpu_ids"] if isinstance(config["gpu_ids"], int) and config["gpu_ids"] >= 0 else "cpu"
        depth_output = depth_feat_model.forward_3P(resize_mask,
                                                    tensor_depth_dict['context'],
                                                    tensor_depth_dict['zero_mean_depth'],
                                                    tensor_depth_dict['edge'],
                                                    unit_length=128,
                                                    cuda=device)
        depth_output = depth_output.cpu()
    tensor_depth_dict['output'] = torch.exp(depth_output + depth_dict['mean_depth']) * \
                                            tensor_depth_dict['mask'] + tensor_depth_dict['depth']
    depth_dict['condition_image'] = tensor_depth_dict['output']
    patch_depth_dict['output'] = tensor_depth_dict['output'].data.cpu().numpy().squeeze()
    depth_dict['output'] = np.zeros((mesh.graph['H'], mesh.graph['W']))
    depth_dict['output'][union_size['x_min']:union_size['x_max'], union_size['y_min']:union_size['y_max']] = \
        patch_depth_dict['output']
    depth_output = depth_dict['output'] * depth_dict['mask'] + depth_dict['depth'] * depth_dict['context']
    depth_output = smooth_cntsyn_gap(depth_dict['output'].copy() * depth_dict['mask'] + depth_dict['depth'] * depth_dict['context'],
                                    depth_dict['mask'], depth_dict['context'],
                                    init_mask_region=depth_dict['mask'])
    if spdb is True:
        f, ((ax1, ax2)) = plt.subplots(1, 2, sharex=True, sharey=True);
        ax1.imshow(depth_output * depth_dict['mask'] + depth_dict['depth']); ax2.imshow(depth_dict['output'] * depth_dict['mask'] + depth_dict['depth']); plt.show()
        import pdb; pdb.set_trace()
    depth_dict['output'] = depth_output * depth_dict['mask'] + depth_dict['depth'] * depth_dict['context']

    return depth_dict

def update_info(mapping_dict, info_on_pix, *meshes):
    rt_meshes = []
    for mesh in meshes:
        rt_meshes.append(relabel_node(mesh, mesh.nodes, [*mapping_dict.keys()][0], [*mapping_dict.values()][0]))
    x, y, _ = [*mapping_dict.keys()][0]
    info_on_pix[(x, y)][0]['depth'] = [*mapping_dict.values()][0][2]

    return [info_on_pix] + rt_meshes

def build_connection(mesh, cur_node, dst_node):
    if (abs(cur_node[0] - dst_node[0]) + abs(cur_node[1] - dst_node[1])) < 2:
        mesh.add_edge(cur_node, dst_node)
    if abs(cur_node[0] - dst_node[0]) > 1 or abs(cur_node[1] - dst_node[1]) > 1:
        return mesh
    ne_nodes = [*mesh.neighbors(cur_node)].copy()
    for ne_node in ne_nodes:
        if mesh.has_edge(ne_node, dst_node) or ne_node == dst_node:
            continue
        else:
            mesh = build_connection(mesh, ne_node, dst_node)

    return mesh

def recursive_add_edge(edge_mesh, mesh, info_on_pix, cur_node, mark):
    ne_nodes = [(x[0], x[1]) for x in edge_mesh.neighbors(cur_node)]
    for node_xy in ne_nodes:
        node = (node_xy[0], node_xy[1], info_on_pix[node_xy][0]['depth'])
        if mark[node[0], node[1]] != 3:
            continue
        else:
            mark[node[0], node[1]] = 0
            mesh.remove_edges_from([(xx, node) for xx in mesh.neighbors(node)])
            mesh = build_connection(mesh, cur_node, node)
            re_info = dict(depth=0, count=0)
            for re_ne in mesh.neighbors(node):
                re_info['depth'] += re_ne[2]
                re_info['count'] += 1.
            try:
                re_depth = re_info['depth'] / re_info['count']
            except:
                re_depth = node[2]
            re_node = (node_xy[0], node_xy[1], re_depth)
            mapping_dict = {node: re_node}
            info_on_pix, edge_mesh, mesh = update_info(mapping_dict, info_on_pix, edge_mesh, mesh)

            edge_mesh, mesh, mark, info_on_pix = recursive_add_edge(edge_mesh, mesh, info_on_pix, re_node, mark)

    return edge_mesh, mesh, mark, info_on_pix

def resize_for_edge(tensor_dict, largest_size):
    resize_dict = {k: v.clone() for k, v in tensor_dict.items()}
    frac = largest_size / np.array([*resize_dict['edge'].shape[-2:]]).max()
    if frac < 1:
        resize_mark = torch.nn.functional.interpolate(torch.cat((resize_dict['mask'],
                                                        resize_dict['context']),
                                                        dim=1),
                                                        scale_factor=frac,
                                                        mode='bilinear')
        resize_dict['mask'] = (resize_mark[:, 0:1] > 0).float()
        resize_dict['context'] = (resize_mark[:, 1:2] == 1).float()
        resize_dict['context'][resize_dict['mask'] > 0] = 0
        resize_dict['edge'] = torch.nn.functional.interpolate(resize_dict['edge'],
                                                                scale_factor=frac,
                                                                mode='bilinear')
        resize_dict['edge'] = (resize_dict['edge'] > 0).float()
        resize_dict['edge'] = resize_dict['edge'] * resize_dict['context']
        resize_dict['disp'] = torch.nn.functional.interpolate(resize_dict['disp'],
                                                                scale_factor=frac,
                                                                mode='nearest')
        resize_dict['disp'] = resize_dict['disp'] * resize_dict['context']
        resize_dict['rgb'] = torch.nn.functional.interpolate(resize_dict['rgb'],
                                                                    scale_factor=frac,
                                                                    mode='bilinear')
        resize_dict['rgb'] = resize_dict['rgb'] * resize_dict['context']
    return resize_dict

def get_map_from_nodes(nodes, height, width):
    omap = np.zeros((height, width))
    for n in nodes:
        omap[n[0], n[1]] = 1

    return omap

def get_map_from_ccs(ccs, height, width, condition_input=None, condition=None, real_id=False, id_shift=0):
    if condition is None:
        condition = lambda x, condition_input: True

    if real_id is True:
        omap = np.zeros((height, width)) + (-1) + id_shift
    else:
        omap = np.zeros((height, width))
    for cc_id, cc in enumerate(ccs):
        for n in cc:
            if condition(n, condition_input):
                if real_id is True:
                    omap[n[0], n[1]] = cc_id + id_shift
                else:
                    omap[n[0], n[1]] = 1
    return omap

def revise_map_by_nodes(nodes, imap, operation, limit_constr=None):
    assert operation == '+' or operation == '-', "Operation must be '+' (union) or '-' (exclude)"
    omap = copy.deepcopy(imap)
    revise_flag = True
    if operation == '+':
        for n in nodes:
            omap[n[0], n[1]] = 1
        if limit_constr is not None and omap.sum() > limit_constr:
            omap = imap
            revise_flag = False
    elif operation == '-':
        for n in nodes:
            omap[n[0], n[1]] = 0
        if limit_constr is not None and omap.sum() < limit_constr:
            omap = imap
            revise_flag = False

    return omap, revise_flag

def repaint_info(mesh, cc, x_anchor, y_anchor, source_type):
    if source_type == 'rgb':
        feat = np.zeros((3, x_anchor[1] - x_anchor[0], y_anchor[1] - y_anchor[0]))
    else:
        feat = np.zeros((1, x_anchor[1] - x_anchor[0], y_anchor[1] - y_anchor[0]))
    for node in cc:
        if source_type == 'rgb':
            feat[:, node[0] - x_anchor[0], node[1] - y_anchor[0]] = np.array(mesh.nodes[node]['color']) / 255.
        elif source_type == 'd':
            feat[:, node[0] - x_anchor[0], node[1] - y_anchor[0]] = abs(node[2])

    return feat

def get_context_from_nodes(mesh, cc, H, W, source_type=''):
    if 'rgb' in source_type or 'color' in source_type:
        feat = np.zeros((H, W, 3))
    else:
        feat = np.zeros((H, W))
    context = np.zeros((H, W))
    for node in cc:
        if 'rgb' in source_type or 'color' in source_type:
            feat[node[0], node[1]] = np.array(mesh.nodes[node]['color']) / 255.
            context[node[0], node[1]] = 1
        else:
            feat[node[0], node[1]] = abs(node[2])

    return feat, context

def get_mask_from_nodes(mesh, cc, H, W):
    mask = np.zeros((H, W))
    for node in cc:
        mask[node[0], node[1]] = abs(node[2])

    return mask


def get_edge_from_nodes(context_cc, erode_context_cc, mask_cc, edge_cc, extend_edge_cc, H, W, mesh):
    context = np.zeros((H, W))
    mask = np.zeros((H, W))
    rgb = np.zeros((H, W, 3))
    disp = np.zeros((H, W))
    depth = np.zeros((H, W))
    real_depth = np.zeros((H, W))
    edge = np.zeros((H, W))
    comp_edge = np.zeros((H, W))
    fpath_map = np.zeros((H, W)) - 1
    npath_map = np.zeros((H, W)) - 1
    near_depth = np.zeros((H, W))
    for node in context_cc:
        rgb[node[0], node[1]] = np.array(mesh.nodes[node]['color'])
        disp[node[0], node[1]] = mesh.nodes[node]['disp']
        depth[node[0], node[1]] = node[2]
        context[node[0], node[1]] = 1
    for node in erode_context_cc:
        rgb[node[0], node[1]] = np.array(mesh.nodes[node]['color'])
        disp[node[0], node[1]] = mesh.nodes[node]['disp']
        depth[node[0], node[1]] = node[2]
        context[node[0], node[1]] = 1
    rgb = rgb / 255.
    disp = np.abs(disp)
    disp = disp / disp.max()
    real_depth = depth.copy()
    for node in context_cc:
        if mesh.nodes[node].get('real_depth') is not None:
            real_depth[node[0], node[1]] = mesh.nodes[node]['real_depth']
    for node in erode_context_cc:
        if mesh.nodes[node].get('real_depth') is not None:
            real_depth[node[0], node[1]] = mesh.nodes[node]['real_depth']
    for node in mask_cc:
        mask[node[0], node[1]] = 1
        near_depth[node[0], node[1]] = node[2]
    for node in edge_cc:
        edge[node[0], node[1]] = 1
    for node in extend_edge_cc:
        comp_edge[node[0], node[1]] = 1
    rt_dict = {'rgb': rgb, 'disp': disp, 'depth': depth, 'real_depth': real_depth, 'self_edge': edge, 'context': context,
               'mask': mask, 'fpath_map': fpath_map, 'npath_map': npath_map, 'comp_edge': comp_edge, 'valid_area': context + mask,
               'near_depth': near_depth}

    return rt_dict

def get_depth_from_maps(context_map, mask_map, depth_map, H, W, log_depth=False):
    context = context_map.astype(np.uint8)
    mask = mask_map.astype(np.uint8).copy()
    depth = np.abs(depth_map)
    real_depth = depth.copy()
    zero_mean_depth = np.zeros((H, W))

    if log_depth is True:
        log_depth = np.log(real_depth + 1e-8) * context
        mean_depth = np.mean(log_depth[context > 0])
        zero_mean_depth = (log_depth - mean_depth) * context
    else:
        zero_mean_depth = real_depth
        mean_depth = 0
    edge = np.zeros_like(depth)

    rt_dict = {'depth': depth, 'real_depth': real_depth, 'context': context, 'mask': mask,
               'mean_depth': mean_depth, 'zero_mean_depth': zero_mean_depth, 'edge': edge}

    return rt_dict

def get_depth_from_nodes(context_cc, erode_context_cc, mask_cc, H, W, mesh, log_depth=False):
    context = np.zeros((H, W))
    mask = np.zeros((H, W))
    depth = np.zeros((H, W))
    real_depth = np.zeros((H, W))
    zero_mean_depth = np.zeros((H, W))
    for node in context_cc:
        depth[node[0], node[1]] = node[2]
        context[node[0], node[1]] = 1
    for node in erode_context_cc:
        depth[node[0], node[1]] = node[2]
        context[node[0], node[1]] = 1
    depth = np.abs(depth)
    real_depth = depth.copy()
    for node in context_cc:
        if mesh.nodes[node].get('real_depth') is not None:
            real_depth[node[0], node[1]] = mesh.nodes[node]['real_depth']
    for node in erode_context_cc:
        if mesh.nodes[node].get('real_depth') is not None:
            real_depth[node[0], node[1]] = mesh.nodes[node]['real_depth']
    real_depth = np.abs(real_depth)
    for node in mask_cc:
        mask[node[0], node[1]] = 1
    if log_depth is True:
        log_depth = np.log(real_depth + 1e-8) * context
        mean_depth = np.mean(log_depth[context > 0])
        zero_mean_depth = (log_depth - mean_depth) * context
    else:
        zero_mean_depth = real_depth
        mean_depth = 0

    rt_dict = {'depth': depth, 'real_depth': real_depth, 'context': context, 'mask': mask,
               'mean_depth': mean_depth, 'zero_mean_depth': zero_mean_depth}

    return rt_dict

def get_rgb_from_nodes(context_cc, erode_context_cc, mask_cc, H, W, mesh):
    context = np.zeros((H, W))
    mask = np.zeros((H, W))
    rgb = np.zeros((H, W, 3))
    erode_context = np.zeros((H, W))
    for node in context_cc:
        rgb[node[0], node[1]] = np.array(mesh.nodes[node]['color'])
        context[node[0], node[1]] = 1
    rgb = rgb / 255.
    for node in mask_cc:
        mask[node[0], node[1]] = 1
    for node in erode_context_cc:
        erode_context[node[0], node[1]] = 1
        mask[node[0], node[1]] = 1
    rt_dict = {'rgb': rgb, 'context': context, 'mask': mask,
               'erode': erode_context}

    return rt_dict

def crop_maps_by_size(size, *imaps):
    omaps = []
    for imap in imaps:
        omaps.append(imap[size['x_min']:size['x_max'], size['y_min']:size['y_max']].copy())

    return omaps

def convert2tensor(input_dict):
    rt_dict = {}
    for key, value in input_dict.items():
        if 'rgb' in key or 'color' in key:
            rt_dict[key] = torch.FloatTensor(value).permute(2, 0, 1)[None, ...]
        else:
            rt_dict[key] = torch.FloatTensor(value)[None, None, ...]

    return rt_dict



================================================
FILE: inpainting/networks.py
================================================
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F


class BaseNetwork(nn.Module):
    def __init__(self):
        super(BaseNetwork, self).__init__()

    def init_weights(self, init_type='normal', gain=0.02):
        '''
        initialize network's weights
        init_type: normal | xavier | kaiming | orthogonal
        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39
        '''

        def init_func(m):
            classname = m.__class__.__name__
            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):
                if init_type == 'normal':
                    nn.init.normal_(m.weight.data, 0.0, gain)
                elif init_type == 'xavier':
                    nn.init.xavier_normal_(m.weight.data, gain=gain)
                elif init_type == 'kaiming':
                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
                elif init_type == 'orthogonal':
                    nn.init.orthogonal_(m.weight.data, gain=gain)

                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias.data, 0.0)

            elif classname.find('BatchNorm2d') != -1:
                nn.init.normal_(m.weight.data, 1.0, gain)
                nn.init.constant_(m.bias.data, 0.0)

        self.apply(init_func)

def weights_init(init_type='gaussian'):
    def init_fun(m):
        classname = m.__class__.__name__
        if (classname.find('Conv') == 0 or classname.find(
                'Linear') == 0) and hasattr(m, 'weight'):
            if init_type == 'gaussian':
                nn.init.normal_(m.weight, 0.0, 0.02)
            elif init_type == 'xavier':
                nn.init.xavier_normal_(m.weight, gain=math.sqrt(2))
            elif init_type == 'kaiming':
                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')
            elif init_type == 'orthogonal':
                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))
            elif init_type == 'default':
                pass
            else:
                assert 0, "Unsupported initialization: {}".format(init_type)
            if hasattr(m, 'bias') and m.bias is not None:
                nn.init.constant_(m.bias, 0.0)

    return init_fun

class PartialConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=0, dilation=1, groups=1, bias=True):
        super().__init__()
        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size,
                                    stride, padding, dilation, groups, bias)
        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size,
                                   stride, padding, dilation, groups, False)
        self.input_conv.apply(weights_init('kaiming'))
        self.slide_winsize = in_channels * kernel_size * kernel_size

        torch.nn.init.constant_(self.mask_conv.weight, 1.0)

        # mask is not updated
        for param in self.mask_conv.parameters():
            param.requires_grad = False

    def forward(self, input, mask):
        # http://masc.cs.gmu.edu/wiki/partialconv
        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)
        # W^T* (M .* X) / sum(M) + b = [C(M .* X) – C(0)] / D(M) + C(0)
        output = self.input_conv(input * mask)
        if self.input_conv.bias is not None:
            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(
                output)
        else:
            output_bias = torch.zeros_like(output)

        with torch.no_grad():
            output_mask = self.mask_conv(mask)

        no_update_holes = output_mask == 0

        mask_sum = output_mask.masked_fill_(no_update_holes, 1.0)

        output_pre = ((output - output_bias) * self.slide_winsize) / mask_sum + output_bias
        output = output_pre.masked_fill_(no_update_holes, 0.0)

        new_mask = torch.ones_like(output)
        new_mask = new_mask.masked_fill_(no_update_holes, 0.0)

        return output, new_mask


class PCBActiv(nn.Module):
    def __init__(self, in_ch, out_ch, bn=True, sample='none-3', activ='relu',
                 conv_bias=False):
        super().__init__()
        if sample == 'down-5':
            self.conv = PartialConv(in_ch, out_ch, 5, 2, 2, bias=conv_bias)
        elif sample == 'down-7':
            self.conv = PartialConv(in_ch, out_ch, 7, 2, 3, bias=conv_bias)
        elif sample == 'down-3':
            self.conv = PartialConv(in_ch, out_ch, 3, 2, 1, bias=conv_bias)
        else:
            self.conv = PartialConv(in_ch, out_ch, 3, 1, 1, bias=conv_bias)

        if bn:
            self.bn = nn.BatchNorm2d(out_ch)
        if activ == 'relu':
            self.activation = nn.ReLU()
        elif activ == 'leaky':
            self.activation = nn.LeakyReLU(negative_slope=0.2)

    def forward(self, input, input_mask):
        h, h_mask = self.conv(input, input_mask)
        if hasattr(self, 'bn'):
            h = self.bn(h)
        if hasattr(self, 'activation'):
            h = self.activation(h)
        return h, h_mask

class Inpaint_Depth_Net(nn.Module):
    def __init__(self, layer_size=7, upsampling_mode='nearest'):
        super().__init__()
        in_channels = 4
        out_channels = 1
        self.freeze_enc_bn = False
        self.upsampling_mode = upsampling_mode
        self.layer_size = layer_size
        self.enc_1 = PCBActiv(in_channels, 64, bn=False, sample='down-7', conv_bias=True)
        self.enc_2 = PCBActiv(64, 128, sample='down-5', conv_bias=True)
        self.enc_3 = PCBActiv(128, 256, sample='down-5')
        self.enc_4 = PCBActiv(256, 512, sample='down-3')
        for i in range(4, self.layer_size):
            name = 'enc_{:d}'.format(i + 1)
            setattr(self, name, PCBActiv(512, 512, sample='down-3'))

        for i in range(4, self.layer_size):
            name = 'dec_{:d}'.format(i + 1)
            setattr(self, name, PCBActiv(512 + 512, 512, activ='leaky'))
        self.dec_4 = PCBActiv(512 + 256, 256, activ='leaky')
        self.dec_3 = PCBActiv(256 + 128, 128, activ='leaky')
        self.dec_2 = PCBActiv(128 + 64, 64, activ='leaky')
        self.dec_1 = PCBActiv(64 + in_channels, out_channels,
                              bn=False, activ=None, conv_bias=True)
    def add_border(self, input, mask_flag, PCONV=True):
        with torch.no_grad():
            h = input.shape[-2]
            w = input.shape[-1]
            require_len_unit = 2 ** self.layer_size
            residual_h = int(np.ceil(h / float(require_len_unit)) * require_len_unit - h) # + 2*require_len_unit
            residual_w = int(np.ceil(w / float(require_len_unit)) * require_len_unit - w) # + 2*require_len_unit
            enlarge_input = torch.zeros((input.shape[0], input.shape[1], h + residual_h, w + residual_w)).to(input.device)
            if mask_flag:
                if PCONV is False:
                    enlarge_input += 1.0
                enlarge_input = enlarge_input.clamp(0.0, 1.0)
            else:
                enlarge_input[:, 2, ...] = 0.0
            anchor_h = residual_h//2
            anchor_w = residual_w//2
            enlarge_input[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w] = input

        return enlarge_input, [anchor_h, anchor_h+h, anchor_w, anchor_w+w]

    def forward_3P(self, mask, context, depth, edge, unit_length=128, cuda=None):
        with torch.no_grad():
            input = torch.cat((depth, edge, context, mask), dim=1)
            n, c, h, w = input.shape
            residual_h = int(np.ceil(h / float(unit_length)) * unit_length - h)
            residual_w = int(np.ceil(w / float(unit_length)) * unit_length - w)
            anchor_h = residual_h//2
            anchor_w = residual_w//2
            enlarge_input = torch.zeros((n, c, h + residual_h, w + residual_w)).to(cuda)
            enlarge_input[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w] = input
            # enlarge_input[:, 3] = 1. - enlarge_input[:, 3]
            depth_output = self.forward(enlarge_input)
            depth_output = depth_output[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w]
            # import pdb; pdb.set_trace()

        return depth_output

    def forward(self, input_feat, refine_border=False, sample=False, PCONV=True):
        input = input_feat
        input_mask = (input_feat[:, -2:-1] + input_feat[:, -1:]).clamp(0, 1).repeat(1, input.shape[1], 1, 1)

        vis_input = input.cpu().data.numpy()
        vis_input_mask = input_mask.cpu().data.numpy()
        H, W = input.shape[-2:]
        if refine_border is True:
            input, anchor = self.add_border(input, mask_flag=False)
            input_mask, anchor = self.add_border(input_mask, mask_flag=True, PCONV=PCONV)
        h_dict = {}  # for the output of enc_N
        h_mask_dict = {}  # for the output of enc_N
        h_dict['h_0'], h_mask_dict['h_0'] = input, input_mask

        h_key_prev = 'h_0'
        for i in range(1, self.layer_size + 1):
            l_key = 'enc_{:d}'.format(i)
            h_key = 'h_{:d}'.format(i)
            h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(
                h_dict[h_key_prev], h_mask_dict[h_key_prev])
            h_key_prev = h_key

        h_key = 'h_{:d}'.format(self.layer_size)
        h, h_mask = h_dict[h_key], h_mask_dict[h_key]

        for i in range(self.layer_size, 0, -1):
            enc_h_key = 'h_{:d}'.format(i - 1)
            dec_l_key = 'dec_{:d}'.format(i)

            h = F.interpolate(h, scale_factor=2, mode=self.upsampling_mode)
            h_mask = F.interpolate(h_mask, scale_factor=2, mode='nearest')

            h = torch.cat([h, h_dict[enc_h_key]], dim=1)
            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim=1)
            h, h_mask = getattr(self, dec_l_key)(h, h_mask)
        output = h
        if refine_border is True:
            h_mask = h_mask[..., anchor[0]:anchor[1], anchor[2]:anchor[3]]
            output = output[..., anchor[0]:anchor[1], anchor[2]:anchor[3]]

        return output

class Inpaint_Edge_Net(BaseNetwork):
    def __init__(self, residual_blocks=8, init_weights=True):
        super(Inpaint_Edge_Net, self).__init__()
        in_channels = 7
        out_channels = 1
        self.encoder = []
        # 0
        self.encoder_0 = nn.Sequential(
                            nn.ReflectionPad2d(3),
                            spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, padding=0), True),
                            nn.InstanceNorm2d(64, track_running_stats=False),
                            nn.ReLU(True))
        # 1
        self.encoder_1 = nn.Sequential(
                            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), True),
                            nn.InstanceNorm2d(128, track_running_stats=False),
                            nn.ReLU(True))
        # 2
        self.encoder_2 = nn.Sequential(
                            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), True),
                            nn.InstanceNorm2d(256, track_running_stats=False),
                            nn.ReLU(True))
        # 3
        blocks = []
        for _ in range(residual_blocks):
            block = ResnetBlock(256, 2)
            blocks.append(block)

        self.middle = nn.Sequential(*blocks)
        # + 3
        self.decoder_0 = nn.Sequential(
                            spectral_norm(nn.ConvTranspose2d(in_channels=256+256, out_channels=128, kernel_size=4, stride=2, padding=1), True),
                            nn.InstanceNorm2d(128, track_running_stats=False),
                            nn.ReLU(True))
        # + 2
        self.decoder_1 = nn.Sequential(
                            spectral_norm(nn.ConvTranspose2d(in_channels=128+128, out_channels=64, kernel_size=4, stride=2, padding=1), True),
                            nn.InstanceNorm2d(64, track_running_stats=False),
                            nn.ReLU(True))
        # + 1
        self.decoder_2 = nn.Sequential(
                            nn.ReflectionPad2d(3),
                            nn.Conv2d(in_channels=64+64, out_channels=out_channels, kernel_size=7, padding=0),
                            )

        if init_weights:
            self.init_weights()

    def add_border(self, input, channel_pad_1=None):
        h = input.shape[-2]
        w = input.shape[-1]
        require_len_unit = 16
        residual_h = int(np.ceil(h / float(require_len_unit)) * require_len_unit - h) # + 2*require_len_unit
        residual_w = int(np.ceil(w / float(require_len_unit)) * require_len_unit - w) # + 2*require_len_unit
        enlarge_input = torch.zeros((input.shape[0], input.shape[1], h + residual_h, w + residual_w)).to(input.device)
        if channel_pad_1 is not None:
            for channel in channel_pad_1:
                enlarge_input[:, channel] = 1
        anchor_h = residual_h//2
        anchor_w = residual_w//2
        enlarge_input[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w] = input

        return enlarge_input, [anchor_h, anchor_h+h, anchor_w, anchor_w+w]

    def forward_3P(self, mask, context, rgb, disp, edge, unit_length=128, cuda=None):
        with torch.no_grad():
            input = torch.cat((rgb, disp/disp.max(), edge, context, mask), dim=1)
            n, c, h, w = input.shape
            residual_h = int(np.ceil(h / float(unit_length)) * unit_length - h)
            residual_w = int(np.ceil(w / float(unit_length)) * unit_length - w)
            anchor_h = residual_h//2
            anchor_w = residual_w//2
            enlarge_input = torch.zeros((n, c, h + residual_h, w + residual_w)).to(cuda)
            enlarge_input[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w] = input
            edge_output = self.forward(enlarge_input)
            edge_output = edge_output[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w]

        return edge_output

    def forward(self, x, refine_border=False):
        if refine_border:
            x, anchor = self.add_border(x, [5])
        x1 = self.encoder_0(x)
        x2 = self.encoder_1(x1)
        x3 = self.encoder_2(x2)
        x4 = self.middle(x3)
        x5 = self.decoder_0(torch.cat((x4, x3), dim=1))
        x6 = self.decoder_1(torch.cat((x5, x2), dim=1))
        x7 = self.decoder_2(torch.cat((x6, x1), dim=1))
        x = torch.sigmoid(x7)
        if refine_border:
            x = x[..., anchor[0]:anchor[1], anchor[2]:anchor[3]]

        return x

class Inpaint_Color_Net(nn.Module):
    def __init__(self, layer_size=7, upsampling_mode='nearest', add_hole_mask=False, add_two_layer=False, add_border=False):
        super().__init__()
        self.freeze_enc_bn = False
        self.upsampling_mode = upsampling_mode
        self.layer_size = layer_size
        in_channels = 6
        self.enc_1 = PCBActiv(in_channels, 64, bn=False, sample='down-7')
        self.enc_2 = PCBActiv(64, 128, sample='down-5')
        self.enc_3 = PCBActiv(128, 256, sample='down-5')
        self.enc_4 = PCBActiv(256, 512, sample='down-3')
        self.enc_5 = PCBActiv(512, 512, sample='down-3')
        self.enc_6 = PCBActiv(512, 512, sample='down-3')
        self.enc_7 = PCBActiv(512, 512, sample='down-3')

        self.dec_7 = PCBActiv(512+512, 512, activ='leaky')
        self.dec_6 = PCBActiv(512+512, 512, activ='leaky')

        self.dec_5A = PCBActiv(512 + 512, 512, activ='leaky')
        self.dec_4A = PCBActiv(512 + 256, 256, activ='leaky')
        self.dec_3A = PCBActiv(256 + 128, 128, activ='leaky')
        self.dec_2A = PCBActiv(128 + 64, 64, activ='leaky')
        self.dec_1A = PCBActiv(64 + in_channels, 3, bn=False, activ=None, conv_bias=True)
        '''
        self.dec_5B = PCBActiv(512 + 512, 512, activ='leaky')
        self.dec_4B = PCBActiv(512 + 256, 256, activ='leaky')
        self.dec_3B = PCBActiv(256 + 128, 128, activ='leaky')
        self.dec_2B = PCBActiv(128 + 64, 64, activ='leaky')
        self.dec_1B = PCBActiv(64 + 4, 1, bn=False, activ=None, conv_bias=True)
        '''
    def cat(self, A, B):
        return torch.cat((A, B), dim=1)

    def upsample(self, feat, mask):
        feat = F.interpolate(feat, scale_factor=2, mode=self.upsampling_mode)
        mask = F.interpolate(mask, scale_factor=2, mode='nearest')

        return feat, mask

    def forward_3P(self, mask, context, rgb, edge, unit_length=128, cuda=None):
        with torch.no_grad():
            input = torch.cat((rgb, edge, context, mask), dim=1)
            n, c, h, w = input.shape
            residual_h = int(np.ceil(h / float(unit_length)) * unit_length - h) # + 128
            residual_w = int(np.ceil(w / float(unit_length)) * unit_length - w) # + 256
            anchor_h = residual_h//2
            anchor_w = residual_w//2
            enlarge_input = torch.zeros((n, c, h + residual_h, w + residual_w)).to(cuda)
            enlarge_input[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w] = input
            # enlarge_input[:, 3] = 1. - enlarge_input[:, 3]
            enlarge_input = enlarge_input.to(cuda)
            rgb_output = self.forward(enlarge_input)
            rgb_output = rgb_output[..., anchor_h:anchor_h+h, anchor_w:anchor_w+w]

        return rgb_output

    def forward(self, input, add_border=False):
        input_mask = (input[:, -2:-1] + input[:, -1:]).clamp(0, 1)
        H, W = input.shape[-2:]
        f_0, h_0 = input, input_mask.repeat((1,input.shape[1],1,1))
        f_1, h_1 = self.enc_1(f_0, h_0)
        f_2, h_2 = self.enc_2(f_1, h_1)
        f_3, h_3 = self.enc_3(f_2, h_2)
        f_4, h_4 = self.enc_4(f_3, h_3)
        f_5, h_5 = self.enc_5(f_4, h_4)
        f_6, h_6 = self.enc_6(f_5, h_5)
        f_7, h_7 = self.enc_7(f_6, h_6)

        o_7, k_7 = self.upsample(f_7, h_7)
        o_6, k_6 = self.dec_7(self.cat(o_7, f_6), self.cat(k_7, h_6))
        o_6, k_6 = self.upsample(o_6, k_6)
        o_5, k_5 = self.dec_6(self.cat(o_6, f_5), self.cat(k_6, h_5))
        o_5, k_5 = self.upsample(o_5, k_5)
        o_5A, k_5A = o_5, k_5
        o_5B, k_5B = o_5, k_5
        ###############
        o_4A, k_4A = self.dec_5A(self.cat(o_5A, f_4), self.cat(k_5A, h_4))
        o_4A, k_4A = self.upsample(o_4A, k_4A)
        o_3A, k_3A = self.dec_4A(self.cat(o_4A, f_3), self.cat(k_4A, h_3))
        o_3A, k_3A = self.upsample(o_3A, k_3A)
        o_2A, k_2A = self.dec_3A(self.cat(o_3A, f_2), self.cat(k_3A, h_2))
        o_2A, k_2A = self.upsample(o_2A, k_2A)
        o_1A, k_1A = self.dec_2A(self.cat(o_2A, f_1), self.cat(k_2A, h_1))
        o_1A, k_1A = self.upsample(o_1A, k_1A)
        o_0A, k_0A = self.dec_1A(self.cat(o_1A, f_0), self.cat(k_1A, h_0))

        return torch.sigmoid(o_0A)

    def train(self, mode=True):
        """
        Override the default train() to freeze the BN parameters
        """
        super().train(mode)
        if self.freeze_enc_bn:
            for name, module in self.named_modules():
                if isinstance(module, nn.BatchNorm2d) and 'enc' in name:
                    module.eval()

class Discriminator(BaseNetwork):
    def __init__(self, use_sigmoid=True, use_spectral_norm=True, init_weights=True, in_channels=None):
        super(Discriminator, self).__init__()
        self.use_sigmoid = use_sigmoid
        self.conv1 = self.features = nn.Sequential(
            spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.conv2 = nn.Sequential(
            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.conv3 = nn.Sequential(
            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.conv4 = nn.Sequential(
            spectral_norm(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.conv5 = nn.Sequential(
            spectral_norm(nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),
        )

        if init_weights:
            self.init_weights()

    def forward(self, x):
        conv1 = self.conv1(x)
        conv2 = self.conv2(conv1)
        conv3 = self.conv3(conv2)
        conv4 = self.conv4(conv3)
        conv5 = self.conv5(conv4)

        outputs = conv5
        if self.use_sigmoid:
            outputs = torch.sigmoid(conv5)

        return outputs, [conv1, conv2, conv3, conv4, conv5]

class ResnetBlock(nn.Module):
    def __init__(self, dim, dilation=1):
        super(ResnetBlock, self).__init__()
        self.conv_block = nn.Sequential(
            nn.ReflectionPad2d(dilation),
            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not True), True),
            nn.InstanceNorm2d(dim, track_running_stats=False),
            nn.LeakyReLU(negative_slope=0.2),

            nn.ReflectionPad2d(1),
            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not True), True),
            nn.InstanceNorm2d(dim, track_running_stats=False),
        )

    def forward(self, x):
        out = x + self.conv_block(x)

        # Remove ReLU at the end of the residual block
        # http://torch.ch/blog/2016/02/04/resnets.html

        return out


def spectral_norm(module, mode=True):
    if mode:
        return nn.utils.spectral_norm(module)

    return module



================================================
FILE: inpainting/pano2mesh.py
================================================
import numpy as np
import argparse
import glob
import os
from functools import partial
#import vispy
import scipy.misc as misc
from tqdm import tqdm
import yaml
import time
import sys
from mesh import  write_ply_no_inpainting
from utils import get_MiDaS_samples, read_MiDaS_depth, read_real_depth
import torch
import cv2
from skimage.transform import resize
import imageio
import copy
from MiDaS.run import run_depth
from boostmonodepth_utils import run_boostmonodepth
from MiDaS.monodepth_net import MonoDepthNet
import MiDaS.MiDaS_utils as MiDaS_utils
from bilateral_filtering import sparse_bilateral_filtering
from skimage import color


parser = argparse.ArgumentParser()
parser.add_argument('--config', type=str, default='argument_p2m.yml',help='Configure of post processing')
args = parser.parse_args()
config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)
#if config['offscreen_rendering'] is True:
    #vispy.use(app='egl')
os.makedirs(config['mesh_folder'], exist_ok=True)
os.makedirs(config['video_folder'], exist_ok=True)
os.makedirs(config['depth_folder'], exist_ok=True)
sample_list = get_MiDaS_samples(config['src_folder'], config['depth_folder'], config, config['specific'])
normal_canvas, all_canvas = None, None

if isinstance(config["gpu_ids"], int) and (config["gpu_ids"] >= 0):
    device = config["gpu_ids"]
else:
    device = "cpu"

print(f"running on device {device}")

for idx in tqdm(range(len(sample_list))):
    depth = None
    sample = sample_list[idx]
    print("Current Source ==> ", sample['src_pair_name'])
    mesh_fi = os.path.join(config['mesh_folder'], sample['src_pair_name'] + "_p2m" +'.ply')
    image = imageio.imread(sample['ref_img_fi'])[:,:,:3]

    print(f"Running depth extraction at {time.time()}")
    if config['use_boostmonodepth'] is True:
        run_boostmonodepth(sample['ref_img_fi'], config['src_folder'], config['depth_folder'])
    elif config['require_midas'] is True:
        run_depth([sample['ref_img_fi']], config['src_folder'], config['depth_folder'],
                  config['MiDaS_model_ckpt'], MonoDepthNet, MiDaS_utils, target_w=640)
    print(sample['depth_fi'])
    print("Depth shape", np.load(sample['depth_fi']).shape)

    if 'npy' in config['depth_format']:
        config['output_h'], config['output_w'] = np.load(sample['depth_fi']).shape[:2]
    else:
        config['output_h'], config['output_w'] = imageio.imread(sample['depth_fi']).shape[:2]
    frac = config['longer_side_len'] / max(config['output_h'], config['output_w'])
    config['output_h'], config['output_w'] = int(config['output_h'] * frac), int(config['output_w'] * frac)
    config['original_h'], config['original_w'] = config['output_h'], config['output_w']
    if image.ndim == 2:
        image = image[..., None].repeat(3, -1)
    if np.sum(np.abs(image[..., 0] - image[..., 1])) == 0 and np.sum(np.abs(image[..., 1] - image[..., 2])) == 0:
        config['gray_image'] = True
    else:
        config['gray_image'] = False
    print("Width, height:", config['output_w'],config['output_h'])
    image = cv2.resize(image, (config['output_w'], config['output_h']), interpolation=cv2.INTER_AREA)
    if config["use_real_depth"] is False:
        depth = read_MiDaS_depth(sample['depth_fi'], 3.0, config['output_h'], config['output_w'])
    else:
        depth = read_real_depth(sample['depth_fi'], h=config["output_h"], w=config['output_w'])
    mean_loc_depth = depth[depth.shape[0]//2, depth.shape[1]//2]
    if not(config['load_ply'] is True and os.path.exists(mesh_fi)):
        vis_photos, vis_depths = sparse_bilateral_filtering(depth.copy(), image.copy(), config, num_iter=config['sparse_iter'], spdb=False)
        depth = vis_depths[-1]
        model = None
        torch.cuda.empty_cache()
        print("Start Running Pano2Mesh ...")
        print(f"Writing depth ply (and basically doing everything) at {time.time()}")
        rt_info = write_ply_no_inpainting(image,
                              depth,
                              sample['int_mtx'],
                              mesh_fi,
                              config)

      

        # if rt_info is False:
        #     continue
        # rgb_model = None
        # color_feat_model = None
        # depth_edge_model = None
        # depth_feat_model = None
        # torch.cuda.empty_cache()
    #if config['save_ply'] is True or config['load_ply'] is True:
        #verts, colors, faces, Height, Width, hFov, vFov = read_ply(mesh_fi)
    #else:
        #verts, colors, faces, Height, Width, hFov, vFov = rt_info


    # print(f"Making video at {time.time()}")
    # videos_poses, video_basename = copy.deepcopy(sample['tgts_poses']), sample['tgt_name']
    # top = (config.get('original_h') // 2 - sample['int_mtx'][1, 2] * config['output_h'])
    # left = (config.get('original_w') // 2 - sample['int_mtx'][0, 2] * config['output_w'])
    # down, right = top + config['output_h'], left + config['output_w']
    # border = [int(xx) for xx in [top, down, left, right]]
    # normal_canvas, all_canvas = output_3d_photo(verts.copy(), colors.copy(), faces.copy(), copy.deepcopy(Height), copy.deepcopy(Width), copy.deepcopy(hFov), copy.deepcopy(vFov),
    #                     copy.deepcopy(sample['tgt_pose']), sample['video_postfix'], copy.deepcopy(sample['ref_pose']), copy.deepcopy(config['video_folder']),
    #                     image.copy(), copy.deepcopy(sample['int_mtx']), config, image,
    #                     videos_poses, video_basename, config.get('original_h'), config.get('original_w'), border=border, depth=depth, normal_canvas=normal_canvas, all_canvas=all_canvas,
    #                     mean_loc_depth=mean_loc_depth)



================================================
FILE: inpainting/run_3d_photo_inpainting.sh
================================================
python3 inpainting/time.py
docker build -t 3d-photo-inpainting inpainting/. 
python3 inpainting/time.py
docker run --runtime=nvidia -u $(xid -u):$(id -g) -e USER=$USER -it --rm -e CUDA_VISIBLE_DEVICES=3 -e HF_HOME=/inpainting/checkpoint -v ./data:/inpainting/data -v ./results:/inpainting/results 3d-photo-inpainting sh -c "cd /inpainting; python3 main.py"
python3 inpainting/time.py


================================================
FILE: inpainting/run_pano2mesh.sh
================================================
python3 inpainting/time.py
docker build -t 3d-photo-inpainting inpainting/. 
python3 inpainting/time.py
docker run --runtime=nvidia -u $(xid -u):$(id -g) -e USER=$USER -it --rm -e CUDA_VISIBLE_DEVICES=2 -e HF_HOME=/inpainting/checkpoint -v ./data:/inpainting/data -v ./results:/inpainting/results 3d-photo-inpainting sh -c "cd /inpainting; python3 pano2mesh.py"
python3 inpainting/time.py


================================================
FILE: inpainting/time.py
================================================
import time
curr = time.ctime(time.time())
print(curr)


================================================
FILE: inpainting/utils.py
================================================
import os
import json
import glob
import cv2
import scipy.misc as misc
from skimage.transform import resize
import numpy as np
from functools import reduce
from operator import mul
import torch
from torch import nn
import matplotlib.pyplot as plt
import re
try:
    import cynetworkx as netx
except ImportError:
    import networkx as netx
from scipy.ndimage import gaussian_filter
from skimage.feature import canny
import collections
import shutil
import imageio
import copy
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
from scipy.interpolate import interp1d
from collections import namedtuple

def path_planning(num_frames, x, y, z, path_type=''):
    if path_type == 'straight-line':
        corner_points = np.array([[0, 0, 0], [(0 + x) * 0.5, (0 + y) * 0.5, (0 + z) * 0.5], [x, y, z]])
        corner_t = np.linspace(0, 1, len(corner_points))
        t = np.linspace(0, 1, num_frames)
        cs = interp1d(corner_t, corner_points, axis=0, kind='quadratic')
        spline = cs(t)
        xs, ys, zs = [xx.squeeze() for xx in np.split(spline, 3, 1)]
    elif path_type == 'double-straight-line':
        corner_points = np.array([[-x, -y, -z], [0, 0, 0], [x, y, z]])
        corner_t = np.linspace(0, 1, len(corner_points))
        t = np.linspace(0, 1, num_frames)
        cs = interp1d(corner_t, corner_points, axis=0, kind='quadratic')
        spline = cs(t)
        xs, ys, zs = [xx.squeeze() for xx in np.split(spline, 3, 1)]        
    elif path_type == 'circle':
        xs, ys, zs = [], [], []
        for frame_id, bs_shift_val in enumerate(np.arange(-2.0, 2.0, (4./num_frames))):
            xs += [np.cos(bs_shift_val * np.pi) * 1 * x]
            ys += [np.sin(bs_shift_val * np.pi) * 1 * y]
            zs += [np.cos(bs_shift_val * np.pi/2.) * 1 * z]
        xs, ys, zs = np.array(xs), np.array(ys), np.array(zs)

    return xs, ys, zs

def open_small_mask(mask, context, open_iteration, kernel):
    np_mask = mask.cpu().data.numpy().squeeze().astype(np.uint8)
    raw_mask = np_mask.copy()
    np_context = context.cpu().data.numpy().squeeze().astype(np.uint8)
    np_input = np_mask + np_context
    for _ in range(open_iteration):
        np_input = cv2.erode(cv2.dilate(np_input, np.ones((kernel, kernel)), iterations=1), np.ones((kernel,kernel)), iterations=1)
    np_mask[(np_input - np_context) > 0] = 1
    out_mask = torch.FloatTensor(np_mask).to(mask)[None, None, ...]
    
    return out_mask

def filter_irrelevant_edge_new(self_edge, comp_edge, other_edges, other_edges_with_id, current_edge_id, context, depth, mesh, context_cc, spdb=False):
    other_edges = other_edges.squeeze().astype(np.uint8)
    other_edges_with_id = other_edges_with_id.squeeze()
    self_edge = self_edge.squeeze()
    dilate_bevel_self_edge = cv2.dilate((self_edge + comp_edge).astype(np.uint8), np.array([[1,1,1],[1,1,1],[1,1,1]]), iterations=1)
    dilate_cross_self_edge = cv2.dilate((self_edge + comp_edge).astype(np.uint8), np.array([[0,1,0],[1,1,1],[0,1,0]]).astype(np.uint8), iterations=1)
    edge_ids = np.unique(other_edges_with_id * context + (-1) * (1 - context)).astype(np.int32)
    end_depth_maps = np.zeros_like(self_edge)
    self_edge_ids = np.sort(np.unique(other_edges_with_id[self_edge > 0]).astype(np.int32))
    self_edge_ids = self_edge_ids[1:] if self_edge_ids.shape[0] > 0  and self_edge_ids[0] == -1 else self_edge_ids
    self_comp_ids = np.sort(np.unique(other_edges_with_id[comp_edge > 0]).astype(np.int32))
    self_comp_ids = self_comp_ids[1:] if self_comp_ids.shape[0] > 0  and self_comp_ids[0] == -1 else self_comp_ids
    edge_ids = edge_ids[1:] if edge_ids[0] == -1 else edge_ids
    other_edges_info = []
    extend_other_edges = np.zeros_like(other_edges)
    if spdb is True:
        f, ((ax1, ax2, ax3)) = plt.subplots(1, 3, sharex=True, sharey=True); ax1.imshow(self_edge); ax2.imshow(context); ax3.imshow(other_edges_with_id * context + (-1) * (1 - context)); plt.show()
        import pdb; pdb.set_trace()
    filter_self_edge = np.zeros_like(self_edge)
    for self_edge_id in self_edge_ids:
        filter_self_edge[other_edges_with_id == self_edge_id] = 1
    dilate_self_comp_edge = cv2.dilate(comp_edge, kernel=np.ones((3, 3)), iterations=2)
    valid_self_comp_edge = np.zeros_like(comp_edge)
    for self_comp_id in self_comp_ids:
        valid_self_comp_edge[self_comp_id == other_edges_with_id] = 1
    self_comp_edge = dilate_self_comp_edge * valid_self_comp_edge
    filter_self_edge = (filter_self_edge + self_comp_edge).clip(0, 1)
    for edge_id in edge_ids:
        other_edge_locs = (other_edges_with_id == edge_id).astype(np.uint8)
        condition = (other_edge_locs * other_edges * context.astype(np.uint8))
        end_cross_point = dilate_cross_self_edge * condition * (1 - filter_self_edge)
        end_bevel_point = dilate_bevel_self_edge * condition * (1 - filter_self_edge)
        if end_bevel_point.max() != 0:
            end_depth_maps[end_bevel_point != 0] = depth[end_bevel_point != 0]
            if end_cross_point.max() == 0:
                nxs, nys = np.where(end_bevel_point != 0)
                for nx, ny in zip(nxs, nys):
                    bevel_node = [xx for xx in context_cc if xx[0] == nx and xx[1] == ny][0]
                for ne in mesh.neighbors(bevel_node):
                    if other_edges_with_id[ne[0], ne[1]] > -1 and dilate_cross_self_edge[ne[0], ne[1]] > 0:
                        extend_other_edges[ne[0], ne[1]] = 1
                        break
        else:
            other_edges[other_edges_with_id == edge_id] = 0
    other_edges = (other_edges + extend_other_edges).clip(0, 1) * context

    return other_edges, end_depth_maps, other_edges_info

def clean_far_edge_new(input_edge, end_depth_maps, mask, context, global_mesh, info_on_pix, self_edge, inpaint_id, config):
    mesh = netx.Graph()
    hxs, hys = np.where(input_edge * mask > 0)
    valid_near_edge = (input_edge != 0).astype(np.uint8) * context
    valid_map = mask + context
    invalid_edge_ids = []
    for hx, hy in zip(hxs, hys):
        node = (hx ,hy)
        mesh.add_node((hx, hy))
        eight_nes = [ne for ne in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1), \
                                   (hx + 1, hy + 1), (hx - 1, hy - 1), (hx - 1, hy + 1), (hx + 1, hy - 1)]\
                        if 0 <= ne[0] < input_edge.shape[0] and 0 <= ne[1] < input_edge.shape[1] and 0 < input_edge[ne[0], ne[1]]] # or end_depth_maps[ne[0], ne[1]] != 0]
        for ne in eight_nes:
            mesh.add_edge(node, ne, length=np.hypot(ne[0] - hx, ne[1] - hy))
            if end_depth_maps[ne[0], ne[1]] != 0:
                mesh.nodes[ne[0], ne[1]]['cnt'] = True
                if end_depth_maps[ne[0], ne[1]] == 0:
                    import pdb; pdb.set_trace()
                mesh.nodes[ne[0], ne[1]]['depth'] = end_depth_maps[ne[0], ne[1]]
            elif mask[ne[0], ne[1]] != 1:
                four_nes = [nne for nne in [(ne[0] + 1, ne[1]), (ne[0] - 1, ne[1]), (ne[0], ne[1] + 1), (ne[0], ne[1] - 1)]\
                                 if nne[0] < end_depth_maps.shape[0] and nne[0] >= 0 and nne[1] < end_depth_maps.shape[1] and nne[1] >= 0]
                for nne in four_nes:
                    if end_depth_maps[nne[0], nne[1]] != 0:
                        mesh.add_edge(nne, ne, length=np.hypot(nne[0] - ne[0], nne[1] - ne[1]))
                        mesh.nodes[nne[0], nne[1]]['cnt'] = True
                        mesh.nodes[nne[0], nne[1]]['depth'] = end_depth_maps[nne[0], nne[1]]
    ccs = [*netx.connected_components(mesh)]
    end_pts = []
    for cc in ccs:
        end_pts.append(set())
        for node in cc:
            if mesh.nodes[node].get('cnt') is not None:
                end_pts[-1].add((node[0], node[1], mesh.nodes[node]['depth']))    
    predef_npaths = [None for _ in range(len(ccs))]
    fpath_map = np.zeros_like(input_edge) - 1
    npath_map = np.zeros_like(input_edge) - 1
    npaths, fpaths = dict(), dict()
    break_flag = False
    end_idx = 0
    while end_idx < len(end_pts):
        end_pt, cc = [*zip(end_pts, ccs)][end_idx]
        end_idx += 1
        sorted_end_pt = []
        fpath = []
        iter_fpath = []
        if len(end_pt) > 2 or len(end_pt) == 0:
            if len(end_pt) > 2:
                continue
            continue
        if len(end_pt) == 2:
            ravel_end = [*end_pt]
            tmp_sub_mesh = mesh.subgraph(list(cc)).copy()
            tmp_npath = [*netx.shortest_path(tmp_sub_mesh, (ravel_end[0][0], ravel_end[0][1]), (ravel_end[1][0], ravel_end[1][1]), weight='length')]
            fpath_map1, npath_map1, disp_diff1 = plan_path(mesh, info_on_pix, cc, ravel_end[0:1], global_mesh, input_edge, mask, valid_map, inpaint_id, npath_map=None, fpath_map=None, npath=tmp_npath)
            fpath_map2, npath_map2, disp_diff2 = plan_path(mesh, info_on_pix, cc, ravel_end[1:2], global_mesh, input_edge, mask, valid_map, inpaint_id, npath_map=None, fpath_map=None, npath=tmp_npath)
            tmp_disp_diff = [disp_diff1, disp_diff2]
            self_end = []
            edge_len = []
            ds_edge = cv2.dilate(self_edge.astype(np.uint8), np.ones((3, 3)), iterations=1)
            if ds_edge[ravel_end[0][0], ravel_end[0][1]] > 0:
                self_end.append(1)
            else:
                self_end.append(0)
            if ds_edge[ravel_end[1][0], ravel_end[1][1]] > 0:
                self_end.append(1)
            else:
                self_end.append(0)
            edge_len = [np.count_nonzero(npath_map1), np.count_nonzero(npath_map2)]
            sorted_end_pts = [xx[0] for xx in sorted(zip(ravel_end, self_end, edge_len, [disp_diff1, disp_diff2]), key=lambda x: (x[1], x[2]), reverse=True)]
            re_npath_map1, re_fpath_map1 = (npath_map1 != -1).astype(np.uint8), (fpath_map1 != -1).astype(np.uint8)
            re_npath_map2, re_fpath_map2 = (npath_map2 != -1).astype(np.uint8), (fpath_map2 != -1).astype(np.uint8)
            if np.count_nonzero(re_npath_map1 * re_npath_map2 * mask) / \
                (np.count_nonzero((re_npath_map1 + re_npath_map2) * mask) + 1e-6) > 0.5\
                and np.count_nonzero(re_fpath_map1 * re_fpath_map2 * mask) / \
                     (np.count_nonzero((re_fpath_map1 + re_fpath_map2) * mask) + 1e-6) > 0.5\
                and tmp_disp_diff[0] != -1 and tmp_disp_diff[1] != -1:
                my_fpath_map, my_npath_map, npath, fpath = \
                    plan_path_e2e(mesh, cc, sorted_end_pts, global_mesh, input_edge, mask, valid_map, inpaint_id, npath_map=None, fpath_map=None)
                npath_map[my_npath_map != -1] = my_npath_map[my_npath_map != -1]
                fpath_map[my_fpath_map != -1] = my_fpath_map[my_fpath_map != -1]
                if len(fpath) > 0:
                    edge_id = global_mesh.nodes[[*sorted_end_pts][0]]['edge_id']
                    fpaths[edge_id] = fpath
                    npaths[edge_id] = npath
                invalid_edge_ids.append(edge_id)
            else:
                if tmp_disp_diff[0] != -1:
                    ratio_a = tmp_disp_diff[0] / (np.sum(tmp_disp_diff) + 1e-8)
                else:
                    ratio_a = 0
                if tmp_disp_diff[1] != -1:
                    ratio_b = tmp_disp_diff[1] / (np.sum(tmp_disp_diff) + 1e-8)
                else:
                    ratio_b = 0
                npath_len = len(tmp_npath)
                if npath_len > config['depth_edge_dilate_2'] * 2:
                    npath_len = npath_len - (config['depth_edge_dilate_2'] * 1)
                tmp_npath_a = tmp_npath[:int(np.floor(npath_len * ratio_a))]
                tmp_npath_b = tmp_npath[::-1][:int(np.floor(npath_len * ratio_b))]
                tmp_merge = []
                if len(tmp_npath_a) > 0 and sorted_end_pts[0][0] == tmp_npath_a[0][0] and sorted_end_pts[0][1] == tmp_npath_a[0][1]:
                    if len(tmp_npath_a) > 0 and mask[tmp_npath_a[-1][0], tmp_npath_a[-1][1]] > 0:
                        tmp_merge.append([sorted_end_pts[:1], tmp_npath_a])
                    if len(tmp_npath_b) > 0 and mask[tmp_npath_b[-1][0], tmp_npath_b[-1][1]] > 0:
                        tmp_merge.append([sorted_end_pts[1:2], tmp_npath_b])
                elif len(tmp_npath_b) > 0 and sorted_end_pts[0][0] == tmp_npath_b[0][0] and sorted_end_pts[0][1] == tmp_npath_b[0][1]:
                    if len(tmp_npath_b) > 0 and mask[tmp_npath_b[-1][0], tmp_npath_b[-1][1]] > 0:
                        tmp_merge.append([sorted_end_pts[:1], tmp_npath_b])
                    if len(tmp_npath_a) > 0 and mask[tmp_npath_a[-1][0], tmp_npath_a[-1][1]] > 0:
                        tmp_merge.append([sorted_end_pts[1:2], tmp_npath_a])
                for tmp_idx in range(len(tmp_merge)):
                    if len(tmp_merge[tmp_idx][1]) == 0:
                        continue
                    end_pts.append(tmp_merge[tmp_idx][0])
                    ccs.append(set(tmp_merge[tmp_idx][1]))
        if len(end_pt) == 1:
            sub_mesh = mesh.subgraph(list(cc)).copy()
            pnodes = netx.periphery(sub_mesh)
            if len(end_pt) == 1:
                ends = [*end_pt]
            elif len(sorted_end_pt) == 1:
                ends = [*sorted_end_pt]
            else:
                import pdb; pdb.set_trace()
            try:
                edge_id = global_mesh.nodes[ends[0]]['edge_id']
            except:
                import pdb; pdb.set_trace()
            pnodes = sorted(pnodes, 
                            key=lambda x: np.hypot((x[0] - ends[0][0]), (x[1] - ends[0][1])),
                            reverse=True)[0]
            npath = [*netx.shortest_path(sub_mesh, (ends[0][0], ends[0][1]), pnodes, weight='length')]
            for np_node in npath:
                npath_map[np_node[0], np_node[1]] = edge_id
            fpath = []
            if global_mesh.nodes[ends[0]].get('far') is None:
                print("None far")
            else:
                fnodes = global_mesh.nodes[ends[0]].get('far')
                dmask = mask + 0
                did = 0
                while True:
                    did += 1
                    dmask = cv2.dilate(dmask, np.ones((3, 3)), iterations=1)
                    if did > 3:
                        break
                    ffnode = [fnode for fnode in fnodes if (dmask[fnode[0], fnode[1]] > 0 and mask[fnode[0], fnode[1]] == 0 and\
                                                            global_mesh.nodes[fnode].get('inpaint_id') != inpaint_id + 1)]
                    if len(ffnode) > 0:
                        fnode = ffnode[0]
                        break
                if len(ffnode) == 0:
                    continue
                fpath.append((fnode[0], fnode[1]))
                barrel_dir = np.array([[1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0], [-1, -1], [0, -1], [1, -1]])
                n2f_dir = (int(fnode[0] - npath[0][0]), int(fnode[1] - npath[0][1]))
                while True:
                    if barrel_dir[0, 0] == n2f_dir[0] and barrel_dir[0, 1] == n2f_dir[1]:
                        n2f_barrel = barrel_dir.copy()
                        break
                    barrel_dir = np.roll(barrel_dir, 1, axis=0)
                for step in range(0, len(npath)):
                    if step == 0:
                        continue
                    elif step == 1:
                        next_dir = (npath[step][0] - npath[step - 1][0], npath[step][1] - npath[step - 1][1])
                        while True:
                            if barrel_dir[0, 0] == next_dir[0] and barrel_dir[0, 1] == next_dir[1]:
                                next_barrel = barrel_dir.copy()
                                break
                            barrel_dir = np.roll(barrel_dir, 1, axis=0)
                        barrel_pair = np.stack((n2f_barrel, next_barrel), axis=0)
                        n2f_dir = (barrel_pair[0, 0, 0], barrel_pair[0, 0, 1])
                    elif step > 1:
                        next_dir = (npath[step][0] - npath[step - 1][0], npath[step][1] - npath[step - 1][1])
                        while True:
                            if barrel_pair[1, 0, 0] == next_dir[0] and barrel_pair[1, 0, 1] == next_dir[1]:
                                next_barrel = barrel_pair.copy()
                                break
                            barrel_pair = np.roll(barrel_pair, 1, axis=1)
                        n2f_dir = (barrel_pair[0, 0, 0], barrel_pair[0, 0, 1])
                    new_locs = []
                    if abs(n2f_dir[0]) == 1:
                        new_locs.append((npath[step][0] + n2f_dir[0], npath[step][1]))
                    if abs(n2f_dir[1]) == 1:
                        new_locs.append((npath[step][0], npath[step][1] + n2f_dir[1]))
                    if len(new_locs) > 1:
                        new_locs = sorted(new_locs, key=lambda xx: np.hypot((xx[0] - fpath[-1][0]), (xx[1] - fpath[-1][1])))
                    break_flag = False
                    for new_loc in new_locs:
                        new_loc_nes = [xx for xx in [(new_loc[0] + 1, new_loc[1]), (new_loc[0] - 1, new_loc[1]), 
                                                    (new_loc[0], new_loc[1] + 1), (new_loc[0], new_loc[1] - 1)]\
                                            if xx[0] >= 0 and xx[0] < fpath_map.shape[0] and xx[1] >= 0 and xx[1] < fpath_map.shape[1]]
                        if np.all([(fpath_map[nlne[0], nlne[1]] == -1) for nlne in new_loc_nes]) != True:
                            break
                        if new_loc[1] >= npath_map.shape[1]:
                            continue
                        if npath_map[new_loc[0], new_loc[1]] != -1:
                            if npath_map[new_loc[0], new_loc[1]] != edge_id:
                                break_flag = True
                                break
                            else:
                                continue
                        if valid_map[new_loc[0], new_loc[1]] == 0:
                            break_flag = True
                            break
                        fpath.append(new_loc)
                    if break_flag is True:
                        break
                if step != len(npath) - 1:
                    for xx in npath[step:]:
                        if npath_map[xx[0], xx[1]] == edge_id:
                            npath_map[xx[0], xx[1]] = -1
                    npath = npath[:step]
            if len(fpath) > 0:
                for fp_node in fpath:
                    fpath_map[fp_node[0], fp_node[1]] = edge_id
                fpaths[edge_id] = fpath
                npaths[edge_id] = npath
        fpath_map[valid_near_edge != 0] = -1
        if len(fpath) > 0:
            iter_fpath = copy.deepcopy(fpaths[edge_id])
        for node in iter_fpath:
            if valid_near_edge[node[0], node[1]] != 0:
                fpaths[edge_id].remove(node)

    return fpath_map, npath_map, False, npaths, fpaths, invalid_edge_ids

def plan_path_e2e(mesh, cc, end_pts, global_mesh, input_edge, mask, valid_map, inpaint_id, npath_map=None, fpath_map=None):
    my_npath_map = np.zeros_like(input_edge) - 1
    my_fpath_map = np.zeros_like(input_edge) - 1
    sub_mesh = mesh.subgraph(list(cc)).copy()
    ends_1, ends_2 = end_pts[0], end_pts[1]
    edge_id = global_mesh.nodes[ends_1]['edge_id']
    npath = [*netx.shortest_path(sub_mesh, (ends_1[0], ends_1[1]), (ends_2[0], ends_2[1]), weight='length')]
    for np_node in npath:
        my_npath_map[np_node[0], np_node[1]] = edge_id
    fpath = []
    if global_mesh.nodes[ends_1].get('far') is None:
        print("None far")
    else:
        fnodes = global_mesh.nodes[ends_1].get('far')
        dmask = mask + 0
        while True:
            dmask = cv2.dilate(dmask, np.ones((3, 3)), iterations=1)
            ffnode = [fnode for fnode in fnodes if (dmask[fnode[0], fnode[1]] > 0 and mask[fnode[0], fnode[1]] == 0 and\
                                                            global_mesh.nodes[fnode].get('inpaint_id') != inpaint_id + 1)]
            if len(ffnode) > 0:
                fnode = ffnode[0]
                break
        e_fnodes = global_mesh.nodes[ends_2].get('far')
        dmask = mask + 0
        while True:
            dmask = cv2.dilate(dmask, np.ones((3, 3)), iterations=1)
            e_ffnode = [e_fnode for e_fnode in e_fnodes if (dmask[e_fnode[0], e_fnode[1]] > 0 and mask[e_fnode[0], e_fnode[1]] == 0 and\
                                                            global_mesh.nodes[e_fnode].get('inpaint_id') != inpaint_id + 1)]
            if len(e_ffnode) > 0:
                e_fnode = e_ffnode[0]
                break            
        fpath.append((fnode[0], fnode[1]))
        if len(e_ffnode) == 0 or len(ffnode) == 0:
            return my_npath_map, my_fpath_map, [], []
        barrel_dir = np.array([[1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0], [-1, -1], [0, -1], [1, -1]])
        n2f_dir = (int(fnode[0] - npath[0][0]), int(fnode[1] - npath[0][1]))
        while True:
            if barrel_dir[0, 0] == n2f_dir[0] and barrel_dir[0, 1] == n2f_dir[1]:
                n2f_barrel = barrel_dir.copy()
                break
            barrel_dir = np.roll(barrel_dir, 1, axis=0)
        for step in range(0, len(npath)):
            if step == 0:
                continue
            elif step == 1:
                next_dir = (npath[step][0] - npath[step - 1][0], npath[step][1] - npath[step - 1][1])
                while True:
                    if barrel_dir[0, 0] == next_dir[0] and barrel_dir[0, 1] == next_dir[1]:
                        next_barrel = barrel_dir.copy()
                        break
                    barrel_dir = np.roll(barrel_dir, 1, axis=0)
                barrel_pair = np.stack((n2f_barrel, next_barrel), axis=0)
                n2f_dir = (barrel_pair[0, 0, 0], barrel_pair[0, 0, 1])
            elif step > 1:
                next_dir = (npath[step][0] - npath[step - 1][0], npath[step][1] - npath[step - 1][1])
                while True:
                    if barrel_pair[1, 0, 0] == next_dir[0] and barrel_pair[1, 0, 1] == next_dir[1]:
                        next_barrel = barrel_pair.copy()
                        break
                    barrel_pair = np.roll(barrel_pair, 1, axis=1)
                n2f_dir = (barrel_pair[0, 0, 0], barrel_pair[0, 0, 1])
            new_locs = []
            if abs(n2f_dir[0]) == 1:
                new_locs.append((npath[step][0] + n2f_dir[0], npath[step][1]))
            if abs(n2f_dir[1]) == 1:
                new_locs.append((npath[step][0], npath[step][1] + n2f_dir[1]))
            if len(new_locs) > 1:
                new_locs = sorted(new_locs, key=lambda xx: np.hypot((xx[0] - fpath[-1][0]), (xx[1] - fpath[-1][1])))
            break_flag = False
            for new_loc in new_locs:
                new_loc_nes = [xx for xx in [(new_loc[0] + 1, new_loc[1]), (new_loc[0] - 1, new_loc[1]),
                                            (new_loc[0], new_loc[1] + 1), (new_loc[0], new_loc[1] - 1)]\
                                    if xx[0] >= 0 and xx[0] < my_fpath_map.shape[0] and xx[1] >= 0 and xx[1] < my_fpath_map.shape[1]]
                if fpath_map is not None and np.sum([fpath_map[nlne[0], nlne[1]] for nlne in new_loc_nes]) != 0:
                    break_flag = True
                    break
                if my_npath_map[new_loc[0], new_loc[1]] != -1:
                    continue
                if npath_map is not None and npath_map[new_loc[0], new_loc[1]] != edge_id:
                    break_flag = True
                    break
                fpath.append(new_loc)
            if break_flag is True:
                break
        if (e_fnode[0], e_fnode[1]) not in fpath:
            fpath.append((e_fnode[0], e_fnode[1]))
        if step != len(npath) - 1:
            for xx in npath[step:]:
                if my_npath_map[xx[0], xx[1]] == edge_id:
                    my_npath_map[xx[0], xx[1]] = -1
            npath = npath[:step]
        if len(fpath) > 0:
            for fp_node in fpath:
                my_fpath_map[fp_node[0], fp_node[1]] = edge_id
    
    return my_fpath_map, my_npath_map, npath, fpath

def plan_path(mesh, info_on_pix, cc, end_pt, global_mesh, input_edge, mask, valid_map, inpaint_id, npath_map=None, fpath_map=None, npath=None):
    my_npath_map = np.zeros_like(input_edge) - 1
    my_fpath_map = np.zeros_like(input_edge) - 1
    sub_mesh = mesh.subgraph(list(cc)).copy()
    pnodes = netx.periphery(sub_mesh)
    ends = [*end_pt]
    edge_id = global_mesh.nodes[ends[0]]['edge_id']
    pnodes = sorted(pnodes, 
                    key=lambda x: np.hypot((x[0] - ends[0][0]), (x[1] - ends[0][1])),
                    reverse=True)[0]
    if npath is None:
        npath = [*netx.shortest_path(sub_mesh, (ends[0][0], ends[0][1]), pnodes, weight='length')]
    else:
        if (ends[0][0], ends[0][1]) == npath[0]:
            npath = npath
        elif (ends[0][0], ends[0][1]) == npath[-1]:
            npath = npath[::-1]
        else:
            import pdb; pdb.set_trace()
    for np_node in npath:
        my_npath_map[np_node[0], np_node[1]] = edge_id
    fpath = []
    if global_mesh.nodes[ends[0]].get('far') is None:
        print("None far")
    else:
        fnodes = global_mesh.nodes[ends[0]].get('far')
        dmask = mask + 0
        did = 0
        while True:
            did += 1
            if did > 3:
                return my_fpath_map, my_npath_map, -1
            dmask = cv2.dilate(dmask, np.ones((3, 3)), iterations=1)
            ffnode = [fnode for fnode in fnodes if (dmask[fnode[0], fnode[1]] > 0 and mask[fnode[0], fnode[1]] == 0 and\
                                                            global_mesh.nodes[fnode].get('inpaint_id') != inpaint_id + 1)]
            if len(ffnode) > 0:
                fnode = ffnode[0]
                break
        
        fpath.append((fnode[0], fnode[1]))
        disp_diff = 0.
        for n_loc in npath:
            if mask[n_loc[0], n_loc[1]] != 0:
                disp_diff = abs(abs(1. / info_on_pix[(n_loc[0], n_loc[1])][0]['depth']) - abs(1. / ends[0][2]))
                break
        barrel_dir = np.array([[1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0], [-1, -1], [0, -1], [1, -1]])
        n2f_dir = (int(fnode[0] - npath[0][0]), int(fnode[1] - npath[0][1]))
        while True:
            if barrel_dir[0, 0] == n2f_dir[0] and barrel_dir[0, 1] == n2f_dir[1]:
                n2f_barrel = barrel_dir.copy()
                break
            barrel_dir = np.roll(barrel_dir, 1, axis=0)
        for step in range(0, len(npath)):
            if step == 0:
                continue
            elif step == 1:
                next_dir = (npath[step][0] - npath[step - 1][0], npath[step][1] - npath[step - 1][1])
                while True:
                    if barrel_dir[0, 0] == next_dir[0] and barrel_dir[0, 1] == next_dir[1]:
                        next_barrel = barrel_dir.copy()
                        break
                    barrel_dir = np.roll(barrel_dir, 1, axis=0)
                barrel_pair = np.stack((n2f_barrel, next_barrel), axis=0)
                n2f_dir = (barrel_pair[0, 0, 0], barrel_pair[0, 0, 1])
            elif step > 1:
                next_dir = (npath[step][0] - npath[step - 1][0], npath[step][1] - npath[step - 1][1])
                while True:
                    if barrel_pair[1, 0, 0] == next_dir[0] and barrel_pair[1, 0, 1] == next_dir[1]:
                        next_barrel = barrel_pair.copy()
                        break
                    barrel_pair = np.roll(barrel_pair, 1, axis=1)
                n2f_dir = (barrel_pair[0, 0, 0], barrel_pair[0, 0, 1])
            new_locs = []
            if abs(n2f_dir[0]) == 1:
                new_locs.append((npath[step][0] + n2f_dir[0], npath[step][1]))
            if abs(n2f_dir[1]) == 1:
                new_locs.append((npath[step][0], npath[step][1] + n2f_dir[1]))
            if len(new_locs) > 1:
                new_locs = sorted(new_locs, key=lambda xx: np.hypot((xx[0] - fpath[-1][0]), (xx[1] - fpath[-1][1])))
            break_flag = False
            for new_loc in new_locs:
                new_loc_nes = [xx for xx in [(new_loc[0] + 1, new_loc[1]), (new_loc[0] - 1, new_loc[1]),
                                        (new_loc[0], new_loc[1] + 1), (new_loc[0], new_loc[1] - 1)]\
                                if xx[0] >= 0 and xx[0] < my_fpath_map.shape[0] and xx[1] >= 0 and xx[1] < my_fpath_map.shape[1]]
                if fpath_map is not None and np.all([(fpath_map[nlne[0], nlne[1]] == -1) for nlne in new_loc_nes]) != True:
                    break_flag = True
                    break
                if np.all([(my_fpath_map[nlne[0], nlne[1]] == -1) for nlne in new_loc_nes]) != True:
                    break_flag = True
                    break 
                if my_npath_map[new_loc[0], new_loc[1]] != -1:
                    continue
                if npath_map is not None and npath_map[new_loc[0], new_loc[1]] != edge_id:
                    break_flag = True
                    break
                if valid_map[new_loc[0], new_loc[1]] == 0:
                    break_flag = True
                    break
                fpath.append(new_loc)
            if break_flag is True:
                break
        if step != len(npath) - 1:
            for xx in npath[step:]:
                if my_npath_map[xx[0], xx[1]] == edge_id:
                    my_npath_map[xx[0], xx[1]] = -1
            npath = npath[:step]
        if len(fpath) > 0:
            for fp_node in fpath:
                my_fpath_map[fp_node[0], fp_node[1]] = edge_id

    return my_fpath_map, my_npath_map, disp_diff

def refresh_node(old_node, old_feat, new_node, new_feat, mesh, stime=False):
    mesh.add_node(new_node)
    mesh.nodes[new_node].update(new_feat)
    mesh.nodes[new_node].update(old_feat)
    for ne in mesh.neighbors(old_node):
        mesh.add_edge(new_node, ne)
    if mesh.nodes[new_node].get('far') is not None:
        tmp_far_nodes = mesh.nodes[new_node]['far']
        for far_node in tmp_far_nodes:
            if mesh.has_node(far_node) is False:
                mesh.nodes[new_node]['far'].remove(far_node)
                continue
            if mesh.nodes[far_node].get('near') is not None:
                for idx in range(len(mesh.nodes[far_node].get('near'))):
                    if mesh.nodes[far_node]['near'][idx][0] == new_node[0] and mesh.nodes[far_node]['near'][idx][1] == new_node[1]:
                        if len(mesh.nodes[far_node]['near'][idx]) == len(old_node):
                            mesh.nodes[far_node]['near'][idx] = new_node
    if mesh.nodes[new_node].get('near') is not None:
        tmp_near_nodes = mesh.nodes[new_node]['near']
        for near_node in tmp_near_nodes:
            if mesh.has_node(near_node) is False:
                mesh.nodes[new_node]['near'].remove(near_node)
                continue        
            if mesh.nodes[near_node].get('far') is not None:
                for idx in range(len(mesh.nodes[near_node].get('far'))):
                    if mesh.nodes[near_node]['far'][idx][0] == new_node[0] and mesh.nodes[near_node]['far'][idx][1] == new_node[1]:
                        if len(mesh.nodes[near_node]['far'][idx]) == len(old_node):
                            mesh.nodes[near_node]['far'][idx] = new_node
    if new_node != old_node:
        mesh.remove_node(old_node)
    if stime is False:
        return mesh
    else:
        return mesh, None, None


def create_placeholder(context, mask, depth, fpath_map, npath_map, mesh, inpaint_id, edge_ccs, extend_edge_cc, all_edge_maps, self_edge_id):
    add_node_time = 0
    add_edge_time = 0
    add_far_near_time = 0
    valid_area = context + mask
    H, W = mesh.graph['H'], mesh.graph['W']
    edge_cc = edge_ccs[self_edge_id]
    num_com = len(edge_cc) + len(extend_edge_cc)
    hxs, hys = np.where(mask > 0)
    for hx, hy in zip(hxs, hys):
        mesh.add_node((hx, hy), inpaint_id=inpaint_id + 1, num_context=num_com)
    for hx, hy in zip(hxs, hys):
        four_nes = [(x, y) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] if\
                        0 <= x < mesh.graph['H'] and 0 <= y < mesh.graph['W'] and valid_area[x, y] != 0]
        for ne in four_nes:
            if mask[ne[0], ne[1]] != 0:
                if not mesh.has_edge((hx, hy), ne):
                    mesh.add_edge((hx, hy), ne)
            elif depth[ne[0], ne[1]] != 0:
                if mesh.has_node((ne[0], ne[1], depth[ne[0], ne[1]])) and\
                    not mesh.has_edge((hx, hy), (ne[0], ne[1], depth[ne[0], ne[1]])):
                    mesh.add_edge((hx, hy), (ne[0], ne[1], depth[ne[0], ne[1]]))
                else:
                    print("Undefined context node.")
                    import pdb; pdb.set_trace()
    near_ids = np.unique(npath_map)
    if near_ids[0] == -1: near_ids = near_ids[1:]
    for near_id in near_ids:
        hxs, hys = np.where((fpath_map == near_id) & (mask > 0))
        if hxs.shape[0] > 0:
            mesh.graph['max_edge_id'] = mesh.graph['max_edge_id'] + 1
        else:
            break
        for hx, hy in zip(hxs, hys):
            mesh.nodes[(hx, hy)]['edge_id'] = int(round(mesh.graph['max_edge_id']))
            four_nes = [(x, y) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] if\
                        x < mesh.graph['H'] and x >= 0 and y < mesh.graph['W'] and y >= 0 and npath_map[x, y] == near_id]
            for xx in four_nes:
                xx_n = copy.deepcopy(xx)
                if not mesh.has_node(xx_n):
                    if mesh.has_node((xx_n[0], xx_n[1], depth[xx_n[0], xx_n[1]])):
                        xx_n = (xx_n[0], xx_n[1], depth[xx_n[0], xx_n[1]])
                if mesh.has_edge((hx, hy), xx_n):
                    # pass
                    mesh.remove_edge((hx, hy), xx_n)
                if mesh.nodes[(hx, hy)].get('near') is None:
                    mesh.nodes[(hx, hy)]['near'] = []
                mesh.nodes[(hx, hy)]['near'].append(xx_n)
        connect_point_exception = set()
        hxs, hys = np.where((npath_map == near_id) & (all_edge_maps > -1))
        for hx, hy in zip(hxs, hys):
            unknown_id = int(round(all_edge_maps[hx, hy]))
            if unknown_id != near_id and unknown_id != self_edge_id:
                unknown_node = set([xx for xx in edge_ccs[unknown_id] if xx[0] == hx and xx[1] == hy])
                connect_point_exception |= unknown_node
        hxs, hys = np.where((npath_map == near_id) & (mask > 0))                
        if hxs.shape[0] > 0:
            mesh.graph['max_edge_id'] = mesh.graph['max_edge_id'] + 1
        else:
            break
        for hx, hy in zip(hxs, hys):
            mesh.nodes[(hx, hy)]['edge_id'] = int(round(mesh.graph['max_edge_id']))
            mesh.nodes[(hx, hy)]['connect_point_id'] = int(round(near_id)) 
            mesh.nodes[(hx, hy)]['connect_point_exception'] = connect_point_exception
            four_nes = [(x, y) for x, y in [(hx + 1, hy), (hx - 1, hy), (hx, hy + 1), (hx, hy - 1)] if\
                        x < mesh.graph['H'] and x >= 0 and y < mesh.graph['W'] and y >= 0 and fpath_map[x, y] == near_id]
            for xx in four_nes:
                xx_n = copy.deepcopy(xx)
                if not mesh.has_node(xx_n):
                    if mesh.has_node((xx_n[0], xx_n[1], depth[xx_n[0], xx_n[1]])):
                        xx_n = (xx_n[0], xx_n[1], depth[xx_n[0], xx_n[1]])
                if mesh.has_edge((hx, hy), xx_n):
                    mesh.remove_edge((hx, hy), xx_n)
                if mesh.nodes[(hx, hy)].get('far') is None:
                    mesh.nodes[(hx, hy)]['far'] = []
                mesh.nodes[(hx, hy)]['far'].append(xx_n)

    return mesh, add_node_time, add_edge_time, add_far_near_time

def clean_far_edge(mask_edge, mask_edge_with_id, context_edge, mask, info_on_pix, global_mesh, anchor):
    if isinstance(mask_edge, torch.Tensor):
        if mask_edge.is_cuda:
            mask_edge = mask_edge.cpu()
        mask_edge = mask_edge.data
        mask_edge = mask_edge.numpy()
    if isinstance(context_edge, torch.Tensor):
        if context_edge.is_cuda:
            context_edge = context_edge.cpu()
        context_edge = context_edge.data
        context_edge = context_edge.numpy()
    if isinstance(mask, torch.Tensor):
        if mask.is_cuda:
            mask = mask.cpu()
        mask = mask.data
        mask = mask.numpy()
    mask = mask.squeeze()
    mask_edge = mask_edge.squeeze()
    context_edge = context_edge.squeeze()
    valid_near_edge = np.zeros_like(mask_edge)
    far_edge = np.zeros_like(mask_edge)
    far_edge_with_id = np.ones_like(mask_edge) * -1
    near_edge_with_id = np.ones_like(mask_edge) * -1
    uncleaned_far_edge = np.zeros_like(mask_edge)
    # Detect if there is any valid pixel mask_edge, if not ==> return default value
    if mask_edge.sum() == 0:
        return far_edge, uncleaned_far_edge, far_edge_with_id, near_edge_with_id
    mask_edge_ids = dict(collections.Counter(mask_edge_with_id.flatten())).keys()
    for edge_id in mask_edge_ids:
        if edge_id < 0:
            continue
        specific_edge_map = (mask_edge_with_id == edge_id).astype(np.uint8)
        _, sub_specific_edge_maps = cv2.connectedComponents(specific_edge_map.astype(np.uint8), connectivity=8)
        for sub_edge_id in range(1, sub_specific_edge_maps.max() + 1):
            specific_edge_map = (sub_specific_edge_maps == sub_edge_id).astype(np.uint8)
            edge_pxs, edge_pys = np.where(specific_edge_map > 0)
            edge_mesh = netx.Graph()
            for edge_px, edge_py in zip(edge_pxs, edge_pys):
                edge_mesh.add_node((edge_px, edge_py))
                for ex in [edge_px-1, edge_px, edge_px+1]:
                    for ey in [edge_py-1, edge_py, edge_py+1]:
                        if edge_px == ex and edge_py == ey:
                            continue
                        if ex < 0 or ex >= specific_edge_map.shape[0] or ey < 0 or ey >= specific_edge_map.shape[1]:
                            continue
                        if specific_edge_map[ex, ey] == 1:
                            if edge_mesh.has_node((ex, ey)):
                                edge_mesh.add_edge((ex, ey), (edge_px, edge_py))
            periphery_nodes = netx.periphery(edge_mesh)
            path_diameter = netx.diameter(edge_mesh)
            start_near_node = None
            for node_s in periphery_nodes:
                for node_e in periphery_nodes:
                    if node_s != node_e:
                        if netx.shortest_path_length(edge_mesh, node_s, node_e) == path_diameter:
                            if np.any(context_edge[node_s[0]-1:node_s[0]+2, node_s[1]-1:node_s[1]+2].flatten()):
                                start_near_node = (node_s[0], node_s[1])
                                end_near_node = (node_e[0], node_e[1])
                                break
                            if np.any(context_edge[node_e[0]-1:node_e[0]+2, node_e[1]-1:node_e[1]+2].flatten()):
                                start_near_node = (node_e[0], node_e[1])
                                end_near_node = (node_s[0], node_s[1])
                                break
                if start_near_node is not None:
                    break
            if start_near_node is None:
                continue
            new_specific_edge_map = np.zeros_like(mask)
            for path_node in netx.shortest_path(edge_mesh, start_near_node, end_near_node):
                new_specific_edge_map[path_node[0], path_node[1]] = 1
            context_near_pxs, context_near_pys = np.where(context_edge[start_near_node[0]-1:start_near_node[0]+2, start_near_node[1]-1:start_near_node[1]+2] > 0)
            distance = np.abs((context_near_pxs - 1)) + np.abs((context_near_pys - 1))
            if (np.where(distance == distance.min())[0].shape[0]) > 1:
                closest_pxs = context_near_pxs[np.where(distance == distance.min())[0]]
                closest_pys = context_near_pys[np.where(distance == distance.min())[0]]
                closest_depths = []
                for closest_px, closest_py in zip(closest_pxs, closest_pys):
                    if info_on_pix.get((closest_px + start_near_node[0] - 1 + anchor[0], closest_py + start_near_node[1] - 1 + anchor[2])) is not None:
                        for info in info_on_pix.get((closest_px + start_near_node[0] - 1 + anchor[0], closest_py + start_near_node[1] - 1 + anchor[2])):
                            if info['synthesis'] is False:
                                closest_depths.append(abs(info['depth']))
                context_near_px, context_near_py = closest_pxs[np.array(closest_depths).argmax()], closest_pys[np.array(closest_depths).argmax()]
            else:
                context_near_px, context_near_py = context_near_pxs[distance.argmin()], context_near_pys[distance.argmin()]
            context_near_node = (start_near_node[0]-1 + context_near_px, start_near_node[1]-1 + context_near_py)
            far_node_list = []
            global_context_near_node = (context_near_node[0] + anchor[0], context_near_node[1] + anchor[2])
            if info_on_pix.get(global_context_near_node) is not None:
                for info in info_on_pix[global_context_near_node]:
                    if info['synthesis'] is False:
                        context_near_node_3d = (global_context_near_node[0], global_context_near_node[1], info['depth'])
                        if global_mesh.nodes[context_near_node_3d].get('far') is not None:
                            for far_node in global_mesh.nodes[context_near_node_3d].get('far'):
                                far_node = (far_node[0] - anchor[0], far_node[1] - anchor[2], far_node[2])
                                if mask[far_node[0], far_node[1]] == 0:
                                    far_node_list.append([far_node[0], far_node[1]])
            if len(far_node_list) > 0:
                far_nodes_dist = np.sum(np.abs(np.array(far_node_list) - np.array([[edge_px, edge_py]])), axis=1)
                context_far_node = tuple(far_node_list[far_nodes_dist.argmin()])
                corresponding_far_edge = np.zeros_like(mask_edge)
                corresponding_far_edge[context_far_node[0], context_far_node[1]] = 1
                surround_map = cv2.dilate(new_specific_edge_map.astype(np.uint8), 
                                            np.array([[1,1,1],[1,1,1],[1,1,1]]).astype(np.uint8), 
                                            iterations=1)
                specific_edge_map_wo_end_pt = new_specific_edge_map.copy()
                specific_edge_map_wo_end_pt[end_near_node[0], end_near_node[1]] = 0
                surround_map_wo_end_pt = cv2.dilate(specific_edge_map_wo_end_pt.astype(np.uint8), 
                                                    np.array([[1,1,1],[1,1,1],[1,1,1]]).astype(np.uint8), 
                                                    iterations=1)
                surround_map_wo_end_pt[new_specific_edge_map > 0] = 0
                surround_map_wo_end_pt[context_near_node[0], context_near_node[1]] = 0
                surround_map = surround_map_wo_end_pt.copy()
                _, far_edge_cc = cv2.connectedComponents(surround_map.astype(np.uint8), connectivity=4)
                start_far_node = None
                accompany_far_node = None
                if surround_map[context_far_node[0], context_far_node[1]] == 1:
                    start_far_node = context_far_node
                else:
                    four_nes = [(context_far_node[0] - 1, context_far_node[1]), 
                            (context_far_node[0] + 1, context_far_node[1]), 
                            (context_far_node[0], context_far_node[1] - 1), 
                            (context_far_node[0], context_far_node[1] + 1)]
                    candidate_bevel = []            
                    for ne in four_nes:
                        if surround_map[ne[0], ne[1]] == 1:
                            start_far_node = (ne[0], ne[1])
                            break
                        elif (ne[0] != context_near_node[0] or ne[1] != context_near_node[1]) and \
                                (ne[0] != start_near_node[0] or ne[1] != start_near_node[1]):
                            candidate_bevel.append((ne[0], ne[1]))
                    if start_far_node is None:
                        for ne in candidate_bevel:
                            if ne[0] == context_far_node[0]:
                                bevel_xys = [[ne[0] + 1, ne[1]], [ne[0] - 1, ne[1]]]
                            if ne[1] == context_far_node[1]:
                                bevel_xys = [[ne[0], ne[1] + 1], [ne[0], ne[1] - 1]]
                            for bevel_x, bevel_y in bevel_xys:
                                if surround_map[bevel_x, bevel_y] == 1:
                                    start_far_node = (bevel_x, bevel_y)
                                    accompany_far_node = (ne[0], ne[1])
                                    break
                            if start_far_node is not None:
                                break
                if start_far_node is not None:
                    for far_edge_id in range(1, far_edge_cc.max() + 1):
                        specific_far_edge = (far_edge_cc == far_edge_id).astype(np.uint8)
                        if specific_far_edge[start_far_node[0], start_far_node[1]] == 1:
                            if accompany_far_node is not None:
                                specific_far_edge[accompany_far_node] = 1
                            far_edge[specific_far_edge > 0] = 1
                            far_edge_with_id[specific_far_edge > 0] = edge_id
                            end_far_candidates = np.zeros_like(far_edge)
                            end_far_candidates[end_near_node[0], end_near_node[1]] = 1
                            end_far_candidates = cv2.dilate(end_far_candidates.astype(np.uint8), 
                                                            np.array([[0,1,0],[1,1,1],[0,1,0]]).astype(np.uint8), 
                                                            iterations=1)
                            end_far_candidates[end_near_node[0], end_near_node[1]] = 0
                            invalid_nodes = (((far_edge_cc != far_edge_id).astype(np.uint8) * \
                                              (far_edge_cc != 0).astype(np.uint8)).astype(np.uint8) + \
                                             (new_specific_edge_map).astype(np.uint8) + \
                                             (mask == 0).astype(np.uint8)).clip(0, 1)
                            end_far_candidates[invalid_nodes > 0] = 0
                            far_edge[end_far_candidates > 0] = 1
                            far_edge_with_id[end_far_candidates > 0] = edge_id
                            
                    far_edge[context_far_node[0], context_far_node[1]] = 1
                    far_edge_with_id[context_far_node[0], context_far_node[1]] = edge_id
                near_edge_with_id[(mask_edge_with_id == edge_id) > 0] = edge_id
    uncleaned_far_edge = far_edge.copy()
    far_edge[mask == 0] = 0

    return far_edge, uncleaned_far_edge, far_edge_with_id, near_edge_with_id

def get_MiDaS_samples(image_folder, depth_folder, config, specific=None, aft_certain=None):
    lines = [os.path.splitext(os.path.basename(xx))[0] for xx in glob.glob(os.path.join(image_folder, '*' + config['img_format']))]
    samples = []
    generic_pose = np.eye(4)
    assert len(config['traj_types']) == len(config['x_shift_range']) ==\
           len(config['y_shift_range']) == len(config['z_shift_range']) == len(config['video_postfix']), \
           "The number of elements in 'traj_types', 'x_shift_range', 'y_shift_range', 'z_shift_range' and \
               'video_postfix' should be equal."
    tgt_pose = [[generic_pose * 1]]
    tgts_poses = []
    for traj_idx in range(len(config['traj_types'])):
        tgt_poses = []
        sx, sy, sz = path_planning(config['num_frames'], config['x_shift_range'][traj_idx], config['y_shift_range'][traj_idx],
                                   config['z_shift_range'][traj_idx], path_type=config['traj_types'][traj_idx])
        for xx, yy, zz in zip(sx, sy, sz):
            tgt_poses.append(generic_pose * 1.)
            tgt_poses[-1][:3, -1] = np.array([xx, yy, zz])
        tgts_poses += [tgt_poses]    
    tgt_pose = generic_pose * 1
    
    aft_flag = True
    if aft_certain is not None and len(aft_certain) > 0:
        aft_flag = False
    for seq_dir in lines:
        if specific is not None and len(specific) > 0:
            if specific != seq_dir:
                continue
        if aft_certain is not None and len(aft_certain) > 0:
            if aft_certain == seq_dir:
                aft_flag = True
            if aft_flag is False:
                continue
        samples.append({})
        sdict = samples[-1]            
        sdict['depth_fi'] = os.path.join(depth_folder, seq_dir + config['depth_format'])
        sdict['ref_img_fi'] = os.path.join(image_folder, seq_dir + config['img_format'])
        H, W = imageio.imread(sdict['ref_img_fi']).shape[:2]
        if os.path.exists('./0001_rgb_cam_all.json'):
            f = open('./0001_rgb_cam_all.json')
            data = json.load(f)
            sdict['int_mtx'] = np.array(data[str(int(seq_dir[-3:]))]["intrinsics"]["matrix"]).astype(np.float32)
            print(sdict['int_mtx'])
        else:
            sdict['int_mtx'] = np.array([[max(H, W), 0, W//2], [0, max(H, W), H//2], [0, 0, 1]]).astype(np.float32)
        if sdict['int_mtx'].max() > 1:
            sdict['int_mtx'][0, :] = sdict['int_mtx'][0, :] / float(W)
            sdict['int_mtx'][1, :] = sdict['int_mtx'][1, :] / float(H)
        sdict['ref_pose'] = np.eye(4)
        sdict['tgt_pose'] = tgt_pose
        sdict['tgts_poses'] = tgts_poses
        sdict['video_postfix'] = config['video_postfix']
        sdict['tgt_name'] = [os.path.splitext(os.path.basename(sdict['depth_fi']))[0]]
        sdict['src_pair_name'] = sdict['tgt_name'][0]

    return samples

def get_valid_size(imap):
    x_max = np.where(imap.sum(1).squeeze() > 0)[0].max() + 1
    x_min = np.where(imap.sum(1).squeeze() > 0)[0].min()
    y_max = np.where(imap.sum(0).squeeze() > 0)[0].max() + 1
    y_min = np.where(imap.sum(0).squeeze() > 0)[0].min()
    size_dict = {'x_max':x_max, 'y_max':y_max, 'x_min':x_min, 'y_min':y_min}
    
    return size_dict

def dilate_valid_size(isize_dict, imap, dilate=[0, 0]):
    osize_dict = copy.deepcopy(isize_dict)
    osize_dict['x_min'] = max(0, osize_dict['x_min'] - dilate[0])
    osize_dict['x_max'] = min(imap.shape[0], osize_dict['x_max'] + dilate[0])
    osize_dict['y_min'] = max(0, osize_dict['y_min'] - dilate[0])
    osize_dict['y_max'] = min(imap.shape[1], osize_dict['y_max'] + dilate[1])

    return osize_dict

def crop_maps_by_size(size, *imaps):
    omaps = []
    for imap in imaps:
        omaps.append(imap[size['x_min']:size['x_max'], size['y_min']:size['y_max']].copy())
    
    return omaps

def smooth_cntsyn_gap(init_depth_map, mask_region, context_region, init_mask_region=None):
    if init_mask_region is not None:
        curr_mask_region = init_mask_region * 1
    else:
        curr_mask_region = mask_region * 0
    depth_map = init_depth_map.copy()
    for _ in range(2):
        cm_mask = context_region + curr_mask_region
        depth_s1 = np.roll(depth_map, 1, 0)
        depth_s2 = np.roll(depth_map, -1, 0)
        depth_s3 = np.roll(depth_map, 1, 1)
        depth_s4 = np.roll(depth_map, -1, 1)
        mask_s1 = np.roll(cm_mask, 1, 0)
        mask_s2 = np.roll(cm_mask, -1, 0)
        mask_s3 = np.roll(cm_mask, 1, 1)
        mask_s4 = np.roll(cm_mask, -1, 1)
        fluxin_depths = (depth_s1 * mask_s1 + depth_s2 * mask_s2 + depth_s3 * mask_s3 + depth_s4 * mask_s4) / \
                        ((mask_s1 + mask_s2 + mask_s3 + mask_s4) + 1e-6)
        fluxin_mask = (fluxin_depths != 0) * mask_region
        init_mask = (fluxin_mask * (curr_mask_region >= 0).astype(np.float32) > 0).astype(np.uint8)
        depth_map[init_mask > 0] = fluxin_depths[init_mask > 0]
        if init_mask.shape[-1] > curr_mask_region.shape[-1]:
            curr_mask_region[init_mask.sum(-1, keepdims=True) > 0] = 1
        else:
            curr_mask_region[init_mask > 0] = 1
        depth_map[fluxin_mask > 0] = fluxin_depths[fluxin_mask > 0]

    return depth_map

def read_MiDaS_depth(disp_fi, disp_rescale=10., h=None, w=None):
    if 'npy' in os.path.splitext(disp_fi)[-1]:
        disp = np.load(disp_fi)
    else:
        disp = imageio.imread(disp_fi).astype(np.float32)
    disp = disp - disp.min()
    disp = cv2.blur(disp / disp.max(), ksize=(3, 3)) * disp.max()
    disp = (disp / disp.max()) * disp_rescale
    if h is not None and w is not None:
        disp = resize(disp / disp.max(), (h, w), order=1) * disp.max()
    depth = 1. / np.maximum(disp, 0.05)

    return depth

def read_real_depth(disp_fi, h=None, w=None):
    if 'npy' in os.path.splitext(disp_fi)[-1]:
        disp = np.load(disp_fi)
    else:
        disp = imageio.imread(disp_fi).astype(np.float32)
    
    if h is not None and w is not None:
        disp = resize(disp, (h, w), order=1)
    return np.maximum(disp, 1e-8)

def follow_image_aspect_ratio(depth, image):
    H, W = image.shape[:2]
    image_aspect_ratio = H / W
    dH, dW = depth.shape[:2]
    depth_aspect_ratio = dH / dW
    if depth_aspect_ratio > image_aspect_ratio:
        resize_H = dH
        resize_W = dH / image_aspect_ratio
    else:
        resize_W = dW
        resize_H = dW * image_aspect_ratio
    depth = resize(depth / depth.max(), 
                    (int(resize_H), 
                    int(resize_W)), 
                    order=0) * depth.max()
    
    return depth

def depth_resize(depth, origin_size, image_size):
    if origin_size[0] is not 0:
        max_depth = depth.max()
        depth = depth / max_depth
        depth = resize(depth, origin_size, order=1, mode='edge')
        depth = depth * max_depth
    else:
        max_depth = depth.max()
        depth = depth / max_depth
        depth = resize(depth, image_size, order=1, mode='edge')
        depth = depth * max_depth

    return depth
    
def filter_irrelevant_edge(self_edge, other_edges, other_edges_with_id, current_edge_id, context, edge_ccs, mesh, anchor):
    other_edges = other_edges.squeeze()
    other_edges_with_id = other_edges_with_id.squeeze()
    
    self_edge = self_edge.squeeze()
    dilate_self_edge = cv2.dilate(self_edge.astype(np.uint8), np.array([[1,1,1],[1,1,1],[1,1,1]]).astype(np.uint8), iterations=1)
    edge_ids = collections.Counter(other_edges_with_id.flatten()).keys()
    other_edges_info = []
    # import ipdb
    # ipdb.set_trace()
    for edge_id in edge_ids:
        edge_id = int(edge_id)
        if edge_id >= 0:
            condition = ((other_edges_with_id == edge_id) * other_edges * context).astype(np.uint8)
            if dilate_self_edge[condition > 0].sum() == 0:
                other_edges[other_edges_with_id == edge_id] = 0
            else:
                num_condition, condition_labels = cv2.connectedComponents(condition, connectivity=8)
                for condition_id in range(1, num_condition):
                    isolate_condition = ((condition_labels == condition_id) > 0).astype(np.uint8)
                    num_end_group, end_group = cv2.connectedComponents(((dilate_self_edge * isolate_condition) > 0).astype(np.uint8), connectivity=8)
                    if num_end_group == 1:
                        continue
                    for end_id in range(1, num_end_group):
                        end_pxs, end_pys = np.where((end_group == end_id))
                        end_px, end_py = end_pxs[0], end_pys[0]
                        other_edges_info.append({})
                        other_edges_info[-1]['edge_id'] = edge_id
                        # other_edges_info[-1]['near_depth'] = None
                        other_edges_info[-1]['diff'] = None
                        other_edges_info[-1]['edge_map'] = np.zeros_like(self_edge)
                        other_edges_info[-1]['end_point_map'] = np.zeros_like(self_edge)
                        other_edges_info[-1]['end_point_map'][(end_group == end_id)] = 1
                        other_edges_info[-1]['forbidden_point_map'] = np.zeros_like(self_edge)
                        other_edges_info[-1]['forbidden_point_map'][(end_group != end_id) * (end_group != 0)] = 1
                        other_edges_info[-1]['forbidden_point_map'] = cv2.dilate(other_edges_info[-1]['forbidden_point_map'], kernel=np.array([[1,1,1],[1,1,1],[1,1,1]]), iterations=2)
                        for x in edge_ccs[edge_id]:
                            nx = x[0] - anchor[0]
                            ny = x[1] - anchor[1]
                            if nx == end_px and ny == end_py:
                                # other_edges_info[-1]['near_depth'] = abs(nx)
                                if mesh.nodes[x].get('far') is not None and len(mesh.nodes[x].get('far')) == 1:
                                    other_edges_info[-1]['diff'] = abs(1./abs([*mesh.nodes[x].get('far')][0][2]) - 1./abs(x[2]))
                                else:
                                    other_edges_info[-1]['diff'] = 0
                                # if end_group[nx, ny] != end_id and end_group[nx, ny] > 0:
                                #     continue
                            try:
                                if isolate_condition[nx, ny] == 1:
                                    other_edges_info[-1]['edge_map'][nx, ny] = 1
                            except:
                                pass
    try:
        other_edges_info = sorted(other_edges_info, key=lambda x : x['diff'], reverse=True)
    except:
        import pdb
        pdb.set_trace()
    # import pdb
    # pdb.set_trace()
    # other_edges = other_edges[..., None]
    for other_edge in other_edges_info:
        if other_edge['end_point_map'] is None:
            import pdb
            pdb.set_trace()

    other_edges = other_edges * context

    return other_edges, other_edges_info

def require_depth_edge(context_edge, mask):
    dilate_mask = cv2.dilate(mask, np.array([[1,1,1],[1,1,1],[1,1,1]]).astype(np.uint8), iterations=1)
    if (dilate_mask * context_edge).max() == 0:
        return False
    else:
        return True

def refine_color_around_edge(mesh, info_on_pix, edge_ccs, config, spdb=False):
    H, W = mesh.graph['H'], mesh.graph['W']
    tmp_edge_ccs = copy.deepcopy(edge_ccs)
    for edge_id, edge_cc in enumerate(edge_ccs):
        if len(edge_cc) == 0:
            continue
        near_maps = np.zeros((H, W)).astype(bool)
        far_maps = np.zeros((H, W)).astype(bool)
        tmp_far_nodes = set()
        far_nodes = set()
        near_nodes = set()
        end_nodes = set()        
        for i in range(5):
            if i == 0:
                for edge_node in edge_cc:
                    if mesh.nodes[edge_node].get('depth_edge_dilate_2_color_flag') is not True:
                        break
                    if mesh.nodes[edge_node].get('inpaint_id') == 1:
                        near_nodes.add(edge_node)
                        tmp_node = mesh.nodes[edge_node].get('far')
                        tmp_node = set(tmp_node) if tmp_node is not None else set()
                        tmp_far_nodes |= tmp_node
                rmv_tmp_far_nodes = set()
                for far_node in tmp_far_nodes:
                    if not(mesh.has_node(far_node) and mesh.nodes[far_node].get('inpaint_id') == 1):
                        rmv_tmp_far_nodes.add(far_node)
                if len(tmp_far_nodes - rmv_tmp_far_nodes) == 0:
                    break                        
                else:
                    for near_node in near_nodes:
                        near_maps[near_node[0], near_node[1]] = True
                        mesh.nodes[near_node]['refine_rgbd'] = True
                        mesh.nodes[near_node]['backup_depth'] = near_node[2] \
                                    if mesh.nodes[near_node].get('real_depth') is None else mesh.nodes[near_node]['real_depth']
                        mesh.nodes[near_node]['backup_color'] = mesh.nodes[near_node]['color']
                for far_node in tmp_far_nodes:
                    if mesh.has_node(far_node) and mesh.nodes[far_node].get('inpaint_id') == 1:
                        far_nodes.add(far_node)
                        far_maps[far_node[0], far_node[1]] = True
                        mesh.nodes[far_node]['refine_rgbd'] = True
                        mesh.nodes[far_node]['backup_depth'] = far_node[2] \
                                    if mesh.nodes[far_node].get('real_depth') is None else mesh.nodes[far_node]['real_depth']
                        mesh.nodes[far_node]['backup_color'] = mesh.nodes[far_node]['color']
                tmp_far_nodes = far_nodes
                tmp_near_nodes = near_nodes
            else:
                tmp_far_nodes = new_tmp_far_nodes
                tmp_near_nodes = new_tmp_near_nodes
                new_tmp_far_nodes = None
                new_tmp_near_nodes = None
            new_tmp_far_nodes = set()
            new_tmp_near_nodes = set()
            for node in tmp_near_nodes:
                for ne_node in mesh.neighbors(node):
                    if far_maps[ne_node[0], ne_node[1]] == False and \
                        near_maps[ne_node[0], ne_node[1]] == False:
                        if mesh.nodes[ne_node].get('inpaint_id') == 1:
                            new_tmp_near_nodes.add(ne_node)
                            near_maps[ne_node[0], ne_node[1]] = True
                            mesh.nodes[ne_node]['refine_rgbd'] = True
                            mesh.nodes[ne_node]['backup_depth'] = ne_node[2] \
                                    if mesh.nodes[ne_node].get('real_depth') is None else mesh.nodes[ne_node]['real_depth']
                            mesh.nodes[ne_node]['backup_color'] = mesh.nodes[ne_node]['color']
                        else:
                            mesh.nodes[ne_node]['backup_depth'] = ne_node[2] \
                                    if mesh.nodes[ne_node].get('real_depth') is None else mesh.nodes[ne_node]['real_depth']
                            mesh.nodes[ne_node]['backup_color'] = mesh.nodes[ne_node]['color']
                            end_nodes.add(node)
            near_nodes.update(new_tmp_near_nodes)
            for node in tmp_far_nodes:
                for ne_node in mesh.neighbors(node):
                    if far_maps[ne_node[0], ne_node[1]] == False and \
                        near_maps[ne_node[0], ne_node[1]] == False:
                        if mesh.nodes[ne_node].get('inpaint_id') == 1:
                            new_tmp_far_nodes.add(ne_node)
                            far_maps[ne_node[0], ne_node[1]] = True
                            mesh.nodes[ne_node]['refine_rgbd'] = True
                            mesh.nodes[ne_node]['backup_depth'] = ne_node[2] \
                                    if mesh.nodes[ne_node].get('real_depth') is None else mesh.nodes[ne_node]['real_depth']
                            mesh.nodes[ne_node]['backup_color'] = mesh.nodes[ne_node]['color']
                        else:
                            mesh.nodes[ne_node]['backup_depth'] = ne_node[2] \
                                    if mesh.nodes[ne_node].get('real_depth') is None else mesh.nodes[ne_node]['real_depth']
                            mesh.nodes[ne_node]['backup_color'] = mesh.nodes[ne_node]['color']
                            end_nodes.add(node)
            far_nodes.update(new_tmp_far_nodes)
        if len(far_nodes) == 0:
            tmp_edge_ccs[edge_id] = set()
            continue
        for node in new_tmp_far_nodes | new_tmp_near_nodes:
            for ne_node in mesh.neighbors(node):
                if far_maps[ne_node[0], ne_node[1]] == False and near_maps[ne_node[0], ne_node[1]] == False:
                    end_nodes.add(node)
                    mesh.nodes[ne_node]['backup_depth'] = ne_node[2] \
                            if mesh.nodes[ne_node].get('real_depth') is None else mesh.nodes[ne_node]['real_depth']
                    mesh.nodes[ne_node]['backup_color'] = mesh.nodes[ne_node]['color']
        tmp_end_nodes = end_nodes
        
        refine_nodes = near_nodes | far_nodes
        remain_refine_nodes = copy.deepcopy(refine_nodes)
        accum_idx = 0
        while len(remain_refine_nodes) > 0:
            accum_idx += 1
            if accum_idx > 100:
                break
            new_tmp_end_nodes = None
            new_tmp_end_nodes = set()
            survive_tmp_end_nodes = set()
            for node in tmp_end_nodes:
                re_depth, re_color, re_count = 0, np.array([0., 0., 0.]), 0
                for ne_node in mesh.neighbors(node):
                    if mesh.nodes[ne_node].get('refine_rgbd') is True:
                        if ne_node not in tmp_end_nodes:
                            new_tmp_end_nodes.add(ne_node)
                    else:
                        try:
                            re_depth += mesh.nodes[ne_node]['backup_depth']
                            re_color += mesh.nodes[ne_node]['backup_color'].astype(np.float32)
                            re_count += 1.
                        except:
                            import pdb; pdb.set_trace()
                if re_count > 0:
                    re_depth = re_depth / re_count
                    re_color = re_color / re_count
                    mesh.nodes[node]['backup_depth'] = re_depth
                    mesh.nodes[node]['backup_color'] = re_color
                    mesh.nodes[node]['refine_rgbd'] = False
                else:
                    survive_tmp_end_nodes.add(node)
            for node in tmp_end_nodes - survive_tmp_end_nodes:
                if node in remain_refine_nodes:
                    remain_refine_nodes.remove(node)
            tmp_end_nodes = new_tmp_end_nodes
        if spdb == True:
            bfrd_canvas = np.zeros((H, W))
            bfrc_canvas = np.zeros((H, W, 3)).astype(np.uint8)
            aftd_canvas = np.zeros((H, W))
            aftc_canvas = np.zeros((H, W, 3)).astype(np.uint8)
            for node in refine_nodes:
                bfrd_canvas[node[0], node[1]] = abs(node[2])
                aftd_canvas[node[0], node[1]] = abs(mesh.nodes[node]['backup_depth'])
                bfrc_canvas[node[0], node[1]] = mesh.nodes[node]['color'].astype(np.uint8)
                aftc_canvas[node[0], node[1]] = mesh.nodes[node]['backup_color'].astype(np.uint8)
            f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharex=True, sharey=True); 
            ax1.imshow(bfrd_canvas); 
            ax2.imshow(aftd_canvas); 
            ax3.imshow(bfrc_canvas); 
            ax4.imshow(aftc_canvas); 
            plt.show()
            import pdb; pdb.set_trace()
        for node in refine_nodes:
            if mesh.nodes[node].get('refine_rgbd') is not None:
                mesh.nodes[node].pop('refine_rgbd')
                mesh.nodes[node]['color'] = mesh.nodes[node]['backup_color']
                for info in info_on_pix[(node[0], node[1])]:
                    if info['depth'] == node[2]:
                        info['color'] = mesh.nodes[node]['backup_color']

    return mesh, info_on_pix

def refine_depth_around_edge(mask_depth, far_edge, uncleaned_far_edge, near_edge, mask, all_depth, config):
    if isinstance(mask_depth, torch.Tensor):
        if mask_depth.is_cuda:
            mask_depth = mask_depth.cpu()
        mask_depth = mask_depth.data
        mask_depth = mask_depth.numpy()
    if isinstance(far_edge, torch.Tensor):
        if far_edge.is_cuda:
            far_edge = far_edge.cpu()
        far_edge = far_edge.data
        far_edge = far_edge.numpy()
    if isinstance(uncleaned_far_edge, torch.Tensor):
        if uncleaned_far_edge.is_cuda:
            uncleaned_far_edge = uncleaned_far_edge.cpu()
        uncleaned_far_edge = uncleaned_far_edge.data
        uncleaned_far_edge = uncleaned_far_edge.numpy()
    if isinstance(near_edge, torch.Tensor):
        if near_edge.is_cuda:
            near_edge = near_edge.cpu()
        near_edge = near_edge.data
        near_edge = near_edge.numpy()
    if isinstance(mask, torch.Tensor):
        if mask.is_cuda:
            mask = mask.cpu()
        mask = mask.data
        mask = mask.numpy()
    mask = mask.squeeze()
    uncleaned_far_edge = uncleaned_far_edge.squeeze()
    far_edge = far_edge.squeeze()
    near_edge = near_edge.squeeze()
    mask_depth = mask_depth.squeeze()
    dilate_far_edge = cv2.dilate(uncleaned_far_edge.astype(np.uint8), kernel=np.array([[0,1,0],[1,1,1],[0,1,0]]).astype(np.uint8), iterations=1)
    near_edge[dilate_far_edge == 0] = 0
    dilate_near_edge = cv2.dilate(near_edge.astype(np.uint8), kernel=np.array([[0,1,0],[1,1,1],[0,1,0]]).astype(np.uint8), iterations=1)
    far_edge[dilate_near_edge == 0] = 0
    init_far_edge = far_edge.copy()
    init_near_edge = near_edge.copy()
    for i in range(config['depth_edge_dilate_2']):
        init_far_edge = cv2.dilate(init_far_edge, kernel=np.array([[0,1,0],[1,1,1],[0,1,0]]).astype(np.uint8), iterations=1)
        init_far_edge[init_near_edge == 1] = 0
        init_near_edge = cv2.dilate(init_near_edge, kernel=np.array([[0,1,0],[1,1,1],[0,1,0]]).astype(np.uint8), iterations=1)
        init_near_edge[init_far_edge == 1] = 0
    init_far_edge[mask == 0] = 0
    init_near_edge[mask == 0] = 0
    hole_far_edge = 1 - init_far_edge
    hole_near_edge = 1 - init_near_edge
    change = None
    while True:
        change = False
        hole_far_edge[init_near_edge == 1] = 0
        hole_near_edge[init_far_edge == 1] = 0
        far_pxs, far_pys = np.where((hole_far_edge == 0) * (init_far_edge == 1) > 0)
        current_hole_far_edge = hole_far_edge.copy()
        for far_px, far_py in zip(far_pxs, far_pys):
            min_px = max(far_px - 1, 0) 
            max_px = min(far_px + 2, mask.shape[0]-1)
            min_py = max(far_py - 1, 0) 
            max_py = min(far_py + 2, mask.shape[1]-1)
            hole_far = current_hole_far_edge[min_px: max_px, min_py: max_py]
            tmp_mask = mask[min_px: max_px, min_py: max_py]
            all_depth_patch = all_depth[min_px: max_px, min_py: max_py] * 0
            all_depth_mask = (all_depth_patch != 0).astype(np.uint8)
            cross_element = np.array([[0,1,0],[1,1,1],[0,1,0]])[min_px - (far_px - 1): max_px - (far_px - 1), min_py - (far_py - 1): max_py - (far_py - 1)]
            combine_mask = (tmp_mask + all_depth_mask).clip(0, 1) * hole_far * cross_element
            tmp_patch = combine_mask * (mask_depth[min_px: max_px, min_py: max_py] + all_depth_patch)
            number = np.count_nonzero(tmp_patch)
            if number > 0:
                mask_depth[far_px, far_py] = np.sum(tmp_patch).astype(np.float32) / max(number, 1e-6)
                hole_far_edge[far_px, far_py] = 1
                change = True
        near_pxs, near_pys = np.where((hole_near_edge == 0) * (init_near_edge == 1) > 0)
        current_hole_near_edge = hole_near_edge.copy()
        for near_px, near_py in zip(near_pxs, near_pys):
            min_px = max(near_px - 1, 0) 
            max_px = min(near_px + 2, mask.shape[0]-1)
            min_py = max(near_py - 1, 0) 
            max_py = min(near_py + 2, mask.shape[1]-1)
            hole_near = current_hole_near_edge[min_px: max_px, min_py: max_py]
            tmp_mask = mask[min_px: max_px, min_py: max_py]
            all_depth_patch = all_depth[min_px: max_px, min_py: max_py] * 0
            all_depth_mask = (all_depth_patch != 0).astype(np.uint8)            
            cross_element = np.array([[0,1,0],[1,1,1],[0,1,0]])[min_px - near_px + 1:max_px - near_px + 1, min_py - near_py + 1:max_py - near_py + 1]
            combine_mask = (tmp_mask + all_depth_mask).clip(0, 1) * hole_near * cross_element
            tmp_patch = combine_mask * (mask_depth[min_px: max_px, min_py: max_py] + all_depth_patch)
            number = np.count_nonzero(tmp_patch)
            if number > 0:                
                mask_depth[near_px, near_py] = np.sum(tmp_patch) / max(number, 1e-6)
                hole_near_edge[near_px, near_py] = 1
                change = True
        if change is False:
            break
        
    return mask_depth



def vis_depth_edge_connectivity(depth, config):
    disp = 1./depth
    u_diff = (disp[1:, :] - disp[:-1, :])[:-1, 1:-1]
    b_diff = (disp[:-1, :] - disp[1:, :])[1:, 1:-1]
    l_diff = (disp[:, 1:] - disp[:, :-1])[1:-1, :-1]
    r_diff = (disp[:, :-1] - disp[:, 1:])[1:-1, 1:]
    u_over = (np.abs(u_diff) > config['depth_threshold']).astype(np.float32)
    b_over = (np.abs(b_diff) > config['depth_threshold']).astype(np.float32)
    l_over = (np.abs(l_diff) > config['depth_threshold']).astype(np.float32)
    r_over = (np.abs(r_diff) > config['depth_threshold']).astype(np.float32)
    concat_diff = np.stack([u_diff, b_diff, r_diff, l_diff], axis=-1)
    concat_over = np.stack([u_over, b_over, r_over, l_over], axis=-1)
    over_diff = concat_diff * concat_over
    pos_over = (over_diff > 0).astype(np.float32).sum(-1).clip(0, 1)
    neg_over = (over_diff < 0).astype(np.float32).sum(-1).clip(0, 1)
    neg_over[(over_diff > 0).astype(np.float32).sum(-1) > 0] = 0
    _, edge_label = cv2.connectedComponents(pos_over.astype(np.uint8), connectivity=8)
    T_junction_maps = np.zeros_like(pos_over)
    for edge_id in range(1, edge_label.max() + 1):
        edge_map = (edge_label == edge_id).astype(np.uint8)
        edge_map = np.pad(edge_map, pad_width=((1,1),(1,1)), mode='constant')
        four_direc = np.roll(edge_map, 1, 1) + np.roll(edge_map, -1, 1) + np.roll(edge_map, 1, 0) + np.roll(edge_map, -1, 0)
        eight_direc = np.roll(np.roll(edge_map, 1, 1), 1, 0) + np.roll(np.roll(edge_map, 1, 1), -1, 0) + \
                      np.roll(np.roll(edge_map, -1, 1), 1, 0) + np.roll(np.roll(edge_map, -1, 1), -1, 0)
        eight_direc = (eight_direc + four_direc)[1:-1,1:-1]
        pos_over[eight_direc > 2] = 0
        T_junction_maps[eight_direc > 2] = 1
    _, edge_label = cv2.connectedComponents(pos_over.astype(np.uint8), connectivity=8)
    edge_label = np.pad(edge_label, 1, mode='constant')

    return edge_label



def max_size(mat, value=0):
    if not (mat and mat[0]): return (0, 0)
    it = iter(mat)
    prev = [(el==value) for el in next(it)]
    max_size = max_rectangle_size(prev)
    for row in it:
        hist = [(1+h) if el == value else 0 for h, el in zip(prev, row)]
        max_size = max(max_size, max_rectangle_size(hist), key=get_area)
        prev = hist                                               
    return max_size

def max_rectangle_size(histogram):
    Info = namedtuple('Info', 'start height')
    stack = []
    top = lambda: stack[-1]
    max_size = (0, 0) # height, width of the largest rectangle
    pos = 0 # current position in the histogram
    for pos, height in enumerate(histogram):
        start = pos # position where rectangle starts
        while True:
            if not stack or height > top().height:
                stack.append(Info(start, height)) # push
            if stack and height < top().height:
                max_size = max(max_size, (top().height, (pos-top().start)),
                               key=get_area)
                start, _ = stack.pop()
                continue
            break # height == top().height goes here
                
    pos += 1
    for start, height in stack:
        max_size = max(max_size, (height, (pos-start)),
                       key=get_area)

    return max_size

def get_area(size):
    return reduce(mul, size)

def find_anchors(matrix):
    matrix = [[*x] for x in matrix]
    mh, mw = max_size(matrix)
    matrix = np.array(matrix)
    # element = np.zeros((mh, mw))
    for i in range(matrix.shape[0] + 1 - mh):
        for j in range(matrix.shape[1] + 1 - mw):
            if matrix[i:i + mh, j:j + mw].max() == 0:
                return i, i + mh, j, j + mw

def find_largest_rect(dst_img, bg_color=(128, 128, 128)):
    valid = np.any(dst_img[..., :3] != bg_color, axis=-1) 
    dst_h, dst_w = dst_img.shape[:2]
    ret, labels = cv2.connectedComponents(np.uint8(valid == False)) 
    red_mat = np.zeros_like(labels) 
    # denoise 
    for i in range(1, np.max(labels)+1, 1): 
        x, y, w, h = cv2.boundingRect(np.uint8(labels==i)) 
        if x == 0 or (x+w) == dst_h or y == 0 or (y+h) == dst_w: 
            red_mat[labels==i] = 1 
    # crop 
    t, b, l, r = find_anchors(red_mat) 

    return t, b, l, r



================================================
FILE: inpainting/MiDaS/MiDaS_utils.py
================================================
"""Utils for monoDepth.
"""
import sys
import re
import numpy as np
import cv2
import torch
import imageio


def read_pfm(path):
    """Read pfm file.

    Args:
        path (str): path to file

    Returns:
        tuple: (data, scale)
    """
    with open(path, "rb") as file:

        color = None
        width = None
        height = None
        scale = None
        endian = None

        header = file.readline().rstrip()
        if header.decode("ascii") == "PF":
            color = True
        elif header.decode("ascii") == "Pf":
            color = False
        else:
            raise Exception("Not a PFM file: " + path)

        dim_match = re.match(r"^(\d+)\s(\d+)\s$", file.readline().decode("ascii"))
        if dim_match:
            width, height = list(map(int, dim_match.groups()))
        else:
            raise Exception("Malformed PFM header.")

        scale = float(file.readline().decode("ascii").rstrip())
        if scale < 0:
            # little-endian
            endian = "<"
            scale = -scale
        else:
            # big-endian
            endian = ">"

        data = np.fromfile(file, endian + "f")
        shape = (height, width, 3) if color else (height, width)

        data = np.reshape(data, shape)
        data = np.flipud(data)

        return data, scale


def write_pfm(path, image, scale=1):
    """Write pfm file.

    Args:
        path (str): pathto file
        image (array): data
        scale (int, optional): Scale. Defaults to 1.
    """

    with open(path, "wb") as file:
        color = None

        if image.dtype.name != "float32":
            raise Exception("Image dtype must be float32.")

        image = np.flipud(image)

        if len(image.shape) == 3 and image.shape[2] == 3:  # color image
            color = True
        elif (
            len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1
        ):  # greyscale
            color = False
        else:
            raise Exception("Image must have H x W x 3, H x W x 1 or H x W dimensions.")

        file.write("PF\n" if color else "Pf\n".encode())
        file.write("%d %d\n".encode() % (image.shape[1], image.shape[0]))

        endian = image.dtype.byteorder

        if endian == "<" or endian == "=" and sys.byteorder == "little":
            scale = -scale

        file.write("%f\n".encode() % scale)

        image.tofile(file)


def read_image(path):
    """Read image and output RGB image (0-1).

    Args:
        path (str): path to file

    Returns:
        array: RGB image (0-1)
    """
    img = cv2.imread(path)

    if img.ndim == 2:
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0

    return img


def resize_image(img):
    """Resize image and make it fit for network.

    Args:
        img (array): image

    Returns:
        tensor: data ready for network
    """
    height_orig = img.shape[0]
    width_orig = img.shape[1]
    unit_scale = 384.

    if width_orig > height_orig:
        scale = width_orig / unit_scale
    else:
        scale = height_orig / unit_scale

    height = (np.ceil(height_orig / scale / 32) * 32).astype(int)
    width = (np.ceil(width_orig / scale / 32) * 32).astype(int)

    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)

    img_resized = (
        torch.from_numpy(np.transpose(img_resized, (2, 0, 1))).contiguous().float()
    )
    img_resized = img_resized.unsqueeze(0)

    return img_resized


def resize_depth(depth, width, height):
    """Resize depth map and bring to CPU (numpy).

    Args:
        depth (tensor): depth
        width (int): image width
        height (int): image height

    Returns:
        array: processed depth
    """
    depth = torch.squeeze(depth[0, :, :, :]).to("cpu")
    depth = cv2.blur(depth.numpy(), (3, 3))
    depth_resized = cv2.resize(
        depth, (width, height), interpolation=cv2.INTER_AREA
    )

    return depth_resized

def write_depth(path, depth, bits=1):
    """Write depth map to pfm and png file.

    Args:
        path (str): filepath without extension
        depth (array): depth
    """
    # write_pfm(path + ".pfm", depth.astype(np.float32))

    depth_min = depth.min()
    depth_max = depth.max()

    max_val = (2**(8*bits))-1

    if depth_max - depth_min > np.finfo("float").eps:
        out = max_val * (depth - depth_min) / (depth_max - depth_min)
    else:
        out = 0

    if bits == 1:
        cv2.imwrite(path + ".png", out.astype("uint8"))
    elif bits == 2:
        cv2.imwrite(path + ".png", out.astype("uint16"))
        
    return


================================================
FILE: inpainting/MiDaS/monodepth_net.py
================================================
"""MonoDepthNet: Network for monocular depth estimation trained by mixing several datasets.
This file contains code that is adapted from
https://github.com/thomasjpfan/pytorch_refinenet/blob/master/pytorch_refinenet/refinenet/refinenet_4cascade.py
"""
import torch
import torch.nn as nn
from torchvision import models


class MonoDepthNet(nn.Module):
    """Network for monocular depth estimation.
    """

    def __init__(self, path=None, features=256):
        """Init.

        Args:
            path (str, optional): Path to saved model. Defaults to None.
            features (int, optional): Number of features. Defaults to 256.
        """
        super().__init__()

        resnet = models.resnet50(pretrained=False)

        self.pretrained = nn.Module()
        self.scratch = nn.Module()
        self.pretrained.layer1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu,
                                               resnet.maxpool, resnet.layer1)

        self.pretrained.layer2 = resnet.layer2
        self.pretrained.layer3 = resnet.layer3
        self.pretrained.layer4 = resnet.layer4

        # adjust channel number of feature maps
        self.scratch.layer1_rn = nn.Conv2d(256, features, kernel_size=3, stride=1, padding=1, bias=False)
        self.scratch.layer2_rn = nn.Conv2d(512, features, kernel_size=3, stride=1, padding=1, bias=False)
        self.scratch.layer3_rn = nn.Conv2d(1024, features, kernel_size=3, stride=1, padding=1, bias=False)
        self.scratch.layer4_rn = nn.Conv2d(2048, features, kernel_size=3, stride=1, padding=1, bias=False)

        self.scratch.refinenet4 = FeatureFusionBlock(features)
        self.scratch.refinenet3 = FeatureFusionBlock(features)
        self.scratch.refinenet2 = FeatureFusionBlock(features)
        self.scratch.refinenet1 = FeatureFusionBlock(features)

        # adaptive output module: 2 convolutions and upsampling
        self.scratch.output_conv = nn.Sequential(nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),
                                                 nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1),
                                                 Interpolate(scale_factor=2, mode='bilinear'))

        # load model
        if path:
            self.load(path)

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input data (image)

        Returns:
            tensor: depth
        """
        layer_1 = self.pretrained.layer1(x)
        layer_2 = self.pretrained.layer2(layer_1)
        layer_3 = self.pretrained.layer3(layer_2)
        layer_4 = self.pretrained.layer4(layer_3)

        layer_1_rn = self.scratch.layer1_rn(layer_1)
        layer_2_rn = self.scratch.layer2_rn(layer_2)
        layer_3_rn = self.scratch.layer3_rn(layer_3)
        layer_4_rn = self.scratch.layer4_rn(layer_4)

        path_4 = self.scratch.refinenet4(layer_4_rn)
        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)
        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)
        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)

        out = self.scratch.output_conv(path_1)

        return out

    def load(self, path):
        """Load model from file.

        Args:
            path (str): file path
        """
        parameters = torch.load(path)

        self.load_state_dict(parameters)


class Interpolate(nn.Module):
    """Interpolation module.
    """

    def __init__(self, scale_factor, mode):
        """Init.

        Args:
            scale_factor (float): scaling
            mode (str): interpolation mode
        """
        super(Interpolate, self).__init__()

        self.interp = nn.functional.interpolate
        self.scale_factor = scale_factor
        self.mode = mode

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input

        Returns:
            tensor: interpolated data
        """
        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False)

        return x


class ResidualConvUnit(nn.Module):
    """Residual convolution module.
    """

    def __init__(self, features):
        """Init.

        Args:
            features (int): number of features
        """
        super().__init__()

        self.conv1 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=True)
        self.conv2 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=False)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        """Forward pass.

        Args:
            x (tensor): input

        Returns:
            tensor: output
        """
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)

        return out + x


class FeatureFusionBlock(nn.Module):
    """Feature fusion block.
    """

    def __init__(self, features):
        """Init.

        Args:
            features (int): number of features
        """
        super().__init__()

        self.resConfUnit = ResidualConvUnit(features)

    def forward(self, *xs):
        """Forward pass.

        Returns:
            tensor: output
        """
        output = xs[0]

        if len(xs) == 2:
            output += self.resConfUnit(xs[1])

        output = self.resConfUnit(output)
        output = nn.functional.interpolate(output, scale_factor=2,
                                           mode='bilinear', align_corners=True)

        return output



================================================
FILE: inpainting/MiDaS/run.py
================================================
"""Compute depth maps for images in the input folder.
"""
import os
import glob
import torch
# from monodepth_net import MonoDepthNet
# import utils
import matplotlib.pyplot as plt
import numpy as np
import cv2
import imageio


def run_depth(img_names, input_path, output_path, model_path, Net, utils, target_w=None):
    """Run MonoDepthNN to compute depth maps.

    Args:
        input_path (str): path to input folder
        output_path (str): path to output folder
        model_path (str): path to saved model
    """
    print("initialize")

    # select device
    device = torch.device("cpu")
    print("device: %s" % device)

    # load network
    model = Net(model_path)
    model.to(device)
    model.eval()

    # get input
    # img_names = glob.glob(os.path.join(input_path, "*"))
    num_images = len(img_names)

    # create output folder
    os.makedirs(output_path, exist_ok=True)

    print("start processing")

    for ind, img_name in enumerate(img_names):

        print("  processing {} ({}/{})".format(img_name, ind + 1, num_images))

        # input
        img = utils.read_image(img_name)
        w = img.shape[1]
        scale = 640. / max(img.shape[0], img.shape[1])
        target_height, target_width = int(round(img.shape[0] * scale)), int(round(img.shape[1] * scale))
        img_input = utils.resize_image(img)
        print(img_input.shape)
        img_input = img_input.to(device)
        # compute
        with torch.no_grad():
            out = model.forward(img_input)
        
        depth = utils.resize_depth(out, target_width, target_height)
        img = cv2.resize((img * 255).astype(np.uint8), (target_width, target_height), interpolation=cv2.INTER_AREA)

        filename = os.path.join(
            output_path, os.path.splitext(os.path.basename(img_name))[0]
        )
        np.save(filename + '.npy', depth)
        utils.write_depth(filename, depth, bits=2)

    print("finished")


# if __name__ == "__main__":
#     # set paths
#     INPUT_PATH = "image"
#     OUTPUT_PATH = "output"
#     MODEL_PATH = "model.pt"

#     # set torch options
#     torch.backends.cudnn.enabled = True
#     torch.backends.cudnn.benchmark = True

#     # compute depth maps
#     run_depth(INPUT_PATH, OUTPUT_PATH, MODEL_PATH, Net, target_w=640)



================================================
FILE: inpainting/pano/credits.txt
================================================
basel_stapfelberg_panorama
https://commons.wikimedia.org/wiki/File:Basel_Stapfelberg_Panorama.jpg
DerMische, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons

soissons_cathedral_interior
https://commons.wikimedia.org/wiki/File:Soissons_Cathedral_Interior_360x180,_Picardy,_France_-_Diliff.jpg
Diliff, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons



================================================
FILE: mesh/estimate_scale_histogram.py
================================================
import sklearn
import numpy as np
import trimesh
from argparse import ArgumentParser

parser = ArgumentParser()
parser.add_argument('input',help='path to input mesh')
parser.add_argument('output',help='output path for scaled mesh')
parser.add_argument('--bins',default=500,help='number of bins in histogram')
parser.add_argument('--camera_height',default=2,help='camera height in meters')
args = parser.parse_args()

print('loading mesh...')
mesh = trimesh.load(args.input)
vertices = mesh.vertices
X = vertices[:, 0]
Y = vertices[:, 1]
plane = vertices[:, :2]

print(f'building histogram with {args.bins} bins...')
hist, bin_edges = np.histogram(Y, bins=args.bins)

print(hist)
max_bin_index = np.argmax(hist)

Y_max_bin = Y[(Y>bin_edges[max_bin_index])&(Y<=bin_edges[max_bin_index+1])]

cluster_height = np.mean(Y_max_bin)

print(f'height of largest cluster: {cluster_height}')

translated = Y - cluster_height

vertices[:, 1] = translated

mesh.vertices = vertices * 2 / abs(cluster_height)

print('exporting mesh...')
mesh.export(args.output)



================================================
FILE: mesh/subdivide.py
================================================
#import xatlas
import numpy as np
import trimesh
from trimesh.scene.scene import Scene
#from sklearn.cluster import KMeans
#from matplotlib import pyplot as plt
from argparse import ArgumentParser
import math


def fibonacci_sphere(samples=1000):

    points = []
    phi = math.pi * (math.sqrt(5.) - 1.)  # golden angle in radians

    for i in range(samples):
        y = 1 - (i / float(samples - 1)) * 2  # y goes from 1 to -1
        radius = math.sqrt(1 - y * y)  # radius at y

        theta = phi * i  # golden angle increment

        x = math.cos(theta) * radius
        z = math.sin(theta) * radius

        points.append((x, y, z))

    return np.array(points)


if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('input',help='path to input mesh')
    parser.add_argument('output',help='output path for scaled mesh')
    parser.add_argument('--num',type=int,default=20,help='number of models to divide into')
    args = parser.parse_args()

    print('loading mesh...')
    mesh = trimesh.load(args.input)
    if type(mesh) is Scene:
        mesh = mesh.geometry.popitem()[-1]
        print(type(mesh))
    print('done.')

    print(f'mesh has {len(mesh.vertices)} vertices, {len(mesh.faces)} faces')
    
    print('centering vertices...')
    verts = mesh.vertices - np.mean(mesh.vertices,axis=0,keepdims=True)

    print('calculating centroids...')
    face_verts = np.stack([
        verts[mesh.faces[:,0]],
        verts[mesh.faces[:,1]],
        verts[mesh.faces[:,2]]
    ],axis=0)
    centroids = np.mean(face_verts,axis=0) # [N,3]
    #centroids = centroids/np.sqrt(np.sum(centroids**2,axis=1,keepdims=True))
    print('done.')
    
    #label = KMeans(args.num).fit_predict(centroids)
    #plt.hist(label)
    #plt.show()

    vecs = fibonacci_sphere(args.num) # [M,3]
    #print(vecs)
    dots = []
    for vec in vecs:
        dots.append(np.sum(centroids*vec[None,:],axis=1))
    dots = np.array(dots)
    #print('dots',dots.shape)
    
    labels = np.argmax(dots,axis=0)
    #plt.hist(labels)
    #plt.show()
    
    meshes = []
    for label in range(args.num):
        label_mesh = mesh.copy()
        label_mesh.update_faces( (labels==label) )
        label_mesh.remove_unreferenced_vertices()

        print(f'mesh {label} has {len(label_mesh.vertices)} vertices, {len(label_mesh.faces)} faces')
        
        #print('running xatlas...')
        #vmapping, indices, uvs = xatlas.parametrize(label_mesh.vertices, label_mesh.faces)
        #print('done.')
        
        #new_mesh = trimesh.Trimesh( label_mesh.vertices[vmapping], indices)
        #new_mesh.visual = trimesh.visual.texture.TextureVisuals(uv=uvs)

        #new_mesh.export(f'mesh{label}.ply')
        #meshes.append(new_mesh)
        
        #label_mesh.export(f'mesh{label}.glb')
        meshes.append(label_mesh)
    scene = Scene(meshes)
    scene.export(args.output)




================================================
FILE: results/README.md
================================================
result files will be placed here


