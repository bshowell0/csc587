question,answer
"What is the overall purpose of the bshowell0-587 repository?","This repository contains the coursework for a deep learning class, likely numbered 587. It includes four major homework assignments and a project proposal, covering topics from basic CNNs to advanced generative models and reinforcement learning using TensorFlow and Keras."
"What are the main deep learning topics covered in the four homework assignments?","The homework assignments cover: 1) Foundational CNNs and an analysis of rotational invariance; 2) Unsupervised representation learning with autoencoders for generation and pre-training; 3) Advanced generative models, specifically a DCGAN and a Latent Diffusion Model; and 4) Reinforcement learning with table-based Q-learning and Deep Q-Learning."
"What is the primary technology stack used throughout this repository?","The projects are primarily implemented using Python with TensorFlow 2 and its Keras API. The reinforcement learning assignments also use the Gymnasium library, and the Q-learning implementation uses NumPy."
"What was the key analysis performed in Homework 1 beyond standard digit classification?","Homework 1 featured a detailed analysis of a CNN's robustness to image rotation. It systematically rotated test images and plotted the model's confidence and predictions, comparing a standard model to one trained with a RandomRotation data augmentation layer to empirically show the benefits of augmentation."
"How did the hw1/main.ipynb notebook visualize the internal workings of its CNN?","The notebook included an intermediate 2-dimensional Dense layer with a linear activation, named 'embedding'. By plotting the output of this layer for the training data, it was able to visualize how the network learned to separate the different digit classes in a 2D space."
"What was the outcome of adding a RandomRotation layer to the model in Homework 1?","Adding a RandomRotation layer and retraining the model made it significantly more robust to rotational transformations. The analysis showed that the augmented model maintained higher confidence in the correct class across a much wider range of rotation angles compared to the original model."
"What were the two main applications of autoencoders demonstrated in Homework 2?","Homework 2 demonstrated two key applications: 1) Using an MLP autoencoder on the Frey face dataset for image compression, reconstruction, and generation from its 2D latent space. 2) Using an autoencoder for unsupervised pre-training on MNIST digits to learn efficient, low-dimensional embeddings that could be used to train a highly accurate classifier."
"How was the concept of unsupervised pre-training demonstrated in hw2/main.ipynb?","An autoencoder was first trained on MNIST images of digits 0 and 1 without using their labels. This trained encoder was then used as a feature extractor to convert the 784-dimensional images into 16-dimensional embeddings. Finally, a separate, very simple classifier was trained on these low-dimensional embeddings, achieving 99.91% accuracy and demonstrating the effectiveness of the learned features."
"What did the latent space visualization of the Frey face autoencoder reveal in Homework 2?","Visualizing the 2D latent space as a scatter plot showed that the data points formed a distinct, curved manifold, not just a random cloud. Generating images from a grid in this space revealed that different regions corresponded to different facial poses and expressions, confirming that the autoencoder had learned a structured representation of the faces."
"What two advanced generative models were built in Homework 3?","Homework 3 involved building and training two generative models on the Frey face dataset: a Deep Convolutional Generative Adversarial Network (DCGAN) and a more complex Latent Diffusion Model."
"Describe the two-stage process used to implement the Latent Diffusion Model in Homework 3.","The Latent Diffusion Model was implemented in a two-stage process: 1) An MLP autoencoder was first trained on the Frey dataset to compress the images into a 2D latent space. 2) A diffusion model was then trained not on the images themselves, but on the 2D latent embeddings produced by the autoencoder, learning the distribution of the latent space."
"Why was the diffusion model in Homework 3 trained on a latent space instead of directly on image pixels?","Training a diffusion model on the low-dimensional latent space of an autoencoder is more computationally efficient and stable than training it directly on high-dimensional pixel data. This approach allows the diffusion model to focus on learning the core data manifold captured by the autoencoder, which can lead to better results with simpler models."
"What are the two reinforcement learning algorithms implemented in Homework 4?","Homework 4 implements two RL algorithms: 1) Table-based Q-learning, written from scratch in NumPy, to solve the discrete state space of the Frozen Lake environment. 2) N-step Deep Q-Learning (DQL), using a Keras MLP, to solve the continuous state space of the CartPole environment."
"What was the main finding when training a Q-learning agent on the 'slippery' versus the 'non-slippery' Frozen Lake?","The main finding was that the stochastic 'slippery' environment requires significantly more exploration and training time to learn a successful policy. The agent's success rate jumped from 0% to over 70% only after increasing the training steps from 2,000 to 200,000 and slowing down the epsilon decay rate, highlighting the challenge of learning in non-deterministic environments."
"How does the update mechanism in the Deep Q-Learning implementation in hw4/main.ipynb work?","The implementation uses n-step, episodic updates. It plays an entire episode to completion, collecting all states, actions, and rewards. Then, it calculates the discounted cumulative reward (return) for every step by iterating backwards through the episode. Finally, the entire episode's data is used to update the Q-network in a single batch, which is more stable than single-step updates."
"What is the main goal of the final project described in the project directory?","The final project proposal aims to create a personalized, on-device AI assistant. The plan involves fine-tuning a small language model (Qwen3-0.6B) with personal data and integrating it into the Gosling Android agent framework to enable it to perform tasks by interacting with other applications on a smartphone."
"What is the Gosling framework and how is it used in the project proposal?","Gosling is an experimental Android agent framework. The project proposes to fork Gosling and integrate the fine-tuned, local LLM as its reasoning engine. Gosling would provide the mechanisms for the LLM to interact with the device's capabilities, such as using Accessibility Services to automate tasks in other apps."
"What two different fine-tuning approaches does the project proposal suggest comparing?","The project proposal suggests exploring and comparing two fine-tuning methods: 1) Full Fine-tuning, where all the weights of the Qwen3-0.6B model are updated. 2) Parameter-Efficient Fine-Tuning (PEFT), such as using Adapters or LoRA, where the base model is frozen and only a small number of additional parameters are trained."
"What is the final project proposed in this repository?","The proposal is to build a personalized, on-device AI assistant by fine-tuning the Qwen3-0.6B small language model with personal data and integrating it into the Gosling Android agent framework to automate tasks on a smartphone."
"What were the two generative models built in Homework 3, and what dataset were they trained on?","Homework 3 involved building a Deep Convolutional Generative Adversarial Network (DCGAN) and a Latent Diffusion Model. Both models were trained on the Frey face dataset."
"In Homework 4's Q-learning implementation, what was the key difference in training required for the 'slippery' Frozen Lake environment compared to the non-slippery one?","The 'slippery' (stochastic) environment required a much longer training time (200,000 steps vs. 2,000) and a slower epsilon decay rate to allow for sufficient exploration, which was necessary to learn a robust policy and achieve a high success rate."
"How does the n-step Deep Q-Learning agent in hw4/main.ipynb calculate its target values for training?","It uses an episodic, n-step approach. After an episode completes, it calculates the discounted cumulative reward (return) for each step by iterating backwards through the episode's rewards. These calculated returns are then used as the target values to train the Q-network."
"What specific Python libraries are central to the implementation of the projects in this repository?","The core libraries used are TensorFlow 2 (with its Keras API) for building and training neural networks, NumPy for numerical operations, and Gymnasium for the reinforcement learning environments."
"According to hw2/main.ipynb, what was the final test accuracy for the classifier trained on MNIST embeddings, and what did this result signify?","The classifier achieved a test accuracy of 99.91%. This high accuracy demonstrates that unsupervised pre-training with an autoencoder can effectively learn meaningful, low-dimensional features that make a subsequent supervised classification task much more efficient and successful."
"In the project proposal, what is Parameter-Efficient Fine-Tuning (PEFT), and why is it relevant?","PEFT refers to techniques like Adapter Tuning or LoRA that adapt a pre-trained model by training only a small number of additional parameters. It's relevant for the project because it's a computationally efficient way to personalize a model for resource-constrained devices like smartphones."
"What visualization technique in hw2/main.ipynb demonstrated the structure of the learned representation of the Frey faces?","The notebook generated new faces by decoding points from a regular grid in the 2D latent space. This visualization clearly showed how different regions of the latent space corresponded to different facial poses and expressions, proving the model learned a structured manifold."
"How did the GAN implementation in hw3/main.ipynb handle the training of the generator and discriminator?","It used a custom training loop with tf.GradientTape to alternate between updating the two networks. The discriminator was trained on batches of real and fake images, while the generator was trained by trying to produce images that the discriminator would classify as real."
"What was the architecture of the Q-network used for Deep Q-Learning in Homework 4?","The Q-network was a multi-layer perceptron (MLP) with an input layer matching the state size (4), a single hidden layer with 32 units and ReLU activation, and an output layer with a linear activation that produced a Q-value for each of the two possible actions."
"What specific change was made in the hw1/main.ipynb model to improve its robustness to rotation?","A layers.RandomRotation layer was added as the first layer after the input in the tf.keras.Sequential model. This layer applies random rotations to the training images on-the-fly, a form of data augmentation that teaches the model to be invariant to rotation."
"Why did the second experiment in the Deep Q-Learning part of Homework 4 fail to learn a good policy?","The experiment failed due to a combination of premature exploitation (from a high epsilon decay rate) and insufficient training data (from only 100 episodes). The agent stopped exploring too early and did not have enough experience to learn an effective strategy."
"What is the role of the decoder model in the Latent Diffusion Model implemented in Homework 3?","After the diffusion model samples a new 2D vector from the learned latent space, the pre-trained autoencoder's decoder is used to transform that 2D vector back into a full-size, 28x20 pixel image."
"What were the proposed evaluation metrics for the personalized LLM in Phase 1 of the final project?","The evaluation included a qualitative assessment of personalization and accuracy, and a quantitative assessment of resource usage (storage size, latency, memory) and perplexity on a held-out personal dataset. A key part was comparing the trade-offs between a fully fine-tuned model and an adapter-based one."
"What is the 587/project/notes.md file about?","The notes.md file documents a change in scope for the final project. The author decided to pivot from a general personalized assistant to one focused on professional context, training the model on personal facts relevant to interviews and on the content of their GitHub repositories."
